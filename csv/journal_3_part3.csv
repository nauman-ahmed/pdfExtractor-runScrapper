link,title,published_year,keywords,author_email,abstract,publication_title,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S0950584922000672,An empirical study of emoji use in software development communication,August 2022,Not Found,Shiyue=Rong: shiyuer@uci.edu; Weisheng=Wang: weishew@uci.edu; Umme Ayda=Mannan: mannanu@oregonstate.edu; Eduardo Santana=de Almeida: esa@rise.com.br; Shurui=Zhou: shuruiz@ece.utoronto.ca; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
Similar to 
social media platforms
, people use emojis in software development related communication to enrich the context and convey additional emotion. With the increasing emoji use in software development-related communication, it has become important to understand why software developers are using emojis and their impact.
Objective:
Gaining a 
deeper understanding
 is essential because the intention of emoji usage might be affected by the demographics and experience of developers; also, frequency and the distribution of emoji usage might change depending on the activity, stage of the development, and nature of the conversation, etc.
Methods:
We present a large-scale empirical study on the intention of emoji usage conducted on 2,712 
Open Source Software
 (OSS) projects. We build a 
machine learning
 model to automate classifying the intentions behind emoji usage in 39,980 posts. We also surveyed 60 open-source software developers from 17 countries to understand developers’ perceptions of why and when emojis are used.
Results:
Our results show that we can classify the intention of emoji usage with high accuracy (AUC of 0.97). In addition, the results indicate that developers use emoji for varying intentions, and emoji usage intention changes throughout a conversation.
Conclusion:
Our study opens a new avenue in 
Software Engineering
 research related to automatically identifying the intention of the emoji use that can help improve the communication efficiency and help project maintainers monitor and ensure the quality of communication. Another thread of future research could look into what intentions of emoji usage or what kind of emojis are more likely to attract users and how that is associated with emoji usage diffusion in different levels (threads, projects, etc.)",Information and Software Technology,18 Mar 2025,7.0,"The study provides insights into the intention of emoji usage in software development, potentially improving communication efficiency and project quality."
https://www.sciencedirect.com/science/article/pii/S0950584922000787,Preventing technical debt with the TAP framework for Technical Debt Aware Management,August 2022,Not Found,Marion=Wiese: marion.wiese@uni-hamburg.de; Paula=Rachow: paula.rachow@uni-hamburg.de; Matthias=Riebisch: matthias.riebisch@uni-hamburg.de; Julian=Schwarze: schwarze.julian@guj.de,"Abstract
Context:
Technical Debt (TD) is a metaphor for technical problems that are not visible to users and customers but hinder developers in their work, making future changes more difficult. TD is often incurred due to tight project deadlines and can make future changes more costly or impossible. Project Management usually focuses on customer benefits and pays less attention to their IT systems’ internal quality. TD prevention should be preferred over 
TD repayment
 because subsequent refactoring and re-engineering are expensive.
Objective:
This paper evaluates a framework focusing on both TD prevention and 
TD repayment
 in the context of agile-managed projects. The framework was developed and applied in an IT unit of a publishing house. The unique contribution of this framework is the integration of TD management into project management.
Method:
The evaluation was performed as a comparative 
case study
 based on ticket statistics and two structured surveys. The surveys were conducted in the observed IT unit using the framework and a comparison unit not using the framework. The first survey targeted team members, the second one IT managers.
Results:
The evaluation shows that in this IT unit the TAP framework led to a raised awareness for the incurrence of TD. Decisions to incur TD are intentional, and TD is repaid timelier. Unintentional TD incurred by unconscious decisions is prevented. Furthermore, better communication and better planning of the project pipeline can be observed.
Conclusion:
We provide an insight into practitioners’ ways to identify, monitor, prevent and repay TD. The presented framework includes a feasible method for TD prevention despite tight timelines by making TD repayment part of project management.",Information and Software Technology,18 Mar 2025,9.0,The framework for TD prevention and repayment in agile-managed projects shows tangible benefits in raising awareness and improving communication and planning.
https://www.sciencedirect.com/science/article/pii/S0950584922000787,Preventing technical debt with the TAP framework for Technical Debt Aware Management,August 2022,Not Found,Marion=Wiese: marion.wiese@uni-hamburg.de; Paula=Rachow: paula.rachow@uni-hamburg.de; Matthias=Riebisch: matthias.riebisch@uni-hamburg.de; Julian=Schwarze: schwarze.julian@guj.de,"Abstract
Context:
Technical Debt (TD) is a metaphor for technical problems that are not visible to users and customers but hinder developers in their work, making future changes more difficult. TD is often incurred due to tight project deadlines and can make future changes more costly or impossible. Project Management usually focuses on customer benefits and pays less attention to their IT systems’ internal quality. TD prevention should be preferred over 
TD repayment
 because subsequent refactoring and re-engineering are expensive.
Objective:
This paper evaluates a framework focusing on both TD prevention and 
TD repayment
 in the context of agile-managed projects. The framework was developed and applied in an IT unit of a publishing house. The unique contribution of this framework is the integration of TD management into project management.
Method:
The evaluation was performed as a comparative 
case study
 based on ticket statistics and two structured surveys. The surveys were conducted in the observed IT unit using the framework and a comparison unit not using the framework. The first survey targeted team members, the second one IT managers.
Results:
The evaluation shows that in this IT unit the TAP framework led to a raised awareness for the incurrence of TD. Decisions to incur TD are intentional, and TD is repaid timelier. Unintentional TD incurred by unconscious decisions is prevented. Furthermore, better communication and better planning of the project pipeline can be observed.
Conclusion:
We provide an insight into practitioners’ ways to identify, monitor, prevent and repay TD. The presented framework includes a feasible method for TD prevention despite tight timelines by making TD repayment part of project management.",Information and Software Technology,18 Mar 2025,9.0,The framework for TD prevention and repayment in agile-managed projects shows tangible benefits in raising awareness and improving communication and planning.
https://www.sciencedirect.com/science/article/pii/S0950584922000039,Multi-objective integer programming approaches to Next Release Problem — Enhancing exact methods for finding whole pareto front,July 2022,Not Found,Shi=Dong: dongshi@mail.ustc.edu.cn; Yinxing=Xue: yxxue@ustc.edu.cn; Sjaak=Brinkkemper: s.brinkkemper@uu.nl,"Abstract
Context:
Project planning is a crucial part of software engineering, it involves selecting requirements to develop for the next release. How to make a good release plan is an optimization problem to maximize the goal of revenue under the condition of cost, time, or other aspects, namely Next Release Problem (NRP). 
Genetic
 and exact algorithms are used since it was proposed.
Objective:
We model NRP as bi-objective (revenue, cost) and tri-objective (revenue, cost, urgency) form, and investigate whether exact methods could solve bi-objective and tri-objective instances more efficiently.
Methods:
The state-of-art integer linear programming (ILP) approach to the bi-objective NRP is 
ε
-constraint for finding all non-dominate solutions. To improve its efficiency, we employ CWMOIP (Constrained Weighted Multi-Objective Integer Programming) and I-EC (improved 
ε
-constraint) for solving bi-objective instances. In tri-objective form, we introduce SolRep, an ILP method that optimizes the 
reference points
 from sampling, for finding solutions subset within a short time. NSGA-II is implemented as the 
evolutionary algorithm
 for the comparison with former methods and it adopts the seeding mechanism.
Results
: I-EC can find all non-dominated solutions with better performance than both 
ε
-constraint and CWMOIP on all instances except for one. I-EC reduces solving time by 19.7% (large instances) and 91.5% (small instances) on average separately compared with 
ε
-constraint. SolRep can find evenly distributed solutions and exceed NSGA-II illustrated by several indicators (such as HyperVolume) on tri-objective instances. And each method has its merit in the aspect of speed and number of the solutions.
Conclusion:
(1) The I-EC can solve all non-dominated solutions with better performance than the state-of-art exact method. (2) SolRep solves large tri-objective instances with more non-dominated solutions and solves small instances with less time compared with seeded NSGA-II. (3) Seeded NSGA-II shows its advantage on the number of non-dominated solutions on smaller tri-objective instances.",Information and Software Technology,18 Mar 2025,6.0,"The study on optimizing the Next Release Problem presents improved methods, although the practical application impact is slightly unclear."
https://www.sciencedirect.com/science/article/pii/S0950584922000039,Multi-objective integer programming approaches to Next Release Problem — Enhancing exact methods for finding whole pareto front,July 2022,Not Found,Shi=Dong: dongshi@mail.ustc.edu.cn; Yinxing=Xue: yxxue@ustc.edu.cn; Sjaak=Brinkkemper: s.brinkkemper@uu.nl,"Abstract
Context:
Project planning is a crucial part of software engineering, it involves selecting requirements to develop for the next release. How to make a good release plan is an optimization problem to maximize the goal of revenue under the condition of cost, time, or other aspects, namely Next Release Problem (NRP). 
Genetic
 and exact algorithms are used since it was proposed.
Objective:
We model NRP as bi-objective (revenue, cost) and tri-objective (revenue, cost, urgency) form, and investigate whether exact methods could solve bi-objective and tri-objective instances more efficiently.
Methods:
The state-of-art integer linear programming (ILP) approach to the bi-objective NRP is 
ε
-constraint for finding all non-dominate solutions. To improve its efficiency, we employ CWMOIP (Constrained Weighted Multi-Objective Integer Programming) and I-EC (improved 
ε
-constraint) for solving bi-objective instances. In tri-objective form, we introduce SolRep, an ILP method that optimizes the 
reference points
 from sampling, for finding solutions subset within a short time. NSGA-II is implemented as the 
evolutionary algorithm
 for the comparison with former methods and it adopts the seeding mechanism.
Results
: I-EC can find all non-dominated solutions with better performance than both 
ε
-constraint and CWMOIP on all instances except for one. I-EC reduces solving time by 19.7% (large instances) and 91.5% (small instances) on average separately compared with 
ε
-constraint. SolRep can find evenly distributed solutions and exceed NSGA-II illustrated by several indicators (such as HyperVolume) on tri-objective instances. And each method has its merit in the aspect of speed and number of the solutions.
Conclusion:
(1) The I-EC can solve all non-dominated solutions with better performance than the state-of-art exact method. (2) SolRep solves large tri-objective instances with more non-dominated solutions and solves small instances with less time compared with seeded NSGA-II. (3) Seeded NSGA-II shows its advantage on the number of non-dominated solutions on smaller tri-objective instances.",Information and Software Technology,18 Mar 2025,6.0,"The study on optimizing the Next Release Problem presents improved methods, although the practical application impact is slightly unclear."
https://www.sciencedirect.com/science/article/pii/S0950584922000556,A DQN-based agent for automatic software refactoring,July 2022,Not Found,Hamidreza=Ahmadi: h_ahmadi@comp.iust.ac.ir; Mehrdad=Ashtiani: m_ashtiani@iust.ac.ir; Mohammad Abdollahi=Azgomi: azgomi@iust.ac.ir; Raana=Saheb-Nassagh: r_sahebnassagh@comp.iust.ac.ir,"Abstract
Context
Nowadays, technical debt has become a very important issue in 
software project management
. The main mechanism to repay this debt is through refactoring. Refactoring software projects usually comes at a high cost. As a result, researchers have always looked for ways to minimize this cost, and a good potential candidate to reduce the cost of a process is to automate it.
Objective
One of the automatic software refactoring methods that recently has received a lot of attention is based on search-based software engineering (SBSE) methods. Although because of comprehensiveness and versatility 
SBSE
 is considered an appropriate method for automatic refactoring, it has its downsides, the most important of which are the uncertainty of the results and the exponential execution time.
Method
In this research, a solution is proposed inspired by search-based refactoring while taking advantage of exploitation in 
reinforcement learning
 techniques. This work aims to solve the uncertainty problems and execution time for large programs. In the proposed approach, the problem of uncertainty is solved by targeting the selection of refactoring actions used in the search-based approach. Also, due to the reduction of the dependency between the choice of the appropriate refactoring and its execution time, the time problem in large software refactoring has been greatly improved.
Results
Amongst the performed evaluations and specifically for the refactoring of the largest 
case study
, the proposed approach managed to increase the accuracy to more than twice of the 
SBSE
 refactoring approaches, while reducing the execution time of refactoring by more than 98%.
Conclusion
The results of the tests show that with increasing the volume and size of the software, the performance of the proposed approach also improves compared to the methods based on SBSE, both in terms of reducing technical debt and speeding up the 
refactoring process
.",Information and Software Technology,18 Mar 2025,8.0,"The research proposes a solution to automate software refactoring using a combination of search-based methods and reinforcement learning, significantly improving accuracy and reducing execution time, which can have a high practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000532,API-m-FAMM: A focus area maturity model for API Management,July 2022,"API Management, Maturity model, Focus area maturity models",Michiel=Overeem: michiel.overeem@afas.nl; Max=Mathijssen: max.mathijssen@afas.nl; Slinger=Jansen: slinger.jansen@uu.nl,"Abstract
Context:
Organizations are increasingly connecting 
software applications
 using 
Application Programming Interfaces
 (APIs) to share data, services, functionality, and even complete business processes. However, the creation and management of APIs is non-trivial. Aspects such as traffic management, community engagement, documentation, and version management are often rushed afterthoughts.
Objective:
In this research, we present and evaluate a focus area maturity model for API Management (API-m-FAMM). A focus area maturity model can be used to establish the maturity level of an organization in a specific functional domain described through a number of areas. The API-m-FAMM addresses the areas 
Lifecycle Management
, Security, Performance, Observability, Community, and Commercial.
Method:
The model is constructed using established methods for the design of a focus area maturity model. It is grounded in literature and practice, and was developed and evaluated through a 
systematic literature Review
, eleven expert interviews, and five 
case studies
 at software producing organizations.
Result:
The model is described in detail, and its application is illustrated by six 
case studies
.
Conclusions:
The evaluations are reported on, and show that the API-m-FAMM is an efficient tool for aiding organizations in gaining a better understanding of their current implementation of API management practices, and provides them with guidance towards higher levels of maturity. The detailed description of the construction of the API-m-FAMM gives researchers an example to further support the available methodologies, specifically how to combine design science research with these methodologies. Additionally, this study’s unique case study design shows that maturity models can be successfully deployed in practice with minimal involvement of researchers. The focus area maturity model for API Management is maintained on 
www.maturitymodels.org
, allowing practitioners to benefit from its useful insights.",Information and Software Technology,18 Mar 2025,7.0,"The focus area maturity model for API management can aid organizations in understanding and improving their API practices, which is relevant for startups working on software applications leveraging APIs."
https://www.sciencedirect.com/science/article/pii/S0950584922000544,Aligned metric representation based balanced multiset ensemble learning for heterogeneous defect prediction,July 2022,Not Found,Haowen=Chen: hwc_zzu@126.com; Yuming=Zhou: zhouyuming@nju.edu.cn; Bing=Li: bingli@whu.edu.cn; Baowen=Xu: bwxu@nju.edu.cn,"Abstract
Context:
Heterogeneous 
defect prediction
 (HDP) refers to the 
defect prediction
 across projects with different metrics. Most existing HDP methods map source and target data into a common metric space where each dimension has no actual meaning, which weakens their 
interpretability
. Besides, HDP always suffers from the 
class imbalance problem
.
Objective:
For deficiencies of current HDP methods, we intend to propose a novel HDP approach that can reduce the heterogeneity of source and target data and deal with 
imbalanced data
 while retaining the actual meaning for each dimension of constructed common metric space.
Method:
We propose an Aligned Metric Representation based Balanced Multiset 
Ensemble learning
 (BMEL+ AMR) approach for HDP. AMR consists of shared, source-specific, and target-specific metrics. It is built by learning the translation from shared to specific metrics and reducing the distribution difference. To deal with 
imbalanced data
, we design BMEL that constructs multiple balanced subsets for source data and produces an aggregated classifier for predicting labels of target data.
Result:
Experimental results on 22 public projects indicate that (1) among all competing methods, BMEL+AMR achieves the best performance on all indicators except 
Popt
, followed by AMR; (2) compared with AMR, the introduction of BMEL improves the performance on non-effort-aware indicators statistically significantly except 
F1-score
; compared with BMEL, the introduction of AMR improves the performance throughout all indicators statistically significantly.
Conclusion:
BMEL+AMR can effectively improve HDP performance by eliminating heterogeneity and dealing with imbalanced data, and AMR is helpful to explain the prediction model.",Information and Software Technology,18 Mar 2025,6.0,"The novel HDP approach addresses the interpretability and class imbalance issues in defect prediction, which can be beneficial for startups dealing with software quality and maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584922000507,"SE
M: A model for software effort estimation using pre-trained embedding models",July 2022,Not Found,Eliane Maria=De Bortoli Fávero: elianedb@utfpr.edu.br; Dalcimar=Casanova: dalcimar@utfpr.edu.br; Andrey Ricardo=Pimentel: andrey@inf.ufpr.br,"Abstract
Context:
Software effort estimation from requirements texts, presents many challenges, mainly in getting viable features to infer effort. The most recent 
Natural Language Processing
 (NLP) initiatives for this purpose apply context-less embedding models, which are often not sufficient to adequately discriminate each analyzed sentence. Contextualized pre-trained embedding models have emerged quite recently and have been shown to be far more effective than context-less models in representing textual features.
Objective:
This paper proposes evaluating the effectiveness of pre-trained embedding models, to explore a more effective technique for representing textual requirements, which are used to infer effort estimates by analogy.
Method:
Generic pre-trained models went through a fine-tuning process for both approaches — context-less and contextualized. The generated models were used as input in the applied 
deep learning
 architecture, with linear output. The results were very promising, realizing that contextualized pre-trained embedding models can be used to estimate software effort based only on requirements texts.
Results:
We highlight the results obtained to apply the contextualized pre-trained model 
BERT
 with fine-tuning, applied in a single repository containing different projects, whose 
Mean Absolute Error
 (MAE) value is 4.25 and the standard deviation is only 0.17. This represents a result very positive when compared to similar works.
Conclusion:
The main advantages of the proposed estimation method are reliability, the possibility of generalization, speed, and low computational cost. Such advantages are provided by the fine-tuning process, enabling to infer effort estimation for new or existing requirements.",Information and Software Technology,18 Mar 2025,9.0,"The evaluation of pre-trained embedding models for software effort estimation shows promising results, offering reliability, generalization, and low computational cost, which can be highly valuable for startups in resource-constrained environments."
https://www.sciencedirect.com/science/article/pii/S095058492200060X,Locality-based security bug report identification via active learning,July 2022,Not Found,Xiuting=Ge: dg20320002@smail.nju.edu.cn; Chunrong=Fang: fangchunrong@nju.edu.cn; Meiyuan=Qian: mf20320109@smail.nju.edu.cn; Yu=Ge: 920397425@qq.com; Mingshuang=Qing: qingms@mails.swust.edu.cn,"Abstract
Context:
Security 
bug report
 (SBR) identification is a crucial way to eliminate security-critical vulnerabilities during software development.
Objective:
In recent years, many approaches have utilized supervised machine learning (SML) techniques in the SBR identification. However, such approaches often require a large number of labelled bug reports, which are often hard to obtain in practice. Active learning is a potential approach to reducing the manual labelling cost while maintaining 
good performance
. Nevertheless, the existing active learning-based SBR identification approach still yields poor performance due to ignoring the locality in bug reports.
Method:
To address the above problems, we propose locality-based SBR identification via active learning. Our approach recommends a small part of instances based on locality in bug reports, asks for their labels, and learns the SBR classifier. Specifically, our approach relies on the locality to construct the initial training set, which is designed to address how to start during active learning. Moreover, our approach applies the locality into the query process, which is designed to improve which instance should be queried next during active learning.
Result:
We conduct experiments on large-scale bug reports (nearly 125K) from six real-world projects. In comparison with three state-of-the-art SML-based and active learning-based SBR identification approaches, our approach can obtain the maximum values of F-Measure (0.8176) and AUC (0.8631). Moreover, our approach requires 16.60% to 71.40% of all bug reports when achieving the 
optimal performance
 in these six projects, which improves three approaches from 9.82% to 64.19% on average.
Conclusion:
As shown from the experimental results, our approach can be more effective and efficient to identify SBRs than the existing approaches.",Information and Software Technology,18 Mar 2025,7.0,"The locality-based SBR identification approach through active learning demonstrates improved performance and efficiency in identifying security bug reports, which can be critical for startups focused on software security."
https://www.sciencedirect.com/science/article/pii/S0950584922000611,Mind the product owner: An action research project into agile release planning,July 2022,"Canonical action research, Agile release planning, Product owner",Konsta=Kantola: konsta.kantola@finago.com; Jari=Vanhanen: Not Found; Jussi=Tolvanen: Not Found,"Abstract
Context:
This paper studies agile release planning in a software development organization with 13 development teams. It is important for software development organizations to be able to plan work in an efficient way that supports development work.
Objective:
The research aims to understand issues within agile release planning in the studied organization, and to make some improvement to the agile release planning practices there.
Method:
The study followed canonical 
action research
 methodology completing one cycle of diagnosis, action planning, intervention, evaluation, and learning. Qualitative methods were used during these phases to identify preliminary issues, to support the choice of action, and the evaluation of those actions.
Results:
The research identified issues of strain on the role of Product Owners. Sources of strain in the organization include changing priorities, the effort required to build up domain competence for new projects, and external pressure to push out new features. Additionally, there was difficulty for people participating in agile release planning to suggest improvements to the used practices due in part to the complexity and scale of planning practices in a multi-team development organization. The actions taken as part of the research provided ways for Product Owners to share knowledge between themselves, to better affect the working practices in the organization, and promoted a sense of team spirit between the Product Owners.
Conclusion:
Organizations should be mindful of their Product Owners when looking at their release planning practices. Problems for Product Owners are problems in planning for the whole organization. Having an active, collective, and structured channel for continuous improvement for Product Owners can help drive improvements to agile release planning.",Information and Software Technology,18 Mar 2025,5.0,"The research provides insights into agile release planning practices, particularly focusing on issues related to Product Owners. This can be valuable for software development organizations but doesn't introduce groundbreaking solutions."
https://www.sciencedirect.com/science/article/pii/S0950584922000635,Continuous verification of system of systems with collaborative MAPE-K pattern and probability model slicing,July 2022,Not Found,Jiyoung=Song: jysong@se.kaist.ac.kr; Jeehoon=Kang: Not Found; Sangwon=Hyun: Not Found; Eunkyoung=Jee: Not Found; Doo-Hwan=Bae: Not Found,"Abstract
The phenomenon of cooperation among independent systems to achieve common goals has been growing. In this regard, the concept of 
system of systems
 (SoS), wherein numerous independent systems cooperate with each other, has been proposed. The key characteristic of an SoS is the 
operational and managerial (O/M) independence
 of each 
constituent system
 (CS). Each CS of a 
collaborative SoS
 with high O/M independence provides different levels of 
internal-knowledge
 sharing and is entitled to voluntary participation in the SoS (
i.e.
, 
dynamic reconfiguration
). To increase goal-achievement rate, we need to verify SoS considering the knowledge-sharing and 
dynamic reconfiguration
 constraints.
The 
dynamic reconfiguration
 of SoSs can be managed using 
continuous verification
, which involves environment monitoring, modeling systems for operation in changing environments, and verifying the model runtimes. However, O/M independence introduces the following challenges: (1) the low knowledge-sharing level causes inaccurate modeling, which leads to inaccurate verification results, and (2) dynamic reconfiguration requires frequent re-verification at runtime, which incurs high verification costs.
In this paper, we propose a continuous-verification-of-SoS (CVSoS) approach to solve these two challenges. To address the low knowledge-sharing level, we propose the 
collaborative MAPE-K
 pattern. The key to collaborative MAPE-K is the retrieval of knowledge from the other collaborating CSs. To address dynamic reconfiguration, we propose a new slicing algorithm for SoS models. This algorithm promotes 
synchronization
 dependence
, which is essential for representing interactions between CSs. Furthermore, we demonstrate the accuracy of this algorithm.
We evaluated CVSoS across multiple SoS domains, which revealed that the SoS goal-achievement rate increases by up to 64% using the collaborative MAPE-K pattern and that slicing the benchmark and SoS models improved the verification time by an average of 67%.",Information and Software Technology,18 Mar 2025,7.0,The proposal of a continuous-verification-of-SoS approach to improve system of systems collaboration and achieve higher goal-achievement rates could have significant practical value for early-stage ventures by enhancing operational efficiency and effectiveness.
https://www.sciencedirect.com/science/article/pii/S0950584922000593,The practical roles of enterprise architecture artifacts: A classification and relationship,July 2022,Not Found,Svyatoslav=Kotusev: kotusev@kotusev.com; Sherah=Kurnia: Not Found; Rod=Dilnutt: Not Found,"Abstract
Context
Enterprise architecture (EA) is a description of an enterprise from an integrated business and IT perspective. EA is typically defined as a comprehensive blueprint of an organization covering its business, data, applications and technology domains and consisting of diverse EA artifacts. EA has numerous potential stakeholders and 
usage scenarios
 in organizations. However, the existing EA literature does not offer any consistent theories explaining the practical roles of individual EA artifacts and fails to explain how exactly different types of EA artifacts are used in practice.
Objective
This study intends to explore the roles of different EA artifacts in organizations and develop a generic descriptive theory explaining these roles. The theory purports to cover various properties of EA artifacts as well as the relationships between them.
Method
The research method of this study follows two consecutive phases: theory construction and theory validation. First, theory construction is based on the qualitative in-depth analysis of five case organizations with established EA practices. Next, theory validation includes confirmatory interviews with ten EA experts.
Results
This study develops a descriptive theory explaining the roles of different EA artifacts in an EA practice. The resulting theory defines six general types of EA artifacts (Considerations, Standards, Visions, Landscapes, Outlines and Designs, CSVLOD) and explains their type-specific practical roles, including their 
informational contents
, typical usage, ensuing organizational benefits and interrelationships with each other.
Conclusions
This study presents the first systematic theory describing the usage of EA artifacts in organizations. Our theory facilitates better theoretical understanding of the concept of EA and also provides evidence-based solutions to the commonly reported practical problems with EA. This study suggests that the EA research community should focus on studying individual EA artifacts instead of studying EA in general and calls for further research on EA artifacts and their usage as part of EA practices.",Information and Software Technology,18 Mar 2025,6.0,The exploration of different EA artifacts and the development of a theory explaining their practical roles could provide valuable insights for organizations looking to optimize their enterprise architecture. This can have a positive impact on early-stage ventures' scalability and integration strategies.
https://www.sciencedirect.com/science/article/pii/S0950584922000647,CASMS: Combining clustering with attention semantic model for identifying security bug reports,July 2022,Not Found,Xiaoxue=Ma: xiaoxuema3-c@my.cityu.edu.hk; Jacky=Keung: jacky.keung@cityu.edu.hk; Zhen=Yang: zhyang8-c@my.cityu.edu.hk; Xiao=Yu: xiaoyu@whut.edu.cn; Yishu=Li: yishuli5-c@my.cityu.edu.hk; Hao=Zhang: hzhang339-c@my.cityu.edu.hk,"Abstract
Context:
Inappropriate public disclosure of security 
bug reports
 (SBRs) is likely to attract malicious attackers to invade software systems; hence being able to detect SBRs has become increasingly important for software maintenance. Due to the 
class imbalance problem
 that the number of non-security 
bug reports
 (NSBRs) exceeds the number of SBRs, insufficient training information, and weak performance robustness, the existing techniques for identifying SBRs are still less than desirable.
Objective:
This prompted us to overcome the challenges of the most advanced SBR detection methods.
Method:
In this work, we propose the CASMS approach to efficiently alleviate the imbalance problem and predict bug reports. CASMS first converts bug reports into weighted 
word embeddings
 based on 
t
f
−
i
d
f
 and 
w
o
r
d
2
v
e
c
 techniques. Unlike the previous studies selecting the NSBRs that are the most dissimilar to SBRs, CASMS then automatically finds a certain number of diverse NSBRs via the Elbow method and 
k
-means clustering algorithm. Finally, the selected NSBRs and all SBRs train an effective Attention CNN–BLSTM model to extract contextual and sequential information.
Results:
The experimental results have shown that CASMS is superior to the three baselines (i.e., FARSEC, SMOTUNED, and LTRWES) in assessing the overall performance (
g
-measure) and correctly identifying SBRs (
recall
), with improvements of 4.09%–24.26% and 10.33%–36.24%, respectively. The best results are easily obtained under the limited ratio ranges of the two-class training set (1:1 to 3:1), with around 20 experiments for each project. By evaluating the robustness of CASMS via the standard deviation indicator, CASMS is more stable than LTRWES.
Conclusion:
Overall, CASMS can alleviate the data imbalance problem and extract more semantic information to improve performance and robustness. Therefore, CASMS is recommended as a practical approach for identifying SBRs.",Information and Software Technology,18 Mar 2025,8.0,The CASMS approach addressing the imbalance problem in security bug report detection and demonstrating superior performance over existing techniques provides a practical solution that can significantly benefit early-stage ventures by enhancing software security and maintenance.
https://www.sciencedirect.com/science/article/pii/S0950584922000659,Successful combination of database search and snowballing for identification of primary studies in systematic literature studies,July 2022,"Systematic literature reviews, Hybrid search, Snowballing, Scopus",Claes=Wohlin: claes.wohlin@bth.se; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Katia=Romero Felizardo: katiascannavino@utfpr.edu.br; Emilia=Mendes: emilia.mendes@bth.se,"Abstract
Background:
A good search strategy is essential for a successful systematic literature study. Historically, database searches have been the norm, which was later complemented with snowball searches. Our conjecture is that we can perform even better searches if combining these two search approaches, referred to as a hybrid search strategy.
Objective:
Our main objective was to compare and evaluate a hybrid search strategy. Furthermore, we compared four alternative hybrid search strategies to assess whether we could identify more cost-efficient ways of searching for relevant primary studies.
Methods:
To compare and evaluate the hybrid search strategy, we replicated the search procedure in a 
systematic literature review
 (SLR) on industry–academia collaboration in 
software engineering
. The SLR used a more “traditional” approach to searching for relevant articles for an SLR, while our replication was executed using a hybrid search strategy.
Results:
In our evaluation, the hybrid search strategy was superior in identifying relevant primary studies. It identified 30% more primary studies and even more studies when focusing only on peer-reviewed articles. To embrace individual viewpoints when assessing research articles and minimise the risk of missing primary studies, we introduced two new concepts, 
wild cards
 and 
borderline articles
, when performing systematic literature studies.
Conclusions:
The hybrid search strategy is a strong contender for being used when performing systematic literature studies. Furthermore, alternative hybrid search strategies may be viable if selected wisely in relation to the start set for snowballing. Finally, the two new concepts were judged as essential to cater for different individual judgements and to minimise the risk of excluding primary studies that ought to be included.",Information and Software Technology,18 Mar 2025,7.0,The comparison and evaluation of a hybrid search strategy for systematic literature studies and the introduction of new concepts like wild cards and borderline articles could offer valuable guidance for early-stage ventures in conducting more comprehensive and efficient literature reviews.
https://www.sciencedirect.com/science/article/pii/S0950584922000581,Quantum computing challenges in the software industry. A fuzzy AHP-based approach,July 2022,"Fuzzy analytic hierarchy process (F-AHP), Software process automation, Multiple-criteria decision-making (MCDM), Quantum software requirement, Quantum computing",Usama=Awan: usama.awan@lut.fi; Lea=Hannola: Not Found; Anushree=Tandon: Not Found; Raman Kumar=Goyal: Not Found; Amandeep=Dhir: Not Found,"Abstract
Context
The current technology revolution has posed unexpected challenges for the software 
industry
. In recent years, the field of 
quantum computing
 (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise 
software engineering
. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented.
Objective
The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy 
analytic hierarchy process
 (F-AHP).
Method
First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers.
Results
The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC.
Conclusion
By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges.",Information and Software Technology,18 Mar 2025,7.0,"The study on identifying challenges in quantum computing for the software industry can have a significant impact on the future of software engineering and technological innovation, benefiting European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000568,Toward successful DevSecOps in software development organizations: A decision-making framework,July 2022,"DevOps, DevSecOps, Challenges, Multivocal literature review, Fuzzy analytical hierarchy process",Muhammad Azeem=Akbar: azeem.akbar@lut.fi; Kari=Smolander: kari.smolander@lut.fi; Sajjad=Mahmood: smahmood@kfupm.edu.sa; Ahmed=Alsanad: aasanad@ksu.edu.sa,"Abstract
Context
Development and Operations (DevOps) is a methodology that aims to establish collaboration between programmers and operators to automate the continuous delivery of new software to reduce the 
development life cycle
 and produce quality software. Development, Security, and Operations (DevSecOps) is developing the DevOps concept, which integrates security methods into a 
DevOps process
. DevSecOps is a software development process where security is built in to ensure application confidentiality, integrity, and availability.
Objective
This paper aims to identify and prioritize the challenges associated with implementing the DevSecOps process.
Method
We performed a multivocal literature review (MLR) and conducted a questionnaire-based survey to identify challenges associated with DevSecOps-based projects. Moreover, interpretive structure modeling (ISM) was applied to study the relationships among the core categories of the challenges. Finally, we used the fuzzy technique for 
order preference
 by similarity to an ideal solution (TOPSIS) to prioritize the identified challenges associated with DevSecOps projects.
Results
We identified 18 challenges for the DevSecOps process and mapped them to 10 core categories. The ISM results indicate that the “standards” category has the most decisive influence on the other nine core categories of the identified challenges. Moreover, the fuzzy TOPSIS indicates that “lack of secure coding standards,” “lack of automated testing tools for security in DevOps,” and “ignorance in static testing for security due to lack of knowledge” are the highest priority challenges for the DevSecOps paradigm.
Conclusion
Organizations using DevOps should consider the identified challenges in developing secure software.",Information and Software Technology,18 Mar 2025,8.0,"Identifying challenges in implementing DevSecOps can improve the security and quality of software development processes, which is crucial for startups dealing with sensitive data and information."
https://www.sciencedirect.com/science/article/pii/S0950584922000428,An end-to-end deep learning system for requirements classification using recurrent neural networks,July 2022,Not Found,Osamah=AlDhafer: Not Found; Irfan=Ahmad: irfan.ahmad@kfupm.edu.sa; Sajjad=Mahmood: Not Found,"Abstract
Context:
Existing requirements 
classification approaches
 mainly use lexical and syntactical features to classify requirements using both traditional 
machine learning
 and 
deep learning
 approaches with promising results. However, the existing techniques depend on word and sentence structures and employ preprocessing and feature engineering techniques to classify requirements from textual natural language documents. Moreover, existing studies deal with requirements classification as binary or 
multiclass classification
 problems and not as multilabel classification, although a given requirement can belong to multiple classes at the same time.
Objective:
The objective of this study is to classify requirements into functional and different non-functional types with minimal preprocessing and to model the task as a multilabel classification problem.
Method:
In this paper, we use Bidirectional Gated 
Recurrent Neural Networks
 (BiGRU) to classify requirements using raw text. We investigated two different approaches: (i) using word sequences as tokens and (ii) using character sequences as tokens.
Results:
Experiments conducted on the publicly available PROMISE and 
EHR
 datasets show the effectiveness of the presented techniques. We achieve state-of-the-art results on most of the tasks using word sequences as tokens.
Conclusion:
Requirements can be effectively classified into functional and different non-functional categories using the presented recurrent neural networks-based deep 
learning system
, which involves minimal text prepossessing and no feature engineering.",Information and Software Technology,18 Mar 2025,6.0,"The study on classifying requirements using deep learning techniques can contribute to more efficient software development processes, potentially benefiting startups by optimizing resource allocation."
https://www.sciencedirect.com/science/article/pii/S0950584922000350,Defining adaptivity and logical architecture for engineering (smart) self-adaptive cyber–physical systems,July 2022,Not Found,Ana=Petrovska: ana.petrovska@tum.de; Stefan=Kugele: stefan.kugele@thi.de; Thomas=Hutzelmann: t.hutzelmann@tum.de; Theo=Beffart: theo.beffart@tum.de; Sebastian=Bergemann: sebastian.bergemann@tum.de; Alexander=Pretschner: alexander.pretschner@tum.de,"Abstract
Context:
Modern cyber–physical systems (CPSs) are embedded in the physical world and intrinsically operate in a continuously changing and uncertain environment or 
operational context
. To meet their business goals and preserve or even improve specific adaptation goals, besides the variety of run-time uncertainties and changes to which the CPSs are exposed—the systems need to self-adapt.
Objective:
The current literature in this domain still lacks a precise definition of what self-adaptive systems are and how they differ from those considered non-adaptive. Therefore, in order to answer 
how
 to engineer self-adaptive CPSs or self-adaptive systems in general, we first need to answer 
what
 is adaptivity, correspondingly self-adaptive systems.
Method:
In this paper, we first formally define the notion of adaptivity. Second, within the frame of the formal definitions, we propose a logical architecture for engineering decentralised self-adaptive CPSs that operate in dynamic, uncertain, and partially observable operational contexts. This logical architecture provides a structure and serves as a foundation for the implementation of a class of self-adaptive CPSs.
Results:
First, our results show that in order to answer if a system is adaptive, the right framing is necessary: the system’s adaptation goals, 
its context
, and the time period in which the system is adaptive. Second, we discuss the benefits of our architecture by comparing it with the MAPE-K conceptual model.
Conclusion:
Commonly accepted definitions of adaptivity and self-adaptive systems are necessary for work in this domain to be compared and discussed since the same terms are often used with different semantics. Furthermore, in modern self-adaptive CPSs, which operate in dynamic and uncertain contexts, it is insufficient if the adaptation logic is specified during the system’s design, but instead, the adaptation logic itself needs to adapt and “learn” during run-time.",Information and Software Technology,18 Mar 2025,5.0,"The research on self-adaptive systems in cyber-physical systems can have implications for innovative technologies, but the practical impact on early-stage ventures may be more indirect and long-term."
https://www.sciencedirect.com/science/article/pii/S0950584921002457,Tailoring the Scrum framework for software development: Literature mapping and feature-based support,June 2022,Not Found,Luciano A.=Garcia: lucianogarcia11@hotmail.com; Edson=OliveiraJr: edson@din.uem.br; Marcelo=Morandini: m.morandini@usp.br,"Abstract
Context:
Literature faces the lack of studies relating which characteristics of the Scrum framework are adapted. Understanding such variations is useful for prospective software development projects and guiding teams at conducting Scrum 
customizations
.
Objective:
We aimed at identifying how the Scrum framework has been adapted to the context of 
Agile software development
 projects and how adaptations might be represented to aid researchers and practitioner at analyzing Scrum processes deployed or to be deployed.
Method:
We carried out a 
systematic mapping study
 in five electronic sources, 11 journals and 15 conferences/workshops. We submitted the 281 returned studies to various filters, which resulted in 50 studies with data extracted, analyzed, and quality evaluated.
Results:
SMS provides a panorama on the Scrum characteristics adapted to roles, events, and artifacts. We decided to adopt feature models for hierarchically accommodating found Scrum adaptations as it supports adaptations in the form of variability. We evaluated the resulting feature model with practitioners from different companies in the perspective of Perceived Usefulness and Perceived Ease of Use considering the Technology Acceptance Model (TAM). Therefore, we demonstrated the produced feature model aids users to better visualize and understand the documented Scrum adaptations.
Conclusions:
The panorama on Scrum adaptations and the problems during Scrum adoption are discussed to providing a means to practically understand and tailor (configure) such adaptations. Such adaptations are an essential source of information on the variety of Scrum elements, thus researchers and practitioners may take the results of this work as a guide to understand how different adaptations occur in different contexts during software development. In addition, the conceived feature model is an important asset to guide such users at selecting Scrum characteristics and respective adaptations to perform. The feature model also promotes reuse of knowledge gathered up from several different 
information sources
.",Information and Software Technology,18 Mar 2025,6.0,"Understanding adaptations of the Scrum framework can help agile software development projects, providing insights for startups on how to customize their processes more effectively."
https://www.sciencedirect.com/science/article/pii/S0950584921002457,Tailoring the Scrum framework for software development: Literature mapping and feature-based support,June 2022,Not Found,Luciano A.=Garcia: lucianogarcia11@hotmail.com; Edson=OliveiraJr: edson@din.uem.br; Marcelo=Morandini: m.morandini@usp.br,"Abstract
Context:
Literature faces the lack of studies relating which characteristics of the Scrum framework are adapted. Understanding such variations is useful for prospective software development projects and guiding teams at conducting Scrum 
customizations
.
Objective:
We aimed at identifying how the Scrum framework has been adapted to the context of 
Agile software development
 projects and how adaptations might be represented to aid researchers and practitioner at analyzing Scrum processes deployed or to be deployed.
Method:
We carried out a 
systematic mapping study
 in five electronic sources, 11 journals and 15 conferences/workshops. We submitted the 281 returned studies to various filters, which resulted in 50 studies with data extracted, analyzed, and quality evaluated.
Results:
SMS provides a panorama on the Scrum characteristics adapted to roles, events, and artifacts. We decided to adopt feature models for hierarchically accommodating found Scrum adaptations as it supports adaptations in the form of variability. We evaluated the resulting feature model with practitioners from different companies in the perspective of Perceived Usefulness and Perceived Ease of Use considering the Technology Acceptance Model (TAM). Therefore, we demonstrated the produced feature model aids users to better visualize and understand the documented Scrum adaptations.
Conclusions:
The panorama on Scrum adaptations and the problems during Scrum adoption are discussed to providing a means to practically understand and tailor (configure) such adaptations. Such adaptations are an essential source of information on the variety of Scrum elements, thus researchers and practitioners may take the results of this work as a guide to understand how different adaptations occur in different contexts during software development. In addition, the conceived feature model is an important asset to guide such users at selecting Scrum characteristics and respective adaptations to perform. The feature model also promotes reuse of knowledge gathered up from several different 
information sources
.",Information and Software Technology,18 Mar 2025,8.0,"This abstract provides valuable insights into adapting the Scrum framework for Agile software development projects, which can be highly beneficial for early-stage ventures looking to streamline their processes."
https://www.sciencedirect.com/science/article/pii/S0950584922000246,Detecting privacy requirements from User Stories with NLP transfer learning models,June 2022,Not Found,Francesco=Casillo: fcasillo@unisa.it; Vincenzo=Deufemia: deufemia@unisa.it; Carmine=Gravino: gravino@unisa.it,"Abstract
Context:
To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems.
Objective:
We present an approach to decrease privacy risks during 
agile software development
 by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile 
Requirement Engineering
 (RE).
Methods:
The proposed approach combines 
Natural Language Processing
 (NLP) and linguistic resources with 
deep learning algorithms
 to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and 
syntactic
 structure of the text. This information is then processed by a pre-trained 
convolutional neural network
, which paved the way for the implementation of a 
Transfer Learning
 technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories.
Results:
The experimental results show that 
deep learning algorithms
 allow to obtain better predictions than those achieved with conventional (shallow) 
machine learning methods
. Moreover, the application of 
Transfer Learning
 allows to considerably improve the accuracy of the predictions, ca. 10%.
Conclusions:
Our study contributes to encourage 
software engineering
 researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models.",Information and Software Technology,18 Mar 2025,7.0,"Automatically detecting privacy-related information in agile software development is a useful contribution, but the impact may be slightly lower compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000209,Featherweight assisted vulnerability discovery,June 2022,"Model interpretability, Vulnerability prediction, Identifier splitting, Source code vocabulary, Software security",David=Binkley: binkley@cs.loyola.edu; Leon=Moonen: Not Found; Sibren=Isaacman: Not Found,"Abstract
Predicting vulnerable 
source code
 helps to focus the attention of a developer, or a program analysis technique, on those parts of the code that need to be examined with more scrutiny. Recent work proposed the use of function names as semantic cues that can be learned by a 
deep neural network
 (DNN) to aid in the hunt for vulnerability of functions.
Combining identifier splitting, which we use to split each function name into its constituent words, with a novel frequency-based algorithm, we explore the extent to which the words that make up a function’s name can be used to predict potentially vulnerable functions. In contrast to the 
lightweight
 prediction provided by a DNN considering only function names, avoiding the need for a DNN provides 
featherweight
 prediction. The underlying idea is that function names that contain certain “dangerous” words are more likely to accompany vulnerable functions. Of course, this assumes that the frequency-based algorithm can be properly tuned to focus on truly dangerous words.
Because it is more transparent than a DNN, which behaves as a “black box” and thus provides no insight into the rationalization underlying its decisions, the frequency-based algorithm enables us to investigate the inner workings of the DNN. If successful, this investigation into what the DNN does and does not learn will help us train more effective future models.
We empirically evaluate our approach on a heterogeneous dataset containing over 73
 
000 functions labeled vulnerable, and over 950
 
000 functions labeled benign. Our analysis shows that words alone account for a significant portion of the DNN’s classification ability. We also find that words are of greatest value in the datasets with a more homogeneous vocabulary. Thus, when working within the scope of a given project, where the vocabulary is unavoidably homogeneous, our approach provides a cheaper, potentially complementary, technique to aid in the hunt for source-code vulnerabilities. Finally, this approach has the advantage that it is viable with orders of magnitude less 
training data
.",Information and Software Technology,18 Mar 2025,9.0,"The approach of predicting vulnerable source code using a frequency-based algorithm provides a transparent alternative to DNNs, which can be highly impactful for startups in need of cost-effective ways to improve code security."
https://www.sciencedirect.com/science/article/pii/S0950584922000258,An empirical study on self-admitted technical debt in modern code review,June 2022,Not Found,Yutaro=Kashiwa: kashiwa@ait.kyushu-u.ac.jp; Ryoma=Nishikawa: Not Found; Yasutaka=Kamei: Not Found; Masanari=Kondo: Not Found; Emad=Shihab: Not Found; Ryosuke=Sato: Not Found; Naoyasu=Ubayashi: Not Found,"Abstract
Technical debt is a sub-optimal state of development in projects. In particular, the type of technical debt incurred by developers themselves (e.g., comments that mean the implementation is imperfect and should be replaced with another implementation) is called self-admitted technical debt (SATD). In theory, technical debt should not be left for a long period because it accumulates more cost over time, making it more difficult to process. Accordingly, developers have traditionally conducted code reviews to find technical debt. In fact, we observe that many SATD comments are often introduced during modern code reviews (MCR) that are light-weight reviews with web applications. However, it is uncertain about the nature of SATD comments that are introduced in the review process: impact, frequency, characteristics, and triggers. Herein, this study empirically examines the relationship between SATD and MCR.
Our 
case study
 of 156,372 review records from the Qt and OpenStack systems shows that (i) review records involving SATD are about 6%–7% less likely to be accepted by reviews than those without SATD; (ii) review records involving SATD tend to require two to three more revisions compared with those without SATD; (iii) 28–48% of SATD comments are introduced during code reviews; (iv) SATD during reviews works for communicating between authors and reviewers; and (v) 20% of the SATD comments are introduced due to reviewers’ requests.",Information and Software Technology,18 Mar 2025,6.0,"The study on self-admitted technical debt during code reviews provides insights into the relationship between SATD and MCR, but the practical impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584922000349,A checklist for the evaluation of software process line approaches,June 2022,Not Found,Halimeh=Agh: agh.halime@gmail.com; Félix=García: Not Found; Mario=Piattini: Not Found,"Abstract
Context
A Software 
Process Line
 (SPrL) can help organisations to construct bespoke software development processes for specific project situations by reusing core assets. However, as there are diverse approaches for SPrL Engineering (SPrLE), this necessitates proper assistance to organisations in selecting the SPrL approach best suited to their needs.
Objective
This paper aims to identify an 
evaluation checklist
 that can be used for evaluating SPrLs.
Method
The checklist was constructed in five stages: first, relevant aspects for managing process variability in the context of SPrLs were identified; based on these, research questions were then formed in the second stage. In the third stage, to answer the research questions, a literature review was conducted that focused on analysing 39 primary studies. In the fourth stage, the checklist was built by synthesising the literature results. In the fifth stage, the checklist was applied to two SPrL approaches as a 
proof of concept
.
Results
The checklist includes seven main aspects, including the 
modelling language
 used, the type of the approach based on the number of artefacts produced, the language constructs provided for 
variability modelling
, the process perspectives covered, the tool used for supporting the SPrL approach, the variability-specific features provided to support process variability throughout the SPrL lifecycle, and the empirical evaluation conducted to evaluate the approach.
Conclusion
The checklist can be used by organisations to compare SPrLs and then select the most suitable SPrL approach; furthermore, it can be used by researchers to propose novel SPrL approaches that consider important aspects for variability management throughout the SPrL lifecycle. Although we have provided an example of the use of the checklist to compare SPrLs, an empirical evaluation of the checklist is required to get feedback from the organisations regarding the strengths and weaknesses of the checklist.",Information and Software Technology,18 Mar 2025,7.0,"The development of an evaluation checklist for Software Process Lines can be valuable for organisations looking to select the best approach, but the impact on startups may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000210,Predicting the precise number of software defects: Are we there yet?,June 2022,Not Found,Xiao=Yu: xiaoyu@whut.edu.cn; Jacky=Keung: jacky.keung@cityu.edu.hk; Yan=Xiao: dcsxan@nus.edu.sg; Shuo=Feng: shuofeng5-c@my.cityu.edu.hk; Fuyang=Li: fyli@whut.edu.cn; Heng=Dai: daiheng726@163.com,"Abstract
Context:
Defect Number Prediction (DNP) models can offer more benefits than classification-based 
defect prediction
. Recently, many researchers proposed to employ regression algorithms for DNP, and found that the algorithms achieve low 
Average Absolute Error
 (AAE) and high Pred(0.3) values. However, since the defect datasets generally contain many non-defective modules, even if a DNP model predicts the number of defects in all modules as zero, the AAE value of the model will be low and Pred(0.3) value will be high. Therefore, the 
good performance
 of the regression algorithms in terms of AAE and Pred(0.3) may be questioned due to the imbalanced distribution of the number of defects.
Objective:
To revisit the impact of regression algorithms for predicting the precise number of defects.
Method:
We examine the practical effects of 12 widely-used regression algorithms, two data resampling algorithm (SmoteR and ROS), and three 
ensemble learning algorithms
 (gradient boosting regression, 
AdaBoost
.R2, and Bagging), one feature selection method (information gain) and one parameter optimization method (grid search) for predicting the precise number of defects on the 18 PROMISE datasets. We propose to evaluate the AAE and Pred(0.3) values for the modules with different numbers of defects separately.
Results:
The AAE values for defective modules are very high and the Pred(0.3) values are very low, i.e., the regression algorithms are very inaccurate for predicting the precise number of defects in defective modules.
Conclusion:
The problem of predicting the precise number of defects via regression algorithms is far from being solved. We recommend that software testers use regression algorithms to rank modules for testing 
resource allocation
, rather than predict the precise number of defects to evaluate the 
software reliability
 and maintenance effort. In addition, most existing DNP studies employing the whole AAE and Pred(0.3) values of all modules as the 
evaluation metrics
 for the proposed DNP algorithms should be revisited.",Information and Software Technology,18 Mar 2025,3.0,"The research focuses on defect prediction using regression algorithms, which may not have a direct practical value for most European early-stage ventures or startups."
https://www.sciencedirect.com/science/article/pii/S0950584922000404,A Delphi study to recognize and assess systems of systems vulnerabilities,June 2022,"Delphi, Expert judgment, Security, Systems of systems",Miguel A.=Olivero: molivero@us.es; Antonia=Bertolino: antonia.bertolino@isti.cnr.it; Francisco José=Dominguez-Mayo: fjdominguez@us.es; Ilaria=Matteucci: ilaria.matteucci@iit.cnr.it; María José=Escalona: mjescalona@us.es,"Abstract
Context
System of Systems (SoS) is an emerging paradigm by which independent systems collaborate by sharing resources and processes to achieve objectives that they could not achieve on their own. In this context, a number of emergent behaviors may arise that can undermine the security of the 
constituent systems
.
Objective
We apply the Delphi method with the aims to improve our understanding of SoS security and related problems, and to investigate their possible causes and remedies.
Method
Experts on SoS expressed their opinions and reached consensus in a series of rounds by following a structured questionnaire.
Results
The results show that the experts found more consensus in disagreement than in agreement about some SoS characteristics, and on how SoS vulnerabilities could be identified and prevented.
Conclusions
From this study we learn that more work is needed to reach a shared understanding of SoS vulnerabilities, and we leverage expert feedback to outline some future research directions.",Information and Software Technology,18 Mar 2025,5.0,The study addresses security issues in System of Systems (SoS) which could have practical implications for early-stage ventures dealing with cybersecurity.
https://www.sciencedirect.com/science/article/pii/S0950584922000362,Towards privacy compliance: A design science study in a small organization,June 2022,Not Found,Ze Shi=Li: lize@uvic.ca; Colin=Werner: Not Found; Neil=Ernst: Not Found; Daniela=Damian: Not Found,"Abstract
Context:
Complying with privacy regulations has taken on new importance with the introduction of the EU’s 
General Data Protection Regulation
 (GDPR) and other privacy regulations. Privacy measures are becoming a paramount requirement demanding software organizations’ attention as recent 
privacy breaches
 such as the Capital One data breach affected millions of customers. Software organizations, however, struggle with achieving privacy compliance. In particular, there is a lack of research into the organizational practices and challenges involved in compliance, particularly for 
small and medium enterprises
 (SMEs), which represent a sizeable portion of organizations. Many SMEs use a continuous 
software engineering
 (CSE) approach, which introduces additional adoption and application challenges. For example, the fast pace of CSE makes it harder for SMEs that are already more resource constrained to prioritize non-functional requirements such as privacy.
Objective:
This paper aims to fill a gap in the under-researched area of continuous compliance with privacy requirements in practice, by investigating how a continuous practicing SME dealt with GDPR compliance.
Method:
Using design science, we conducted an in-depth ethnographically informed study over the span of 16 months and iteratively developed two artifacts to help address the organization’s challenges in addressing GDPR compliance.
Results:
We identified 3 main challenges that our collaborating organization experienced when trying to comply with the GDPR. To help mitigate the challenges, we developed two design science artifacts, which include a list of privacy requirements that operationalized the GDPR principles for automated verification, and an automated testing tool that helps to verify these privacy requirements. We validated these artifacts through close collaboration with our partner organization and applying our artifacts to the partner organization’s system.
Conclusions:
We conclude with a discussion of opportunities and obstacles in leveraging CSE to achieve continuous compliance with the GDPR. We also highlight the importance of building a shared understanding of privacy non-functional requirements and how 
risk management
 plays an important role in an organization’s GDPR compliance.",Information and Software Technology,18 Mar 2025,9.0,"The research on continuous compliance with GDPR requirements is highly relevant to startups and SMEs in Europe, providing valuable insights and practical solutions."
https://www.sciencedirect.com/science/article/pii/S0950584922000398,Leveraging execution traces to enhance traceability links recovery in BPMN models,June 2022,Not Found,Raúl=Lapeña: rlapena@usj.es; Francisca=Pérez: mfperez@usj.es; Óscar=Pastor: opastor@pros.upv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Traceability Links
 Recovery has been a topic of interest for many years, resulting in techniques that perform traceability based on the linguistic clues of the software artifacts under study. However, 
BPMN
 models tend to present an overall lack of linguistic clues when compared to code-based artifacts or code generation models. Hence, TLR becomes a harder task when performed among requirements and 
BPMN
 models.
Objective:
This paper proposes a novel approach, called METRA, that leverages the execution traces of BPMN to expand the BPMN models. The expansion of the BPMN models enhances their linguistic clues, bridging the language between BPMN models and other software artifacts, and improving the TLR process between requirements and BPMN models.
Methods:
The proposed approach is evaluated through a real-world industrial 
case study
, comparing its outcomes against two state-of-the-art baselines, TLR and LORE. The paper also evaluates the combination of METRA with LORE against the rest of the approaches, including standalone METRA. The evaluation process generates a report of measurements (precision, recall, f-measure, and MCC), over which a statistical analysis is conducted.
Results:
Results show that approaches based on METRA maintain the excellent precision results obtained by baseline approaches (74.2% for METRA, 78.8% for METRA+LORE), whilst also improving the recall results from the unacceptable values obtained by the baselines to good values (72.4% for METRA, 73.9% for METRA+LORE). Moreover, according to the statistical analysis, the differences in the results obtained by the evaluated approaches are statistically significant.
Conclusions:
This paper opens a novel field of work in TLR by analyzing the improvement of the TLR process through the inclusion of linguistic clues present in execution traces, and discusses ideas for further research that can delve into this promising direction explored by our work.",Information and Software Technology,18 Mar 2025,7.0,The proposed approach for improving traceability links between requirements and BPMN models could benefit startups in terms of software development efficiency and quality.
https://www.sciencedirect.com/science/article/pii/S095058492200026X,Context2Vector: Accelerating security event triage via context representation learning,June 2022,Not Found,Jia=Liu: Not Found; Runzi=Zhang: runzi_zhang@163.com; Wenmao=Liu: Not Found; Yinghua=Zhang: Not Found; Dujuan=Gu: Not Found; Mingkai=Tong: Not Found; Xingkai=Wang: Not Found; Jianxin=Xue: Not Found; Huanran=Wang: Not Found,"Abstract
Context:
Security teams are overwhelmed by thousands of alerts and events everyday, which are comprehensively collected for threat analysis in 
security operations center
. Although methods based on rules, intelligence and data mining are utilized, the alert fatigue situation is still a challenging problem, slowing down the overall threat investigation process.
Objective:
‘Event polysemy’ phenomenon broadly exists in large-scale event dataset, which means that events of the same category can reveal different purposes in different contexts. This paper aims at exploring, revealing and evaluating the latent patterns embedding in the event contexts, to gain insight on context semantics and reduce manual intervention in event triage tasks.
Method:
A context 
representation learning
 based method, named Context2Vector, is proposed. Contexts are extracted from multiple behavioral views. Then, both dense event representations and sparse topic representations are learnt at the same time and in the same space. A human-in-the-loop topic annotation process is involved and finally, a context deviation detection based method is integrated to generate explainable and informative labels for automated context semantic decoding.
Results:
Various experiments are conducted on a enterprise-scale event dataset. The topic annotation, context related feature importance and top-N event ranking evaluation results show that Context2Vector outperforms traditional methods on the high-risk event identification problems, improving the attacker recall rate by up to 2.25 times within limited events to be investigated.
Conclusion:
It is concluded that event contexts imply practicable and abundant information in regard to behaviors and intents of real threat actors. More precise profiling of network entities can be extracted from contexts, compared to rules, intelligence, and 
anomaly detectors
 used in practice.",Information and Software Technology,18 Mar 2025,6.0,The research on event polysemy and context representation learning for threat analysis can provide valuable insights for startups dealing with security operations and alert fatigue.
https://www.sciencedirect.com/science/article/pii/S0950584922000180,An evaluation of the effectiveness of personalization and self-adaptation for e-Health apps,June 2022,"Self-adaptive systems, Personalization, Reference architecture, Mobile apps, e-Health",Eoin Martino=Grua: e.m.grua@vu.nl; Martina=De Sanctis: martina.desanctis@gssi.it; Ivano=Malavolta: i.malavolta@vu.nl; Mark=Hoogendoorn: m.hoogendoorn@vu.nl; Patricia=Lago: p.lago@vu.nl,"Abstract
Context.
There are many e-Health mobile apps on the apps store, from apps to improve a user’s lifestyle to mental coaching. Whilst these apps might consider user context when they give their interventions, prompts, and encouragements, they still tend to be rigid 
e.g.,
 not using user context and experience to tailor themselves to the user.
Objective.
To better engage and tailor to the user, we have previously proposed a Reference Architecture for enabling self-adaptation and 
AI
 personalization in e-Health mobile apps. In this work we evaluate the end users’ perception, usability, performance impact, and energy consumption contributed by this Reference Architecture.
Method.
We do so by implementing a Reference Architecture compliant app and conducting two experiments: a user study and a measurement-based experiment.
Results.
Although limited in the number of participants, the results of our user study show that usability of the Reference Architecture compliant app is similar to the control app. Users’ perception was found to be positively influenced by the compliant app when compared to the control group. Results of our measurement-based experiment showed some differences in performance and energy consumption measurements between the two apps. The differences are, however, deemed minimal.
Conclusions.
Our experiments show promising results for an app implemented following our proposed Reference Architecture. This is preliminary evidence that the use of personalization and self-adaptation techniques can be beneficial within the domain of e-Health apps.",Information and Software Technology,18 Mar 2025,8.0,The research on enabling self-adaptation and personalization in e-Health mobile apps shows promising results that could have a significant impact on the user experience and effectiveness of such apps.
https://www.sciencedirect.com/science/article/pii/S0950584922000416,Prioritization of model smell refactoring using a covariance matrix-based adaptive evolution algorithm,June 2022,Not Found,Amjad=AbuHassan: amjad.abuhassan@najah.edu; Mohammad=Alshayeb: alshayeb@kfupm.edu.sa; Lahouari=Ghouti: lghouti@psu.edu.sa,"Abstract
Context
The 
refactoring process
 enhances the 
software design
 by modifying the structure of design parts impaired with 
model smells
 without altering the overall software behavior. However, handling these smells without proper prioritization will not produce the anticipated effects.
Objective
In this paper, we solve the prioritization of the model smell refactoring using a multi-objective optimization (MOO) algorithm called the multi-objective (MO) 
covariance matrix
 adaptation evolution strategy (MO
CMA-ES). Our formulation relies on the refactoring of 
unified modeling language
 (UML) 
class diagrams
 to mitigate the negative effect of design smells.
Method
We treat the prioritization problem as a real-valued MOO where we propose novel data encoding procedures. We use two conflicting objectives, quality, and 
maintainability
, to balance the refactoring. We first build a new solution representation that guarantees smell fixing and eliminates the rejection limitation. Furthermore, we suggest a custom mapping scheme to properly encode real-valued quantities using unique representations. For performance evaluation purposes, we developed a large custom dataset with more than 30,000 class records, using seven popular open-source software projects. A novel relative coverage metric is proposed to mitigate the limitations associated with the standard coverage. For benchmarking purposes, we also consider an improved version of the nondominated sorting 
genetic algorithm
 (NSGA-II(.
Results
The reported performance scores confirm the superiority of the MO
CMA-ES algorithm over NSGA-II. The former successfully identified the refactoring sequences that lead to the best improvements in software quality and 
maintainability
 while it is able to fix all identified design smells. These improvements are quantified in terms of hypervolume, coverage, spacing metrics, and 
execution time
.
Conclusion
The MO
CMA-ES attained the highest average maximum quality score of 1149 while keeping the average 
maintainability
 at the lowest score of 13.8. In all experiment settings, the proposed solution leads to longer refactoring sequences at no additional computational cost.",Information and Software Technology,18 Mar 2025,10.0,"The use of a multi-objective optimization algorithm for model smell refactoring in UML class diagrams demonstrates a high practical value and potential impact on software quality and maintainability, making it highly valuable for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000416,Prioritization of model smell refactoring using a covariance matrix-based adaptive evolution algorithm,June 2022,Not Found,Amjad=AbuHassan: amjad.abuhassan@najah.edu; Mohammad=Alshayeb: alshayeb@kfupm.edu.sa; Lahouari=Ghouti: lghouti@psu.edu.sa,"Abstract
Context
The 
refactoring process
 enhances the 
software design
 by modifying the structure of design parts impaired with 
model smells
 without altering the overall software behavior. However, handling these smells without proper prioritization will not produce the anticipated effects.
Objective
In this paper, we solve the prioritization of the model smell refactoring using a multi-objective optimization (MOO) algorithm called the multi-objective (MO) 
covariance matrix
 adaptation evolution strategy (MO
CMA-ES). Our formulation relies on the refactoring of 
unified modeling language
 (UML) 
class diagrams
 to mitigate the negative effect of design smells.
Method
We treat the prioritization problem as a real-valued MOO where we propose novel data encoding procedures. We use two conflicting objectives, quality, and 
maintainability
, to balance the refactoring. We first build a new solution representation that guarantees smell fixing and eliminates the rejection limitation. Furthermore, we suggest a custom mapping scheme to properly encode real-valued quantities using unique representations. For performance evaluation purposes, we developed a large custom dataset with more than 30,000 class records, using seven popular open-source software projects. A novel relative coverage metric is proposed to mitigate the limitations associated with the standard coverage. For benchmarking purposes, we also consider an improved version of the nondominated sorting 
genetic algorithm
 (NSGA-II(.
Results
The reported performance scores confirm the superiority of the MO
CMA-ES algorithm over NSGA-II. The former successfully identified the refactoring sequences that lead to the best improvements in software quality and 
maintainability
 while it is able to fix all identified design smells. These improvements are quantified in terms of hypervolume, coverage, spacing metrics, and 
execution time
.
Conclusion
The MO
CMA-ES attained the highest average maximum quality score of 1149 while keeping the average 
maintainability
 at the lowest score of 13.8. In all experiment settings, the proposed solution leads to longer refactoring sequences at no additional computational cost.",Information and Software Technology,18 Mar 2025,,
https://www.sciencedirect.com/science/article/pii/S0950584922000179,Short communication: Evolution of secondary studies in software engineering,May 2022,"Systematic review, Mapping study, Qualitative study, Experience of authors",David=Budgen: david.budgen@durham.ac.uk; Pearl=Brereton: o.p.brereton@keele.ac.uk,"Abstract
Context:
Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by 
software engineering
 in 2004, many have been undertaken by researchers.
Objective:
To assess how the role of secondary studies in software engineering has evolved.
Methods:
We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their 
type
 (e.g. mapping study), their 
research focus
 (quantitative/qualitative and practice/methodological), as well as the experience of the first authors.
Results:
Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study.
Conclusion:
Existing guidelines
 largely focus upon quantitative 
systematic reviews
. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies.",Information and Software Technology,18 Mar 2025,6.0,"The assessment of the role of secondary studies in software engineering provides valuable insights, but the practical impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000179,Short communication: Evolution of secondary studies in software engineering,May 2022,"Systematic review, Mapping study, Qualitative study, Experience of authors",David=Budgen: david.budgen@durham.ac.uk; Pearl=Brereton: o.p.brereton@keele.ac.uk,"Abstract
Context:
Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by 
software engineering
 in 2004, many have been undertaken by researchers.
Objective:
To assess how the role of secondary studies in software engineering has evolved.
Methods:
We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their 
type
 (e.g. mapping study), their 
research focus
 (quantitative/qualitative and practice/methodological), as well as the experience of the first authors.
Results:
Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study.
Conclusion:
Existing guidelines
 largely focus upon quantitative 
systematic reviews
. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies.",Information and Software Technology,18 Mar 2025,6.0,"While the evaluation of secondary studies in software engineering is informative, its immediate practical value for early-stage ventures in Europe may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000234,Translating quality-driven code change selection to an instance of multiple-criteria decision making,May 2022,Not Found,Christos P.=Lamprakos: cplamprakos@microlab.ntua.gr; Charalampos=Marantos: hmarantos@microlab.ntua.gr; Miltiadis=Siavvas: siavvasm@iti.gr; Lazaros=Papadopoulos: lpapadop@microlab.ntua.gr; Angeliki-Agathi=Tsintzira: angeliki.agathi.tsintzira@gmail.com; Apostolos=Ampatzoglou: ampatzoglou@uom.edu.gr; Alexander=Chatzigeorgiou: achat@uom.edu.gr; Dionysios=Kehagias: diok@iti.gr; Dimitrios=Soudris: dsoudris@microlab.ntua.gr,"Abstract
Context:
The definition and assessment 
of software quality
 have not converged to a single specification. Each team may formulate its own notion of quality and tools and methodologies for measuring it. Software quality can be improved via code changes, most often as part of a software maintenance loop.
Objective:
This manuscript contributes towards providing decision support for code change selection given a) a set of preferences on a software product’s qualities and b) a pool of heterogeneous code changes to select from.
Method:
We formulate the problem as an instance of Multiple-Criteria Decision Making, for which we provide both an abstract flavor and a prototype implementation. Our prototype targets energy efficiency, technical debt and dependability.
Results:
This prototype achieved inconsistent results, in the sense of not always recommending changes reflecting the decision maker’s preferences. Encouraged from some positive cases and cognizant of our prototype’s shortcomings, we propose directions for future research.
Conclusion:
This paper should thus be viewed as an imperfect first step towards quality-driven, code change-centered decision support and, simultaneously, as a curious yet pragmatic enough gaze on the road ahead.",Information and Software Technology,18 Mar 2025,8.0,"The abstract presents a practical approach to improving software quality through decision support for code change selection, which can have a significant impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921002378,A systematic literature review on counterexample explanation,May 2022,Not Found,Arut Prakash=Kaleeswaran: arutprakash.kaleeswaran@de.bosch.com; Arne=Nordmann: arne.nordmann@de.bosch.com; Thomas=Vogel: thomas.vogel@informatik.hu-berlin.de; Lars=Grunske: grunske@informatik.hu-berlin.de,"Abstract
Context:
Safety is of 
paramount importance
 for cyber–physical systems in domains such as automotive, robotics, and avionics. Formal methods such as model checking are one way to ensure the safety of cyber–physical systems. However, adoption of formal methods in industry is hindered by 
usability issues
, particularly the difficulty of understanding model checking results.
Objective:
We want to provide an overview of the state of the art for counterexample explanation by investigating the contexts, techniques, and evaluation of research approaches in this field. This overview shall provide an understanding of current and guide future research.
Method:
To provide this overview, we conducted a systematic literature review. The survey comprises 116 publications that address counterexample explanations for model checking.
Results:
Most primary studies provide counterexample explanations graphically or as traces, minimize counterexamples to reduce complexity, localize errors in the models expressed in the input formats of 
model checkers
, support 
linear temporal logic
 or computation tree logic specifications, and use 
model checkers
 of the Symbolic Model Verifier family. Several studies evaluate their approaches in safety-critical domains with industrial applications.
Conclusion:
We notably see a lack of research on counterexample explanation that targets probabilistic and real-time systems, leverages the explanations to domain-specific models, and evaluates approaches in user studies. We conclude by discussing the adequacy of different types of explanations for users with varying domain and formal methods expertise, showing the need to support laypersons in understanding model checking results to increase adoption of formal methods in industry.",Information and Software Technology,18 Mar 2025,7.0,"The abstract addresses usability issues in formal methods for cyber-physical systems, which can be important for European startups in automotive, robotics, and avionics industries."
https://www.sciencedirect.com/science/article/pii/S0950584922000167,Response time evaluation of mobile applications combining network protocol analysis and information fusion,May 2022,Not Found,Pan=Liu: Not Found; Yihao=Li: yihao.li@ldu.edu.cn,"Abstract
The 
response time
 of a mobile application (app), especially a mobile stock trading app, is an important factor that affects customer satisfaction. However, it is considerably difficult to accurately evaluate the performance of mobile apps owing to numerous real-world settings such as operating systems, hardware, and test environments. This paper presents a novel method to evaluate the 
response time
 of mobile apps on different 
mobile phones
 through combining network protocol analysis and information fusion. To make the 
data collected
 from the mobile app more reliable and credible, we recruited some volunteers to collect data on their 
mobile phones
. Then we used the network protocol analysis method to obtain the response time of the mobile app on a mobile phone. Next, we adopted information fusion technology using the rank-score 
characteristic function
 to evaluate the response time of mobile apps on different mobile phones. Experiments were conducted to evaluate our approach on three types of mobile apps. The results showed that the proposed method can effectively evaluate the response time of mobile apps with low cost.",Information and Software Technology,18 Mar 2025,6.0,"The abstract introduces a novel method to evaluate the response time of mobile apps, which can benefit startups developing mobile applications in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584921002433,A unifying framework for the systematic analysis of Git workflows,May 2022,Not Found,Julio César=Cortés Ríos: juliocesar.cortesrios@manchester.ac.uk; Suzanne M.=Embury: suzanne.m.embury@manchester.ac.uk; Sukru=Eraslan: seraslan@metu.edu.tr,"Abstract
Context:
Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed.
Objective:
In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows’ comparison, to discern how, and to what extent, they exploit Git capabilities for 
collaborative software development
.
Methods:
The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using 
hierarchical clustering
 to determine workflows’ similarities and to identify which features are most popular, and more relevant for discriminatory purposes.
Results:
Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused.
Conclusion:
The characterisation and 
clustering analysis
 demonstrated that our framework can be used to compare and analyse Git workflows.",Information and Software Technology,18 Mar 2025,7.0,"The abstract proposes a feature-based framework for describing Git workflows, which can be valuable for startups collaborating on software development projects in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584921002469,"Drivers, barriers and impacts of digitalisation in rural areas from the viewpoint of experts",May 2022,"68-02, 68U35, 68N99",Alessio=Ferrari: alessio.ferrari@isti.cnr.it; Manlio=Bacco: Not Found; Kirsten=Gaber: Not Found; Andreas=Jedlitschka: Not Found; Steffen=Hess: Not Found; Jouni=Kaipainen: Not Found; Panagiota=Koltsida: Not Found; Eleni=Toli: Not Found; Gianluca=Brunori: Not Found,"Abstract
Context:
The domain of rural areas, including rural communities, 
agriculture
, and 
forestry
, is going through a process of deep digital transformation. 
Digitalisation
 can have positive impacts on 
sustainability
 in terms of greater environmental control, and community prosperity. At the same time, it can also have disruptive effects, with the 
marginalisation
 of actors that cannot cope with the change. When developing a novel system for rural areas, requirements engineers should carefully consider the specific socio-economic characteristics of the domain, so that potential positive effects can be maximised, while mitigating 
negative impacts
.
Objective:
The goal of this paper is to support requirements engineers with a reference catalogue of 
drivers
, 
barriers
 and potential 
impacts
 associated to the introduction of novel ICT solutions in rural areas.
Method:
To this end, we interview 30 cross-disciplinary experts in digitalisation of rural areas, and we analyse the transcripts to identify common themes.
Results:
According to the experts, main 
drivers
 are economic, with the possibility of reducing costs, and regulatory, as institutions push for more precise tracing and monitoring of production; 
barriers
 are the limited connectivity, but also distrust towards technology and other socio-cultural aspects; positive 
impacts
 are socio-economic (e.g., reduction of manual labour, greater productivity), while negative ones include potential dependency from technology, with loss of hands-on expertise, and marginalisation of certain actors (e.g., 
small farms
, subjects with limited education).
Conclusion:
This paper contributes to the literature with a domain-specific catalogue that characterises digitalisation in rural areas. The catalogue can be used as a reference baseline for 
requirements elicitation
 endeavours in rural areas, to support domain analysis prior to the development of novel solutions, as well as fit-gap analysis for the adaptation of existing technologies.",Information and Software Technology,18 Mar 2025,5.0,"The abstract focuses on digital transformation in rural areas, which may have indirect relevance to European startups, but may not directly impact early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000015,Predicting vulnerability inducing function versions using node embeddings and graph neural networks,May 2022,Not Found,Sefa Eren=Şahin: sahinsef@itu.edu.tr; Ecem Mine=Özyedierler: ozyedierlere@itu.edu.tr; Ayse=Tosun: tosunay@itu.edu.tr,"Abstract
Context:
Predicting software vulnerabilities over code changes is a difficult task due to obtaining real vulnerability data and their associated code fixes from software projects as software organizations are often reluctant to report those.
Objective:
We aim to propose a vulnerability prediction model that runs after every code change, and identifies vulnerability inducing functions in that version. We also would like to assess the success of node and token based source code representations over 
abstract syntax trees
 (ASTs) on predicting vulnerability inducing functions.
Method:
We train 
neural networks
 to represent 
node embeddings
 and token embeddings over ASTs in order to obtain feature representations. Then, we build two 
Graph Neural Networks
 (GNNs) with 
node embeddings
, and compare them against 
Convolutional Neural Network
 (CNN) and 
Support Vector Machine
 (SVM) with token representations.
Results:
We report our empirical analysis over the change history of vulnerability inducing functions of 
Wireshark
 project. GraphSAGE model using source code representation via ASTs achieves the highest AUC rate, while 
CNN models
 using token representations achieves the highest recall, precision and F1 measure.
Conclusion:
Representing functions with their structural information extracted from ASTs, either in token form or in complete graph form, is great at predicting vulnerability inducing function versions. Transforming source code into token frequencies as a natural language text fails to build successful models for vulnerability prediction in a real software project.",Information and Software Technology,18 Mar 2025,8.0,"The proposed vulnerability prediction model using GraphSAGE achieves high AUC rates and provides valuable insights into predicting vulnerability inducing functions, which can benefit early-stage ventures in improving software security."
https://www.sciencedirect.com/science/article/pii/S0950584921002470,Enhancing software modularization via semantic outliers filtration and label propagation,May 2022,Not Found,Kaiyuan=Yang: Not Found; Junfeng=Wang: wangjf@scu.edu.cn; Zhiyang=Fang: Not Found; Peng=Wu: Not Found; Zihua=Song: Not Found,"Abstract
Context:
Software systems’ modular structure often drifts from the intended design throughout evolution. To improve the modular structure of a software system, the 
software clustering
 technology aiming to partition a software system into meaningful modules is demanding. Many 
clustering approaches
 rely on semantic information, which cluster software entities that use similar vocabulary. However, the existence of semantic outliers obstructing the 
clustering process
 is hardly considered.
Objective:
To overcome the existence of semantic outliers, this paper proposes a two-stage 
software clustering
 approach named EVOL (Enhancing Via Outliers filtration and Label propagation).
Methods:
A feature density-based 
outliers detecting
 algorithm is used to compute the 
local outlier factor
 of each feature. Accordingly, we filter out the semantic outliers and cluster remaining high-quality features to construct a partition skeleton; After that, assign each outlier into a suitable cluster by label propagation.
Results:
To assess the effectiveness of the proposed approach, this paper conducts experiments on six folders from Mozilla Firefox and other four software systems, referring to the original design concepts and modular structure provided by the developers. The average of the 
evaluation metric
 MoJoFM shows significant improvement from 6% to 35% over the other six state-of-art 
clustering techniques
. The results demonstrate that the filtration of the outliers facilitates the 
clustering results
, and label propagation could place the outliers into a suitable cluster.
Conclusion:
In this paper, we propose EVOL, a new software clustering approach that considers semantic outliers filtration and label propagation. The experiment results show that the proposed approach EVOL can be very useful to enhance the quality of the software modularization.",Information and Software Technology,18 Mar 2025,9.0,"The EVOL software clustering approach significantly improves the modular structure of software systems by addressing semantic outliers, offering a practical solution that can have a positive impact on European early-stage ventures in enhancing software modularization."
https://www.sciencedirect.com/science/article/pii/S0950584922000027,An automated test data generation method for void pointers and function pointers in C/C++ libraries and embedded projects,May 2022,Not Found,Lam Nguyen=Tung: tunglam@vnu.edu.vn; Hoang-Viet=Tran: thv@vnu.edu.vn; Khoi Nguyen=Le: khoi.n.le@vnu.edu.vn; Pham Ngoc=Hung: hungpn@vnu.edu.vn,"Abstract
Automated test data generation for unit testing C/C++ functions using concolic methods is well-known for improving software quality while reducing human testing effort. However, there have been only a few researches related to generating test data for void pointers and 
function pointers
 which are commonly used in C/C++ libraries and embedded projects. This paper proposes a concolic-based method named VFP (
V
oid and 
F
unction 
P
ointers test data generation) to generate test data for void pointers and 
function pointers
. The key idea of VFP method is to preprocess the 
source code
 of the project under test to find all possible types of void pointers and references of function pointers. These types and references are used in the initial test data generating phase of the concolic 
testing method
. VFP method is implemented in VFP verification tool to test on various C/C++ libraries and embedded projects. The experimental results show that VFP significantly improves the coverage of the generated test data in comparison with existing methods.",Information and Software Technology,18 Mar 2025,7.0,"The VFP method for generating test data for void and function pointers in C/C++ libraries is a valuable contribution to improving software quality, especially for embedded projects, which can be beneficial for startups focusing on such technologies."
https://www.sciencedirect.com/science/article/pii/S0950584922000192,Personalizing label prediction for GitHub issues,May 2022,Not Found,Jun=Wang: 20194227028@stu.suda.edu.cn; Xiaofang=Zhang: xfzhang@suda.edu.cn; Lin=Chen: lchen@nju.edu.cn; Xiaoyuan=Xie: xxie@whu.edu.cn,"Abstract
Context:
Automated label prediction tools can help developers manage and categorize issues on GitHub. However, different open-source projects use various forms of labels with the same meaning. Previous label prediction methods mainly solve the problem of the synonymous labels by manual preprocessing rules, but these preprocessing rules can only identify synonyms with the same prefix or suffix.
Objective:
These factors inspire us to propose a method to identify these synonymous labels automatically and recommend personalized labels for different open-source projects.
Method:
In this paper, we propose a Personalizing Label Prediction framework for Issues named PLPI. PLPI identifies labels with similar meanings by representing labels as 
semantic vectors
 and applying 
clustering methods
. PLPI can predict personalized labels from the 
existing labels
 in the open-source project.
Result:
We conduct a comprehensive study to compare seven commonly adopted labeling models with our approach. The experimental results demonstrate the advantages of our approach. Finally, we show some representative examples and discuss the visualization results of synonyms clustering by dimension reduction.
Conclusion:
The experimental results show that our method PLPI can improve label prediction performance and provide personalized label recommendation results for different open-source projects.",Information and Software Technology,18 Mar 2025,6.0,"The PLPI framework for personalized label prediction addresses the issue of synonymous labels in open-source projects, providing a practical solution, although it may have a limited direct impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921002391,Inferring data model from service interactions for response generation in service virtualization,May 2022,Not Found,Md. Arafat=Hossain: mdarafathossain@swin.edu.au; Jiaojiao=Jiang: jiaojiao.jiang@unsw.edu.au; Jun=Han: jhan@swin.edu.au; Muhammad Ashad=Kabir: akabir@csu.edu.au; Jean-Guy=Schneider: jeanguy.schneider@deakin.edu.au; Chengfei=Liu: cliu@swin.edu.au,"Abstract
Context:
 Service 
virtualization
 has become a popular tool to provide testing environments for highly connected enterprise software systems. It enables the enterprise system under test to interact with and obtain responses from model-based service emulations instead of the actual services they use in production environments, providing accessibility and realness. Existing approaches consider only the 
control dependencies
 between messages (
i.e.
, the service’s control model) and do not consider the relationships between data values of the messages (
i.e.
, the service’s data model), limiting the accuracy of service emulation.
Objective:
 In this paper, we present an approach to deriving the service’s data model from its interaction traces and using it in determining the payloads for 
response messages
, therefore achieving more accurate service emulation.
Method:
 The derivation of a service’s data model is achieved by discovering the data entities and their key attribute(s) from the service interaction messages. It is then used, together with the control model, to synthesize 
response messages
 for incoming 
request messages
 at runtime. While the control model help to identify the types of responses, the data model keeps track of the changes to the service’s data entities and provides the basis for populating accurate payloads for the responses.
Results:
 A number of experiments have been conducted on message traces collected from a range of stateful and stateless services. With the use of both the control and data models in 
response generation
, our approach consistently outperforms the existing state-of-the-art approaches. In particular, it generates 100% identical responses (compared to actual services) for most of the datasets, while the highest accuracy achieved by existing approaches was 88%.
Conclusion:
 The experimental results have shown that the inferred data model provides an effective means in determining the payloads for response messages, significantly improving the accuracy of service emulation and providing more realistic testing environments.",Information and Software Technology,18 Mar 2025,8.0,"The approach to deriving service data models for accurate service emulation offers a significant advancement in service virtualization, which can benefit startups utilizing service emulation for testing enterprise software systems."
https://www.sciencedirect.com/science/article/pii/S0950584921002445,Feature toggles as code: Heuristics and metrics for structuring feature toggles,May 2022,Not Found,Rezvan=Mahdavi-Hezaveh: rmahdav@ncsu.edu; Nirav=Ajmeri: Not Found; Laurie=Williams: Not Found,"Abstract
Context:
Using feature toggles is a technique to turn a feature either on or off in program code by checking the value of a variable in a 
conditional statement
. This technique is increasingly used by software practitioners to support continuous integration and continuous delivery (CI/CD). However, using feature toggles may increase code complexity, create dead code, and decrease the quality of a codebase.
Objective:
The goal of this research is to aid software practitioners in structuring feature toggles in the codebase by proposing and evaluating a set of heuristics and corresponding complexity, 
comprehensibility
, and 
maintainability
 metrics based upon an empirical study of open source repositories.
Method:
We identified 80 GitHub repositories that use feature toggles in their 
development cycle
. We conducted a qualitative analysis using 60 of the 80 repositories to identify heuristics and metrics. Then, we conducted a survey of practitioners of 80 repositories to obtain their feedback that the proposed heuristics can be used to guide the structure of feature toggles and to reduce technical debt. We also conducted a 
case study
 of the all 80 repositories to analyze relations between heuristics and metrics.
Results:
From the qualitative analysis, we proposed 7 heuristics to guide structuring feature toggles and identified 12 metrics to support the principles embodied in the heuristics. Our survey result shows that practitioners agree that managing feature toggles is difficult, and using identified heuristics can reduce technical debt. Based on our 
case study
, we find a relationship between the adoption of heuristics and the values of metrics.
Conclusions:
Our results support that practitioners should have self-descriptive feature toggles, use feature toggles sparingly, avoid duplicate code in using feature toggles, and ensure complete removal of a feature toggle.",Information and Software Technology,18 Mar 2025,5.0,"The research on structuring feature toggles can provide valuable insights for software practitioners, but may have limited direct impact on early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584922000040,Ambiguity in user stories: A systematic literature review,May 2022,Not Found,Anis R.=Amna: AnisRahmawati.Amna@UGent.be; Geert=Poels: Geert.Poels@UGent.be,"Abstract
Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, 
syntactic
, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.",Information and Software Technology,18 Mar 2025,4.0,"While addressing ambiguity in user stories is a relevant problem, the research focus may not directly translate to practical value for early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584922000040,Ambiguity in user stories: A systematic literature review,May 2022,Not Found,Anis R.=Amna: AnisRahmawati.Amna@UGent.be; Geert=Poels: Geert.Poels@UGent.be,"Abstract
Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, 
syntactic
, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.",Information and Software Technology,18 Mar 2025,4.0,"Similar to abstract 47, addressing ambiguity in user stories is important but may not have immediate practical impact on early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584922000052,Proactive hybrid learning and optimisation in self-adaptive systems: The swarm-fleet infrastructure scenario,May 2022,Not Found,Christian=Krupitzer: christian.krupitzer@uni-hohenheim.de; Christian=Gruhl: Not Found; Bernhard=Sick: Not Found; Sven=Tomforde: Not Found,"Abstract
Context:
Smart and adaptive Systems, such as self-adaptive and self-organising (SASO) systems, typically consist of a large set of highly autonomous and heterogeneous subsystems that are able to adapt their behaviour to the requirements of ever-changing, dynamic environments. Their successful operation is based on appropriate modelling of the internal and external conditions.
Objective:
The control problem for establishing a near-to-optimal coordinated behaviour of systems with multiple, potentially conflicting objectives is either approached in a distributed (i.e., fully autonomous by the autonomous subsystems) or in a centralised way (i.e. one instance controlling the optimisation and planning process). In the distributed approach, selfish behaviour and being limited to local knowledge may lead to sub-optimal 
system behaviour
, while the 
centralised approach
 ignores the 
autonomy
 and the coordination efforts of parts of the system.
Method:
In this article, we present a concept for a hybrid (i.e., integrating a central optimisation with a distributed decision-making process) 
system management
 that combines local 
reinforcement learning
 and self-awareness mechanisms of fully autonomous subsystems with external system-wide planning and optimisation of adaptation freedom that steers the behaviour dynamically by issuing plans and guidelines augmented with incentivisation schemes.
Results:
This work addresses the inherent uncertainty of the dynamic 
system behaviour
, the local autonomous and context-aware learning of subsystems, and proactive control based on adaptiveness. We provide the ‘swarm-fleet infrastructure’ – a self-organised taxi service established by autonomous, privately-owned cars – as a 
testbed
 for structured comparison of systems.
Conclusion:
The ‘swarm-fleet infrastructure’ supports the advantages of a proactive hybrid self-adaptive and self-organising system operation. Further, we provide a system model to combine the system-wide optimisation while ensuring local decision-making through 
reinforcement learning
 for individualised configurations.",Information and Software Technology,18 Mar 2025,6.0,The concept of a hybrid system management approach can potentially provide practical value for early-stage ventures dealing with dynamic environments and system coordination.
https://www.sciencedirect.com/science/article/pii/S0950584921002317,A systematic process for Mining Software Repositories: Results from a systematic literature review,April 2022,Not Found,M.=Vidoni: melina.vidoni@anu.edu.au,"Abstract
Context:
Mining Software Repositories
 (MSR) is a growing area of 
Software Engineering
 (SE) research. Since their emergence in 2004, many investigations have analysed different aspects of these studies. However, there are no guidelines on how to conduct systematic MSR studies. There is a need to evaluate how MSR research is approached to provide a framework to do so systematically.
Objective:
To identify how MSR studies are conducted in terms of repository selection and 
data extraction
. To uncover potential for improvement in directing systematic research and providing guidelines to do so.
Method:
A 
systematic literature review
 of MSR studies was conducted following the guidelines and 
template
 proposed by Mian et al. (which refines those provided by Kitchenham and Charters). These guidelines were extended and revised to provide a framework for systematic MSR studies.
Results:
MSR studies typically do not follow a systematic approach for repository selection, and many do not report selection or 
data extraction
 protocols. Furthermore, few manuscripts discuss threats to the study’s validity due to the selection or data extraction steps followed.
Conclusions:
Although MSR studies are evidence-based research, they seldom follow a systematic process. Hence, there is a need for guidelines on how to conduct systematic MSR studies. New guidelines and a 
template
 have been proposed, consolidating related studies in the MSR field and strategies for systematic literature reviews.",Information and Software Technology,18 Mar 2025,5.0,The evaluation of how MSR studies are conducted can offer valuable insights for researchers but may have limited immediate impact on European early-stage ventures and startups.
https://www.sciencedirect.com/science/article/pii/S0950584921002317,A systematic process for Mining Software Repositories: Results from a systematic literature review,April 2022,Not Found,M.=Vidoni: melina.vidoni@anu.edu.au,"Abstract
Context:
Mining Software Repositories
 (MSR) is a growing area of 
Software Engineering
 (SE) research. Since their emergence in 2004, many investigations have analysed different aspects of these studies. However, there are no guidelines on how to conduct systematic MSR studies. There is a need to evaluate how MSR research is approached to provide a framework to do so systematically.
Objective:
To identify how MSR studies are conducted in terms of repository selection and 
data extraction
. To uncover potential for improvement in directing systematic research and providing guidelines to do so.
Method:
A 
systematic literature review
 of MSR studies was conducted following the guidelines and 
template
 proposed by Mian et al. (which refines those provided by Kitchenham and Charters). These guidelines were extended and revised to provide a framework for systematic MSR studies.
Results:
MSR studies typically do not follow a systematic approach for repository selection, and many do not report selection or 
data extraction
 protocols. Furthermore, few manuscripts discuss threats to the study’s validity due to the selection or data extraction steps followed.
Conclusions:
Although MSR studies are evidence-based research, they seldom follow a systematic process. Hence, there is a need for guidelines on how to conduct systematic MSR studies. New guidelines and a 
template
 have been proposed, consolidating related studies in the MSR field and strategies for systematic literature reviews.",Information and Software Technology,18 Mar 2025,7.0,"The research on conducting systematic MSR studies can provide valuable guidelines to improve the quality and validity of research in the field, benefiting early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S095058492100224X,How far are we from reproducible research on code smell detection? A systematic literature review,April 2022,"Software engineering, Code smells, Reproducibility, Reproducible research",Tomasz=Lewowski: tomasz.lewowski@pwr.edu.pl; Lech=Madeyski: lech.madeyski@pwr.edu.pl,"Abstract
Context:
Code smells are symptoms of wrong design decisions or coding shortcuts that may increase defect rate and decrease 
maintainability
. Research on code smells is accelerating, focusing on code smell detection and using code smells as 
defect predictors
. Recent research shows that even between software developers, agreement on what constitutes a code smell is low, but several publications claim the high performance of detection algorithms—which seems counterintuitive, considering that algorithms should be taught on data labeled by developers.
Objective:
This paper aims to investigate the possible reasons for the inconsistencies between studies in the performance of applied 
machine learning algorithms
 compared to developers. It focuses on the reproducibility of existing studies.
Methods:
A systematic literature review was performed among conference and journal articles published between 1999 and 2020 to assess the state of reproducibility of the research performed in those papers. A quasi-gold standard procedure was used to validate the search. Modeling process descriptions, reproduction scripts, data sets, and techniques used for their creation were analyzed.
Results:
We obtained data from 46 publications. 22 of them contained a detailed description of the modeling process, 17 included any reproduction data (data set, results, or scripts) and 15 used existing data sets. In most of the publications, analyzed projects were hand-picked by the researchers.
Conclusion:
Most studies do not include any form of an online reproduction package, although this has started to change recently—8% of analyzed studies published before 2018 included a full reproduction package, compared to 22% in years 2018–2019. Ones that do include a package usually use a research group website or even a personal one. Dedicated archives are still rarely used for data packages. We recommend that researchers include complete reproduction packages for their studies and use well-established research data archives instead of their own websites.",Information and Software Technology,18 Mar 2025,6.0,"The investigation into the reproducibility of studies on code smells and machine learning algorithms can provide insights into improving code quality and maintainability, which can benefit startups in developing reliable software products."
https://www.sciencedirect.com/science/article/pii/S0950584921002287,Impact of software development processes on the outcomes of student computing projects: A tale of two universities,April 2022,"Software engineering, Comparative study, Capstone project, Student projects, Education, Computer science education",Rafal=Włodarski: Not Found; Aneta=Poniszewska-Marańda: aneta.poniszewska-maranda@p.lodz.pl; Jean-Remy=Falleri: Not Found,"Abstract
Context:
Project-based courses are more and more commonly used as an opportunity to teach students structured methods of developing software. Two well-known approaches in this area – traditional and Agile – have been successfully applied to drive academic projects. However too often the default is still to have no organizational process at all. While a large variety of software development life-cycle models exists, little guidance is available on which one to choose to fit the context of working with students.
Objective:
This paper assesses the impact of iterative, sequential and “hands-off” development approaches on the success of student computing projects. A structured, metric-based assessment scheme was applied to investigate team productivity, teamwork and the quality of the final product.
Method:
Empirical evidence was collected during a controlled experiment carried out at two engineering schools in Europe. More than 100 students at Bachelor’s and Master’s levels participated in the research, with varied software development and teamwork skill sets.
Results:
Similar patterns were observed among both sets of subjects, with iterative teams demonstrating the highest productivity and superior team cohesion but a decline in the quality of the final product. Sequential development led to a considerable improvement in the external 
quality characteristics
 of the software produced, owing to the method’s stress on design activities.
Conclusion:
The findings of this study will be of use to educators interested in applying software development processes to student groupwork. A set of guidelines is provided for applying a structured way of working in a project-based course.",Information and Software Technology,18 Mar 2025,8.0,"Assessing the impact of different software development approaches on student projects can provide valuable guidance for educators and students in early-stage ventures, helping improve project success rates and team productivity."
https://www.sciencedirect.com/science/article/pii/S0950584921002287,Impact of software development processes on the outcomes of student computing projects: A tale of two universities,April 2022,"Software engineering, Comparative study, Capstone project, Student projects, Education, Computer science education",Rafal=Włodarski: Not Found; Aneta=Poniszewska-Marańda: aneta.poniszewska-maranda@p.lodz.pl; Jean-Remy=Falleri: Not Found,"Abstract
Context:
Project-based courses are more and more commonly used as an opportunity to teach students structured methods of developing software. Two well-known approaches in this area – traditional and Agile – have been successfully applied to drive academic projects. However too often the default is still to have no organizational process at all. While a large variety of software development life-cycle models exists, little guidance is available on which one to choose to fit the context of working with students.
Objective:
This paper assesses the impact of iterative, sequential and “hands-off” development approaches on the success of student computing projects. A structured, metric-based assessment scheme was applied to investigate team productivity, teamwork and the quality of the final product.
Method:
Empirical evidence was collected during a controlled experiment carried out at two engineering schools in Europe. More than 100 students at Bachelor’s and Master’s levels participated in the research, with varied software development and teamwork skill sets.
Results:
Similar patterns were observed among both sets of subjects, with iterative teams demonstrating the highest productivity and superior team cohesion but a decline in the quality of the final product. Sequential development led to a considerable improvement in the external 
quality characteristics
 of the software produced, owing to the method’s stress on design activities.
Conclusion:
The findings of this study will be of use to educators interested in applying software development processes to student groupwork. A set of guidelines is provided for applying a structured way of working in a project-based course.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 53, this study on the impact of different development approaches can benefit educators and students in early-stage ventures by providing guidance on improving project success rates and team productivity."
https://www.sciencedirect.com/science/article/pii/S0950584921002020,Erratum: Leveraging Flexible Tree Matching to repair broken locators in web automation scripts,April 2022,Not Found,Sacha=Brisset: sacha.brisset@hotmail.fr; Romain=Rouvoy: romain.rouvoy@univ-lille.fr; Lionel=Seinturier: lionel.seinturier@inria.fr; Renaud=Pawlak: renaud.pawlak@gmail.com,"Abstract
Web applications are constantly evolving to integrate new features and fix reported bugs. Even an imperceptible change can sometimes entail significant modifications of the 
Document Object Model
 (DOM), which is the underlying model used by browsers to render all the elements included in a web application. Scripts that interact with web applications (
e.g.
 web test scripts, crawlers, or robotic process automation) rely on this continuously evolving DOM which means they are often particularly fragile. More precisely, the major cause of breakages observed in automation scripts are 
element locators
, which are identifiers used by automation scripts to navigate across the DOM. When the DOM evolves, these identifiers tend to break, thus causing the related scripts to no longer locate the intended target elements.
For this reason, several contributions explored the idea of automatically repairing broken locators on a page. These works attempt to repair a given broken locator by scanning all elements in the new DOM to find the most similar one. Unfortunately, this approach fails to scale when the complexity of web pages grows, leading either to long 
computation times
 or incorrect element repairs. This article, therefore, adopts a different perspective on this problem by introducing a new locator repair solution that leverages 
tree
 
matching algorithms
 to relocate broken locators. This solution, named 
Erratum
, implements a holistic approach to reduce the element 
search space
, which greatly eases the locator repair task and drastically improves repair accuracy. We compare the robustness of 
Erratum
 on a large-scale benchmark composed of realistic and synthetic mutations applied to popular web applications currently deployed in production. Our empirical results demonstrate that 
Erratum
 outperforms the accuracy of WATER, a state-of-the-art solution, by 67%.",Information and Software Technology,18 Mar 2025,5.0,"While the research on automatically repairing broken locators on web pages is innovative, its direct impact on European early-stage ventures and startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921002342,A model-driven approach to reengineering processes in cloud computing,April 2022,Not Found,Mahdi=Fahmideh: mahdi.fahmideh@usq.edu.au; John=Grundy: Not Found; Ghassan=Beydoun: Not Found; Didar=Zowghi: Not Found; Willy=Susilo: Not Found; Davoud=Mougouei: Not Found,"Abstract
Context
The 
reengineering process
 of large data-intensive legacy 
software applications
 (“legacy applications” for brevity) to cloud platforms involves different interrelated activities. These activities are related to planning, architecture design, re-hosting/lift-shift, 
code refactoring
, and other related ones. In this regard, the 
cloud computing
 literature has seen the emergence of different methods with a disparate point of view of the same underlying legacy application 
reengineering process
 to cloud platforms. As such, the effective interoperability and tailoring of these methods become problematic due to the lack of integrated and consistent standard models.
Objective
We design, implement, and evaluate a novel framework called 
MLSAC (Migration of Legacy Software Applications to the Cloud)
. The core aim of MLSAC is to facilitate the sharing and tailoring of reengineering methods for migrating legacy applications to cloud platforms. MLSAC achieves this by using a collection of coherent and empirically tested cloud-specific method fragments from the literature and practice. A metamodel (or meta-method) together with corresponding 
instantiation
 guidelines is developed from this collection. The metamodel can also be used to create and maintain bespoke reengineering methods in a given scenario of reengineering to cloud platforms.
Approach
MLSAC is underpinned by a metamodeling approach that acts as a representational layer to express reengineering methods. The design and evaluation of MLSAC are informed by the guidelines from the 
design science research
 approach.
Results
Our framework is an accessible guide of what legacy-to-cloud reengineering methods can look like. The efficacy of the framework is demonstrated by modeling real-world reengineering scenarios and obtaining user feedback. Our findings show that the framework provides a fully-fledged domain-specific, yet platform-independent, foundation for the semi-automated representing, maintaining, sharing, and tailoring reengineering methods. MLSAC contributes to the state of the art of 
cloud computing
 and model-driven 
software engineering
 literature through (a) providing a collection of mainstream method fragments for incorporate into various scenarios of reengineering processes and (b) enabling a basis for consistent creation, representation, and maintenance of reengineering methods and processes within the cloud computing community.",Information and Software Technology,18 Mar 2025,8.0,The development of MLSAC framework for migrating legacy applications to cloud platforms has a practical value for startups looking to modernize their software systems efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584921002366,"Prioritizing user concerns in app reviews – A study of requests for new features, enhancements and bug fixes",April 2022,Not Found,Saurabh=Malgaonkar: Not Found; Sherlock A.=Licorish: sherlock.licorish@otago.ac.nz; Bastin Tony Roy=Savarimuthu: tony.savarimuthu@otago.ac.nz,"Abstract
Context
: App developers spend exhaustive manual efforts towards the identification and prioritization of informative end-user reviews. Informative reviews are those that express end-users’ requests for new features, bug fixes and possible enhancements. 
Problem Statement
: While prior studies have proposed approaches to convert app reviews into 
actionable knowledge
, these are limited in utility due to being domain knowledge dependent or manually-based.
Objective
: In this study, in order to facilitate app maintenance and evolution cycles, we develop two novel automated prioritization techniques to rank informative reviews, and also compare their performances.
Method
: We developed the techniques comprising of entropy, frequency, TF-IDF and sentiment scoring methods using reviews from four popular apps comprising more than 1000 informative reviews in each app. Time and accuracy metrics were then used to measure the performance of our techniques. We performed evaluations where the ranking outcomes generated by our techniques were compared to those provided by regular app users and developers using two rounds of evaluations (internal and external evaluations).
Results
: Our results show that the time required for prioritization was in the range of 17–25 s and the accuracy of prioritization was in the range of 73–90%.
Conclusion
: These are promising outcomes when compared to prior work, where our outcomes were 4% and 185% better in terms of accuracy and time respectively. Thus, it is anticipated that our proposed techniques could support app developers in identifying and prioritizing informative reviews.",Information and Software Technology,18 Mar 2025,7.0,"The automated prioritization techniques for app reviews can benefit startups in enhancing their app maintenance and evolution cycles, though the impact may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921002354,Consistent or not? An investigation of using Pull Request Template in GitHub,April 2022,Not Found,Mengxi=Zhang: Not Found; Huaxiao=Liu: liuhuaxiao@jlu.edu.cn; Chunyang=Chen: Not Found; Yuzhou=Liu: Not Found; Shuotong=Bai: Not Found,"Abstract
Context:
The arbitrary usage of pull requests in GitHub may bring many issues such as incomplete, verbose, and duplicated descriptions, which hinder the development and maintenance of the project. Thus, GitHub proposed the Pull Request 
Template
 (PRT) in 2016 so that developers could edit the pull request in a relevant consistent manner. However, whether the PRT has been widely applied to GitHub and what impact it might bring remain little known. Such uninformed cases may affect the efficiency of cooperative development.
Objective:
In this work, we conduct an empirical study on large-scale repositories to explore whether the PRT has been widely applied and what impact it can bring to the GitHub community.
Method:
This work aims to answer four research questions. The first is a statistical experiment with the aim of analyzing the current status of PRTs. The second is an explored experiment, which aims at probing which repositories are suitable to adopt the PRT. The third is the measurement evaluation experiment, focusing on discussing what impact the PRT can bring. The last is an online survey to explain why few PRTs have been adopted. Notably, both the second and third questions are conducted a mixed quantitative and qualitative analysis.
Results and conclusion:
In this work, we find that only 1.2% of repositories contain the PRT in GitHub, and such repositories are mostly in high popularity and contain a large number of PRs. Besides, contributors are willing to accept the PRT that requires pivotal information, including description, test, and check_list. Meanwhile, the PRT can assist developers to manage repositories, reflecting in less time for reviewing, fewer duplicated pull requests, and almost non-existentially invalid comments. Finally, we survey 527 well-reputed developers to explain why few repositories adopt the PRT, and further provide some actionable suggestions.",Information and Software Technology,18 Mar 2025,6.0,"The empirical study on GitHub PRT adoption provides insights that could be valuable for startups collaborating on GitHub, but the direct impact on early-stage ventures may not be as significant."
https://www.sciencedirect.com/science/article/pii/S0950584921002408,Indexing source code and clone detection,April 2022,Not Found,Zdenek=Tronicek: tronicek@tarleton.edu,"Abstract
Context:
Searching 
source code
 is a common task in code recommendation systems as well as in many other areas. Clone detection is used in software maintenance and bug detection.
Objective:
The paper introduces an algorithm for building the index structure of 
abstract syntax trees
. When the index structure is built, a pattern tree can be found in time linear in the length of the pattern. Furthermore, the paper describes 
DrDup2
 and 
DrDupLex
, two open-source tools that use the index structure to find Type-2 clones.
Method:
The index structure presented in this paper is based on the trie, which is a fundamental 
data structure
 in computer science. Evaluation of the presented clone detectors is done on BigCloneBench, which is a well-established benchmark for clone detection.
Results:
Comparison with three state-of-the-art clone detectors (
NiCad
, 
CloneWorks
 and 
SourcererCC
) shows that 
DrDup2
 and 
DrDupLex
 are able to beat them in precision, recall and running time.
Conclusion:
The presented index structure can be used for example to speed up searching for code fragments in code recommendation systems. It is also shown that it can be used to detect Type-2 clones with high precision and recall.",Information and Software Technology,18 Mar 2025,9.0,"The algorithm for building the index structure of abstract syntax trees and the presented clone detectors are highly relevant for startups using code recommendation systems, offering practical solutions with superior performance."
https://www.sciencedirect.com/science/article/pii/S0950584921002421,VUDENC: Vulnerability Detection with Deep Learning on a Natural Codebase for Python,April 2022,Not Found,Laura=Wartschinski: wartschinski@informatik.hu.berlin.de; Yannic=Noller: noller@informatik.hu.berlin.de; Thomas=Vogel: thomas.vogel@informatik.hu.berlin.de; Timo=Kehrer: kehrer@informatik.hu.berlin.de; Lars=Grunske: grunske@informatik.hu.berlin.de,"Abstract
Context:
Identifying potential vulnerable code is important to improve the security of our software systems. However, the manual detection of software vulnerabilities requires expert knowledge and is time-consuming, and must be supported by automated techniques.
Objective:
Such automated 
vulnerability detection
 techniques should achieve a high accuracy, point developers directly to the vulnerable code fragments, scale to real-world software, generalize across the boundaries of a specific software project, and require no or only moderate setup or configuration effort.
Method:
In this article, we present 
Vudenc
 (Vulnerability Detection with 
Deep Learning
 on a Natural Codebase), a deep learning-based 
vulnerability detection
 tool that automatically learns features of vulnerable code from a large and real-world Python codebase. 
Vudenc
 applies a word2vec model to identify semantically similar code tokens and to provide a vector representation. A network of long-short-term memory cells (LSTM) is then used to classify vulnerable code token sequences at a fine-grained level, highlight the specific areas in the source code that are likely to contain vulnerabilities, and provide confidence levels for its predictions.
Results:
To evaluate 
Vudenc
, we used 1,009 vulnerability-fixing commits from different GitHub repositories that contain seven different types of vulnerabilities (SQL injection, XSS, Command injection, XSRF, 
Remote code execution
, Path disclosure, Open redirect) for training. In the experimental evaluation, 
Vudenc
 achieves a recall of 78%–87%, a precision of 82%–96%, and an F1 score of 80%–90%. 
Vudenc
’s code, the datasets for the vulnerabilities, and the Python corpus for the word2vec model are available for reproduction.
Conclusions:
Our experimental results suggest that 
Vudenc
 is capable of outperforming most of its competitors in terms of vulnerably detection capabilities on real-world software. Comparable accuracy was only achieved on synthetic benchmarks, within single projects, or on a much coarser level of 
granularity
 such as entire 
source code files
.",Information and Software Technology,18 Mar 2025,8.0,The Vudenc tool for vulnerability detection with deep learning presents a high accuracy solution that can greatly benefit startups aiming to improve the security of their software systems efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584921002081,iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms,April 2022,Not Found,Mohammad=Hamdaqa: mhamdaqa@polymtl.ca; Lucas Alberto Pineda=Met: Not Found; Ilham=Qasse: Not Found,"Abstract
Context:
Smart contracts
 play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of 
contractual rules
. In addition to business knowledge, coding a smart contract requires strong 
technical knowledge
 in a multiplex of new and rapidly changing domain-specific languages and 
blockchain
 platforms.
Objectives:
The goal of this paper is to assist developers in 
building smart
 contracts independently from the language or the target 
blockchain
 platform. In which, we present our second-generation smart contract language iContractML 2.0.
Methods:
We follow a feature-oriented approach to analyze three different 
blockchain
 platforms and propose an enhanced reference model and a modeling framework for smart contracts (iContractML 2.0). Then, we evaluate the coverage and extensibility of iContractML 2.0, first through mapping the concepts of the reference models to the constructs within each of the platforms used in devising the reference model, and second through mapping its concepts to a new smart contract language not previously considered. Finally, we demonstrate the capabilities of iContractML 2.0 using five 
case studies
 from different business domains.
Results:
iContractML 2.0 extends our first generation language to support 
DAML
, which is another standardized language for smart contracts. This makes iContractML 2.0 supports the platforms that 
DAML
 support by extension. Moreover, iContractML 2.0 supports generating the structural and deployment artifacts in addition to the smart contract behavior by implementing templates for some of the common functions. The results of evaluating the generality of the iContractML 2.0 reference model show that it is 91.7% lucid and 72.2% laconic. Moreover, the reference model is able to capture all the elements of the new language with 83.3% of the components which have a direct one-to-one mapping.
Conclusion:
iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple 
blockchain
 platforms.",Information and Software Technology,18 Mar 2025,8.0,The development of a second-generation smart contract language that can be deployed onto multiple blockchain platforms has practical value for early-stage ventures and startups in the tech industry.
https://www.sciencedirect.com/science/article/pii/S0950584921002081,iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms,April 2022,Not Found,Mohammad=Hamdaqa: mhamdaqa@polymtl.ca; Lucas Alberto Pineda=Met: Not Found; Ilham=Qasse: Not Found,"Abstract
Context:
Smart contracts
 play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of 
contractual rules
. In addition to business knowledge, coding a smart contract requires strong 
technical knowledge
 in a multiplex of new and rapidly changing domain-specific languages and 
blockchain
 platforms.
Objectives:
The goal of this paper is to assist developers in 
building smart
 contracts independently from the language or the target 
blockchain
 platform. In which, we present our second-generation smart contract language iContractML 2.0.
Methods:
We follow a feature-oriented approach to analyze three different 
blockchain
 platforms and propose an enhanced reference model and a modeling framework for smart contracts (iContractML 2.0). Then, we evaluate the coverage and extensibility of iContractML 2.0, first through mapping the concepts of the reference models to the constructs within each of the platforms used in devising the reference model, and second through mapping its concepts to a new smart contract language not previously considered. Finally, we demonstrate the capabilities of iContractML 2.0 using five 
case studies
 from different business domains.
Results:
iContractML 2.0 extends our first generation language to support 
DAML
, which is another standardized language for smart contracts. This makes iContractML 2.0 supports the platforms that 
DAML
 support by extension. Moreover, iContractML 2.0 supports generating the structural and deployment artifacts in addition to the smart contract behavior by implementing templates for some of the common functions. The results of evaluating the generality of the iContractML 2.0 reference model show that it is 91.7% lucid and 72.2% laconic. Moreover, the reference model is able to capture all the elements of the new language with 83.3% of the components which have a direct one-to-one mapping.
Conclusion:
iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple 
blockchain
 platforms.",Information and Software Technology,18 Mar 2025,8.0,The development of a second-generation smart contract language that can be deployed onto multiple blockchain platforms has practical value for early-stage ventures and startups in the tech industry.
https://www.sciencedirect.com/science/article/pii/S095058492100241X,HyMap: Eliciting hypotheses in early-stage software startups using cognitive mapping,April 2022,Not Found,Jorge=Melegati: jorge.melegati@unibz.it; Eduardo=Guerra: eduardo.guerra@unibz.it; Xiaofeng=Wang: xiaofeng.wang@unibz.it,"Abstract
Context:
 Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation, an approach based on validating assumptions about the software product through data obtained from diverse techniques, like A/B tests or interviews, is valuable for these companies. Relying on data rather than opinions reduces the chance of developing unnecessary products or features, improving the likelihood of success, especially in early development stages, when implementing unnecessary features represents a higher risk for companies’ survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. Since the first step of the approach is to define hypotheses, testable statements about the software product features, based on which software development teams will create experiments, eliciting hypotheses is a natural first step to develop practices. 
Objective:
 We aim to develop a systematic technique for identifying hypotheses in early-stage software startups to support experimentation in these companies and, consequently, improve their software products. 
Methods:
 We followed a Design Science approach consisting of an artifact 
construction process
, divided in three phases, and an evaluation within three startups. 
Results:
 We developed the HyMap, a hypotheses 
elicitation
 technique based on 
cognitive mapping
. It consists of a process conducted by a facilitator using pre-defined questions, supported by a visual language to depict a cognitive map representing the founder’s understanding of the product. Our evaluation showed that founders perceived the artifacts as clear, easy to use, and useful leading to hypotheses and facilitating their idea’s visualization. 
Conclusion:
 From a theoretical perspective, our study provides a better understanding of the guidance founders use to develop their startups and, from a practical point of view, a technique to identify hypotheses in early-stage software startups.",Information and Software Technology,18 Mar 2025,7.0,The systematic technique for identifying hypotheses in early-stage software startups to support experimentation may provide valuable insights for startups looking to improve their software products.
https://www.sciencedirect.com/science/article/pii/S095058492100241X,HyMap: Eliciting hypotheses in early-stage software startups using cognitive mapping,April 2022,Not Found,Jorge=Melegati: jorge.melegati@unibz.it; Eduardo=Guerra: eduardo.guerra@unibz.it; Xiaofeng=Wang: xiaofeng.wang@unibz.it,"Abstract
Context:
 Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation, an approach based on validating assumptions about the software product through data obtained from diverse techniques, like A/B tests or interviews, is valuable for these companies. Relying on data rather than opinions reduces the chance of developing unnecessary products or features, improving the likelihood of success, especially in early development stages, when implementing unnecessary features represents a higher risk for companies’ survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. Since the first step of the approach is to define hypotheses, testable statements about the software product features, based on which software development teams will create experiments, eliciting hypotheses is a natural first step to develop practices. 
Objective:
 We aim to develop a systematic technique for identifying hypotheses in early-stage software startups to support experimentation in these companies and, consequently, improve their software products. 
Methods:
 We followed a Design Science approach consisting of an artifact 
construction process
, divided in three phases, and an evaluation within three startups. 
Results:
 We developed the HyMap, a hypotheses 
elicitation
 technique based on 
cognitive mapping
. It consists of a process conducted by a facilitator using pre-defined questions, supported by a visual language to depict a cognitive map representing the founder’s understanding of the product. Our evaluation showed that founders perceived the artifacts as clear, easy to use, and useful leading to hypotheses and facilitating their idea’s visualization. 
Conclusion:
 From a theoretical perspective, our study provides a better understanding of the guidance founders use to develop their startups and, from a practical point of view, a technique to identify hypotheses in early-stage software startups.",Information and Software Technology,18 Mar 2025,7.0,The systematic technique for identifying hypotheses in early-stage software startups to support experimentation may provide valuable insights for startups looking to improve their software products.
https://www.sciencedirect.com/science/article/pii/S095058492100238X,Introduction to the Special Issue on value and waste in software engineering,April 2022,Not Found,Matthias=Galster: Not Found; Clemente=Izurieta: Not Found; Carolyn=Seaman: Not Found,"Abstract
In the context of software engineering, “value” and “waste” can mean different things to different stakeholders. While traditionally value and waste have been considered from a business or economic point of view, there has been a trend in recent years towards a broader perspective that also includes wider human and societal values. This Special Issue explores value and waste aspects in all areas of software engineering, including identifying, quantifying, reasoning about, and representing value and waste, driving value and avoiding waste, and managing value and waste. In this editorial we provide an introduction to the topic and provide an overview of the contributions included in this Special Issue.",Information and Software Technology,18 Mar 2025,5.0,"While the exploration of value and waste aspects in software engineering is valuable, the broader perspective discussed may have limited practical impact on early-stage ventures and startups in the tech industry."
https://www.sciencedirect.com/science/article/pii/S0950584921002263,When should we (not) use the mean magnitude of relative error (MMRE) as an error measure in software development effort estimation?,March 2022,Not Found,Magne=Jørgensen: magnej@simula.no; Torleif=Halkjelsvik: torleif@simula.no; Knut=Liestøl: knut@ifi.uio,"Abstract
Context
The mean magnitude of relative error (MMRE) is an error measure frequently used to evaluate and compare the estimation performance of prediction models and software professionals.
Objective
This paper examines conditions for proper use of MMRE in effort estimation contexts.
Method
We apply research on scoring functions to identify the type of estimates that minimizes the expected value of the MMRE.
Results
We show that the MMRE is a proper error measure for estimates of the most likely (mode) effort, but not for estimates of the median or mean effort, provided that the effort usage is approximately log-normally distributed, which we argue is a reasonable assumption in many software development contexts. The relevance of the findings is demonstrated on real-world software development data.
Conclusion
MMRE is not a proper measure of the accuracy of estimates of the median or mean effort, but may be used for the accuracy evaluation of estimates of most likely effort.",Information and Software Technology,18 Mar 2025,4.0,"The research on MMRE in effort estimation for software development may have some impact on the accuracy evaluation of certain types of effort estimates, but the practical value for startups is limited as it focuses on a specific error measure."
https://www.sciencedirect.com/science/article/pii/S0950584921002263,When should we (not) use the mean magnitude of relative error (MMRE) as an error measure in software development effort estimation?,March 2022,Not Found,Magne=Jørgensen: magnej@simula.no; Torleif=Halkjelsvik: torleif@simula.no; Knut=Liestøl: knut@ifi.uio,"Abstract
Context
The mean magnitude of relative error (MMRE) is an error measure frequently used to evaluate and compare the estimation performance of prediction models and software professionals.
Objective
This paper examines conditions for proper use of MMRE in effort estimation contexts.
Method
We apply research on scoring functions to identify the type of estimates that minimizes the expected value of the MMRE.
Results
We show that the MMRE is a proper error measure for estimates of the most likely (mode) effort, but not for estimates of the median or mean effort, provided that the effort usage is approximately log-normally distributed, which we argue is a reasonable assumption in many software development contexts. The relevance of the findings is demonstrated on real-world software development data.
Conclusion
MMRE is not a proper measure of the accuracy of estimates of the median or mean effort, but may be used for the accuracy evaluation of estimates of most likely effort.",Information and Software Technology,18 Mar 2025,4.0,"Similar to abstract 66, this paper on MMRE in effort estimation may have relevance in software development contexts, but the practical application for startups is restricted due to its specific focus on error measure."
https://www.sciencedirect.com/science/article/pii/S0950584921001865,A comparison of machine learning algorithms on design smell detection using balanced and imbalanced dataset: A study of God class,March 2022,"Software quality, Design smell detection, Machine learning, God class, Balanced data",Khalid=Alkharabsheh: khalidkh@bau.edu.jo; Sadi=Alawadi: sadi.alawadi@it.uu.se; Victor R.=Kebande: victor.kebande@bth.se; Yania=Crespo: yania@infor.uva.es; Manuel=Fernández-Delgado: manuel.fernandez.delgado@usc.es; José A.=Taboada: joseangel.taboada@usc.es,"Abstract
Context:
Design smell detection has proven to be a significant activity that has an aim of not only enhancing the software quality but also increasing its life cycle.
Objective:
This work investigates whether 
machine learning approaches
 can effectively be leveraged for 
software design
 smell detection. Additionally, this paper provides a comparatively study, focused on using balanced datasets, where it checks if avoiding dataset balancing can be of any influence on the accuracy and behavior during design smell detection.
Method:
A set of experiments have been conducted-using 28 
Machine Learning
 classifiers aimed at detecting God classes. This experiment was conducted using a dataset formed from 12,587 classes of 24 software systems, in which 1,958 classes were manually validated.
Results:
Ultimately, most classifiers obtained high performances,-with Cat Boost showing a higher performance. Also, it is evident from the experiments conducted that data balancing does not have any significant influence on the accuracy of detection. This reinforces the application of machine learning in real scenarios where the data is usually imbalanced by the inherent nature of design smells.
Conclusions:
Machine learning approaches can effectively be used as a leverage for God class detection. While in this paper we have employed SMOTE technique for data balancing, it is worth noting that there exist other methods of data balancing and with other design smells. Furthermore, it is also important to note that application of those other methods may improve the results, in our experiments SMOTE did not improve God class detection.
The results are not fully generalizable because only one design smell is studied with projects developed in a single programming language, and only one balancing technique is used to compare with the imbalanced case. But these results are promising for the application in real design smells detection scenarios as mentioned above and the focus on other measures, such as Kappa, ROC, and MCC, have been used in the assessment of the 
classifier behavior
.",Information and Software Technology,18 Mar 2025,8.0,"The study on leveraging machine learning for software design smell detection, with practical experiments and results showing the effectiveness of certain classifiers, has high practical value for startup software development teams looking to enhance software quality."
https://www.sciencedirect.com/science/article/pii/S0950584921002056,An information theoretic notion of software testability,March 2022,Not Found,Krishna=Patel: krishna.patel@sheffield.ac.uk; Robert M.=Hierons: r.hierons@sheffield.ac.uk; David=Clark: david.clark@ucl.ac.uk,"Abstract
Context:
In software testing, Failed 
Error Propagation
 (FEP) is the situation in which a faulty program state occurs during the execution of the system under test (SUT) but this does not lead to incorrect output. It is known that FEP can adversely affect software testing and this has resulted in associated information 
theoretic measures
.
Objective:
To devise measures that can be used to assess the 
testability
 of the SUT. By testability, we mean how likely it is that a faulty program state, that occurs during testing, will lead to incorrect output. Previous work has considered a single program point rather than an entire program.
Method:
New, more fine-grained, measures were devised. Experiments were used to evaluate these and the previously defined measures (Squeeziness and Normalised Squeeziness). The experiments assessed how well these measures correlated with an estimate of the probability of FEP occurring during testing. Mutants were used to estimate this probability.
Results:
A strong rank correlation was found between several of the measures and the probability of FEP. Importantly, this included the Normalised Squeeziness of the whole SUT, which is simpler to compute, or estimate, than most of the other measures considered. Additional experiments found that the measures were relatively insensitive to the choice of mutants and also test suite.
Conclusion:
There is scope to use information 
theoretic measures
 to estimate how prone an SUT is to FEP. As a result, there is potential to use such measures to prioritise testing or estimate how much testing an SUT might require.",Information and Software Technology,18 Mar 2025,7.0,"The investigation into measures for assessing testability in software testing, with experiments showing correlations and potential applications in prioritizing testing efforts, provides practical value for startups aiming to improve their software testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002068,Refactoring embedded software: A study in healthcare domain,March 2022,Not Found,Paraskevi=Smiari: psmiari@uowm.gr; Stamatia=Bibi: sbibi@uowm.gr; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Elvira-Maria=Arvanitou: e.arvanitou@uom.edu.gr,"Abstract
Context
In 
embedded software
 industry, stakeholders usually promote run-time properties (e.g., performance, energy efficiency, etc.) as quality drivers, which in many cases leads to a compromise at the levels of design-time qualities (e.g., 
maintainability
, reusability, etc.). Such a compromise does not come without a cost; since embedded systems need heavy maintenance cycles. To assure effective bug-fixing, shorten the time required for releasing updates, a refactoring of the software codebase needs to take place regularly. Objective: This study aims to investigate how refactorings are applied in ES industry; and propose a systematic approach that can guide refactoring through a 3-step process for refactoring: (a) planning; (b) design; and (c) evaluation.
Method
The aforementioned goals were achieved by conducting a single case study in a company that develops medical applications for bio-impedance devices; and follows a rather systematic 
refactoring process
 in periodic timestamps. Three 
data collection approaches
 have been used: surveys, interviews (10 practitioners), and artifact analysis (51 refactoring activities).
Results
The results of the study suggest that: (a) 
maintainability
 and reusability are the design-time quality attributes that motivate the refactoring of Embedded Software (ES), with 30% of the participants considering them as of “Very High” importance; (b) the refactorings that are most frequently performed are “Extract Method”, “Replace Magic Number with Constant” and “Remove Parameter”. We note that the “Extract Method” refactoring has an applicability of more than over 80%; and (c) to evaluate the 
refactoring process
 engineers use tools producing structural metrics, internal standards, and reviews.
Conclusions
The outcomes of this study can be useful to both researchers and practitioners, in the sense that the former can focus their efforts on aspects that are meaningful to industry, whereas the latter are provided with a systematic refactoring process.",Information and Software Technology,18 Mar 2025,8.0,"The study on refactorings in the embedded software industry, proposing a systematic approach for refactoring with insights from a case study, offers practical guidance for startups in the industry on improving design-time qualities and ensuring effective bug-fixing processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002251,"Relative estimates of software development effort: Are they more accurate or less time-consuming to produce than absolute estimates, and to what extent are they person-independent?",March 2022,Not Found,Magne=Jørgensen: magnej@simula.no; Eban=Escott: Not Found,"Abstract
Context
Estimates of software development effort may be given as judgments of relationships between the use of efforts on different tasks-that is, as relative estimates. The use of relative estimates has increased with the introduction of story points in 
agile software development
 contexts.
Objective
This study examines to what extent relative estimates are likely to be more accurate or less time-consuming to produce than absolute software development effort estimates and to what extent relative estimates can be considered developer-independent.
Method
We conducted two experiments. In the first experiment, we collected estimates from 102 professional software developers estimating the same tasks and randomly allocated to providing relative estimates in story points or absolute estimates in work-hours. In the second experiment, we collected the actual efforts of 20 professional software developers completing the same 5 programming tasks and used these to analyze the variance in relative efforts.
Results
The results from the first experiment indicates that the relative estimates were less accurate than the absolute estimates, and that the time consumed completing the estimation work was higher for those using relative estimation, even when only considering developers with extensive 
prior experience
 in story point–based estimation for both tasks. The second experiment revealed that the relative effort was far from developer-independent, especially for the least productive developers. This suggests that relative estimates to a large extent are developer-dependent.
Conclusions
Although there may be good reasons for the continued use of relative estimates, we interpret our results as not supporting that the use of relative estimates is connected with higher estimation accuracy or less time consumed on producing the estimates. Neither do our results support a high degree of developer-independence in relative estimates.",Information and Software Technology,18 Mar 2025,4.0,"The study highlights the limitations of relative estimates in software development, which can be important for startups to consider when planning their projects."
https://www.sciencedirect.com/science/article/pii/S095058492100210X,Developers’ viewpoints to avoid bug-introducing changes,March 2022,Not Found,Jairo=Souza: jrmcs@ic.ufal.br; Rodrigo=Lima: Not Found; Baldoino=Fonseca: Not Found; Bruno=Cartaxo: Not Found; Márcio=Ribeiro: Not Found; Gustavo=Pinto: Not Found; Rohit=Gheyi: Not Found; Alessandro=Garcia: Not Found,"Abstract
Context:
During software development, developers can make assumptions that guide their development practices to avoid bug-introducing changes. For instance, developers may consider that code with low test coverage is more likely to introduce bugs; and thus, focus their attention on that code to avoid bugs, neglecting other factors during the software development process. However, there is no knowledge about the relevance of these assumptions for developers.
Objective:
This study investigates the developers’ viewpoints on the relevance of certain assumptions to avoid bug-introducing changes. In particular, we analyze which assumptions developers can make during software development; how relevant these assumptions are for developers; the common viewpoints among developers regarding these assumptions; and the main reasons for developers to put more/less relevance for some assumptions.
Method:
We applied the Q-methodology, a mixed-method from the psychometric spectrum, to investigate the relevance of assumptions and extract the developers’ viewpoints systematically. We involved 41 developers analyzing 41 assumptions extracted from literature and personal interviews.
Results:
We identified five viewpoints among developers regarding their assumptions around bug-introducing changes. Despite the differences among the viewpoints, there is also consensus, for example, regarding the importance of being aware of changes invoking high number of features. Moreover, developers rely on personal and technical reasons to put relevance on some assumptions.
Conclusion:
These findings are valuable knowledge for practitioners and researchers towards future research directions and development practices improvements.",Information and Software Technology,18 Mar 2025,6.0,"The findings provide valuable insights for developers on assumptions to avoid bug-introducing changes, which can have practical implications for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492100207X,Summarizing source code with hierarchical code representation,March 2022,Not Found,Ziyi=Zhou: zhouziyi@mail.ecust.edu.cn; Huiqun=Yu: yhq@ecust.edu.cn; Guisheng=Fan: Not Found; Zijie=Huang: Not Found; Xingguang=Yang: Not Found,"Abstract
Context
Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area. Data-driven code summarization models based on 
neural networks
 have proliferated in recent few years.
Objective
Almost all of existing 
neural models
 are built upon the 
granularity
 of token or 
AST
 node. This has several drawbacks: a) Code summarization requires high-level knowledge of code while token representations are limited to provide a global view; b) Such approaches can hardly model the hierarchy of code; c) Long input codes challenge such models to handle long-range dependencies due to the large number of tokens and 
AST
 nodes.
Method
To address these issues, we propose a novel framework to utilize hierarchical representation of code to generate better summaries. We consider two levels of code hierarchy: token-level and statement-level. Our framework contains a pair of customized encoder-decoder models for tokens and AST of code respectively. Each of them has a hierarchical encoder that aims to extract both token and statement-level code features, and an attentional decoder with the ability to attend to those different levels of representation during decoding. They are then combined to predict summaries via 
ensemble learning
.
Results
We conduct extensive experiments to evaluate our models on a large Java corpus. The experimental results show that our approach outperforms several state-of-the-art baselines by a substantial margin.
Conclusion
In conclusion, our approach could better learn global information of code and shift attention between important statements during summary generation. With the help of hierarchical attention, the models are able to locate keywords more accurately in a top-down way. Ensemble learning is also proved to be an effective way to benefit from multiple input sources.",Information and Software Technology,18 Mar 2025,9.0,"The proposed framework shows significant improvements in code summarization, which can greatly benefit startups in improving their code documentation and understanding."
https://www.sciencedirect.com/science/article/pii/S0950584921002275,Task assignment to counter the effect of developer turnover in software maintenance: A knowledge diffusion model,March 2022,Not Found,Vahid=Etemadi: Not Found; Omid=Bushehrian: bushehrian@sutech.ac.ir; Gregorio=Robles: Not Found,"Abstract
Context:
Developer churn is the overall turnover in a software organization’s staff. Existing developers leave and new ones join the project. Retaining the knowledge of the software 
source code
 among the development team in such scenarios is an essential factor to keep the software maintenance cost as low as possible. 
Knowledge diffusion
 is an activity that could mitigate the 
negative impact
 of developer churn, while a task assignment strategy could pay an important role to attain good knowledge diffusion among the team members and effectively lower the likelihood of knowledge loss.
Objective:
In this work, a self-adaptive task assignment (SATA) approach is proposed that adaptively switches between cost-oriented and diffusion-oriented strategies over subsequent rounds of task assignments.
Method:
An entropy-based model is applied to estimate the current conditions of the development team from the knowledge concentration perspective. This model is assisted by a learning 
automata
 and 
evolutionary algorithms
 to offer smart assignments.
Results:
The experimental results show that, particularly in teams with medium churn rates, applying an entropy-aware task assignment model can reduce the total maintenance cost up to slightly over 50%, provided that the knowledge demands in the team over successive rounds of task assignment remain stationary. There are also improvements in terms of the projects’ 
bus factor
 which prevent the project to lose its key knowledge. Even for projects where there is no saving in maintenance costs, SATA results in knowledge being more distributed among the developers, resulting in a more resilient project.
Conclusion:
SATA improves the long-term 
sustainability
 of development teams with developer turnover. Projects and their managers can hence rely on it when there is the risk of knowledge loss due to developer turnover.",Information and Software Technology,18 Mar 2025,8.0,"The self-adaptive task assignment approach can help reduce maintenance costs and improve knowledge diffusion in development teams, which is crucial for startups with limited resources."
https://www.sciencedirect.com/science/article/pii/S0950584921002135,Automated data function extraction from textual requirements by leveraging semi-supervised CRF and language model,March 2022,Not Found,Mingyang=Li: Not Found; Lin=Shi: shilin@iscas.ac.cn; Yawen=Wang: Not Found; Junjie=Wang: Not Found; Qing=Wang: Not Found; Jun=Hu: Not Found; Xinhua=Peng: Not Found; Weimin=Liao: Not Found; Guizhen=Pi: Not Found,"Abstract
Context:
Function Point Analysis (FPA) provides an objective, comparative measure for size estimation in the early stage of software development. When practicing FPA, analysts typically abide by the following steps: data function (DF) extraction, transactional function extraction, function type classification and adjustment factor determination. However, due to lack of approach and tool support, these steps are usually conduct by human efforts in practice. Related approaches can hardly be applied in the FPA due to the following three challenges, i.e., FPA rule-driven extraction, domain-specific 
parsing
, and expensive labeled resources.
Objective:
In this paper, we aim to automate the extraction of DFs, which is the starting and fundamental step in FPA.
Method:
We propose an automated approach named DEX to extract data functions from textual requirements. Specifically, DEX introduces the popularly-used 
conditional random field
 (CRF) model to predict the boundary of a data function. Besides, DEX employs the bootstrapping-based algorithm and DF-oriented 
language model
 to further boost the performance.
Results:
We evaluate DEX from two aspects: evaluation on a real industrial dataset and a manual review by domain experts. The evaluation on the real industrial dataset shows that DEX could achieve 80% precision, 84% recall, and 82% F1, and outperforms three state-of-the-art baselines. The expert review suggests that DEX could increase 16% precision and 13% recall, compared with those produced by engineers.
Conclusion:
DEX could achieve promising results under a small number of labeled requirements and outperform the state-of-the-art approaches. Moreover, DEX could help engineers produce more accurate and complete DFs in the industrial environment.",Information and Software Technology,18 Mar 2025,7.0,"The automated approach for data function extraction in FPA could streamline the size estimation process in software development, providing efficiency benefits for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921002044,Layout merging with relative positioning in Concern-Oriented Reuse hierarchies,March 2022,Not Found,Hyacinth=Ali: hyacinth.ali@mail.mcgill.ca; Gunter=Mussbacher: gunter.mussbacher@mcgill.ca,"Abstract
Context:
The advent of modeling in 
software engineering
, like other engineering fields, has revolutionized the formalism and pace of software development. However, 
software applications
 are not built from scratch, instead, other existing software artifacts are reused and combined with new artifacts. This notion of 
software reuse
 has been in existence for decades. When structural models such as 
class diagrams
 are reused, the reusing and reused models often need to be merged and the result visualized to the modeler. However, layout mechanisms such as GraphViz, JGraphX, and other related layout tools do not retain the original layout and rather arbitrarily layout the merged models. Therefore, important information that corresponds to the mental map of a modeler and is conveyed by the specific layout is currently lost.
Objective:
This paper aims to establish layout algorithms to retain the original layout information from a set of individual but interrelated models after they are merged during 
software reuse
 to preserve a modeler’s mental map of the models.
Method:
In this work, rpGraph uses the 
relative positioning
 of model elements to retain the general layout of a single reusing model and a single reused model (two-model merge). Additionally, rpGraph integrates its two-model merge approach into a multi-model merge in a reuse hierarchy to preserve the general topology of several interrelated models. Our findings are evaluated with 20 example single-model reuses from a library of reusable software model artifacts. We further carry out a 
case study
 in a reuse hierarchy framework, Concern Oriented Reuse (CORE), where rpGraph is applied to the layout of reusable artifacts, which result from a merge of several individual models.
Result:
A comparison of the merged layouts of rpGraph, GraphViz, and JGraphX shows that rpGraph performs better in terms of retaining the original layouts.
Conclusion:
Considering 
relative positioning
 during model merge increases the degree with which original layouts can be preserved.",Information and Software Technology,18 Mar 2025,8.0,"The research addresses a practical issue in software engineering by improving model layout algorithms for software reuse, which can benefit early-stage ventures by increasing efficiency and accuracy in software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002044,Layout merging with relative positioning in Concern-Oriented Reuse hierarchies,March 2022,Not Found,Hyacinth=Ali: hyacinth.ali@mail.mcgill.ca; Gunter=Mussbacher: gunter.mussbacher@mcgill.ca,"Abstract
Context:
The advent of modeling in 
software engineering
, like other engineering fields, has revolutionized the formalism and pace of software development. However, 
software applications
 are not built from scratch, instead, other existing software artifacts are reused and combined with new artifacts. This notion of 
software reuse
 has been in existence for decades. When structural models such as 
class diagrams
 are reused, the reusing and reused models often need to be merged and the result visualized to the modeler. However, layout mechanisms such as GraphViz, JGraphX, and other related layout tools do not retain the original layout and rather arbitrarily layout the merged models. Therefore, important information that corresponds to the mental map of a modeler and is conveyed by the specific layout is currently lost.
Objective:
This paper aims to establish layout algorithms to retain the original layout information from a set of individual but interrelated models after they are merged during 
software reuse
 to preserve a modeler’s mental map of the models.
Method:
In this work, rpGraph uses the 
relative positioning
 of model elements to retain the general layout of a single reusing model and a single reused model (two-model merge). Additionally, rpGraph integrates its two-model merge approach into a multi-model merge in a reuse hierarchy to preserve the general topology of several interrelated models. Our findings are evaluated with 20 example single-model reuses from a library of reusable software model artifacts. We further carry out a 
case study
 in a reuse hierarchy framework, Concern Oriented Reuse (CORE), where rpGraph is applied to the layout of reusable artifacts, which result from a merge of several individual models.
Result:
A comparison of the merged layouts of rpGraph, GraphViz, and JGraphX shows that rpGraph performs better in terms of retaining the original layouts.
Conclusion:
Considering 
relative positioning
 during model merge increases the degree with which original layouts can be preserved.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 76, this research focuses on improving layout algorithms for software reuse, which can have a practical impact on European early-stage ventures by enhancing software development efficiency and preserving original layouts."
https://www.sciencedirect.com/science/article/pii/S0950584921002093,Facilitating the co-evolution of semantic descriptions in standards and models,March 2022,Not Found,Philip=Makedonski: makedonski@cs.uni-goettingen.de; Jens=Grabowski: grabowski@cs.uni-goettingen.de,"Abstract
Context:
Standardised specifications for sophisticated technologies are subdivided in multiple documents maintained by different working groups, typically accompanied by models and other formalised artefacts. As the specifications and the models evolve, ensuring their consistency at scale becomes challenging.
Objective:
While previous work developed a methodology for facilitating the co-evolution of models and standards, based on the Network Function 
Virtualisation
 (NFV) Information Model (IM) and models extracted from the related standardised specifications, the methodology focused on structural aspects only. This article refines the methodology, enabling the alignment of 
semantic descriptions
 of 
information elements
 and attributes, both across specifications and across 
information elements
.
Method:
To enable the alignment of 
semantic descriptions
, we extend the methodology by using statistical and visual analyses of terms used in the specifications. The underlying meta-model for the information extracted from the specifications is extended to accommodate the capturing of additional semantic information.
Results:
We report on our experiences with the application of a prototypical implementation of the methodology during the continued alignment and maintenance of the IM and the related standardised specifications. More than 400 potential inconsistencies were identified, leading to more than 100 contributions, some of which addressed multiple findings. Feedback from the working group provided insights on how to refine the methodology further.
Conclusions:
Models shall play a more central role and be better integrated throughout the specification development and implementation processes, helping to ensure and maintain consistency among specifications. Our experiences may provide useful insights into ongoing and future initiatives where similar challenges are faced.",Information and Software Technology,18 Mar 2025,7.0,"The refinement of the methodology for aligning semantic descriptions in model-based specifications can provide valuable insights for early-stage ventures involved in developing sophisticated technologies, enhancing consistency and efficiency in their processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002305,Programming language implementations for context-oriented self-adaptive systems,March 2022,Not Found,Nicolás=Cardozo: n.cardozo@uniandes.edu.co; Kim=Mens: kim.mens@uclouvain.be,"Abstract
Context
The context-oriented programming paradigm is designed to enable self-adaptation, or dynamic behavior modification of software systems, in response to changes in their surrounding environment. Contextoriented programming offers an adaptation model, from a programming language perspective, that maintains a clean modularisation between the application and adaptation logic, as well as between the components providing adaptations.
Objective
We use three implementation techniques for context-oriented programming languages to assess their appropriateness to foster self-adaptive systems. These approaches take advantage of the capabilities offered by the host programming language to realize self-adaptation as proposed by context-oriented languages.
Method
We evaluate each of these approaches by assessing their modularity and complexity when defining adaptations, and by comparing their run-time performance on a simple benchmark.
Results
Our results show a higher modularity than that for common architecture based self-adaptive systems, while maintaining comparable performance.
Conclusion
We conclude that context-oriented programming is an appropriate paradigm to realize self-adaptation.",Information and Software Technology,18 Mar 2025,5.0,"While the research on context-oriented programming is relevant for enabling self-adaptive systems, the direct practical impact on European early-stage ventures may be limited compared to other abstracts focusing on software development efficiency and consistency."
https://www.sciencedirect.com/science/article/pii/S0950584921002329,An onboarding model for integrating newcomers into agile project teams,March 2022,Not Found,Peggy=Gregory: ajgregory@uclan.ac.uk; Diane E.=Strode: diane.strode@whitireia.ac.nz; Helen=Sharp: helen.sharp@open.ac.uk; Leonor=Barroca: leonor.barroca@open.ac.uk,"Abstract
Context
A stable team is deemed optimal for 
agile software development
 project success; however, all teams change membership over time. Newcomers joining an agile project team must rapidly assimilate into the organisational and project environment. They must do this while learning how to contribute effectively and without seriously interrupting project progress.
Objective
This paper addresses how newcomers integrate into an established agile project team and how agile practices assist with onboarding.
Method
A single, qualitative 
case study
 approach was used, investigating a co-located agile project team in a large IT department who regularly onboard inexperienced newcomers. Analysis was abductive, consisting of inductive coding and theming using categories from an existing onboarding theory.
Results
We describe the team's onboarding practices and adjustments and present an agile onboarding model that encompasses onboarding activities, individual adjustments, and workplace adjustments.
Conclusions
A mixture of general and specific agile onboarding practices contribute to successful onboarding in an agile team. We provide 
practical guidelines
 to improve onboarding practice in agile teams. Our major new contribution is an extended model of onboarding for agile teams.",Information and Software Technology,18 Mar 2025,6.0,"The study on agile onboarding in software development teams provides practical guidelines for integrating newcomers, which can be beneficial for early-stage ventures looking to maintain team stability and project progress, although the direct impact on software development processes may not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921002329,An onboarding model for integrating newcomers into agile project teams,March 2022,Not Found,Peggy=Gregory: ajgregory@uclan.ac.uk; Diane E.=Strode: diane.strode@whitireia.ac.nz; Helen=Sharp: helen.sharp@open.ac.uk; Leonor=Barroca: leonor.barroca@open.ac.uk,"Abstract
Context
A stable team is deemed optimal for 
agile software development
 project success; however, all teams change membership over time. Newcomers joining an agile project team must rapidly assimilate into the organisational and project environment. They must do this while learning how to contribute effectively and without seriously interrupting project progress.
Objective
This paper addresses how newcomers integrate into an established agile project team and how agile practices assist with onboarding.
Method
A single, qualitative 
case study
 approach was used, investigating a co-located agile project team in a large IT department who regularly onboard inexperienced newcomers. Analysis was abductive, consisting of inductive coding and theming using categories from an existing onboarding theory.
Results
We describe the team's onboarding practices and adjustments and present an agile onboarding model that encompasses onboarding activities, individual adjustments, and workplace adjustments.
Conclusions
A mixture of general and specific agile onboarding practices contribute to successful onboarding in an agile team. We provide 
practical guidelines
 to improve onboarding practice in agile teams. Our major new contribution is an extended model of onboarding for agile teams.",Information and Software Technology,18 Mar 2025,7.0,"The study provides practical guidelines for improving onboarding practices in agile teams, which can have a direct impact on the success of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001889,Does it matter who pays back Technical Debt? An empirical study of self-fixed TD,March 2022,Not Found,Jie=Tan: j.tan@rug.nl; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
Technical Debt (TD) can be paid back either by those that incurred it or by others. We call the former self-fixed TD, and it can be particularly effective, as developers are experts in their own code and are well-suited to fix the corresponding TD issues.
Objective:
The goal of our study is to investigate self-fixed technical debt, especially the extent in which TD is self-fixed, which types of TD are more likely to be self-fixed, whether the remediation time of self-fixed TD is shorter than non-self-fixed TD and how development behaviors are related to self-fixed TD.
Method:
We report on an empirical study that analyzes the self-fixed issues of five types of TD (i.e., Code, Defect, Design, Documentation and Test), captured via 
static analysis
, in more than 44,000 commits obtained from 20 Python and 16 Java projects of the Apache Software Foundation.
Results:
The results show that about half of the fixed issues are self-fixed and that the likelihood of contained TD issues being self-fixed is negatively correlated with project size, the number of developers and total issues. Moreover, there is no significant difference of the survival time between self-fixed and non-self-fixed issues. Furthermore, developers are more keen to pay back their own TD when it is related to lower code level issues, e.g., Defect Debt and Code Debt. Finally, developers who are more dedicated to or knowledgeable about the project contribute to a higher chance of self-fixing TD.
Conclusions:
These results can benefit both researchers and practitioners by aiding the prioritization of TD remediation activities and refining strategies within development teams, and by informing the development of TD 
management tools
.",Information and Software Technology,18 Mar 2025,8.0,"The findings on self-fixed technical debt can benefit both researchers and practitioners by aiding the prioritization of TD remediation activities, which is crucial for startups in early stages."
https://www.sciencedirect.com/science/article/pii/S095058492100183X,A mapping study on documentation in Continuous Software Development,February 2022,"Systematic mapping studies, Systematic reviews, Continuous Software Development, Lean, Agile, DevOps, Documentation",Theo=Theunissen: theo.theunissen@gmail.com; Uwe=van Heesch: Not Found; Paris=Avgeriou: Not Found,"Abstract
Context:
With an increase in Agile, Lean, and 
DevOps
 software methodologies over the last years (collectively referred to as Continuous Software Development (CSD)), we have observed that documentation is often poor.
Objective:
This work aims at collecting studies on documentation challenges, documentation practices, and tools that can support documentation in CSD.
Method:
A 
systematic mapping study
 was conducted to identify and analyze research on documentation in CSD, covering publications between 2001 and 2019.
Results:
A total of 63 studies were selected. We found 40 studies related to documentation practices and challenges, and 23 studies related to tools used in CSD. The challenges include: informal documentation is hard to understand, documentation is considered as waste, productivity is measured by working software only, documentation is out-of-sync with the software and there is a short-term focus. The practices include: non-written and informal communication, the usage of development artifacts for documentation, and the use of architecture frameworks. We also made an inventory of numerous tools that can be used for documentation purposes in CSD. Overall, we recommend the usage of executable documentation, modern tools and technologies to retrieve information and transform it into documentation, and the practice of minimal documentation upfront combined with detailed design for knowledge transfer afterwards.
Conclusion:
It is of 
paramount importance
 to increase the quantity and quality of documentation in CSD. While this remains challenging, practitioners will benefit from applying the identified practices and tools in order to mitigate the stated challenges.",Information and Software Technology,18 Mar 2025,6.0,"While documentation in CSD may not directly impact startups, the usage of modern tools and technologies recommended in this study could be beneficial for early-stage ventures looking to scale."
https://www.sciencedirect.com/science/article/pii/S095058492100183X,A mapping study on documentation in Continuous Software Development,February 2022,"Systematic mapping studies, Systematic reviews, Continuous Software Development, Lean, Agile, DevOps, Documentation",Theo=Theunissen: theo.theunissen@gmail.com; Uwe=van Heesch: Not Found; Paris=Avgeriou: Not Found,"Abstract
Context:
With an increase in Agile, Lean, and 
DevOps
 software methodologies over the last years (collectively referred to as Continuous Software Development (CSD)), we have observed that documentation is often poor.
Objective:
This work aims at collecting studies on documentation challenges, documentation practices, and tools that can support documentation in CSD.
Method:
A 
systematic mapping study
 was conducted to identify and analyze research on documentation in CSD, covering publications between 2001 and 2019.
Results:
A total of 63 studies were selected. We found 40 studies related to documentation practices and challenges, and 23 studies related to tools used in CSD. The challenges include: informal documentation is hard to understand, documentation is considered as waste, productivity is measured by working software only, documentation is out-of-sync with the software and there is a short-term focus. The practices include: non-written and informal communication, the usage of development artifacts for documentation, and the use of architecture frameworks. We also made an inventory of numerous tools that can be used for documentation purposes in CSD. Overall, we recommend the usage of executable documentation, modern tools and technologies to retrieve information and transform it into documentation, and the practice of minimal documentation upfront combined with detailed design for knowledge transfer afterwards.
Conclusion:
It is of 
paramount importance
 to increase the quantity and quality of documentation in CSD. While this remains challenging, practitioners will benefit from applying the identified practices and tools in order to mitigate the stated challenges.",Information and Software Technology,18 Mar 2025,6.0,"While documentation in CSD may not directly impact startups, the usage of modern tools and technologies recommended in this study could be beneficial for early-stage ventures looking to scale."
https://www.sciencedirect.com/science/article/pii/S0950584921001932,Patchworking: Exploring the code changes induced by vulnerability fixing activities,February 2022,Not Found,Gerardo=Canfora: canfora@unisannio.it; Andrea=Di Sorbo: disorbo@unisannio.it; Sara=Forootani: forootani@unisannio.it; Matias=Martinez: matias.martinez@uphf.fr; Corrado A.=Visaggio: visaggio@unisannio.it,"Abstract
Context:
Identifying and repairing vulnerable code is a critical software maintenance task. Change impact analysis plays an important role during software maintenance, as it helps software maintainers to figure out the potential effects of a change before it is applied. However, while the 
software engineering
 community has extensively studied techniques and tools for performing impact analysis of change requests, there are no approaches for estimating the impact when the change involves the resolution of a vulnerability bug.
Objective:
We hypothesize that similar vulnerabilities may present similar strategies for patching. More specifically, our work aims at understanding whether the class of the vulnerability to fix may determine the type of impact on the system to repair.
Method:
To verify our conjecture, in this paper, we examine 524 security patches applied to vulnerabilities belonging to ten different weakness categories and extracted from 98 different open-source projects written in Java.
Results:
We obtain empirical evidence that vulnerabilities of the same types are often resolved by applying similar code transformations, and, thus, produce almost the same impact on the codebase.
Conclusion:
On the one hand, our findings open the way to better management of software maintenance activities when dealing with software vulnerabilities. Indeed, vulnerability class information could be exploited to better predict how much code will be affected by the fixing, how the 
structural properties
 of the code (i.e., complexity, coupling, cohesion, size) will change, and the effort required for the fix. On the other hand, our results can be leveraged for improving automated strategies supporting developers when they have to deal with security flaws.",Information and Software Technology,18 Mar 2025,8.0,"Understanding the impact of vulnerability fixes can significantly improve software maintenance activities, which is crucial for startups looking to secure their systems and products."
https://www.sciencedirect.com/science/article/pii/S0950584921001920,The impact of the distance metric and measure on SMOTE-based techniques in software defect prediction,February 2022,Not Found,Shuo=Feng: shuo.feng@hotmail.com; Jacky=Keung: jacky.keung@cityu.edu.hk; Peichang=Zhang: pzhang@szu.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Miao=Zhang: miazhang9-c@my.cityu.edu.hk,"Abstract
Context:
In software 
defect prediction
, SMOTE-based techniques are widely adopted to alleviate the 
class imbalance problem
. SMOTE-based techniques select instances close in the distance to synthesize minority class instances, ensuring few noise instances are generated.
Objective:
However, recent studies show that selecting instances far away effectively increases the diversity and alleviates the overgeneralization brought by SMOTE-based techniques. To investigate the relationship between the distance of the selected instances and the performances of SMOTE-based techniques, we carry out this study.
Method:
We first conduct experiments to empirically investigate the impact of the distance between the instances on the performances of three common SMOTE-based techniques. Based on the experimental result, we improve a recently proposed oversampling technique-SMOTUNED.
Results:
The experimental results on five common classifiers across 30 imbalanced datasets from the PROMISE repository show that (1) the selection of the distance metric has little impact on the performances of SMOTE-based techniques, (2) as long as the number of synthesized noise instances is not beyond the noise-resistant ability of classifiers, the overall performances measured by AUC and 
b
a
l
a
n
c
e
 of SMOTE-based techniques are not significantly affected by the distance between instances, and (3) the 
probability of detection
 (
p
d
) and the 
probability of false alarm
 (
p
f
) values of SMOTE-based techniques are significantly affected by the distance between the selected instances. The larger the distance between the selected instances is, the lower the 
p
d
 and 
p
f
 values SMOTE-based techniques obtain. The performance of the improved SMOTUNED is similar to that of the original SMOTUNED, but the improved SMOTUNED dramatically decreases the 
execution time
 of the original SMOTUNED.
Conclusion:
By controlling the distance, different 
p
d
 and 
p
f
 values can be obtained. The diversity of SMOTE-based techniques can be improved, and the overgeneralization can be avoided.",Information and Software Technology,18 Mar 2025,4.0,"While the study provides insights into improving SMOTE-based techniques, the impact on European early-stage ventures may be limited as it focuses more on technical aspects rather than practical application in startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001944,Towards cost-effective API deprecation: A win–win strategy for API developers and API users,February 2022,Not Found,Chia Hung=Kao: chkao@nttu.edu.tw; Cheng-Ying=Chang: Not Found; Hewijin Christine=Jiau: Not Found,"Abstract
API
 deprecation, which enables 
API
 developers to assist API users in 
migration tasks
, has been widely employed in API removal management. However, mismanaged API deprecation will cause unnecessary cost and bring negligible benefit to API users. Cost-effective investments in API deprecation become challenges for API developers. In this work, an iterative model for cost-effective investments in API deprecation is developed. The model provides a data-driven mechanism for API developers to iteratively make investments in API deprecation. A tool named 
AWARE
 (
A
 
W
in–win 
A
ssistant for API 
RE
moval management) is also developed for API developers to accurately assess the benefit from the perspective of API 
usage statistics
. Based on the prioritized benefit, API developers can allocate appropriate resources on API deprecation. A 
case study
 is performed to evaluate the effectiveness of the iterative model with AWARE. The evaluation result shows that the cost paid by API developers can be reduced significantly while the benefit brought to API users can be increased. A win–win strategy for API deprecation can be achieved.",Information and Software Technology,18 Mar 2025,7.0,"The development of an iterative model for cost-effective investments in API deprecation can have a significant impact on European early-stage ventures by reducing costs and increasing benefits, making it practical and valuable for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001956,SHSE: A subspace hybrid sampling ensemble method for software defect number prediction,February 2022,Not Found,Haonan=Tong: Not Found; Wei=Lu: Not Found; Weiwei=Xing: Not Found; Bin=Liu: Not Found; Shihai=Wang: wangshihai@buaa.edu.cn,"Abstract
Context:
Software defect
 number prediction (SDNP) helps allocate limited testing resources by ranking software modules according to the predicted defect numbers. However, the highly skewed distribution of defects greatly degrades the performance of SDNP models by preventing SDNP models from ranking software modules accurately.
Objective:
This paper introduces a novel subspace 
hybrid sampling
 ensemble (SHSE) method based on feature subspace construction, 
hybrid sampling
, and 
ensemble learning
 for building high-performance SDNP models.
Method:
Specifically, we first construct a series of feature subspace to ensure the diversity of 
base
 learners. In each of feature subspace, we then use the proposed hybrid sampling method to balance the training subset without losing too much information and introducing lots of noisy data caused by only using undersampling or oversampling techniques. Finally, we train each 
base
 learner and combine them by using the proposed weighted ensemble strategy. Experiments are performed on 27 public defect datasets. We compare SHSE with five state-of-the-art resampling-based models and four zero-inflated/hurdle models in terms of the ranking 
performance measure
 fault-percentile-average (FPA). To demonstrate the effectiveness of SHSE, two statistical 
testing methods
 including Wilcoxon Signed-rank test and Scott–Knott 
Effect Size
 Difference test are utilized. Cliff’s 
δ
 is also computed for quantifying the difference when there is significant difference between SHSE and each baseline.
Results:
The experimental results show that SHSE significantly outperforms the baselines and improves the performance over each baseline with as least medium effect size on most datasets. On average, SHSE improves the performance over the resampling-based methods by 8.7%
∼
14.4% and the zero-inflate/hurdle models by 10.3%
∼
15.2%.
Conclusion:
It can be concluded that SHSE is a more promising alternative for software defect number prediction.",Information and Software Technology,18 Mar 2025,9.0,"The novel SHSE method for software defect number prediction presents a promising alternative with significant improvements over baselines, making it highly valuable for early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584921002032,Early prediction for merged vs abandoned code changes in modern code reviews,February 2022,Not Found,Khairul=Islam: Not Found; Toufique=Ahmed: Not Found; Rifat=Shahriyar: Not Found; Anindya=Iqbal: Not Found; Gias=Uddin: gias.uddin@ucalgary.ca,"Abstract
Context:
The modern 
code review process
 is an integral part of the current software development practice. Considerable effort is given here to inspect code changes, find defects, suggest an improvement, and address the suggestions of the reviewers. In a 
code review process
, several iterations usually take place where an author 
submits
 code changes and a reviewer gives feedback until is happy to accept the change. In around 12% cases, the changes are abandoned, eventually wasting all the efforts.
Objective:
In this research, our objective is to design a tool that can predict whether a code change would be merged or abandoned at an early stage to reduce the waste of efforts of all stakeholders (e.g., program author, reviewer, project management, etc.) involved. The real-world demand for such a tool was formally identified by a study by Fan et al. (2018).
Method:
We have mined 146,612 code changes from the code reviews of three large and popular open-source software and trained and tested a suite of supervised 
machine learning
 classifiers, both shallow and deep learning-based. We consider a total of 25 features in each code change during the training and testing of the models. The features are divided into five dimensions: reviewer, author, project, text, and code.
Results:
The best performing model named PredCR (Predicting Code Review), a LightGBM-based classifier achieves around 85% AUC score on average and relatively improves the state-of-the-art (Fan et al., 2018) by 14%–23%. In our extensive empirical study involving PredCR on the 146,612 code changes from the three software projects, we find that (1) The new features like reviewer dimensions that are introduced in PredCR are the most informative. (2) Compared to the baseline, PredCR is more effective towards reducing bias against new developers. (3) PredCR uses 
historical data
 in the code review repository and as such the performance of PredCR improves as a software system evolves with new and more data.
Conclusion:
PredCR can help save time and effort by helping developers/code reviewers to prioritize the code changes that they are asked to review. Project management can use PredCR to determine how code changes can be assigned to the code reviewers (e.g., select code changes that are more likely to be merged for review before the changes that might be abandoned).",Information and Software Technology,18 Mar 2025,8.0,"The development of PredCR for predicting code changes in code reviews can save time and effort for developers and project management, providing practical value for European early-stage ventures, especially startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001907,The use of incentives to promote technical debt management,February 2022,"Technical debt, Software development, Software incentive programs, Empirical study",Terese=Besker: Terese.Besker@ri.se; Antonio=Martini: antonima@ifi.uio.no; Jan=Bosch: Jan.Bosch@chalmers.se,"Abstract
Context
When developing software, it is vitally important to keep the level of technical debt down since, based on several studies, it has been well established that technical debt can lower the development productivity, decrease the developers' morale and 
compromise
 the overall quality of the software, among others. However, even if researchers and practitioners working in today's software development industry are quite familiar with the concept of technical debt and its related negative consequences, there has been no empirical research focusing specifically on how software managers actively communicate and manage the need to keep the level of technical debt as low as possible.
Objective
This study aims to understand how software companies give incentives to manage technical debt. This is carried out by exploring how companies encourage and reward practitioners for actively keeping the level of technical debt down add whether the companies use any 
forcing
 or 
penalising
 initiatives when managing technical debt.
Method
As a first step, this paper reports the results of both an online survey providing quantitative data from 258 participants and interviews with 32 software practitioners. As a second step, this study sets out to specifically provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based on an encouraging mindset and attitude from both managers and technical roles to understand 
how, when and by whom
 such strategies are adopted in practice.
Results
Our findings show that having a technical debt management strategy (specially based on encouragement) can significantly impact the amount of technical debt related to the software.
Conclusion
The result indicates that there is considerable unfulfilled potential to influence how software practitioners can further limit and reduce technical debt by adopting a strategy based explicitly on an encouraging mindset from managers where they also specifically dedicate time and resources for technical debt remediation activities.",Information and Software Technology,18 Mar 2025,6.0,"While the study on managing technical debt provides valuable insights, the focus on software companies may limit its direct impact on European early-stage ventures. However, the encouraging mindset strategy could be beneficial for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001907,The use of incentives to promote technical debt management,February 2022,"Technical debt, Software development, Software incentive programs, Empirical study",Terese=Besker: Terese.Besker@ri.se; Antonio=Martini: antonima@ifi.uio.no; Jan=Bosch: Jan.Bosch@chalmers.se,"Abstract
Context
When developing software, it is vitally important to keep the level of technical debt down since, based on several studies, it has been well established that technical debt can lower the development productivity, decrease the developers' morale and 
compromise
 the overall quality of the software, among others. However, even if researchers and practitioners working in today's software development industry are quite familiar with the concept of technical debt and its related negative consequences, there has been no empirical research focusing specifically on how software managers actively communicate and manage the need to keep the level of technical debt as low as possible.
Objective
This study aims to understand how software companies give incentives to manage technical debt. This is carried out by exploring how companies encourage and reward practitioners for actively keeping the level of technical debt down add whether the companies use any 
forcing
 or 
penalising
 initiatives when managing technical debt.
Method
As a first step, this paper reports the results of both an online survey providing quantitative data from 258 participants and interviews with 32 software practitioners. As a second step, this study sets out to specifically provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based on an encouraging mindset and attitude from both managers and technical roles to understand 
how, when and by whom
 such strategies are adopted in practice.
Results
Our findings show that having a technical debt management strategy (specially based on encouragement) can significantly impact the amount of technical debt related to the software.
Conclusion
The result indicates that there is considerable unfulfilled potential to influence how software practitioners can further limit and reduce technical debt by adopting a strategy based explicitly on an encouraging mindset from managers where they also specifically dedicate time and resources for technical debt remediation activities.",Information and Software Technology,18 Mar 2025,8.0,"The study addresses an important issue in software development, providing insights on how to manage technical debt effectively, which can have a significant impact on startup ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001877,Towards a taxonomy of code review smells,February 2022,Not Found,Emre=Doğan: emredogan7@outlook.com; Eray=Tüzün: Not Found,"Abstract
Context:
Code review is a crucial step of the 
software development life cycle
 in order to detect possible problems in 
source code
 before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the 
code review process
, many companies and 
open source software
 (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective 
code review process
. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are 
code review (CR) smells
.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight 
OSS projects
.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the 
OSS projects
 are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.",Information and Software Technology,18 Mar 2025,7.0,"The study focuses on identifying bad practices in the code review process, which can help startups improve their code quality and development efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584921001816,An approach to explore sequential interactions in cognitive activities of software engineering,January 2022,Not Found,Joelma=Choma: jh.choma@hotmail.com; Eduardo M.=Guerra: guerraem@gmail.com; Tiago S.=da Silva: silvadasilva@unifesp.br; Luciana M.=Zaina: lzaina@ufscar.br,"Abstract
Context
: The study of cognitive aspects around software activities can provide valuable insights to improve 
software engineering
 practices. Objective: This paper presents an approach based on distributed cognition and sequential analysis to explore cognitive activities in the software development context by analyzing the interactions between software practitioners and the resources used to support them. Method: We conducted nine laboratory-based observation sessions to record qualitative audio/video data of interactions between the study participants and at-hand resources during the planning and managing of 
software analytics
 tasks. Results: The interaction strategies of the resources model included 21 emergent actions, and the sequential analysis revealed two different patterns of interaction over time. Conclusion: Our approach has been useful for evaluating how well an artifact works to support a team in 
software analytics
 activities. Furthermore, it can be applied to explore and discover interaction patterns in different software activities.",Information and Software Technology,18 Mar 2025,5.0,"The study on cognitive aspects around software activities provides valuable insights, but may have limited immediate practical application for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001543,Challenges and solutions when adopting DevSecOps: A systematic review,January 2022,Not Found,Roshan N.=Rajapakse: roshan.rajapakse@adelaide.edu.au; Mansooreh=Zahedi: mansooreh.zahedi@adelaide.edu.au; M. Ali=Babar: ali.babar@adelaide.edu.au; Haifeng=Shen: Haifeng.Shen@acu.edu.au,"Abstract
Context:
DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge.
Objective:
This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future.
Method:
We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data.
Results:
We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied.
Conclusions:
We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.",Information and Software Technology,18 Mar 2025,9.0,"The study on adopting DevSecOps addresses a critical issue in software development, offering practical solutions and recommendations that can benefit startups in ensuring secure software delivery."
https://www.sciencedirect.com/science/article/pii/S0950584921001543,Challenges and solutions when adopting DevSecOps: A systematic review,January 2022,Not Found,Roshan N.=Rajapakse: roshan.rajapakse@adelaide.edu.au; Mansooreh=Zahedi: mansooreh.zahedi@adelaide.edu.au; M. Ali=Babar: ali.babar@adelaide.edu.au; Haifeng=Shen: Haifeng.Shen@acu.edu.au,"Abstract
Context:
DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge.
Objective:
This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future.
Method:
We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data.
Results:
We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied.
Conclusions:
We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.",Information and Software Technology,18 Mar 2025,9.0,"Similar to abstract 94, the study on adopting DevSecOps is highly relevant and provides actionable insights for startups to address security challenges in software delivery."
https://www.sciencedirect.com/science/article/pii/S0950584921001580,Relationships between software architecture and source code in practice: An exploratory survey and interview,January 2022,Not Found,Fangchao=Tian: tianfangchao@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Muhammad Ali=Babar: ali.babar@adelaide.edu.au,"Abstract
Context
Software Architecture (SA) and Source Code (SC) are two intertwined artefacts that represent the interdependent design decisions made at different levels of abstractions - High-Level (HL) and Low-Level (LL). An understanding of the relationships between SA and SC is expected to bridge the gap between SA and SC for supporting maintenance and evolution of software systems.
Objective
We aimed at exploring practitioners’ understanding about the relationships between SA and SC.
Method
We used a mixed-method that combines an online survey with 87 respondents and an interview with 8 participants to collect the views of practitioners from 37 countries about the relationships between SA and SC.
Results
Our results reveal that: practitioners mainly discuss five features of relationships between SA and SC; a few practitioners have adopted dedicated approaches and tools in the literature for identifying and analyzing the relationships between SA and SC despite recognizing the importance of such information for improving a system's quality attributes, especially 
maintainability
 and reliability. It is felt that cost and effort are the major impediments that prevent practitioners from identifying, analyzing, and using the relationships between SA and SC.
Conclusions
The results have empirically identified five features of relationships between SA and SC reported in the literature from the perspective of practitioners and a systematic framework to manage the five features of relationships should be developed with dedicated approaches and tools considering the cost and benefit of maintaining the relationships.",Information and Software Technology,18 Mar 2025,6.0,"The study explores the relationships between Software Architecture and Source Code, providing insights for practitioners to improve system quality attributes. The development of a systematic framework is proposed, which could have practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001579,Automatically inferring user behavior models in large-scale web applications,January 2022,Not Found,Saeedeh Sadat Sajjadi=Ghaemmaghami: sajjadig@ualberta.ca; Seyedeh Sepideh=Emam: emam@ualberta.ca; James=Miller: jimm@ualberta.ca,"Abstract
Context
Inferring a behavioral model from users’ navigation patterns in a web application helps application providers to understand their users’ interests. It is essential to automatically identify and generate such models as the volume of daily interactions with applications are enormous.
Objective
The goal of this paper is to incrementally generate such an automated user behavior model with no instrumentation for understanding users’ interests in large-scale mobile and desktop applications.
Method
We propose an approach to fully automate the behavioral model generation for large-scale web applications. Our proposed solution infers a reward augmented behavioral model using a reinforcement learning method by 1) dynamically generating a set of probabilistic Markov models from the users’ interactions, 2) augmenting the state of the model with reward values. Our analysis engine of the proposed solution evaluates the evolving properties of interaction patterns against the inferred behavioral models using probabilistic model checking.
Results
We evaluate the utility of our approach by using it on a large-scale mobile and desktop application. In order to show that it is assigning meaningful reward values, we compare these values with results from Google Analytics (as a state-of-the-art approach). Empirical results indicate that our approach is not only compatible with the results from Google Analytics, but also can provide information in situations, where Google Analytics data is not available.
Conclusion
In this paper, we present a novel stochastic approach to (1) generate user behavioral models for mobile and desktop web applications, (2) automatically calculate the states’ rewards, (3) annotate and analyze the models to verify their quantitative properties, and (4) address many limitations found in existing approaches.",Information and Software Technology,18 Mar 2025,8.0,"The paper presents a novel stochastic approach to generating user behavioral models for web applications, addressing limitations of existing approaches. The automated calculation of states' rewards and compatibility with Google Analytics results could be valuable for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001695,Supporting refactoring of BDD specifications—An empirical study,January 2022,"Refactoring, Normalized Compression Distance (NCD), Normalized Compression Similarity (NCS), Reuse, Similarity ratio (SR), BDD, Behavior-driven development, Specifications, Testing",Mohsin=Irshad: mohsin.irshad@bth.se; Jürgen=Börstler: Not Found; Kai=Petersen: Not Found,"Abstract
Context:
Behavior-driven development (BDD) is a variant of test-driven development where specifications are described in a structured domain-specific natural language. Although refactoring is a crucial activity of BDD, little research is available on the topic.
Objective:
To support practitioners in refactoring BDD specifications by (1) proposing semi-automated approaches to identify 
refactoring candidates
; (2) defining refactoring techniques for BDD specifications; and (3) evaluating the proposed identification approaches in an industry context.
Method:
Using Action Research, we have developed an approach for identifying refactoring candidates in BDD specifications based on two measures of similarity and applied the approach in two projects of a large software organization. The accuracy of the measures for identifying refactoring candidates was then evaluated against an approach based on 
machine learning
 and a manual approach based on practitioner perception.
Results:
We proposed two measures of similarity to support the identification of refactoring candidates in a BDD specification base; (1) normalized compression similarity (NCS) and (2) 
similarity ratio
 (SR). A semi-automated approach based on NCS and SR was developed and applied to two industrial cases to identify refactoring candidates. Our results show that our approach can identify candidates for refactoring 6o times faster than a manual approach. Our results furthermore showed that our measures accurately identified refactoring candidates compared with a manual identification by software practitioners and outperformed an ML-based text 
classification approach
. We also described four types of refactoring techniques applicable to BDD specifications; merging candidates, restructuring candidates, deleting duplicates, and renaming specification titles.
Conclusion:
Our results show that NCS and SR can help practitioners in accurately identifying BDD specifications that are suitable candidates for refactoring, which also decreases the time for identifying refactoring candidates.",Information and Software Technology,18 Mar 2025,7.0,"The research focuses on supporting practitioners in refactoring Behavior-driven development specifications, proposing semi-automated approaches. The identification of refactoring candidates 60 times faster than manual approaches could be beneficial for startups with limited resources."
https://www.sciencedirect.com/science/article/pii/S0950584921001804,How resource utilization influences UI responsiveness of Android software,January 2022,Not Found,Jiaojiao=Fu: jjfu15@fudan.edu.cn; Yaohui=Wang: 17210240047@fudan.edu.cn; Yangfan=Zhou: zyf@fudan.edu.cn; Xin=Wang: xinw@fudan.edu.cn,"Abstract
Context:
The rapid responsiveness of smartphones is critical to 
user experience
. Excessive 
resource utilization
 is typically considered as one of the major factors leading to laggy-UI. Much work focuses on modifying the design of 
Android
 systems and software to reduce their 
resource utilization
. However, laggy-UI is still quite common on 
Android
 devices, especially the low-end ones. One reason is that developers still lack a clear understanding about how the utilization of various resources (
e.g.
, CPU and memory) affects Android responsiveness, which leads to the inadequacy of existing 
performance optimization
 measures.
Objective:
The objective of this paper is to obtain a systematical understanding of how the utilization of various resources (
e.g.
, CPU and memory) affects Android responsiveness. Then accordingly, we aim to figure out the root cause(s) of laggy-UI.
Methods:
First, we conduct a set of controlled experiments on two Android devices with a stress test tool. Second, we further test 36 real-life Android software to study whether the competition of resource(s), the root factor(s) causing laggy-UI, is severe in real-life scenarios.
Results:
The experimental results show that CPU competition is the root cause and other resources have no observable impact on Android responsiveness, except in extreme cases, 
e.g.
, utilization reaches almost 100%. We also find out CPU competition is quite common for existing Android software when it is running in the background.
Conclusion:
Through stress testing and real-life Android software testing, this work unveils that CPU competition should be the main problem to be solved. Our experimental results deepen and update previous perceptions of resources’ impact on Android responsiveness. Based on these findings, we provide a set of suggestions for designing high-performance Android systems and software, and effective 
performance optimization
 tools.",Information and Software Technology,18 Mar 2025,9.0,"The study provides insights into how resource utilization affects Android responsiveness, identifying CPU competition as a main issue. The findings offer suggestions for designing high-performance Android systems and software, which could be crucial for startups developing mobile applications."
https://www.sciencedirect.com/science/article/pii/S0950584921001853,Engineering Web Augmentation software: A development method for enabling end-user maintenance,January 2022,Not Found,Diego=Firmenich: dfirmenich@tw.unp.edu.ar; Sergio=Firmenich: Not Found; Gustavo=Rossi: Not Found; Manuel=Wimmer: Not Found; Irene=Garrigós: Not Found; César=González-Mora: Not Found,"Abstract
Nowadays, end-users are able to adapt Web applications when some of their requirements have not been taken into account by developers. One possible way to do adaptations is by using Web Augmentation techniques. Web Augmentation allows end-users to modify the Web sites’ user interfaces once these are loaded on the client-side, i.e., in the browser. They achieve these adaptations by developing and/or installing Web browser plugins (“augmenters”) that modify the user interface with new functionalities. This particular kind of software artifacts requires 
special attention
 regarding maintenance as–in most cases–they depend on third-party resources, such as HTML pages. When these resources are upgraded, unexpected results during the augmentation process may occur. Many communities have arisen around Web Augmentation, and today there are large repositories where developers share their augmenters; end-users may give feedback about existing augmentations and even ask for new ones. Maintenance is a key phase in the augmenters’ life-cycle, and currently, this task falls (as usual) on the developers. In this paper, we present a participatory approach for allowing end-users without programming skills to participate in the augmenters’ maintenance phase. In order to allow this, we also provide support for the development phase to bootstrap a first version of the augmenter and to reduce the load on developers in both phases, development and maintenance. We present an analysis of more than eight thousand augmenters, which helped us devise the approach. Finally, we present an experiment with 48 participants to validate our approach.",Information and Software Technology,18 Mar 2025,5.0,"The paper presents a participatory approach for end-users to participate in Web Augmentation maintenance, reducing the burden on developers. While the approach has potential value, the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492100152X,Introduction to the Special Issue on: Grey Literature and Multivocal Literature Reviews (MLRs) in software engineering,January 2022,Not Found,Vahid=Garousi: v.garousi@qub.ac.uk; Michael=Felderer: michael.felderer@uibk.ac.at; Mika V.=Mäntylä: mika.mantyla@oulu.fi,"Abstract
In parallel to academic (peer-reviewed) literature (e.g., journal and conference papers), an enormous extent of grey literature (GL) has accumulated since the inception of software engineering (SE). GL is often defined as “literature that is not formally published in sources such as books or journal articles”, e.g., in the form of trade magazines, online blog-posts, technical reports, and online videos such as tutorial and presentation videos. GL is typically produced by SE practitioners. We have observed that researchers are increasingly using and benefitting from the knowledge available within GL. Related to the notion of GL is the notion of Multivocal Literature Reviews (MLRs) in SE, i.e., a MLR is a form of a Systematic Literature Review (SLR) which includes knowledge and/or evidence from the GL in addition to the peer-reviewed literature. MLRs are useful for both researchers and practitioners because they provide summaries of both the state-of-the-art and -practice in a given area. MLRs are popular in other fields and have started to appear in SE community. It is timely then for a Special Issue (SI) focusing on GL and MLRs in SE. From the pool of 13 submitted papers, and after following a rigorous peer review process, seven papers were accepted for this SI. In this introduction we provide a brief overview of GL and MLRs in SE, and then a brief summary of the seven papers published in this SI.",Information and Software Technology,18 Mar 2025,5.0,"While the focus on grey literature and Multivocal Literature Reviews is relevant for researchers, the practical value for European early-stage ventures or startups is limited."
https://www.sciencedirect.com/science/article/pii/S0950584921001506,A closer look at process-based simulation with stackless coroutines,January 2022,Not Found,Dorian=Weber: weber@informatik.hu-berlin.de; Paula=Wiesner: wiesnerp@informatik.hu-berlin.de; Joachim=Fischer: fischer@informatik.hu-berlin.de,"Abstract
Context
Validating discrete-event 
computer simulations
 for a particular problem domain often involves the help of a domain expert. This means that a certain structural closeness between the simulator’s inner workings and the modeled system is needed in order to allow the expert to follow the implementation in analogy. Process-based simulation imposes an object-oriented view onto a modeled system which allows for a high degree of structural closeness in most cases. In comparison, event-based simulation requires a procedural definition with a relatively low degree of structural closeness for many cases, but outperforms the process-based approach both in terms of performance and portability. Recent advances in compiler technology have introduced a portable way of rewriting thread-based code into event-based code, effectively providing the means to implement portable green-threads in compiled system languages.
Objective
This work aims to cover the historical, mechanical, and implementation specific aspects as well as practical measurements of runtime performance of a library based solution to process-based discrete-event simulation in comparison to alternative solutions.
Method
We explain how to use the stackless coroutines introduced into the 
Rust
 programming language to implement a minimal simulator core and discuss aesthetic as well as performance implications through systematic benchmarking using the three simulation scenarios 
Barbershop
, 
Car Ferry
 and 
Dining Philosophers
 by comparing their implementations to equivalent ones in the simulation language 
SLX
 and the 
C


++
 library 
ODEMx
.
Results
Our results indicate that stackless coroutines enable structurally equivalent formulations to pure process-based simulations while still delivering close to equivalent or – depending on the use-case – even superior performance and portability compared to the aforementioned solutions.
Conclusion
We show that stackless coroutines can be used to bridge the gap between process- and event-based simulators, affording modelers a level of abstraction close to the former approach while delivering the performance and portability of the latter one.",Information and Software Technology,18 Mar 2025,7.0,The work on stackless coroutines and their impact on simulation performance and portability can be beneficial for European early-stage ventures working on software simulations.
https://www.sciencedirect.com/science/article/pii/S0950584921001506,A closer look at process-based simulation with stackless coroutines,January 2022,Not Found,Dorian=Weber: weber@informatik.hu-berlin.de; Paula=Wiesner: wiesnerp@informatik.hu-berlin.de; Joachim=Fischer: fischer@informatik.hu-berlin.de,"Abstract
Context
Validating discrete-event 
computer simulations
 for a particular problem domain often involves the help of a domain expert. This means that a certain structural closeness between the simulator’s inner workings and the modeled system is needed in order to allow the expert to follow the implementation in analogy. Process-based simulation imposes an object-oriented view onto a modeled system which allows for a high degree of structural closeness in most cases. In comparison, event-based simulation requires a procedural definition with a relatively low degree of structural closeness for many cases, but outperforms the process-based approach both in terms of performance and portability. Recent advances in compiler technology have introduced a portable way of rewriting thread-based code into event-based code, effectively providing the means to implement portable green-threads in compiled system languages.
Objective
This work aims to cover the historical, mechanical, and implementation specific aspects as well as practical measurements of runtime performance of a library based solution to process-based discrete-event simulation in comparison to alternative solutions.
Method
We explain how to use the stackless coroutines introduced into the 
Rust
 programming language to implement a minimal simulator core and discuss aesthetic as well as performance implications through systematic benchmarking using the three simulation scenarios 
Barbershop
, 
Car Ferry
 and 
Dining Philosophers
 by comparing their implementations to equivalent ones in the simulation language 
SLX
 and the 
C


++
 library 
ODEMx
.
Results
Our results indicate that stackless coroutines enable structurally equivalent formulations to pure process-based simulations while still delivering close to equivalent or – depending on the use-case – even superior performance and portability compared to the aforementioned solutions.
Conclusion
We show that stackless coroutines can be used to bridge the gap between process- and event-based simulators, affording modelers a level of abstraction close to the former approach while delivering the performance and portability of the latter one.",Information and Software Technology,18 Mar 2025,7.0,"The alternative to traditional logging presented in Cronista can offer more concise historical records for software systems, potentially aiding European early-stage ventures in tracking changes."
https://www.sciencedirect.com/science/article/pii/S095058492100149X,Cronista: A multi-database automated provenance collection system for runtime-models,January 2022,Not Found,Owen=Reynolds: 180200041@aston.ac.uk; Antonio=García-Domínguez: a.garcia-dominguez@aston.ac.uk; Nelly=Bencomo: n.bencomo@aston.ac.uk,"Abstract
Context:
Decision making by software systems that face uncertainty needs tracing to support 
understandability
, as accountability is crucial. While logging has been essential to support explanations and 
understandability
 of behaviour, several issues still persist, such as the high cost for managing large logs, not knowing what to log, and the inability of logging techniques to relate events to each other or to specific occurrences of high-level activities in the system.
Objective:
Cronista
 is an alternative to logging for systems that act on top of runtime models. Instead of targeting the running systems, 
Cronista
 automatically collects the provenance of changes made to the runtime models, which aim at leveraging high-level representations, to produce more concise historical records. The provenance graphs capture causal links between those changes and the activities of the system, which are used to investigate issues.
Method:
Cronista
’s architecture is described with the current design and the implementation of its high-level components for single-machine, multi-threaded systems. 
Cronista
 is applied to a traffic-simulation 
case study
. The trade-offs of two different storage solutions are evaluated, i.e. the CDO 
model repositories
, and JanusGraph 
graph databases
.
Results:
Integrating 
Cronista
 into the 
case study
 requires only minor code changes. 
Cronista
 collected provenance graphs for the simulations as they ran, using both CDO and JanusGraph. Both solutions highlighted the cause of a seeded defect in the system. For the longer executions, both CDO and JanusGraph showed negligible overhead on the simulation times. Querying and visualisation tools were more user-friendly in JanusGraph than in CDO.
Conclusion:
Cronista
 demonstrates the feasibility of recording fine-grained provenance for the evolution of runtime models, while using it to investigate issues. User convenience and resource requirements need to be balanced. The paper present how the available technologies studied offer different trade-offs to satisfy the balance required.",Information and Software Technology,18 Mar 2025,8.0,"The combination of Agile Software Development, UCD, and Lean Startup in the development approach can provide valuable insights for European early-stage ventures navigating product development and market validation."
https://www.sciencedirect.com/science/article/pii/S0950584921001701,Improving Agile Software Development using User-Centered Design and Lean Startup,January 2022,Not Found,Maximilian=Zorzetti: maximilian.zorzetti@acad.pucrs.br; Ingrid=Signoretti: ingrid.manfrim@acad.pucrs.br; Larissa=Salerno: larissa.salerno@acad.pucrs.br; Sabrina=Marczak: sabrina.marczak@pucrs.br; Ricardo=Bastos: bastos@pucrs.br,"Abstract
Context:
Agile methods have limitations concerning problem understanding and solution finding, which can cause organizations to push misguided products and accrue waste. Some authors suggest combining agile methods with discovery-oriented approaches to overcome this, with notable candidates being User-Centered Design (UCD) and Lean Startup, a combination of which there is yet not a demonstrated, comprehensive study on how it works.
Objective:
To characterize a development approach combination of 
Agile Software Development
, UCD, and Lean Startup; exposing how the three approaches can be intertwined in a single 
development process
 and how they affect development.
Method:
We conducted a 
case study
 with two industry software development teams that use this combined approach, investigating them through interviews, observation, focus groups, and a workshop during a nine-month period in which they were stationed in a custom-built development lab.
Results:
The teams are made up of user advocates, business advocates, and solution builders; while their development approach emphasizes experimentation by making heavy use of build-measure-learn cycles. The combined approach promotes a problem-oriented mindset, encouraging team members to work together and engage with the entire 
development process
, actively discovering stakeholders needs and how to fulfill them. Each of its approaches provide a unique contribution to the development process: UCD fosters empathy with stakeholders and enables teams to better understand the problem they are tasked with solving; Lean Startup introduces experimentation as the guiding force of development; and 
Extreme Programming
 (the teams’ agile method) provides support to experimentation and achieving technical excellence.
Conclusion:
The combined approach pushes teams to think critically throughout the development effort. Our practical example provides insight on its essence and might inspire industry practitioners to seek a similar development approach based on the same precepts.",Information and Software Technology,18 Mar 2025,6.0,"The exploration of a combined approach of Agile, UCD, and Lean Startup in software development can offer insights for European early-stage ventures looking to improve problem understanding and solution finding."
https://www.sciencedirect.com/science/article/pii/S0950584921001841,A model-driven framework to support strategic agility: Value-added perspective,January 2022,"Strategic agility, IT governance, Strategic value, Model-driven development, Agile, Agility, i* framework, Scaled agile framework, SAFe, MoDrIGo, StratAMoDrIGo",Konstantinos=Tsilionis: Not Found; Yves=Wautelet: yves.wautelet@kuleuven.be,"Abstract
Context:
The Covid-19 pandemic has shown the entire world that the habits of work, freedom, and consumption can change quickly and significantly for an undetermined amount of time. A dynamic environment as such, prompts organizations to move fast in order to leverage changing circumstances as sources of opportunity rather than deadly threats. Drastic changes in work organization, consumption habits, compliance, etc., may require firms to quickly adopt new technology delivering all sorts of added value.
Objective:
The development and adoption of new technology – structurally impacting the way the organization conducts its activities – requires a considerable amount of effort in a short time frame, thus rendering it a governance decision where the alignment of the technology’s adoption and use to the long term strategy needs to be evaluated. The short time frame requiring fast response implies that agility should not remain a development or management/operational concept but should also be adopted onto the strategic layer.
Method:
Design Science Research (DSR) has been applied to build-up a framework supporting strategic agility in a model-driven fashion called 
Strategic Agile Model Driven IT Governance
 (
StratAMoDrIGo
). The relevance, rigor and design cycles of DSR have been applied and presented.
Results:
StratAMoDrIGo is based on the identification of sources of value for the organization’s strategy, its stakeholders and the users of the implemented/adopted technology. Relevant concepts are consolidated in an ontology of which the application uses the 
NFR
 Model at strategic-level and the i* Strategic Rationale Model at management-level. The proposal is applied on the case of an hospital facing the Covid-19 pandemic.
Conclusion:
The value brought by strategic opportunities’ adoption to the organization, stakeholders and users can be evaluated 
ex ante
 through conceptual models.",Information and Software Technology,18 Mar 2025,8.0,The abstract addresses the impact of the Covid-19 pandemic on organizations and the need for strategic agility. The development of a framework to support this agility is relevant and practical for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921001841,A model-driven framework to support strategic agility: Value-added perspective,January 2022,"Strategic agility, IT governance, Strategic value, Model-driven development, Agile, Agility, i* framework, Scaled agile framework, SAFe, MoDrIGo, StratAMoDrIGo",Konstantinos=Tsilionis: Not Found; Yves=Wautelet: yves.wautelet@kuleuven.be,"Abstract
Context:
The Covid-19 pandemic has shown the entire world that the habits of work, freedom, and consumption can change quickly and significantly for an undetermined amount of time. A dynamic environment as such, prompts organizations to move fast in order to leverage changing circumstances as sources of opportunity rather than deadly threats. Drastic changes in work organization, consumption habits, compliance, etc., may require firms to quickly adopt new technology delivering all sorts of added value.
Objective:
The development and adoption of new technology – structurally impacting the way the organization conducts its activities – requires a considerable amount of effort in a short time frame, thus rendering it a governance decision where the alignment of the technology’s adoption and use to the long term strategy needs to be evaluated. The short time frame requiring fast response implies that agility should not remain a development or management/operational concept but should also be adopted onto the strategic layer.
Method:
Design Science Research (DSR) has been applied to build-up a framework supporting strategic agility in a model-driven fashion called 
Strategic Agile Model Driven IT Governance
 (
StratAMoDrIGo
). The relevance, rigor and design cycles of DSR have been applied and presented.
Results:
StratAMoDrIGo is based on the identification of sources of value for the organization’s strategy, its stakeholders and the users of the implemented/adopted technology. Relevant concepts are consolidated in an ontology of which the application uses the 
NFR
 Model at strategic-level and the i* Strategic Rationale Model at management-level. The proposal is applied on the case of an hospital facing the Covid-19 pandemic.
Conclusion:
The value brought by strategic opportunities’ adoption to the organization, stakeholders and users can be evaluated 
ex ante
 through conceptual models.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 106, this abstract emphasizes the importance of strategic agility in the face of changing circumstances. The development of a framework for strategic agility is valuable for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001828,Human values in software development artefacts: A case study on issue discussions in three Android applications,January 2022,Not Found,Arif=Nurwidyantoro: Arif.Nurwidyantoro@monash.edu; Mojtaba=Shahin: Mojtaba.Shahin@monash.edu; Michel R.V.=Chaudron: m.r.v.chaudron@tue.nl; Waqar=Hussain: Waqar.Hussain@monash.edu; Rifat=Shams: Rifat.Shams@monash.edu; Harsha=Perera: Harsha.Perera@monash.edu; Gillian=Oliver: Gillian.Oliver@monash.edu; Jon=Whittle: Jon.Whittle@data61.csiro.au,"Abstract
Context:
Human values such as inclusion, privacy, and accessibility need to be considered during software development to attract and maintain users. However, little effort has been made to study human values consideration in software development, particularly in software development artefacts.
Objective:
Issue discussion is potentially a rich source for human values analysis because it is a common place for users and developers to share and communicate their concerns. This paper aims to investigate the extent to which human values are discussed and whether the presence of values differs across projects.
Method:
We carried out a 
case study
 to discover human values in 1,097 issues collected from three 
Android
 projects: Signal, K-9, and Focus.
Results:
We identified 20 value themes and proposed a contextualised 
software engineering
 description for each of them. The analysis shows that privacy, freedom, usability, and efficiency were the prevalent value themes in the issue discussions of these three projects. Meanwhile, Self-direction - Action and Security - Personal are the common prevalent human values found in the projects. Moreover, we found that a statement of values from the apps and their functionalities could contribute to the presence of values.
Conclusion:
The results suggest that human values are present in software development artefacts, for which automated tools can be developed to extract and classify human values from them.",Information and Software Technology,18 Mar 2025,7.0,"The abstract focuses on human values consideration in software development, which is essential for startups to attract and maintain users. The proposed analysis and results are practical for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001348,On preserving the behavior in software refactoring: A systematic mapping study,December 2021,Not Found,Eman Abdullah=AlOmar: eman.alomar@mail.rit.edu; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Christian=Newman: cdnvse@rit.edu,"Abstract
Context:
Refactoring is the art of modifying the design of a system without altering its behavior. The idea is to reorganize variables, classes and methods to facilitate their future adaptations and comprehension. As the concept of behavior preservation is fundamental for refactoring, several studies, using 
formal verification
, language transformation and dynamic analysis, have been proposed to monitor the execution of 
refactoring operations
 and their impact on the program semantics. However, there is no existing study that examines the available behavior preservation strategies for each refactoring operation.
Objective:
This paper identifies behavior preservation approaches in the research literature.
Method:
We conduct, in this paper, a 
systematic mapping study
, to capture all existing behavior preservation approaches that we classify based on several criteria including their methodology, applicability, and their degree of automation.
Results:
The results indicate that several behavior preservation approaches have been proposed in the literature. The approaches vary between using formalisms and techniques, developing automatic refactoring safety tools, and performing a manual analysis of the source code.
Conclusion:
Our taxonomy reveals that there exist some types of 
refactoring operations
 whose behavior preservation is under-researched. Our classification also indicates that several possible strategies can be combined to better detect any violation of the program semantics.",Information and Software Technology,18 Mar 2025,6.0,"The abstract discusses behavior preservation in refactoring operations, which is relevant for startups in the software development space. The systematic mapping study provides insights that can be beneficial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001348,On preserving the behavior in software refactoring: A systematic mapping study,December 2021,Not Found,Eman Abdullah=AlOmar: eman.alomar@mail.rit.edu; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Christian=Newman: cdnvse@rit.edu,"Abstract
Context:
Refactoring is the art of modifying the design of a system without altering its behavior. The idea is to reorganize variables, classes and methods to facilitate their future adaptations and comprehension. As the concept of behavior preservation is fundamental for refactoring, several studies, using 
formal verification
, language transformation and dynamic analysis, have been proposed to monitor the execution of 
refactoring operations
 and their impact on the program semantics. However, there is no existing study that examines the available behavior preservation strategies for each refactoring operation.
Objective:
This paper identifies behavior preservation approaches in the research literature.
Method:
We conduct, in this paper, a 
systematic mapping study
, to capture all existing behavior preservation approaches that we classify based on several criteria including their methodology, applicability, and their degree of automation.
Results:
The results indicate that several behavior preservation approaches have been proposed in the literature. The approaches vary between using formalisms and techniques, developing automatic refactoring safety tools, and performing a manual analysis of the source code.
Conclusion:
Our taxonomy reveals that there exist some types of 
refactoring operations
 whose behavior preservation is under-researched. Our classification also indicates that several possible strategies can be combined to better detect any violation of the program semantics.",Information and Software Technology,18 Mar 2025,6.0,"Similar to abstract 109, this abstract addresses behavior preservation in refactoring operations. The classification of preservation approaches can be useful for startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584921001312,Blurring boundaries: Toward the collective empathic understanding of product requirements,December 2021,"Requirements understanding and validation, Empathy-driven development, Product team organisation, Collective sensemaking, Constructivist Grounded Theory",Robert C.=Fuller: rfuller@ece.ubc.ca; Philippe=Kruchten: pbk@ece.ubc.ca,"Abstract
Context
Many software product companies create cross-functional development teams that own a product or a defined set of features. These product teams often require a deep and collective understanding of the product domain, a rich context within which to understand the product requirements and to make decisions throughout the 
development process
.
Objective
Little is known about what supports or impedes these teams in collectively achieving this 
deep understanding
. This paper identifies certain organisational conditions that impact teams in this respect.
Method
Using Constructivist 
Grounded Theory method
, we studied 18 teams across seven software companies creating products for a diverse range of markets.
Results
The study found certain organisational and planning process factors play a significant role in whether product development teams have the potential to collectively develop deep domain understanding. These factors also impact individual and development team dynamics.
Conclusions
We identify two essential metaphorical dynamics, broadening the lens and blurring boundaries, that cross-functional product teams employ in order to fully embrace product ownership, visioning, and planning towards achieving this rich context for understanding product requirements. We also conclude that the highly specialised nature of many organisational models and development processes is contraindicated for cross-functional product development teams in achieving this deep collective understanding and we call for a revisiting of conventional organisational and product planning practices for software product development.",Information and Software Technology,18 Mar 2025,8.0,"The study provides valuable insights into the organizational conditions impacting product development teams, which can benefit early-stage ventures in improving team dynamics and planning processes."
https://www.sciencedirect.com/science/article/pii/S0950584921001312,Blurring boundaries: Toward the collective empathic understanding of product requirements,December 2021,"Requirements understanding and validation, Empathy-driven development, Product team organisation, Collective sensemaking, Constructivist Grounded Theory",Robert C.=Fuller: rfuller@ece.ubc.ca; Philippe=Kruchten: pbk@ece.ubc.ca,"Abstract
Context
Many software product companies create cross-functional development teams that own a product or a defined set of features. These product teams often require a deep and collective understanding of the product domain, a rich context within which to understand the product requirements and to make decisions throughout the 
development process
.
Objective
Little is known about what supports or impedes these teams in collectively achieving this 
deep understanding
. This paper identifies certain organisational conditions that impact teams in this respect.
Method
Using Constructivist 
Grounded Theory method
, we studied 18 teams across seven software companies creating products for a diverse range of markets.
Results
The study found certain organisational and planning process factors play a significant role in whether product development teams have the potential to collectively develop deep domain understanding. These factors also impact individual and development team dynamics.
Conclusions
We identify two essential metaphorical dynamics, broadening the lens and blurring boundaries, that cross-functional product teams employ in order to fully embrace product ownership, visioning, and planning towards achieving this rich context for understanding product requirements. We also conclude that the highly specialised nature of many organisational models and development processes is contraindicated for cross-functional product development teams in achieving this deep collective understanding and we call for a revisiting of conventional organisational and product planning practices for software product development.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 111, this study offers essential findings on organizational conditions affecting product development teams, highlighting the need to revisit conventional organizational practices, which can be beneficial for startups."
https://www.sciencedirect.com/science/article/pii/S095058492100135X,Topic modeling for feature location in software models: Studying both code generation and interpreted models,December 2021,Not Found,Francisca=Pérez: mfperez@usj.es; Raúl=Lapeña: rlapena@usj.es; Ana C.=Marcén: acmarcen@usj.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
In the last 20 years, the research community has increased its attention to the use of 
topic modeling
 for software maintenance and evolution tasks in code. Topic modeling is a popular and promising information retrieval technique that represents topics by word probabilities. 
Latent Dirichlet Allocation
 (LDA) is one of the most popular 
topic modeling
 methods. However, the use of topic modeling in model-driven software development has been largely neglected. Since software models have less noise (implementation details) than software code, software models might be well-suited for topic modeling.
Objective:
This paper presents our LDA-guided evolutionary approach for feature location in software models. Specifically, we consider two types of software models: models for code generation and interpreted model.
Method:
We evaluate our approach considering two real-world industrial 
case studies
: code-generation models for train control software, and interpreted models for a commercial 
video game
. To study the impact on the results, we compare our approach for feature location in models against random search and a baseline based on Latent Semantic Indexing, which is a popular information retrieval technique. In addition, we perform a statistical analysis of the results to show that this impact is significant. We also discuss the results in terms of the following aspects: data 
sparsity
, implementation complexity, calibration, and stability.
Results:
Our approach significantly outperforms the baseline in terms of recall, precision and F-measure when it comes to interpreted models. This is not the case for code-generation models.
Conclusions:
Our analysis of the results uncovers a recommendation towards results improvement. We also show that calibration approaches can be transferred from code to models. The findings of our work with regards to the compensation of instability have the potential to help not only feature location in models, but also in code.",Information and Software Technology,18 Mar 2025,7.0,"The research presents a novel approach for feature location in software models, showcasing significant improvements in results for interpreted models. This can potentially aid startups in enhancing their software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584921001336,Evaluating the influence of scope on feature location,December 2021,Not Found,África=Domingo: adomingo@usj.es; Jorge=Echeverría: jecheverria@usj.es; Óscar=Pastor: opastor@dsic.upv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Feature Location (FL) is a widespread technique that is used to maintain and evolve a software product. FL is also helpful in reengineering a family of software products into a Software Product Line (SPL). Despite the popularity of FL, there is no study that evaluates the influence of scope (single product or product family) when engineers perform FL.
Objective:
The goal of this paper is to compare the performance, productivity, and perceived difficulty of manual FL when scope changes from a single product to a 
product family
.
Method:
We conducted a crossover experiment to compare the performance, productivity, and perceived difficulty of manual FL when scope changes. The 
experimental objects
 are extracted from a real-world SPL that uses a Domain-Specific Language to generate the firmware of its products.
Results:
Performance and productivity decrease significantly when engineers locate features in a 
product family
 regardless of their experience. For these variables the impact of the FL Scope is medium–large. On contrast, for perceived difficulty, the magnitude of the difference is moderate and is not significant.
Conclusions:
While performance and productivity decrease significantly when engineers locate features in a product family, the difficulty they perceive does not predict the significant worsening of the results. Our work also identifies strengths and weaknesses in FL. This can help in developing better FL approaches and test cases for evaluation.",Information and Software Technology,18 Mar 2025,6.0,"The study evaluates the impact of scope changes on feature location in software products, providing insights into performance and productivity changes. While not directly focused on startups, the findings can inform early-stage ventures on optimizing their product maintenance and evolution strategies."
https://www.sciencedirect.com/science/article/pii/S0950584921001361,Guiding the selection of research methodology in industry–academia collaboration in software engineering,December 2021,"Research methodology, Selecting research methodology, Design Science, Action Research, Technology Transfer Model, Industry–academia collaboration",Claes=Wohlin: claes.wohlin@bth.se; Per=Runeson: per.runeson@cs.lth.se,"Abstract
Background:
The literature concerning research methodologies and methods has increased in 
software engineering
 in the last decade. However, there is limited guidance on selecting an appropriate research methodology for a given research study or project.
Objective:
Based on a selection of research methodologies suitable for software engineering research in collaboration between industry and academia, we present, discuss and compare the methodologies aiming to provide guidance on which research methodology to choose in a given situation to ensure successful industry–academia collaboration in research.
Method:
Three research methodologies were chosen for two main reasons. Design Science and Action Research were selected for their usage in software engineering. We also chose a model emanating from software engineering, i.e., the Technology Transfer Model. An overview of each methodology is provided. It is followed by a discussion and an illustration concerning their use in industry–academia collaborative research. The three methodologies are then compared using a set of criteria as a basis for our guidance.
Results:
The discussion and comparison of the three research methodologies revealed general similarities and distinct differences. All three research methodologies are easily mapped to the general research process describe–solve–practice, while the main driver behind the formulation of the research methodologies is different. Thus, we guide in selecting a research methodology given the primary research objective for a given research study or project in collaboration between industry and academia.
Conclusions:
We observe that the three research methodologies have different main objectives and differ in some characteristics, although still having a lot in common. We conclude that it is vital to make an informed decision concerning which research methodology to use. The presentation and comparison aim to guide selecting an appropriate research methodology when conducting research in collaboration between industry and academia.",Information and Software Technology,18 Mar 2025,7.0,The paper offers guidance on selecting research methodologies for industry-academia collaboration in software engineering research. This can be valuable for startups seeking to conduct research in partnership with academia to drive innovation.
https://www.sciencedirect.com/science/article/pii/S0950584921001440,Prioritizing code documentation effort: Can we do it simpler but better?,December 2021,Not Found,Shiran=Liu: Not Found; Zhaoqiang=Guo: Not Found; Yanhui=Li: yanhuili@nju.edu.cn; Hongmin=Lu: Not Found; Lin=Chen: Not Found; Lei=Xu: Not Found; Yuming=Zhou: zhouyuming@nju.edu.cn; Baowen=Xu: Not Found,"Abstract
Context
. Due to time or economic pressures, code developers are often unable to write documents for all modules in a project. Recently, a supervised artificial neural network (ANN) approach is proposed to prioritize documentation effort “to ensure that sections of code important to 
program comprehension
 are thoroughly explained”.
Objective
. However, as a supervised approach, there is a need to use labeled 
training data
 to train the prediction model, which may not easy to obtain in practice. Furthermore, it is unclear whether the ANN approach is generalizable, as it is only evaluated on several small data sets collected from API libraries.
Method
. In this paper, we propose an unsupervised approach based on improved PageRank to prioritize documentation effort. This approach identifies “important” modules only based on the dependence relationships between modules in a project. As a result, the PageRank approach does not need any 
training data
 to build the prediction model.
Results
. In order to evaluate the effectiveness of the PageRank approach, we use six additional large data sets collected from two larger libraries and four applications to conduct the experiment. The experimental results show that the PageRank approach is superior to the state-of-the-art ANN approach.
Conclusion
. Due to the simplicity and effectiveness, we advocate that the PageRank approach should be used as an easy-to-implement baseline in future research on documentation effort prioritization, and any newly proposed approach should be compared with it to demonstrate its effectiveness.",Information and Software Technology,18 Mar 2025,7.0,The proposed unsupervised approach based on improved PageRank for prioritizing documentation effort can be beneficial for early-stage ventures by providing a more efficient and effective method of code documentation without the need for labeled training data.
https://www.sciencedirect.com/science/article/pii/S0950584921001452,UX work in software startups: A thematic analysis of the literature,December 2021,Not Found,Jullia=Saad: julliasaad01@gmail.com; Suéllen=Martinelli: suellen.martinelli@estudante.ufscar.br; Leticia S.=Machado: leticia.smachado@gmail.com; Cleidson R.B.=de Souza: cleidson.desouza@acm.org; Alexandre=Alvaro: alvaro@ufscar.br; Luciana=Zaina: lzaina@ufscar.br,"Abstract
Context:
Startups are new and fast-growing innovative businesses. These companies also deal with uncertain market conditions and work under constant time and business pressures. Although 
User Experience
 (UX) has been widely adopted in the software industry, this has not been a reality in the context of software startups yet. Several factors might influence whether, which, and how UX is adopted by software startups.
Objective:
The objective of this paper is to investigate in the literature how software startups work with UX and to discover the relationship between software development practices and UX in startups.
Methodology:
Our methodology is composed of three main activities: (1) mapping the literature seeking publications on UX work, 
software engineering
, and startups, which resulted in 21 relevant publications; (2) a thematic analysis based on the output of step 1 (i.e., the relevant literature); and (3) refining the themes found out in step 2 and the design of their relationships to explain the link between UX work and software startups.
Results:
The challenges, opportunities, and practices associated with UX in the context of software startups reported by the literature were organized in a set of themes. As a result, seven themes were defined so as to identify needs and opportunities related to UX work in startups. In addition, we synthesize open questions from the literature and suggest new ones to further research directions about the adoption of UX by software startups.
Conclusion:
Our findings demonstrate that software startups require an approach to UX that is more adherent to the startups’ dynamic and disruptive nature. We also suggest emerging 
open research
 questions which should be answered to promote the evolution of UX as applied to software startups.",Information and Software Technology,18 Mar 2025,5.0,"The investigation into UX in software startups can provide valuable insights, but the findings may not have a direct immediate impact on early-stage ventures as they focus more on understanding the challenges and opportunities related to UX."
https://www.sciencedirect.com/science/article/pii/S0950584921001476,Technical debt payment and prevention through the lenses of software architects,December 2021,Not Found,Boris=Pérez: br.perez41@uniandes.edu.co; Camilo=Castellanos: Not Found; Darío=Correal: Not Found; Nicolli=Rios: Not Found; Sávio=Freire: Not Found; Rodrigo=Spínola: Not Found; Carolyn=Seaman: Not Found; Clemente=Izurieta: Not Found,"Abstract
Context:
Architectural decisions
 are considered one of the most common sources of technical debt (TD). Thus, it is necessary to understand how TD is perceived by software architects, particularly, the practices supporting the elimination of debt items from projects, and the practices used to reduce the chances of TD occurrence.
Objective:
This paper investigates the most commonly used practices to pay off TD and to prevent debt occurrence in software projects from the architect’s point of view.
Method:
We used the available data from InsighTD, which is a globally distributed family of industrial surveys on the causes, effects, and management of TD. We analyze responses from a corpus of 72 software architects from Brazil, Chile, Colombia, and the United States.
Results:
Results showed that refactoring (30.2%) was the main practice related to TD payment, followed by design improvements (14.0%). Refactoring, design improvements, and test improvements are the most cited payment practices among cases of code, design and test debt. Concerning the TD preventive practices, we find that having a well-defined architecture and design is the most cited practice (13.6%), followed by having a well-defined scope and requirements. This last practice is the most cited one for expert software architects. Finally, when comparing preventive practices among the three major roles derived from the survey (software architects, engineer roles, and management roles), we found that none of the roles shared the most cited practice, meaning that each role had its worries and focus on different strategies to reduce TD’s presence in the software.
Conclusion:
The lists of TD payment and prevention practices can guide software teams by having a catalog of practices to keep debt controlled or reduced.",Information and Software Technology,18 Mar 2025,8.0,"The study on architectural decisions and technical debt payment practices can be highly valuable for startups as it provides guidance on how to manage technical debt effectively, which is crucial for the success and growth of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001518,An automatic methodology for the quality enhancement of requirements using genetic algorithms,December 2021,Not Found,Daniel=Adanza Dopazo: 100371746@alumnos.uc3m.es; Valentín=Moreno Pelayo: vmpelayo@kr.inf.uc3m.es; Gonzalo=Génova Fuster: ggenova@uc3m.es,"Abstract
Context
The set of requirements for any project offers common ground where the client and the company agree on the most important features and limitations of the project. Having a set of requirements of the highest possible quality is of enormous importance; benefits include improving project quality, understanding client needs better, reducing costs, and predicting project schedules and results with greater accuracy.
Objective
This paper's primary goal is to create a methodology that can provide effective and efficient solutions for modifying poor requirements integrated into a full-fledged system, extracting the main features of each requirement, assessing their quality at an expert level, and, finally, enhancing the quality of the requirements.
Method
In the first step, a 
machine learning algorithm
 is implemented to classify requirements based on quality and identify those that are the likeliest to be problematic. In the second step, the 
genetic algorithm
 generated solutions to enhance the quality of the requirements identified as inferior.
Results
The results of the 
genetic algorithm
 are compared with the theoretically optimal solution. The paper demonstrates the significant flexibility of genetic algorithms, which create a wide variety of solutions and can adapt to any type of classifier. From the initial dataset of requirements, the genetic algorithm finds the optimal solution in 85% of cases after 10 iterations and achieves 59.8% success after only one iteration.
Conclusions
Genetic algorithms are promising tools for 
requirements engineering
 by delivering benefits such as saving costs, automating tasks, and providing more solid and efficient planning in any project through the generation of new solutions.",Information and Software Technology,18 Mar 2025,6.0,"The methodology proposed for enhancing the quality of project requirements can be beneficial for startups in improving project quality and understanding client needs better, but the direct impact on early-stage ventures may not be immediate or significant."
https://www.sciencedirect.com/science/article/pii/S0950584921001531,A longitudinal study of the impact of refactoring in android applications,December 2021,Not Found,Oumayma=Hamdi: Not Found; Mel Ó=Cinnéide: Not Found; Mohamed Wiem=Mkaouer: Not Found,"Abstract
Context:
Mobile applications have to continuously evolve in order to meet new user requirements and technological changes. Addressing these constraints may lead to poor implementation and design choices, known 
code smells
. 
Code refactoring
 is a key practice that is employed to ensure that the intent of a code change is properly achieved without compromising internal software quality. While previous studies have investigated the impact of refactoring on traditional code smells in 
desktop applications
, little attention has been paid to the impact of refactoring activities in mobile application development.
Objective:
We aim to develop a broader understanding of the impact of refactoring activities on 
Android
 and traditional code smells in 
Android
 apps.
Method:
We conduct a longitudinal empirical study by analyzing the evolution history of five open-source Android apps comprising 652 releases and exhibiting a total of 9,600 
refactoring operations
. We consider 15 common Android smell types and 10 common traditional Object-Oriented (OO) code smell types to provide a broad overview of the relationship between refactoring and code smells.
Results:
We find that code smells are widespread across 
Android applications
, but smelly classes are not particularly targeted by refactoring activities and, when they are, it is rare for refactoring to actually remove a smell.
Conclusions:
These somewhat surprising results indicate that it is critical to understand better the real quality issues that Android developers face, and to develop a model of code smells and refactoring that can better address their needs in practice.",Information and Software Technology,18 Mar 2025,4.0,"While the study on refactoring activities in Android apps is interesting, the findings may not directly impact early-stage ventures as they focus on understanding code smells in mobile applications, which may not be a primary concern for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001567,HYDRA: Feedback-driven black-box exploitation of injection vulnerabilities,December 2021,Not Found,Manuel=Leithner: mleithner@sba-research.org; Bernhard=Garn: bgarn@sba-research.org; Dimitris E.=Simos: dsimos@sba-research.org,"Abstract
Context:
Injection vulnerabilities
 remain an omnipresent threat to web application security. These issues arise when user-supplied input is included in commands constructed by the application without applying adequate validation and filtering, permitting attackers to modify the resulting instructions.
Objective:
Tools used in real-world security assessments commonly employ a static list of 
malicious input
 strings to be submitted to the system under test (SUT) to gauge the presence of vulnerabilities. However, sanitizing filters may cause these simulated attacks to fail, even if they only mitigate a subset of potentially harmful values. This may result in a false sense of security. This work introduces HYDRA, a feedback-driven black-box security testing approach for the exploitation of injection vulnerabilities. It is capable of constructing inputs designed to evade such imperfect filters while allowing users to define and rank 
output contexts
, abstract locations in the output of the SUT that are associated with desirable semantics (for instance, allowing the execution of JavaScript code).
Method:
Starting with an innocuous initial input string that is submitted to the SUT and appears anywhere in the output, HYDRA identifies the initial output context. It extends the input string with the goal of reaching contexts that are deemed ”better” according to domain knowledge. This process continues until an ”ideal” output context is reached, usually corresponding to an exploit that impacts the security of the SUT. In addition to this dynamic approach, we present a static variant based on combinatorial security testing. We instantiate our approach by targeting cross-site scripting (XSS) vulnerabilities, detailing the unique challenges posed by HTML 
parsing
, and implement this application of HYDRA in a prototype tool.
Results:
The evaluation shows that our implementation is able to evade faulty filters and is effective at identifying injection vulnerabilities while remaining more flexible than existing approaches by allowing users to define desirable output contexts.
Conclusion:
Based on the results of our evaluation, we are confident that including the HYDRA approach in security assessments will increase the number of identified XSS vulnerabilities, particularly those that are difficult to exploit. We anticipate that an application to other classes of vulnerabilities such as 
SQL
 injections will significantly advance the state of the art.",Information and Software Technology,18 Mar 2025,8.0,"The HYDRA approach introduces a feedback-driven security testing method that allows for more effective identification of vulnerabilities, particularly XSS vulnerabilities, and offers a more flexible approach compared to existing methods, which can have a significant impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492100166X,MS-QuAAF: A generic evaluation framework for monitoring software architecture quality,December 2021,Not Found,Salim=Kadri: salim8359@yahoo.fr; Sofiane=Aouag: Sofiane.Aouag@gmail.com; Djalal=Hedjazi: Djalal.Hedjazi@gmail.com,"Abstract
Context
In a highly competitive software market, architecture quality is one of the key 
differentiators
 between software systems. Many quantitative and qualitative evaluation frameworks were proposed to measure architecture. However, qualitative evaluation lacks statistical significance, whereas quantitative methods are designed for evaluating 
specific quality attributes
, such as modifiability and performance. Besides, the assessment covers usually a single development stage, either at the design stage or at the implementation stage.
Objective
A lack of generic frameworks that can support the assessment of a broad set of attributes and ensure continuous evaluation by covering the main development stages is addressed. Accordingly, this paper presents MS-QuAAF, a 
quantitative assessment
 framework destined for evaluating software architecture through a set of generic metrics.
Method
The 
quantitative evaluation
 consists of checking architecture facets mapped to quality attributes against the early specified meta-models. This process starts by analyzing rules infringements and calculating architecture defects after accomplishing the design stage. Second, the assigned responsibilities supposed to promote stakeholders’ quality attributes are assessed quantitatively at the end of the implementation stage. Third, the final evaluation report is generated.
Results
We made specifically three main contributions. First, the proposed metrics within the framework are generic, which means that the framework has the ability to assess any inputted quality. Second, the framework proposes a set of evaluation services capable of assessing the architecture at two main development stages, which are design and implementation. Third, we proposed a 
quantitative assessment
 tree within the framework called the Responsibilities Satisfaction Tree (RST) that uses 
NFR
 responsibilities nodes to evaluate the implemented architectures.
Conclusion
The conducted experiment showed that the framework is capable of evaluating quality attributes based on architecture specification using the proposed metrics. Furthermore, these metrics contributed to enhancing architecture quality during the development stages by notifying architects of the discovered anomalies.",Information and Software Technology,18 Mar 2025,6.0,"The MS-QuAAF framework addresses the lack of generic frameworks for evaluating software architecture, which can potentially improve architecture quality during development stages, providing a moderate impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000938,Assessing test artifact quality—A tertiary study,November 2021,"Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance",Huynh Khanh Vi=Tran: huynh.khanh.vi.tran@bth.se; Michael=Unterkalmsteiner: michael.unterkalmsteiner@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Nauman bin=Ali: nauman.ali@bth.se,"Abstract
Context:
Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases.
Objective:
We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives.
Methods:
We have carried out a systematic literature review to identify and analyze existing secondary studies on 
quality aspects
 of software testing artifacts.
Results:
We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the 
quality attributes
 and measurements based on findings in the literature and ISO/IEC 25010:2011.
Conclusion:
The test artifact 
quality model
 presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furthermore, the model can also be used as a framework for documenting context characteristics to make 
research results
 more accessible for research and practice.",Information and Software Technology,18 Mar 2025,5.0,"The comprehensive model for test case/suite quality can support quality assessment and improvement initiatives in practice, but lacks a direct practical impact on early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000938,Assessing test artifact quality—A tertiary study,November 2021,"Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance",Huynh Khanh Vi=Tran: huynh.khanh.vi.tran@bth.se; Michael=Unterkalmsteiner: michael.unterkalmsteiner@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Nauman bin=Ali: nauman.ali@bth.se,"Abstract
Context:
Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases.
Objective:
We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives.
Methods:
We have carried out a systematic literature review to identify and analyze existing secondary studies on 
quality aspects
 of software testing artifacts.
Results:
We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the 
quality attributes
 and measurements based on findings in the literature and ISO/IEC 25010:2011.
Conclusion:
The test artifact 
quality model
 presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furthermore, the model can also be used as a framework for documenting context characteristics to make 
research results
 more accessible for research and practice.",Information and Software Technology,18 Mar 2025,5.0,"Similar to abstract 123, the test artifact quality model presented has relevance for improving software testing artifacts, but lacks a direct practical impact on early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001154,A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models,November 2021,Not Found,Kunsong=Zhao: Not Found; Zhou=Xu: zhouxullx@cqu.edu.cn; Meng=Yan: Not Found; Tao=Zhang: Not Found; Dan=Yang: Not Found; Wei=Li: Not Found,"Abstract
Context:
Software crash is a serious form of the software failure, which often occurs during the software development and maintenance process. As the stack trace reported when the software crashes contains a wealth of information about crashes, recent work utilized 
classification models
 with the collected features from stack traces and 
source code
 to predict whether the fault causing the crash resides in the stack trace. This could speed-up the crash localization task.
Objective:
As the quality of features can affect the performance of the constructed 
classification models
, researchers proposed to use feature selection methods to select a representative feature subset to build models by replacing the original features. However, only limited feature selection methods and classification models were taken into consideration for this issue in previous work. In this work, we look into this topic deeply and find out the best feature selection method for crash fault residence prediction task.
Method:
We study the performance of 24 feature selection techniques with 21 classification models on a benchmark dataset containing crash instances from 7 real-world software projects. We use 4 indicators to evaluate the performance of these feature selection methods which are applied to the classification models.
Results:
The experimental results show that, overall, a probability-based feature selection, called Symmetrical Uncertainty, performs well across the studied classification models and projects. Thus, we recommend such a feature selection method to preprocess the crash instances before constructing classification models to predict the crash fault residence.
Conclusion:
This work conducts a large-scale empirical study to investigate the impact of feature selection methods on the performance of classification models for the crashing fault residence prediction task. The results clearly demonstrate that there exist significant performance differences among these feature selection techniques across different classification models and projects.",Information and Software Technology,18 Mar 2025,7.0,"The study on feature selection methods for crash fault residence prediction task shows significant performance differences, recommending a specific method that can enhance the prediction accuracy, which can have a positive impact on early-stage ventures relying on software stability."
https://www.sciencedirect.com/science/article/pii/S0950584921001154,A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models,November 2021,Not Found,Kunsong=Zhao: Not Found; Zhou=Xu: zhouxullx@cqu.edu.cn; Meng=Yan: Not Found; Tao=Zhang: Not Found; Dan=Yang: Not Found; Wei=Li: Not Found,"Abstract
Context:
Software crash is a serious form of the software failure, which often occurs during the software development and maintenance process. As the stack trace reported when the software crashes contains a wealth of information about crashes, recent work utilized 
classification models
 with the collected features from stack traces and 
source code
 to predict whether the fault causing the crash resides in the stack trace. This could speed-up the crash localization task.
Objective:
As the quality of features can affect the performance of the constructed 
classification models
, researchers proposed to use feature selection methods to select a representative feature subset to build models by replacing the original features. However, only limited feature selection methods and classification models were taken into consideration for this issue in previous work. In this work, we look into this topic deeply and find out the best feature selection method for crash fault residence prediction task.
Method:
We study the performance of 24 feature selection techniques with 21 classification models on a benchmark dataset containing crash instances from 7 real-world software projects. We use 4 indicators to evaluate the performance of these feature selection methods which are applied to the classification models.
Results:
The experimental results show that, overall, a probability-based feature selection, called Symmetrical Uncertainty, performs well across the studied classification models and projects. Thus, we recommend such a feature selection method to preprocess the crash instances before constructing classification models to predict the crash fault residence.
Conclusion:
This work conducts a large-scale empirical study to investigate the impact of feature selection methods on the performance of classification models for the crashing fault residence prediction task. The results clearly demonstrate that there exist significant performance differences among these feature selection techniques across different classification models and projects.",Information and Software Technology,18 Mar 2025,8.0,"This abstract provides valuable insights into improving crash fault residence prediction, which is crucial for software development and maintenance in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000951,"Does shortening the release cycle affect refactoring activities: A case study of the JDT Core, Platform SWT, and UI projects",November 2021,Not Found,Olivier=Nourry: oliviern@posl.ait.kyushu-u.ac.jp; Yutaro=Kashiwa: kashiwa@ait.kyushu-u.ac.jp; Yasutaka=Kamei: kamei@ait.kyushu-u.ac.jp; Naoyasu=Ubayashi: ubayashi@ait.kyushu-u.ac.jp,"Abstract
Context:
Several large-scale companies such as Google and Netflix chose to adopt short release cycles (e.g., rapid releases) in recent years. Although this allows these companies to provide updates and features faster for their users, it also causes developers to have less time to dedicate to development activities other than feature development.
Objective:
In this paper, we investigate how refactoring activities were impacted by the adoption of shorter releases.
Method:
We extract all refactorings applied over a period of two years during traditional yearly releases and almost two years during shorter quarterly releases in three Eclipse projects. We then analyze both time periods’ refactoring activities to understand how refactoring activities can be impacted by shortening the release cycles.
Results:
We observe reduced refactoring activities in one project and a decrease in more complex 
refactoring operations
 after shortening the release cycles. We also find that weekly efforts dedicated to refactoring activities was lower across all projects after shortening the release cycles.
Conclusion:
Shorter releases may impact software development tasks such as refactoring in unintended ways. Not applying specific types of refactoring may also affect the software’s quality in the long term. Using this 
case study
 and past work on shorter releases, potential short release adopters can now better plan their transition to shorter releases knowing which areas of development may be affected.",Information and Software Technology,18 Mar 2025,6.0,"The impact of shorter releases on refactoring activities is relevant for startups, but the practical application may vary based on the specific context of the venture."
https://www.sciencedirect.com/science/article/pii/S0950584921001051,A decision model for programming language ecosystem selection: Seven industry case studies,November 2021,"Programming language ecosystem selection, Decision model, Industry case study, Software production, Multi-criteria decision-making, Decision support system",Siamak=Farshidi: s.farshidi@uva.nl; Slinger=Jansen: slinger.jansen@uu.nl; Mahdi=Deldar: m.deldar@datakavosh.com,"Abstract
Context:
Software development is a continuous decision-making process that mainly relies on the software engineer’s experience and intuition. One of the essential decisions in the early stages of the process is selecting the best fitting programming language ecosystem based on the project requirements. A significant number of criteria, such as developer availability and consistent documentation, in addition to the number of available options in the market, lead to a challenging decision-making process. As the selection of programming language ecosystems depends on the application to be developed and its environment, a decision model is required to analyze the selection problem using systematic identification and evaluation of potential alternatives for a development project.
Method:
Recently, we introduced a framework to build decision models for technology selection problems in software production. Furthermore, we designed and implemented a decision support system that uses such decision models to support software engineers with their decision-making problems. This study presents a decision model based on the framework for the programming language ecosystem selection problem.
Results:
The decision model has been evaluated through seven real-world 
case studies
 at seven software development companies. The case study participants declared that the approach provides significantly more insight into the programming language ecosystem selection process and decreases the decision-making process’s time and cost.
Conclusion:
With the decision model, software engineers can more rapidly evaluate and select programming language ecosystems. Having the knowledge in the decision model readily available supports software engineers in making more efficient and effective decisions that meet their requirements and priorities. Furthermore, such reusable knowledge can be employed by other researchers to develop new concepts and solutions for future challenges.",Information and Software Technology,18 Mar 2025,9.0,The decision support system for programming language ecosystem selection can greatly benefit early-stage ventures by optimizing decision-making processes and reducing time and cost.
https://www.sciencedirect.com/science/article/pii/S0950584921001038,Automatic patch linkage detection in code review using textual content and file location features,November 2021,Not Found,Dong=Wang: wang.dong.vt8@is.naist.jp; Raula Gaikovina=Kula: Not Found; Takashi=Ishio: Not Found; Kenichi=Matsumoto: Not Found,"Abstract
Context:
Contemporary code review tools are a popular choice for 
software quality assurance
. Using these tools, reviewers are able to post a 
linkage
 between two patches during a review discussion. Large development teams that use a review-then-commit model risk being unaware of these linkages.
Objective:
Our objective is to first explore how patch linkage impacts the review process. We then propose and evaluate models that detect patch linkage based on realistic time intervals.
Method:
First, we carry out an 
exploratory study
 on three 
open source projects
 to conduct linkage impact analysis using 942 manually classified linkages. Second, we propose two techniques using textual and file location similarity to build detection models and evaluate their performance.
Results:
The study provides evidence of latency in the linkage notification. We show that a patch with the Alternative Solution linkage (i.e., patches that implement similar functionality) undergoes a quicker review and avoids additional revisions after the team has been notified, compared to other linkage types. Our detection model experiments show promising recall rates for the Alternative Solution linkage (from 32% to 95%), but precision has room for improvement.
Conclusion:
Patch linkage detection is promising, with likely improvements if the practice of posting linkages becomes more prevalent. From our implications, this paper lays the groundwork for future research on how to increase patch linkage awareness to facilitate efficient reviews.",Information and Software Technology,18 Mar 2025,7.0,"The study on patch linkage detection addresses a practical issue in software quality assurance, but the direct impact on European early-stage ventures may be moderate."
https://www.sciencedirect.com/science/article/pii/S0950584921001166,ALBFL: A novel neural ranking model for software fault localization via combining static and dynamic features,November 2021,Not Found,Xi=Xiao: xiaox@sz.tsinghua.edu.cn; Yuqing=Pan: 66panyuqing@sina.com; Bin=Zhang: bin.zhang@pcl.ac.cn; Guangwu=Hu: hugw@sziit.edu.cn; Qing=Li: liq8@sustech.edu.cn; Runiu=Lu: lurn@sz.singhua.edu.cn,"Abstract
Context
Automatic 
software fault
 localization serves as a significant purpose in helping developers solve bugs efficiently. Existing approaches for software 
fault localization
 can be categorized into static methods and dynamic ones, which have improved the fault locating ability greatly by analyzing static features from the 
source code
 or tracking dynamic behaviors during the runtime respectively. However, the accuracy of 
fault localization
 is still unsatisfactory.
Objective
To enhance the capability of detecting software faults with the statement 
granularity
, this paper puts forward ALBFL, a novel neural ranking model that combines the static and dynamic features, which obtains excellent fault 
localization accuracy
. Firstly, ALBFL learns the 
semantic features
 of the 
source code
 by a transformer encoder. Then, it exploits a self-attention layer to integrate those static features and dynamic features. Finally, those integrated features are fed into a LambdaRank model, which can list the suspicious statements in 
descending order
 by their ranked scores.
Method
The experiments are conducted on an authoritative dataset (i.e., Defect4J), which includes 5 open-source projects, 357 faulty programs in total. We evaluate the effectiveness of ALBFL, effectiveness of combining features, effectiveness of model components and aggregation on method level.
Result
The results reflect that ALBFL identifies triple more faulty statements than 11 traditional 
SBFL methods
 and outperforms 2 state-of-the-art approaches by on average 14% on ranking faults in the first position.
Conclusions
To improve the precision of automatic 
software fault
 localization, ALBFL combines 
neural network
 ranking model equipped with the self-attention layer and the transformer encoder, which can take full use of various techniques to judge whether a code statement is fault-inducing or not. Moreover, the 
joint
 architecture of ALBFL is capable of training the integration of these features under various strategies so as to improve accuracy further. In the future, we plan to exploit more features so as to improve our method's efficiency and accuracy.",Information and Software Technology,18 Mar 2025,8.0,"The ALBFL model for software fault localization presents a novel approach that can significantly improve bug solving efficiency, which is crucial for startups dealing with limited resources."
https://www.sciencedirect.com/science/article/pii/S0950584921001257,Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction,November 2021,Not Found,Shuo=Feng: shuofeng5-c@my.cityu.edu.hk; Jacky=Keung: jacky.keung@cityu.edu.hk; Xiao=Yu: xiaoyu@whut.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Miao=Zhang: miazhang9-c@my.cityu.edu.hk,"Abstract
Context:
In practice, software datasets tend to have more non-defective instances than defective ones, which is referred to as the 
class imbalance problem
 in 
software defect
 prediction (SDP). Synthetic Minority Oversampling TEchnique (SMOTE) and its variants alleviate the 
class imbalance problem
 by generating synthetic defective instances. SMOTE-based oversampling techniques were widely adopted as the baselines to compare with the newly proposed oversampling techniques in SDP. However, randomness is introduced during the procedure of SMOTE-based oversampling techniques. If the performance of SMOTE-based oversampling techniques is highly unstable, the conclusion drawn from the comparison between SMOTE-based oversampling techniques and the newly proposed techniques may be misleading and less convincing.
Objective:
This paper aims to investigate the stability of SMOTE-based oversampling techniques. Moreover, a series of stable SMOTE-based oversampling techniques are proposed to improve the stability of SMOTE-based oversampling techniques.
Method:
Stable SMOTE-based oversampling techniques reduce the randomness in each step of SMOTE-based oversampling techniques by selecting defective instances in turn, distance-based selection of 
K
 neighbor instances, and evenly distributed interpolation. Besides, we mathematically prove and also empirically investigate the stability of SMOTE-based and stable SMOTE-based oversampling techniques on four common classifiers across 26 datasets in terms of AUC, 
b
a
l
a
n
c
e
, and 
MCC
.
Results:
The analysis of SMOTE-based and stable SMOTE-based oversampling techniques shows that the performance of stable SMOTE-based oversampling techniques is more stable and better than that of SMOTE-based oversampling techniques. The difference between the worst and best performances of SMOTE-based oversampling techniques is up to 23.3%, 32.6%, and 204.2% in terms of AUC, 
b
a
l
a
n
c
e
, and 
MCC
, respectively.
Conclusion:
Stable SMOTE-based oversampling techniques should be considered as a drop-in replacement for SMOTE-based oversampling techniques.",Information and Software Technology,18 Mar 2025,8.0,The investigation of stable SMOTE-based oversampling techniques in software defect prediction can have a significant impact on improving the stability of predictive models and the overall quality of software systems.
https://www.sciencedirect.com/science/article/pii/S0950584921001282,Architectural design decisions that incur technical debt — An industrial case study,November 2021,"Technical debt, Architectural design decisions, Architectural knowledge, Architectural technical debt",Mohamed=Soliman: m.a.m.soliman@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl; Yikun=Li: yikun.li@rug.nl,"Abstract
Context:
During software development, some 
architectural design decisions
 incur technical debt, either deliberately or inadvertently. These have serious impact on the quality of a software system, and can cost significant time and effort to be changed. While current research efforts have explored general concepts of architectural design decisions and technical debt separately, debt-incurring architectural design decisions have not been specifically explored in practice.
Objective:
In this 
case study
, we explore debt-incurring architectural design decisions (DADDs) in practice. Specifically, we explore the main types of DADDs, why and how they are incurred in a software system, and how practitioners deal with these types of design decisions.
Method:
We performed interviews and a focus group with practitioners working in embedded and enterprise software companies, discussing their concrete experience with such architectural design decisions.
Results:
We provide the following contributions: 1) A categorization for the types of DADDs, which extend a current ontology on architectural design decisions. 2) A process on how deliberate DADDs are made in practice. 3) A conceptual model which shows the relationships between the causes and triggers of inadvertent DADDs. 4) The main factors that influence the way of dealing with DADDs.
Conclusion:
The results can support the development of new approaches and tools for Architecture Technical Debt management from the perspective of Design Decisions. Moreover, they support future research to capture architecture knowledge related to DADDs.",Information and Software Technology,18 Mar 2025,9.0,The exploration of debt-incurring architectural design decisions in practice can greatly benefit software development by providing insights into how to manage technical debt effectively.
https://www.sciencedirect.com/science/article/pii/S0950584921001269,“Won’t We Fix this Issue?” Qualitative characterization and automated identification of wontfix issues on GitHub,November 2021,"Issue tracking, Issue management, Empirical study, Machine learning",Sebastiano=Panichella: panc@zhaw.ch; Gerardo=Canfora: canfora@unisannio.it; Andrea=Di Sorbo: disorbo@unisannio.it,"Abstract
Context
: Addressing user requests in the form of 
bug reports
 and Github issues represents a crucial task of any successful software project. However, user-submitted issue reports tend to widely differ in their quality, and developers spend a considerable amount of time handling them.
Objective
: By collecting a dataset of around 6,000 issues of 279 GitHub projects, we observe that developers take significant time (i.e., about five months, on average) before labeling an issue as a wontfix. For this reason, in this paper, we empirically investigate the nature of wontfix issues and methods to facilitate issue management process.
Method
: We first manually analyze a sample of 667 wontfix issues, extracted from heterogeneous projects, investigating the common reasons behind a “wontfix decision”, the main characteristics of wontfix issues and the potential factors that could be connected with the time to close them. Furthermore, we experiment with approaches enabling the prediction of wontfix issues by analyzing the titles and descriptions of reported issues when submitted.
Results and conclusion
: Our investigation sheds some light on the wontfix issues’ characteristics, as well as the potential factors that may affect the time required to make a “wontfix decision”. Our results also demonstrate that it is possible to perform prediction of wontfix issues with high average values of precision, recall, and F-measure (90%–93%).",Information and Software Technology,18 Mar 2025,7.0,The empirical investigation of wontfix issues in software projects can help streamline the issue management process and improve overall efficiency in handling user-submitted reports.
https://www.sciencedirect.com/science/article/pii/S0950584921001294,On the value of encouraging gender tolerance and inclusiveness in software engineering communities,November 2021,Not Found,Elijah=Zolduoarrati: Not Found; Sherlock A.=Licorish: sherlock.licorish@otago.ac.nz,"Abstract
Context
The recent spike in the growth of online communities is a testament to the technological advancements of the 21st century. People with shared interests, problems, and solutions can now engage via online groups, including the 
software engineering
 community. There is evidence, however, to suggest females are often underrepresented in such online communities, and especially those that are technology related. This comes at a great loss to these communities, and for 
software engineering
 in particular. Females, like males, add much value to the field of software engineering.
Objective
Limited evidence exists to quantify the value of males and females in the software engineering process or relevant communities. This insight could inform evidence-driven inclusiveness strategies. Accordingly, we sought to better understand 
gender differences
 in the Stack Overflow community in order to delineate the value of 
gender diversity
 in the field of software engineering.
Method
This study used 
archival data
 from Stack Overflow over an 11-year period, comprising records from 9.5 million contributors. We employed quantitative and qualitative approaches to examine the role of gender in terms of contributors’ orientation, attitudes, and 
knowledge sharing
 patterns.
Results
The results indicate female contributors on Stack Overflow differed significantly from males in relation to their orientation, attitudes, and 
knowledge sharing
 patterns. We observe that female contributors tend to have a more cooperative orientation. Additionally, females expressed a more supportive and collective outlook and were more willing to share knowledge than their male counterparts.
Conclusion
The software engineering community would benefit from gender tolerance and inclusiveness to promote a knowledge sharing culture. In this regard, 
gender diversity
 should be encouraged for the value it brings to Stack Overflow and the field of software engineering.",Information and Software Technology,18 Mar 2025,6.0,"The study on gender differences in the software engineering community sheds light on the importance of gender diversity, although the practical impact on startups may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000914,On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent,October 2021,Not Found,Islem=Saidani: islem.saidani.1@ens.etsmlt.ca; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 
refactoring operations
 extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.",Information and Software Technology,18 Mar 2025,5.0,"The examination of refactoring practices in the context of Continuous Integration provides valuable insights, but the practical implications on early-stage ventures may be less direct."
https://www.sciencedirect.com/science/article/pii/S0950584921000914,On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent,October 2021,Not Found,Islem=Saidani: islem.saidani.1@ens.etsmlt.ca; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 
refactoring operations
 extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.",Information and Software Technology,18 Mar 2025,7.0,"The study provides insights into the impact of Continuous Integration on refactoring practices, which is valuable for European startups focusing on software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584921000963,Why many challenges with GUI test automation (will) remain,October 2021,Not Found,Michel=Nass: michel.nass@bth.se; Emil=Alégroth: emil.alegroth@bth.se; Robert=Feldt: robert.feldt@chalmers.se,"Abstract
Context:
Automated testing is ubiquitous in modern software development and used to verify requirement conformance on all levels of system abstraction, including the system’s graphical user interface (GUI). GUI-based test automation, like other automation, aims to reduce the cost and time for testing compared to alternative, manual approaches. Automation has been successful in reducing costs for other forms of testing (like unit- or integration testing) in industrial practice. However, we have not yet seen the same convincing results for automated GUI-based testing, which has instead been associated with multiple 
technical challenges
. Furthermore, the software industry has struggled with some of these challenges for more than a decade with what seems like only limited progress.
Objective:
This systematic literature review takes a longitudinal perspective on GUI test automation challenges by identifying them and then investigating why the field has been unable to mitigate them for so many years.
Method:
The review is based on a final set of 49 publications, all reporting empirical evidence from practice or industrial studies. Statements from the publications are synthesized, based on a thematic coding, into 24 challenges related to GUI test automation.
Results:
The most reported challenges were mapped chronologically and further analyzed to determine how they and their proposed solutions have evolved over time. This chronological mapping of reported challenges shows that four of them have existed for almost two decades.
Conclusion:
Based on the analysis, we discuss why the key challenges with GUI-based test automation are still present and why some will likely remain in the future. For others, we discuss possible ways of how the challenges can be addressed. Further research should focus on finding solutions to the identified 
technical challenges
 with GUI-based test automation that can be resolved or mitigated. However, in parallel, we also need to acknowledge and try to overcome non-technical challenges.",Information and Software Technology,18 Mar 2025,5.0,"The research on GUI test automation challenges is relevant, but the focus on a niche technical aspect may limit its immediate practical value for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000987,Metamorphic testing of OpenStreetMap,October 2021,"Metamorphic testing, Quality of maps, OpenStreetMap",Jesús M.=Almendros-Jiménez: jalmen@ual.es; Antonio=Becerra-Terón: abecerra@ual.es; Mercedes G.=Merayo: mgmerayo@fdi.ucm.es; Manuel=Núñez: mn@sip.ucm.es,"Abstract
Context:
OpenStreetMap represents a collaborative effort of many different and unrelated users to create a free map of the world. Although contributors follow some general guidelines, unsupervised additions are prone to include erroneous information. Unfortunately, it is impossible to automatically detect most of these issues because there does not exist an 
oracle
 to evaluate whether the information is correct or not. Metamorphic testing has shown to be very useful in assessing the correctness of very heterogeneous artifacts when oracles are not available.
Objective:
The main goal of our work is to provide a (fully implemented) framework, based on metamorphic testing, that will support the analysis of the information provided in OpenStreetMap with the goal of detecting 
faulty
 information.
Method:
We defined a general metamorphic testing framework to deal with OpenStreetMap. We identified a set of 
good
 metamorphic relations. In order to have as much automation as possible, we paid 
special attention
 to the automatic selection of 
follow-up inputs
 because they are fundamental to diminish manual testing. In order to assess the usefulness of our framework, we applied it to analyze maps of four cities in different continents. The rationale is that we would be dealing with different problems created by different contributors.
Results:
We obtained experimental evidence that shows the potential value of our framework. The application of our framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories.
Conclusion:
The experiments showed the usefulness of our framework to identify potential issues in the information appearing in OpenStreetMap. Although our metamorphic relations are very helpful, future users of the framework might identify other relations to deal with specific situations not covered by our relations. Since we provide a general pattern to define metamorphic relations, it is relatively easy to extend the existing framework. In particular, since all our metamorphic relations are implemented and the code is freely available, users have a pattern to implement new relations.",Information and Software Technology,18 Mar 2025,8.0,The framework for detecting faulty information in OpenStreetMap using metamorphic testing has practical implications for startups needing data quality assurance in their applications.
https://www.sciencedirect.com/science/article/pii/S0950584921001014,Automating user-feedback driven requirements prioritization,October 2021,Not Found,Fitsum Meshesha=Kifetew: kifetew@fbk.eu; Anna=Perini: perini@fbk.eu; Angelo=Susi: susi@fbk.eu; Aberto=Siena: siena@fbk.eu; Denisse=Muñante: denisse_yessica.munante_arzapalo@telecom-sudparis.eu; Itzel=Morales-Ramirez: imoralesr@iingen.unam.mx,"Abstract
Context:
Feedback from end users of 
software applications
 is a valuable resource in understanding what users request, what they value, and what they dislike. Information derived from user-feedback can support software evolution activities, such as requirements prioritization. User-feedback analysis is still mostly performed manually by practitioners, despite growing research in automated analysis.
Objective:
We address two issues in automated user-feedback analysis: (i) most of the existing automated analysis approaches that exploit 
linguistic analysis
 assume that the vocabulary adopted by users (when expressing feedback) and developers (when formulating requirements) are the same; and (ii) user-feedback analysis techniques are usually experimentally evaluated only on some user-feedback dataset, not involving assessment by potential software developers.
Method:
We propose an approach, 
ReFeed
, that computes, for each requirement, the set of related user-feedback, and from such user-feedback extracts quantifiable properties which are relevant for prioritizing the requirement. The extracted properties are propagated to the 
related requirements
, based on which ranks are computed for each requirement. 
ReFeed
 relies on domain knowledge, in the form of an ontology, helping mitigate the gap in the vocabulary of end users and developers. The effectiveness of 
ReFeed
 is evaluated on a realistic requirements prioritization scenario in two experiments involving graduate students from two different universities.
Results:
ReFeed
 is able to synthesize reasonable priorities for a given set of requirements based on properties derived from user-feedback. The implementation of 
ReFeed
 and related resources are publicly available.
Conclusion:
The results from our studies are encouraging in that using only three properties of user-feedback, 
ReFeed
 is able to prioritize requirements with reasonable accuracy. Such automatically determined prioritization could serve as a 
good starting point
 for requirements experts involved in the task of prioritizing requirements Future studies could explore additional user-feedback properties to improve the effectiveness of computed priorities.",Information and Software Technology,18 Mar 2025,6.0,"Automated user-feedback analysis can benefit software evolution activities, but the specific approach discussed may require further validation for wider applicability in European startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000975,Convergence rate of Artificial Neural Networks for estimation in software development projects,October 2021,"Software effort estimation, Convergence rate, Taguchi Orthogonal Arrays, Artificial Neural Networks design, Fuzzification, Clustering",Dragica=Rankovic: drankovic@raf.rs; Nevena=Rankovic: Not Found; Mirjana=Ivanovic: Not Found; Ljubomir=Lazic: Not Found,"Abstract
Context:
Nowadays, companies are investing in brand new software, given that fact they always need help with 
estimating software
 development, effort, costs, and the period of time needed for completing the software itself. In this paper, four different architectures of 
Artificial Neural Networks
 (ANN), as one of the most desired tools for predicting and estimating effort in software development, were used.
Objective:
This paper aims to determine the 
convergence rate of
 each of the proposed ANNs, when obtaining the minimum 
relative error
, first depending on the cost effect function, then on the nature of the data on which the training, testing, and validation is performed.
Method:
Magnitude 
relative error
 (MRE) is calculated based on Taguchi’s orthogonal plans for each of these four proposed 
ANN architectures
. The 
fuzzification
 method, five different datasets, the 
clustering method
 for input values of each dataset, and prediction were used to achieve the best model for estimation.
Results:
Based on performed parts of the experiment, it can be concluded that the convergence rate of each proposed architecture depends on the cost effect function and the nature of projects in different datasets. By following the prediction throughout all experimental parts, it can be further confirmed that ANN-L36 gave the best results in this proposed approach.
Conclusion:
The main advantages of this model are as follows: the number of iterations is less than 10, shortened effort estimation time thanks to convergence rate, simple architecture of each proposed ANN, large coverage of different values of actual project efficiency, and minimal MMRE. This model can also serve as an idea for the construction of a tool that would be able to reliably, efficiently and accurately estimate the effort when developing various software projects.",Information and Software Technology,18 Mar 2025,7.0,The study on estimating software development effort using Artificial Neural Networks offers valuable insights for startups seeking to enhance their project planning and estimation processes.
https://www.sciencedirect.com/science/article/pii/S0950584921001129,Code smell detection using feature selection and stacking ensemble: An empirical investigation,October 2021,Not Found,Amal=Alazba: aalazba@ksu.edu.sa; Hamoud=Aljamaan: hjamaan@kfupm.edu.sa,"Abstract
Context:
Code smell detection is the process of identifying code pieces that are poorly designed and implemented. Recently more research has been directed towards machine learning-based approaches for code smells detection. Many classifiers have been explored in the literature, yet, finding an effective model to detect different code smells types has not yet been achieved.
Objective:
The main objective of this paper is to empirically investigate the capabilities of stacking heterogeneous ensemble model in code smell detection.
Methods:
Gain feature selection technique was applied to select relevant features in code smell detection. Detection performance of 14 
individual classifiers
 was investigated in the context of two class-level and four method-level code smells. Then, three stacking ensembles were built using all 
individual classifiers
 as 
base classifiers
, and three different meta-classifiers (LR, SVM and DT).
Results:
GP, MLP, DT and SVM(Lin) classifiers were among the best performing classifiers in detecting most of the code smells. On the other hand, SVM(Sig), NB(B), NB(M), and SGD were among the least accurate classifiers for most smell types. The stacking ensemble with LR and SVM meta-classifiers achieved a consistent high detection performance in class-level and method-level code smells compared to all individual models.
Conclusion:
This paper concludes that the detection performance of the majority of individual classifiers varied from one code smell type to another. However, the detection performance of the stacking ensemble with LR and SVM meta-classifiers was consistently superior over all individual classifiers in detecting different code smell types.",Information and Software Technology,18 Mar 2025,7.0,The research on stacking ensemble model in code smell detection has practical implications for improving code quality in early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000926,Icon2Code: Recommending code implementations for Android GUI components,October 2021,Not Found,Yanjie=Zhao: Not Found; Li=Li: Li.Li@monash.edu; Xiaoyu=Sun: Not Found; Pei=Liu: Not Found; John=Grundy: Not Found,"Abstract
Context:
Event-driven programming plays a crucial role in implementing GUI-based software systems such as 
Android
 apps. However, such event-driven code is inherently challenging to design and implement correctly. Despite a significant amount of research to help developers efficiently implement such software, improved approaches are still needed to assist developers in better handling events and associated callback methods.
Objective:
This work aims at inventing an intelligent recommendation system for helping app developers efficiently and effectively implement 
Android
 GUI components.
Methods:
To achieve the aforementioned objective, we introduce in this work a novel approach called Icon2Code. Given an icon or UI widget provided by designers as input, Icon2Code first searches from a large-scale app database to locate similar icons used in existing popular apps. It then learns from the implementation of these similar apps and leverages a collaborative filtering model to select and recommend the most relevant APIs.
Results:
Our approach can achieve an 81% success rate when only five recommended APIs are considered, and a 94% success rate if twenty results are considered, based on ten-fold cross-validation with a large-scale dataset containing over 45,000 icons and their code implementations.
Conclusion:
It is feasible to automatically recommend code implementations for Android GUI components and Icon2Code is useful and effective in helping achieve such an objective.",Information and Software Technology,18 Mar 2025,8.0,The intelligent recommendation system for Android GUI components can greatly benefit startups in developing efficient apps.
https://www.sciencedirect.com/science/article/pii/S0950584921000872,TIDY: A PBE-based framework supporting smart transformations for entity consistency in PowerPoint,October 2021,Not Found,Shuguan=Liu: liu_shuguan@126.com; Huiyan=Wang: cocowhy1013@gmail.com; Chang=Xu: changxu@nju.edu.cn,"Abstract
Context:
Programming by Example (PBE) is increasingly assisting human users by recognizing and executing repetitive tasks, such as text editing and spreadsheet manipulation. Yet, existing work falls short on dealing with rich-formatted documents like PowerPoint (PPT) files, when examples are few and collecting them is intrusive.
Objective:
This article presents 
TIDY
, a PBE-based framework, to assist automated entity transformations for their layout and style consistency in rich-formatted documents like PowerPoint, in a way adaptive to entity contexts and flexible with user selections.
Methods:
TIDY
 achieves this by examining entities’ operation histories, and proposes a two-stage framework to first identify user intentions behind histories and then make wise next-operation recommendations for users, in order to maintain the entity consistency for rich-formatted documents.
Results:
We implemented 
TIDY
 as a prototype tool and integrated it into PowerPoint as a plug-in module. We experimentally evaluated 
TIDY
 with real-world user operation data. The evaluation reports that 
TIDY
 achieved promising effectiveness with a hit rate of 77.3% on average, which was stably holding for a variety of editing tasks. Besides, 
TIDY
 took only marginal time overhead, costing several to several tens of milliseconds, to complete each recommendation.
Conclusion:
TIDY
 assists users to complete repetitive tasks in rich-formatted documents by non-intrusive user intention recognition and smart next-operation recommendations, which is effective and practically useful.",Information and Software Technology,18 Mar 2025,9.0,The PBE-based framework for rich-formatted documents like PowerPoint can streamline tasks for startups and early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000859,Analyzing privacy policies through syntax-driven semantic analysis of information types,October 2021,Not Found,Mitra Bokaei=Hosseini: mbokaeihossein@stmarytx.edu; Travis D.=Breaux: breaux@cs.cmu.edu; Rocky=Slavin: Rocky.Slavin@utsa.edu; Jianwei=Niu: Jianwei.Niu@utsa.edu; Xiaoyin=Wang: xiaoyin.wang@utsa.edu,"Abstract
Context:
Several government laws and app markets, such as Google Play, require the disclosure of app data practices to users. These data practices constitute critical privacy requirements statements, since they underpin the app’s functionality while describing how various personal information types are collected, used, and with whom they are shared.
Objective:
Abstract and ambiguous terminology in requirements statements concerning information types (e.g., “we collect your device information”), can reduce shared understanding among app developers, policy writers, and users.
Method:
To address this challenge, we propose a syntax-driven method that first parses a given information type phrase (e.g. mobile device identifier) into its constituents using a context-free grammar and second infers 
semantic relationships
 between constituents using semantic rules. The inferred 
semantic relationships
 between a given phrase and its constituents generate a hierarchy that models the generality and ambiguity of phrases. Through this method, we infer relations from a lexicon consisting of a set of information type phrases to populate a partial ontology. The resulting ontology is a knowledge graph that can be used to guide requirements authors in the selection of the most appropriate information type terms.
Results:
We evaluate the method’s performance using two criteria: (1) expert assessment of relations between information types; and (2) non-expert preferences for relations between information types. The results suggest 
performance improvement
 when compared to a previously proposed method. We also evaluate the reliability of the method considering the information types extracted from different data practices (e.g., collection, usage, sharing, etc.) in privacy policies for mobile or web-based apps in various app domains.
Contributions:
The method achieves average of 89% precision and 87% recall considering information types from various app domains and data practices. Due to these results, we conclude that the method can be generalized reliably in inferring relations and reducing the ambiguity and abstraction in privacy policies.",Information and Software Technology,18 Mar 2025,8.0,The syntax-driven method for inferring semantic relationships in privacy policies can aid startups in complying with data privacy regulations.
https://www.sciencedirect.com/science/article/pii/S0950584921000859,Analyzing privacy policies through syntax-driven semantic analysis of information types,October 2021,Not Found,Mitra Bokaei=Hosseini: mbokaeihossein@stmarytx.edu; Travis D.=Breaux: breaux@cs.cmu.edu; Rocky=Slavin: Rocky.Slavin@utsa.edu; Jianwei=Niu: Jianwei.Niu@utsa.edu; Xiaoyin=Wang: xiaoyin.wang@utsa.edu,"Abstract
Context:
Several government laws and app markets, such as Google Play, require the disclosure of app data practices to users. These data practices constitute critical privacy requirements statements, since they underpin the app’s functionality while describing how various personal information types are collected, used, and with whom they are shared.
Objective:
Abstract and ambiguous terminology in requirements statements concerning information types (e.g., “we collect your device information”), can reduce shared understanding among app developers, policy writers, and users.
Method:
To address this challenge, we propose a syntax-driven method that first parses a given information type phrase (e.g. mobile device identifier) into its constituents using a context-free grammar and second infers 
semantic relationships
 between constituents using semantic rules. The inferred 
semantic relationships
 between a given phrase and its constituents generate a hierarchy that models the generality and ambiguity of phrases. Through this method, we infer relations from a lexicon consisting of a set of information type phrases to populate a partial ontology. The resulting ontology is a knowledge graph that can be used to guide requirements authors in the selection of the most appropriate information type terms.
Results:
We evaluate the method’s performance using two criteria: (1) expert assessment of relations between information types; and (2) non-expert preferences for relations between information types. The results suggest 
performance improvement
 when compared to a previously proposed method. We also evaluate the reliability of the method considering the information types extracted from different data practices (e.g., collection, usage, sharing, etc.) in privacy policies for mobile or web-based apps in various app domains.
Contributions:
The method achieves average of 89% precision and 87% recall considering information types from various app domains and data practices. Due to these results, we conclude that the method can be generalized reliably in inferring relations and reducing the ambiguity and abstraction in privacy policies.",Information and Software Technology,18 Mar 2025,8.0,The syntax-driven method for inferring semantic relationships in privacy policies can aid startups in complying with data privacy regulations.
https://www.sciencedirect.com/science/article/pii/S0950584921000884,Overcoming cultural barriers to being agile in distributed teams,October 2021,Not Found,Darja=Šmite: Darja.Smite@bth.se; Nils Brede=Moe: Nils.B.Moe@bth.se; Javier=Gonzalez-Huerta: Javier.Gonzalez.Huerta@bth.se,"Abstract
Context:
 Agile methods in offshored projects have become increasingly popular. Yet, many companies have found that the use of agile methods in coordination with companies located outside the regions of early agile adopters remains challenging. 
India
 has received particular attention as the leading destination of offshoring contracts due to significant cultural differences between sides of such contracts. Alarming differences are primarily rooted in the hierarchical business culture of Indian organizations and related command-and-control management behavior styles.
Objective:
 In this study, we attempt to understand whether cultural barriers persist in distributed projects in which Indian engineers work with a more empowering Swedish management, and if so, how to overcome them. The present work is an invited extension of a 
conference paper
.
Method:
 We performed a multiple-case study in a mature agile company located in Sweden and a more hierarchical Indian vendor. We 
collected data
 from five group interviews with a total of 34 participants and five workshops with 96 participants in five distributed 
DevOps
 teams, including 36 Indian members, whose preferred behavior in different situations we surveyed.
Results:
 We identified twelve cultural barriers, six of which were classified as impediments to 
agile software development
 practices, and report on the manifestation of these barriers in five 
DevOps
 teams. Finally, we put forward recommendations to overcome the identified barriers and emphasize the importance of 
cultural training
, especially when onboarding new team members.
Conclusions:
 Our findings confirm previously reported behaviors rooted in cultural differences that impede the adoption of agile approaches in offshore collaborations, and identify new barriers not previously reported. In contrast to the existing opinion that cultural characteristics are rigid and unchanging, we found that some barriers present at the beginning of the studied collaboration disappeared over time. Many offshore members reported behaving similarly to their onshore colleagues.",Information and Software Technology,18 Mar 2025,7.0,"This study addresses cultural barriers in distributed agile projects involving Indian and Swedish teams, providing insights and recommendations which could be valuable for European startups working with offshoring contracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000860,Grey Literature in Software Engineering: A critical review,October 2021,Not Found,Fernando=Kamei: fernando.kenji@ifal.edu.br; Igor=Wiese: Not Found; Crescencio=Lima: Not Found; Ivanilton=Polato: Not Found; Vilmar=Nepomuceno: Not Found; Waldemar=Ferreira: Not Found; Márcio=Ribeiro: Not Found; Carolline=Pena: Not Found; Bruno=Cartaxo: Not Found; Gustavo=Pinto: Not Found; Sérgio=Soares: Not Found,"Abstract
Context:
Grey Literature (GL) recently has grown in 
Software Engineering
 (SE) research since the increased use of online communication channels by software engineers. However, there is still a limited understanding of how SE research is taking advantage of GL.
Objective:
This research aimed to understand how SE researchers use GL in their secondary studies.
Methods:
We conducted a tertiary study of studies published between 2011 and 2018 in high-quality software engineering conferences and journals. We then applied qualitative and 
quantitative analysis
 to investigate 446 potential studies.
Results:
From the 446 selected studies, 126 studies cited GL but only 95 of those used GL to answer a specific research question representing almost 21% of all the 446 secondary studies. Interestingly, we identified that few studies employed specific search mechanisms and used additional criteria for assessing GL. Moreover, by the time we conducted this research, 49% of the GL URLs are not working anymore. Based on our findings, we discuss some challenges in using GL and potential 
mitigation plans
.
Conclusion:
In this paper, we summarized the last 10 years of software engineering research that uses GL, showing that GL has been essential for bringing practical new perspectives that are scarce in traditional literature. By drawing the current landscape of use, we also raise some awareness of related challenges (and strategies to deal with them).",Information and Software Technology,18 Mar 2025,5.0,"While the study on the use of Grey Literature in Software Engineering research is interesting, its direct impact on practical aspects for European early-stage ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000860,Grey Literature in Software Engineering: A critical review,October 2021,Not Found,Fernando=Kamei: fernando.kenji@ifal.edu.br; Igor=Wiese: Not Found; Crescencio=Lima: Not Found; Ivanilton=Polato: Not Found; Vilmar=Nepomuceno: Not Found; Waldemar=Ferreira: Not Found; Márcio=Ribeiro: Not Found; Carolline=Pena: Not Found; Bruno=Cartaxo: Not Found; Gustavo=Pinto: Not Found; Sérgio=Soares: Not Found,"Abstract
Context:
Grey Literature (GL) recently has grown in 
Software Engineering
 (SE) research since the increased use of online communication channels by software engineers. However, there is still a limited understanding of how SE research is taking advantage of GL.
Objective:
This research aimed to understand how SE researchers use GL in their secondary studies.
Methods:
We conducted a tertiary study of studies published between 2011 and 2018 in high-quality software engineering conferences and journals. We then applied qualitative and 
quantitative analysis
 to investigate 446 potential studies.
Results:
From the 446 selected studies, 126 studies cited GL but only 95 of those used GL to answer a specific research question representing almost 21% of all the 446 secondary studies. Interestingly, we identified that few studies employed specific search mechanisms and used additional criteria for assessing GL. Moreover, by the time we conducted this research, 49% of the GL URLs are not working anymore. Based on our findings, we discuss some challenges in using GL and potential 
mitigation plans
.
Conclusion:
In this paper, we summarized the last 10 years of software engineering research that uses GL, showing that GL has been essential for bringing practical new perspectives that are scarce in traditional literature. By drawing the current landscape of use, we also raise some awareness of related challenges (and strategies to deal with them).",Information and Software Technology,18 Mar 2025,5.0,"Similar to abstract 147, the research on Grey Literature usage in SE, while valuable for academia, may have limited practical implications for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000689,Joint feature representation learning and progressive distribution matching for cross-project defect prediction,September 2021,Not Found,Quanyi=Zou: zouquanyi2010@163.com; Lu=Lu: lul@scut.edu.cn; Zhanyu=Yang: yangzhanyu@hotmail.com; Xiaowei=Gu: amyxwgu@163.com; Shaojian=Qiu: qiushaojian@outlook.com,"Abstract
Context:
Cross-Project 
Defect Prediction
 (CPDP) aims to leverage the knowledge from label-rich source software projects to promote tasks in a label-poor target software project. Existing CPDP methods have two major flaws. One is that previous CPDP methods only consider global feature representation and ignores local relationship between instances in the same category from different projects, resulting in ambiguous predictions near the decision boundary. The other one is that CPDP methods based on pseudo-labels assume that the conditional distribution can be well matched at one stroke, when instances of target project are correctly annotated pseudo labels. However, due to the great gap between projects, the pseudo-labels seriously deviate from the real labels.
Objective:
To address above issues, this paper proposed a novel CPDP method named 
Joint
 Feature Representation with Double Marginalized 
Denoising
 
Autoencoders
 (DMDA_JFR).
Method:
Our method mainly includes two parts: joint feature 
representation learning
 and progressive distribution matching. We utilize two novel 
autoencoders
 to jointly learn the global and 
local feature
 representations simultaneously. To achieve progressive distribution matching, we introduce a repetitious pseudo-labels strategy, which makes it possible that distributions are matched after each stack layer learning rather than in one stroke.
Results:
The effectiveness of the proposed method was evaluated through experiments conducted on 10 open-source projects, including 29 software releases from PROMISE repository. Overall, experimental results show that our proposed method outperformed several state-of-the-art baseline CPDP methods.
Conclusions:
It can be concluded that (1) joint deep representations are promising for CPDP compared with only considering global feature representation methods, (2) progressive distribution matching is more effective for adapting probability distributions in CPDP compared with existing CPDP methods based on pseudo-labels.",Information and Software Technology,18 Mar 2025,8.0,"The proposed CPDP method introduces novel techniques that could enhance defect prediction practices in software projects, offering potential benefits to European startups focusing on software development."
https://www.sciencedirect.com/science/article/pii/S0950584921000823,Leveraging developer information for efficient effort-aware bug prediction,September 2021,Not Found,Yu=Qu: yuq@ucr.edu; Jianlei=Chi: chijianlei@stu.xjtu.edu.cn; Heng=Yin: hengy@ucr.edu,"Abstract
Context:
Software bug prediction techniques can provide informative guidance in 
software engineering
 practices. Over the past 15 years, developer information has been intensively used in bug prediction as features or basic 
data source
 to construct other useful models.
Objective:
Further leverage developer information from a new and straightforward perspective to improve effort-aware bug prediction.
Methods:
We propose to investigate the direct relations between the number of developers and the probability for a file to be buggy. Based on an empirical study on nine open-source Java systems with 32 versions, we observe a widely-existed and interesting tendency: when there are more developers working on a source file, there will be a stronger possibility for this file to be buggy. Based on the observed tendency, we propose an unsupervised algorithm and a supervised equation both called 
top-dev
 to improve effort-aware bug prediction. The key idea is to prioritize the ranking of files, whose number of developers is large, in the suspicious file list generated by effort-aware models.
Results:
Experimental results show that the proposed 
top-dev
 algorithm and equation significantly outperform the unsupervised and supervised 
baseline models
 (ManualUp, 
R
a
d
, 
R
d
d
, 
R
e
e
, CBS+, and 
top-core
). Moreover, the unsupervised 
top-dev
 algorithm is comparable or superior to existing supervised 
baseline models
.
Conclusion:
The proposed approaches are very useful in effort-aware bug prediction practices. Practitioners can use the 
top-dev
 algorithm to generate a high-quality and informative suspicious file list without training complex 
machine learning
 classifiers. On the other hand, when building supervised bug prediction model, the best practice is to combine existing models with the 
top-dev
 equation.",Information and Software Technology,18 Mar 2025,9.0,The investigation on leveraging developer information for bug prediction provides practical insights and proposed approaches that could benefit European early-stage ventures in improving bug prediction practices.
https://www.sciencedirect.com/science/article/pii/S0950584921000811,"How do developers discuss and support new programming languages in technical Q&A site? An empirical study of Go, Swift, and Rust in Stack Overflow",September 2021,Not Found,Partha=Chakraborty: Not Found; Rifat=Shahriyar: Not Found; Anindya=Iqbal: Not Found; Gias=Uddin: gias.uddin@ucalgary.ca,"Abstract
Context:
New programming languages (e.g., Swift, Go, Rust, etc.) are being introduced to provide a better opportunity for the developers to make software development robust and easy. At the early stage, a programming language is likely to have 
resource constraints
 that encourage the developers to seek help frequently from experienced peers active in Question–Answering (QA) sites such as Stack Overflow (SO).
Objective:
In this study, we have formally studied the discussions on three popular new languages introduced after the inception of SO (2008) and match those with the relevant activities in GitHub whenever appropriate. For that purpose, we have mined 4,17,82,536 questions and answers from SO and 7,846 issue information along with 6,60,965 
repository information
 from Github. Initially, the development of new languages is relatively slow compared to mature languages (e.g., C, C++, Java). The expected outcome of this study is to reveal the difficulties and challenges faced by the developers working with these languages so that appropriate measures can be taken to expedite the generation of relevant resources.
Method:
We have used the 
Latent Dirichlet Allocation
 (LDA) method on SO’s questions and answers to identify different topics of new languages. We have extracted several features of the answer pattern of the new languages from SO (e.g., time to get an accepted answer, time to get an answer, etc.) to study their characteristics. These attributes were used to identify difficult topics. We explored the background of developers who are contributing to these languages. We have created a model by combining Stack Overflow data and issues, repository, user data of Github. Finally, we have used that model to identify factors that affect language evolution.
Results:
The major findings of the study are: (i) migration, data and 
data structure
 are generally the difficult topics of new languages, (ii) the time when adequate resources are expected to be available vary from language to language, (iii) the unanswered question ratio increases regardless of the age of the language, and (iv) there is a relationship between developers’ activity pattern and the growth of a language.
Conclusion:
We believe that the outcome of our study is likely to help the owner/sponsor of these languages to design better features and documentation. It will also help the software developers or students to prepare themselves to work on these languages in an informed way.",Information and Software Technology,18 Mar 2025,7.0,"The study provides valuable insights into the challenges faced by developers working with new programming languages, which can help improve language features and documentation. This can be beneficial for early-stage ventures looking to adopt new technologies."
https://www.sciencedirect.com/science/article/pii/S0950584921000793,From monolithic systems to Microservices: An assessment framework,September 2021,"Microservices, Cloud migration, Software measurement",Florian=Auer: florian.auer@uibk.ac.at; Valentina=Lenarduzzi: valentina.lenarduzzi@lut.fi; Davide=Taibi: davide.taibi@tuni.fi,"Abstract
Context:
Re-architecting 
monolithic systems
 with Microservices-based architecture is a common trend. Various companies are migrating to 
Microservices
 for different reasons. However, making such an important decision like re-architecting an entire system must be based on real facts and not only on gut feelings.
Objective:
The goal of this work is to propose an evidence-based decision support framework for companies that need to migrate to Microservices, based on the analysis of a set of characteristics and metrics they should collect before re-architecting their monolithic system.
Method:
We conducted a survey done in the form of interviews with professionals to derive the assessment framework based on Grounded Theory.
Results:
We identified a set consisting of information and metrics that companies can use to decide whether to migrate to Microservices or not. The proposed assessment framework, based on the aforementioned metrics, could be useful for companies if they need to migrate to Microservices and do not want to run the risk of failing to consider some important information.",Information and Software Technology,18 Mar 2025,6.0,"The framework proposed can be useful for companies considering migration to Microservices, providing them with a structured approach based on evidence. This could be valuable for early-stage ventures undergoing architectural decision-making."
https://www.sciencedirect.com/science/article/pii/S095058492100080X,Source Code Transformations for Improving Security of Time-bounded K-variant Systems,September 2021,Not Found,Berk=Bekiroglu: bbekirog@iit.edu; Bogdan=Korel: Not Found,"Abstract
Context
Source code transformation techniques can improve the security of systems against memory exploitation attacks. As such, the chance of exploitation of 
security vulnerabilities
 can be decreased by using different controlled source code transformation techniques. In K-variant architecture, multiple variants of a program are generated through a controlled source code transformation to improve the security of systems.
Objective
To investigate the effectiveness and practicality of source code program transformations in improving the security of time-bounded K-variant systems for memory exploitation attacks.
Method
The effectiveness of program transformations in improving the security of time-bounded K-variant systems is experimentally investigated for different memory attacks.
Results
The results suggest that generating multiple variants using the presented transformations significantly improves the survivability of time-bounded K-variant systems under memory exploitation attacks.
Conclusion
We conclude that generating multi-variants in time-bounded K-variant systems in accordance with the presented program transformations may improve the security of time-bounded K-variant systems significantly for memory exploitation attacks with a reasonable cost and overhead.",Information and Software Technology,18 Mar 2025,8.0,The investigation on source code transformation techniques for improving security against memory exploitation attacks has practical implications for startups concerned about system security. The findings suggest significant improvements in survivability under attacks.
https://www.sciencedirect.com/science/article/pii/S0950584921000835,Phase-wise migration of multiple legacy applications–A graph-theoretic approach,September 2021,Not Found,Rohit=Punnoose: r14012@astra.xlri.ac.in; Supriya Kumar=De: skde@xlri.ac.in,"Abstract
Context
Many organizations undertake large-scale projects of application migration due to availability of scalable and cost-efficient technologies. Such legacy application migration projects are very complex since the process involves in-depth profiling of the applications.
Objective
During the initial profiling phase, it is imperative to understand the underlying complexities of individual applications, as well as the interdependencies among applications in the organization. This analysis phase can take considerable time and effort, depending on number and complexity of the applications. The main goal of this paper is to provide a framework that provides a cost-effective and quick approach to study the interdependencies between legacy applications with minimal prior knowledge of application usage.
Method
In this paper, we propose a framework that uses community 
detection algorithms
 and other established techniques from graph theory, to discover interdependencies of legacy applications within an organization, group these highly interdependent legacy applications in clusters, and finally sequence the clusters for migration to a modern platform. We study the proposed framework through three case studies, using network datasets from a large US organization.
Results
The experimental results from the proposed framework suggests that legacy applications can be grouped into clusters with high interdependencies between each other. Also, the framework shows how organizations can then appropriately sequence the clusters of legacy applications into a phase-wise migration project, thereby reducing migration costs.
Conclusion
The proposed framework provides a valuable design input to organizations on how to determine the interdependencies between the various legacy applications that are in scope for migration to a modern platform. Such large-scale migration projects can be simplified and broken down to use a systematic approach, thereby reducing migration costs and data integrity challenges.",Information and Software Technology,18 Mar 2025,6.0,"The framework proposed for legacy application migration can help organizations understand application interdependencies and reduce costs. While not directly focused on startups, the approach could be valuable for early-stage ventures considering application migration."
https://www.sciencedirect.com/science/article/pii/S0950584921000720,The do’s and don’ts of infrastructure code: A systematic gray literature review,September 2021,"Infrastructure-as-code, DevOps, Gray literature review",Indika=Kumara: i.p.k.weerasingha.dewage@tue.nl; Martín=Garriga: m.garriga@uvt.nl; Angel Urbano=Romeu: a.urbanoromeu@uvt.nl; Dario=Di Nucci: d.dinucci@uvt.nl; Fabio=Palomba: fpalomba@unisa.it; Damian Andrew=Tamburri: d.a.tamburri@tue.nl; Willem-Jan=van den Heuvel: w.j.a.m.vdnheuvel@uvt.nl,"Abstract
Context:
Infrastructure-as-code (IaC) is the 
DevOps
 tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual 
hardware configuration
 or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the 
relative scarcity
 of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.",Information and Software Technology,18 Mar 2025,7.0,The catalog of best and bad practices for Infrastructure-as-code languages can provide valuable guidance for practitioners and academics. Early-stage ventures looking to adopt IaC practices could benefit from the insights and recommendations presented.
https://www.sciencedirect.com/science/article/pii/S0950584921000720,The do’s and don’ts of infrastructure code: A systematic gray literature review,September 2021,"Infrastructure-as-code, DevOps, Gray literature review",Indika=Kumara: i.p.k.weerasingha.dewage@tue.nl; Martín=Garriga: m.garriga@uvt.nl; Angel Urbano=Romeu: a.urbanoromeu@uvt.nl; Dario=Di Nucci: d.dinucci@uvt.nl; Fabio=Palomba: fpalomba@unisa.it; Damian Andrew=Tamburri: d.a.tamburri@tue.nl; Willem-Jan=van den Heuvel: w.j.a.m.vdnheuvel@uvt.nl,"Abstract
Context:
Infrastructure-as-code (IaC) is the 
DevOps
 tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual 
hardware configuration
 or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the 
relative scarcity
 of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.",Information and Software Technology,18 Mar 2025,7.0,The research on Infrastructure-as-code (IaC) and compilation of best and bad practices for widely used IaC languages can provide valuable insights for early-stage ventures looking to improve their infrastructure management processes.
https://www.sciencedirect.com/science/article/pii/S0950584921000847,"Processes, challenges and recommendations of Gray Literature Review: An experience report",September 2021,Not Found,He=Zhang: Not Found; Runfeng=Mao: Not Found; Huang=Huang: Not Found; Qiming=Dai: Not Found; Xin=Zhou: Not Found; Haifeng=Shen: Not Found; Guoping=Rong: ronggp@nju.edu.cn,"Abstract
Context:
Systematic Literature Review
 (SLR), as a tool of Evidence-Based 
Software Engineering
 (EBSE), has been widely used in 
Software Engineering
 (SE). However, for certain topics in SE, especially those that are trendy or 
industry
 driven, academic literature is generally scarce and consequently Gray Literature (GL) becomes a major source of evidence. In recent years, the adoption of Gray Literature Review (GLR) or Multivocal Literature Review (MLR) is rising steadily to provide the state-of-the-practice of a specific topic where SLR is not a viable option.
Objective:
Although some SLR guidelines recommend the use of GL and several MLR guidelines have already been proposed in SE, researchers still have conflicting views on the value of GL and commonly accepted GLR or MLR studies are generally lacking in terms of publication. This experience report aims to shed some light on GLR through a 
case study
 that uses SLR and MLR guidelines to conduct a GLR on an emerging topic in SE to specifically answer the questions related to the reasons of using GL, the processes of conducting GL, and the impacts of GL on review results.
Method:
We retrospect the review process of conducting a GLR on the topic of DevSecOps with reference to Kitchenham’s SLR and Garousi’s MLR guidelines. We specifically reflect on the processes we had to adapt in order to tackle the challenges we faced. We also compare and contrast our GLR with existing MLRs or GLRs in SE to contextualize our reflections.
Results:
We distill ten challenges in nine activities of a GLR process. We provide reasons for these challenges and further suggest ways to tackle them during a GLR process. We also discuss the decision process of selecting a suitable review methodology among SLR, MLR and GLR and elaborate the impacts of GL on our review results.
Conclusion:
Although our experience on GLR is mainly derived from a specific 
case study
 on DevSecOps, we conjecture that it is relevant and would be beneficial to other GLR or MLR studies. We also expect our experience would contribute to future GLR or MLR guidelines, in a way similar to how SLR guidelines learned from the SLR experience report a dozen years ago. In addition, other researchers may find our 
decision making process
 useful before they conduct their own reviews.",Information and Software Technology,18 Mar 2025,5.0,"While the study on Gray Literature Review (GLR) provides insights into conducting literature reviews in SE, the practical impact on early-stage ventures may not be immediate or significant."
https://www.sciencedirect.com/science/article/pii/S0950584921000707,Automated formalization of structured natural language requirements,September 2021,Not Found,Dimitra=Giannakopoulou: dimitra.giannakopoulou@nasa.gov; Thomas=Pressburger: tom.pressburger@nasa.gov; Anastasia=Mavridou: anastasia.mavridou@nasa.gov; Johann=Schumann: johann.m.schumann@nasa.gov,"Abstract
The use of structured natural languages to capture requirements provides a reasonable trade-off between ambiguous natural language and unintuitive 
formal notations
. There are two major challenges in making structured natural language amenable to formal analysis: (1) formalizing requirements as formulas that can be processed by analysis tools and (2) ensuring that the formulas conform to the semantics of the structured natural language. 
fretish
 is a structured natural language that incorporates features from existing research and from NASA applications. Even though 
fretish
 is quite expressive, its underlying semantics is determined by the types of four fields: 
scope
, 
condition
, 
timing
, and 
response
. Each combination of field types defines a template with Real-Time Graphical Interval Logic (RTGIL) semantics. We have developed a framework that constructs temporal logic formulas for each template compositionally, from its fields. The compositional nature of our algorithms facilitates maintenance and extensibility. Our goal is to be inclusive not only in terms of language expressivity, but also in terms of requirements analysis tools that we can interface with. For this reason we generate metric-temporal logic formulas with (1) exclusively future-time operators, over both finite and infinite traces, and (2) exclusively past-time operators. To establish trust in the produced formalizations for each template, our framework: (1) extensively tests the generated formulas against the template semantics and (2) proves equivalence between its past-time and future-time formulas. Our approach is available through the open-source tool 
fret
 and has been used to capture and analyze requirements for a Lockheed Martin Cyber–Physical System challenge.",Information and Software Technology,18 Mar 2025,8.0,The development of a framework to construct temporal logic formulas for structured natural language requirements can potentially benefit startups by improving their requirements analysis processes.
https://www.sciencedirect.com/science/article/pii/S095058492100046X,Measuring the cognitive load of software developers: An extended Systematic Mapping Study,August 2021,Not Found,Lucian José=Gonçales: lucianj@edu.unisinos.br; Kleinner=Farias: Not Found; Bruno C.=da Silva: Not Found,"Abstract
Context:
Cognitive load in 
software engineering
 refers to the mental effort users spend while reading software artifacts. The cognitive load can vary according to tasks and across developers. Researchers have measured developers’ cognitive load for different purposes, such as understanding its impact on productivity and software quality. Thus, researchers and practitioners can use cognitive load measures for solving many aspects of 
software engineering
 problems.
Problem:
However, a lack of a classification of dimensions on cognitive load measures in software engineering makes it difficult for researchers and practitioners to obtain research trends to advance 
scientific knowledge
 or apply it in software projects.
Objective:
This article aims to classify different aspects of cognitive load measures in software engineering and identify challenges for further research.
Method:
We conducted a 
Systematic Mapping Study
 (SMS), which started with 4,175 articles gathered from 11 search engines and then narrowed down to 63 primary studies.
Results:
Our main findings are: (1) 43% (27/63) of the primary studies focused on applying a combination of sensors; (2) 81% (51/63) of the selected works were validation studies; (3) 83% (52/63) of the primary studies analyzed cognitive load while developers performed programming tasks. Moreover, we created a 
classification scheme
 based on the answers to our research questions.
Conclusion:
despite the production of a significant amount of studies on cognitive load in software engineering, there are still many challenges to be solved in this particular field for effectively measuring the cognitive load in software engineering. Therefore, this work provided directions for future studies on cognitive load measurement in software engineering.",Information and Software Technology,18 Mar 2025,6.0,"The classification of cognitive load measures in software engineering is valuable for researchers and practitioners, but the direct impact on early-stage ventures may be limited in the short term."
https://www.sciencedirect.com/science/article/pii/S095058492100046X,Measuring the cognitive load of software developers: An extended Systematic Mapping Study,August 2021,Not Found,Lucian José=Gonçales: lucianj@edu.unisinos.br; Kleinner=Farias: Not Found; Bruno C.=da Silva: Not Found,"Abstract
Context:
Cognitive load in 
software engineering
 refers to the mental effort users spend while reading software artifacts. The cognitive load can vary according to tasks and across developers. Researchers have measured developers’ cognitive load for different purposes, such as understanding its impact on productivity and software quality. Thus, researchers and practitioners can use cognitive load measures for solving many aspects of 
software engineering
 problems.
Problem:
However, a lack of a classification of dimensions on cognitive load measures in software engineering makes it difficult for researchers and practitioners to obtain research trends to advance 
scientific knowledge
 or apply it in software projects.
Objective:
This article aims to classify different aspects of cognitive load measures in software engineering and identify challenges for further research.
Method:
We conducted a 
Systematic Mapping Study
 (SMS), which started with 4,175 articles gathered from 11 search engines and then narrowed down to 63 primary studies.
Results:
Our main findings are: (1) 43% (27/63) of the primary studies focused on applying a combination of sensors; (2) 81% (51/63) of the selected works were validation studies; (3) 83% (52/63) of the primary studies analyzed cognitive load while developers performed programming tasks. Moreover, we created a 
classification scheme
 based on the answers to our research questions.
Conclusion:
despite the production of a significant amount of studies on cognitive load in software engineering, there are still many challenges to be solved in this particular field for effectively measuring the cognitive load in software engineering. Therefore, this work provided directions for future studies on cognitive load measurement in software engineering.",Information and Software Technology,18 Mar 2025,6.0,"Similar to Abstract 159, the classification of cognitive load measures in software engineering may not have an immediate practical impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000550,BCI-CFI: A context-sensitive control-flow integrity method based on branch correlation integrity,August 2021,Not Found,Ye=Wang: daguoli415@163.com; Qingbao=Li: Not Found; Zhifeng=Chen: Not Found; Ping=Zhang: Not Found; Guimin=Zhang: Not Found; Zhihui=Shi: Not Found,"Abstract
Context
As part of the arms race, one emerging attack methodology has been control-hijacking attacks, e.g., return-oriented programming (ROP). Control-flow integrity (CFI) is a generic and effective defense against most control-hijacking attacks. However, existing CFI mechanisms have poor security as demonstrated by their equivalence class (EC) sizes, which are sets of targets that CFI policies cannot distinguish. Adversaries can choose an illegitimate control transfer within an EC that is included in the resulting 
CFG
 and incorrectly allowed by CFI protection policies.
Objective
The paper introduces a context-sensitive control-flow integrity method, which aims to improve the security of CFI and prevent ROP attacks.
Method
The paper presents BCI-CFI, a context-sensitive CFI technique based on branch correlation integrity (BCI), which can effectively break down EC sizes and improve the security of CFI. BCI-CFI takes the branch correlation relationship (i.e., a new type of context for CFI) as contextual information to refine the CFI policy and identify the BCI pairs in the target program via 
static analysis
. Furthermore, the paper introduces a state machine M
CFI
 for BCI-CFI to conduct target validation for the indirect control-flow transfer (ICT) instructions in the target program at runtime.
Results
Our results show that, (i) BCI-CFI prevented adversaries from manipulating the control data and launching ROP attacks, (ii) protected both forward and backward ICT in the target program, and improved the security and effectiveness of CFI, and (iii) BCI-CFI introduced a 19.67% runtime overhead on average and a maximum runtime overhead of 31.2%
Conclusion
BCI-CFI is a context-sensitive CFI technique aiming to prevent adversaries from manipulating the control data of the target program to launch ROP attacks. BCI-CFI can reduce EC sizes and improve the security of CFI while incurring a moderate runtime overhead on average.",Information and Software Technology,18 Mar 2025,5.0,"The context-sensitive CFI technique introduced has the potential to improve security and prevent ROP attacks, which can be valuable for early-stage ventures dealing with cybersecurity."
https://www.sciencedirect.com/science/article/pii/S0950584921000550,BCI-CFI: A context-sensitive control-flow integrity method based on branch correlation integrity,August 2021,Not Found,Ye=Wang: daguoli415@163.com; Qingbao=Li: Not Found; Zhifeng=Chen: Not Found; Ping=Zhang: Not Found; Guimin=Zhang: Not Found; Zhihui=Shi: Not Found,"Abstract
Context
As part of the arms race, one emerging attack methodology has been control-hijacking attacks, e.g., return-oriented programming (ROP). Control-flow integrity (CFI) is a generic and effective defense against most control-hijacking attacks. However, existing CFI mechanisms have poor security as demonstrated by their equivalence class (EC) sizes, which are sets of targets that CFI policies cannot distinguish. Adversaries can choose an illegitimate control transfer within an EC that is included in the resulting 
CFG
 and incorrectly allowed by CFI protection policies.
Objective
The paper introduces a context-sensitive control-flow integrity method, which aims to improve the security of CFI and prevent ROP attacks.
Method
The paper presents BCI-CFI, a context-sensitive CFI technique based on branch correlation integrity (BCI), which can effectively break down EC sizes and improve the security of CFI. BCI-CFI takes the branch correlation relationship (i.e., a new type of context for CFI) as contextual information to refine the CFI policy and identify the BCI pairs in the target program via 
static analysis
. Furthermore, the paper introduces a state machine M
CFI
 for BCI-CFI to conduct target validation for the indirect control-flow transfer (ICT) instructions in the target program at runtime.
Results
Our results show that, (i) BCI-CFI prevented adversaries from manipulating the control data and launching ROP attacks, (ii) protected both forward and backward ICT in the target program, and improved the security and effectiveness of CFI, and (iii) BCI-CFI introduced a 19.67% runtime overhead on average and a maximum runtime overhead of 31.2%
Conclusion
BCI-CFI is a context-sensitive CFI technique aiming to prevent adversaries from manipulating the control data of the target program to launch ROP attacks. BCI-CFI can reduce EC sizes and improve the security of CFI while incurring a moderate runtime overhead on average.",Information and Software Technology,18 Mar 2025,5.0,"Similar to abstract 161, this context-sensitive CFI technique has the potential to enhance security and prevent ROP attacks, which can benefit startups focusing on cybersecurity."
https://www.sciencedirect.com/science/article/pii/S0950584921000586,BGNN4VD: Constructing Bidirectional Graph Neural-Network for Vulnerability Detection,August 2021,Not Found,Sicong=Cao: Not Found; Xiaobing=Sun: xbsun@yzu.edu.cn; Lili=Bo: Not Found; Ying=Wei: Not Found; Bin=Li: lb@yzu.edu.cn,"Abstract
Context:
Previous studies have shown that existing deep learning-based approaches can significantly improve the performance of 
vulnerability detection
. They represent code in various forms and mine vulnerability features with 
deep learning models
. However, the differences of code representation forms and 
deep learning
 models make various approaches still have some limitations. In practice, their false-positive rate (FPR) and false-negative rate (FNR) are still high.
Objective:
To address the limitations of existing deep learning-based vulnerability detection approaches, we propose 
BGNN4VD
 (Bidirectional 
Graph Neural Network
 for Vulnerability Detection), a vulnerability detection approach by constructing a Bidirectional Graph Neural-Network (BGNN).
Method:
In Phase 1, we extract the syntax and semantic information of source code through 
abstract syntax tree
 (AST), 
control flow graph
 (CFG), and 
data flow graph
 (DFG). Then in Phase 2, we use vectorized source code as input to Bidirectional Graph Neural-Network (BGNN). In Phase 3, we learn the different features between vulnerable code and non-vulnerable code by introducing backward edges on the basis of traditional Graph Neural-Network (GNN). Finally in Phase 4, a Convolutional Neural-Network (CNN) is used to further extract features and detect vulnerabilities through a classifier.
Results:
We evaluate 
BGNN4VD
 on four popular C/C++ projects from NVD and GitHub, and compare it with four state-of-the-art (
Flawfinder
, 
RATS
, 
SySeVR
, and 
VUDDY
) vulnerab ility detection approaches. Experiment results show that, when compared these baselines, 
BGNN4VD
 achieves 4.9%, 11.0%, and 8.4% improvement in F1-measure, accuracy and precision, respectively.
Conclusion:
The proposed 
BGNN4VD
 achieves a higher precision and accuracy than the state-of-the-art methods. In addition, when applied on the latest vulnerabilities reported by CVE, 
BGNN4VD
 can still achieve a precision at 45.1%, which demonstrates the feasibility of 
BGNN4VD
 in practical application.",Information and Software Technology,18 Mar 2025,8.0,"The BGNN4VD vulnerability detection approach shows significant improvements in F1-measure, accuracy, and precision, which could be highly beneficial for startups aiming to enhance their security measures and reduce vulnerabilities."
https://www.sciencedirect.com/science/article/pii/S0950584921000562,An empirical study on clone consistency prediction based on machine learning,August 2021,Not Found,Fanlong=Zhang: izhangfanlong@gmail.com; Siau-cheng=Khoo: khoosc@nus.edu.sg,"Abstract
Context:
Code Clones have been accepted as a common phenomenon in software, thanks to the increasing demand for rapid production of software. The existence of code clones is recognized by developers in the form of 
clone group
, which includes several pieces of clone fragments that are similar to one another. A change in one of these clone fragments may indicate necessary 
“consistent changes”
 are required for the rest of the clones within the same group, which can increase extra maintenance costs. A failure in making such consistent change when it is necessary is commonly known as a “clone consistency-defect”, which can adversely impact software 
maintainability
.
Objective:
Predicting the need for “clone consistent changes” after successful clone-creating or clone-changing operations can help developers maintain clone changes effectively, avoid consistency-defects and reduce maintenance cost.
Method:
In this work, we use several sets of attributes in two scenarios of clone operations (
clone-creating
 and 
clone-changing
), and conduct an empirical study on five different machine-learning methods to assess each of their clone consistency predictability — whether any one of the clone operations will 
require
 or be 
free of
 clone consistency maintenance in future.
Results:
We perform our experiments on 
eight
 open-source projects. Our study shows that such predictions can be reasonably effective both for clone-creating and changing operating instances. We also investigate the use of 
five
 different machine-learning methods for predictions and show that our selected features are effective in predicting the needs of consistency-maintenance across all selected machine-learning methods.
Conclusion:
The empirical study conducted here demonstrates that the models developed by different machine-learning methods with the specified sets of attributes have the ability to perform clone-consistency prediction.",Information and Software Technology,18 Mar 2025,3.0,"While predicting clone consistency changes is valuable for software maintenance, the practical impact on early-stage ventures might be limited compared to cybersecurity-focused solutions."
https://www.sciencedirect.com/science/article/pii/S0950584921000537,From a Scrum Reference Ontology to the Integration of Applications for Data-Driven Software Development,August 2021,Not Found,Paulo Sérgio=Santos Júnior: paulo.junior@ifes.edu.br; Monalessa Perini=Barcellos: monalessa@inf.ufes.edu.br; Ricardo de Almeida=Falbo: falbo@inf.ufes.br; João Paulo A.=Almeida: jpalmeida@ieee.org,"Abstract
Context
Organizations often use different applications to support the Scrum process, including project management tools, source repository and quality assessment tools. These applications store useful data for decision-making. However, data items often remain spread in different applications, each of which adopt different data and behavioral models, posing a barrier for integrated data usage. As a consequence, data-driven decisions in agile development are uncommon, missing valuable opportunities for informed decision making.
Objective
Considering the need to address semantic issues to properly integrate applications that support the agile development process, we aim to provide a common and comprehensive conceptualization about Scrum in the software development context and apply this conceptualization to support application integration.
Method
We have developed the Scrum Reference Ontology (SRO) and used it to semantically integrate Azure DevOps and Clockify.
Results
SRO served as a reference model to build software artifacts in a semantic integration architecture that enables applications to automatically share, exchange and combine data and services. The integrated solution was used in the software development unit of a Brazilian government agency. Results demonstrate that the integrated solution contributed to improving estimates, provided data that helped allocate teams, manage team productivity and project performance, and enabled to identify and fix problems in the Scrum process execution.
Conclusions
SRO can serve as an interlingua for application integration in the context of Scrum-process support. By capturing the conceptualization underlying Scrum, the reference ontology can address semantic conflicts and thereby support the development of integrated data-driven solutions for decision making.",Information and Software Technology,18 Mar 2025,7.0,"The Scrum Reference Ontology provides a practical solution for integrating data from different applications, enabling informed decision-making, which can be highly valuable for startups seeking efficient and data-driven project management."
https://www.sciencedirect.com/science/article/pii/S0950584921000677,Insights on the relationship between decision-making style and personality in software engineering,August 2021,Not Found,Fabiana=Mendes: fabiana.mendes@oulu.fi; Emília=Mendes: emilia.mendes@bth.se; Norsaremah=Salleh: norsaremah@iium.edu.my; Markku=Oivo: markku.oivo@oulu.fi,"Abstract
Context:
Software development involves many activities, and 
decision making
 is an essential one. Various factors can impact a decision-making process, and by understanding such factors, one can improve the process. Since people are the ones making decisions, some human-related aspects are amongst those influencing factors. One such aspect is the decision maker’s personality.
Objective:
This research investigates the relationship between decision-making style and personality within the context of software project development.
Method:
We conducted a survey in a population of Brazilian software engineers to gather data on their personality and decision-making style.
Results:
Data from 63 participants was gathered and resulted in the identification of seven statistically significant correlations between decision-making style and personality (personality factor and personality facets). Furthermore, we built a regression model in which decision-making style (DMS) was the response variable and personality factors the independent variables. The 
backward elimination
 procedure selected only agreeableness to explain 4.2% of DMS variation. The model accuracy was evaluated and deemed good enough. Regarding the moderation effect of demographic variables (age, educational level, experience, and role) on the relationship between DMS and Agreeableness, the analysis showed that only software engineers’ role has such effect.
Conclusion:
This paper contributes toward understanding the relationship between DMS and personality. Results show that the personality variable agreeableness can explain the variation in decision-making style. Furthermore, someone’s role in a software development project can impact the 
strength
 of the relationship between DMS and agreeableness.",Information and Software Technology,18 Mar 2025,6.0,The research on the relationship between decision-making style and personality within software project development can provide valuable insights for startups regarding team dynamics and project management.
https://www.sciencedirect.com/science/article/pii/S0950584921000719,A methodology to automatically translate user requirements into visualizations: Experimental validation,August 2021,"Data visualization, Big data analytics, Model-driven development, Requirements engineering, Experimental validation",Ana=Lavalle: alavalle@dlsi.ua.es; Alejandro=Maté: Not Found; Juan=Trujillo: Not Found; Miguel A.=Teruel: Not Found; Stefano=Rizzi: Not Found,"Abstract
Context:
Information visualization
 is paramount for the analysis of Big Data. The volume of data requiring interpretation is continuously growing. However, users are usually not experts in information visualization. Thus, defining the visualization that best suits a determined context is a very challenging task for them. Moreover, it is often the case that users do not have a clear idea of what objectives they are building the visualizations for. Consequently, it is possible that graphics are misinterpreted, making wrong decisions that lead to missed opportunities. One of the underlying problems in this process is the lack of methodologies and tools that non-expert users in visualizations can use to define their objectives and visualizations.
Objective:
The main objectives of this paper are to (i) enable non-expert users in data visualization to communicate their analytical needs with little effort, (ii) generate the visualizations that best fit their requirements, and (iii) evaluate the impact of our proposal with reference to a 
case study
, describing an experiment with 97 non-expert users in data visualization.
Methods:
We propose a methodology that collects user requirements and semi-automatically creates suitable visualizations. Our proposal covers the whole process, from the definition of requirements to the implementation of visualizations. The methodology has been tested with several groups to measure its effectiveness and perceived usefulness.
Results:
The experiments increase our confidence about the utility of our methodology. It significantly improves over the case when users face the same problem manually. Specifically: (i) users are allowed to cover more analytical questions, (ii) the visualizations produced are more effective, and (iii) the overall satisfaction of the users is larger.
Conclusion:
By following our proposal, non-expert users will be able to more effectively express their analytical needs and obtain the set of visualizations that best suits their goals.",Information and Software Technology,18 Mar 2025,9.0,The proposal to enable non-expert users in data visualization to express their needs effectively and obtain suitable visualizations can significantly impact startups by helping them make informed decisions based on data analysis.
https://www.sciencedirect.com/science/article/pii/S0950584921000549,"Motivations, benefits, and issues for adopting Micro-Frontends: A Multivocal Literature Review",August 2021,"Micro-Frontends, Microservices, Web front-end development, Software architectures, Multivocal Literature Review",Severi=Peltonen: severi.peltonen@gmail.com; Luca=Mezzalira: luca.mezzalira@dazn.com; Davide=Taibi: davide.taibi@tuni.fi,"Abstract
Context:
Micro-Frontends are increasing in popularity, being adopted by several large companies, such as DAZN, Ikea, Starbucks and may others. Micro-Frontends enable splitting of monolithic frontends into independent and smaller micro applications. However, many companies are still hesitant to adopt Micro-Frontends, due to the lack of knowledge concerning their benefits. Additionally, provided online documentation is often times perplexed and contradictory.
Objective:
The goal of this work is to map the existing knowledge on Micro-Frontends, by understanding the motivations of companies when adopting such applications as well as possible benefits and issues.
Method:
For this purpose, we surveyed the academic and grey literature by means of the Multivocal Literature Review process, analysing 173 sources, of which 43 reported motivations, benefits and issues.
Results:
The results show that existing architectural options to build web applications are cumbersome if the application and development team grows, and if multiple teams need to develop the same frontend application. In such cases, companies adopted Micro-Frontends to increase team independence and to reduce the overall complexity of the frontend. The application of the Micro-Frontend, confirmed the expected benefits, and Micro-Frontends resulted to provide the same benefits as 
microservices
 on the back end side, combining the development team into a fully cross-functional development team that can scale processes when needed. However, Micro-Frontends also showed some issues, such as the increased payload size of the application, increased code duplication and coupling between teams, and monitoring complexity.
Conclusions:
Micro-Frontends allow companies to scale development according to business needs in the same way microservices do with the back end side. In addition, Micro-Frontends have a lot of overhead and require careful planning if an advantage is achieved by using Micro-Frontends. Further research is needed to carefully investigate this new hype, by helping practitioners to understand how to use Micro-Frontends as well as understand in which contexts they are the most beneficial.",Information and Software Technology,18 Mar 2025,7.0,The exploration of Micro-Frontends and their benefits for development teams can be valuable for startups looking to scale their applications and understand the implications of adopting such architectural patterns.
https://www.sciencedirect.com/science/article/pii/S0950584921000549,"Motivations, benefits, and issues for adopting Micro-Frontends: A Multivocal Literature Review",August 2021,"Micro-Frontends, Microservices, Web front-end development, Software architectures, Multivocal Literature Review",Severi=Peltonen: severi.peltonen@gmail.com; Luca=Mezzalira: luca.mezzalira@dazn.com; Davide=Taibi: davide.taibi@tuni.fi,"Abstract
Context:
Micro-Frontends are increasing in popularity, being adopted by several large companies, such as DAZN, Ikea, Starbucks and may others. Micro-Frontends enable splitting of monolithic frontends into independent and smaller micro applications. However, many companies are still hesitant to adopt Micro-Frontends, due to the lack of knowledge concerning their benefits. Additionally, provided online documentation is often times perplexed and contradictory.
Objective:
The goal of this work is to map the existing knowledge on Micro-Frontends, by understanding the motivations of companies when adopting such applications as well as possible benefits and issues.
Method:
For this purpose, we surveyed the academic and grey literature by means of the Multivocal Literature Review process, analysing 173 sources, of which 43 reported motivations, benefits and issues.
Results:
The results show that existing architectural options to build web applications are cumbersome if the application and development team grows, and if multiple teams need to develop the same frontend application. In such cases, companies adopted Micro-Frontends to increase team independence and to reduce the overall complexity of the frontend. The application of the Micro-Frontend, confirmed the expected benefits, and Micro-Frontends resulted to provide the same benefits as 
microservices
 on the back end side, combining the development team into a fully cross-functional development team that can scale processes when needed. However, Micro-Frontends also showed some issues, such as the increased payload size of the application, increased code duplication and coupling between teams, and monitoring complexity.
Conclusions:
Micro-Frontends allow companies to scale development according to business needs in the same way microservices do with the back end side. In addition, Micro-Frontends have a lot of overhead and require careful planning if an advantage is achieved by using Micro-Frontends. Further research is needed to carefully investigate this new hype, by helping practitioners to understand how to use Micro-Frontends as well as understand in which contexts they are the most beneficial.",Information and Software Technology,18 Mar 2025,7.0,"Similar to Abstract 168, the research on Micro-Frontends and their impact can provide practical value for startups considering or already using this technology to improve development processes and scalability."
https://www.sciencedirect.com/science/article/pii/S0950584921000574,RLTCP: A reinforcement learning approach to prioritizing automated user interface tests,August 2021,Not Found,Vu=Nguyen: nvu@fit.hcmus.edu.vn; Bach=Le: ldbach@apcs.vn,"Abstract
Context:
User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution.
Objective:
This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test.
Methods:
We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines 
Reinforcement Learning
 (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant 
historical data
, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the 
RL system
 can gain more insights into individual test cases.
Results:
We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods.
Conclusions:
The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history.",Information and Software Technology,18 Mar 2025,8.0,"The novel test prioritization method for user interface testing using Reinforcement Learning and coverage graphs can benefit startups by optimizing testing processes, improving fault detection, and reducing time spent on testing."
https://www.sciencedirect.com/science/article/pii/S0950584921000458,Continuous Systems and Software Engineering for Industry 4.0: A disruptive view,July 2021,Not Found,Elisa Yumi=Nakagawa: elisa@icmc.usp.br; Pablo Oliveira=Antonino: pablo.antonino@iese.fraunhofer.de; Frank=Schnicke: Frank.Schnicke@iese.fraunhofer.de; Peter=Liggesmeyer: peter.liggesmeyer@iese.fraunhofer.de,"Abstract
Context:
Industry 4.0
 has substantially changed the manufacturing processes, leading to smart factories with full 
digitalization
, intelligence, and dynamic production. The need for rigorous and continuous development of highly networked software-intensive 
Industry 4.0
 systems entails great challenges. Hence, Industry 4.0 requires new ways to develop, operate, and evolve these systems accordingly.
Objective:
We introduce the view of 
Continuous Systems
 and 
Software Engineering
 for Industry 4.0 (CSSE I4.0).
Method:
Based on our research and industrial projects, we propose this novel view and its core elements, including continuous twinning, which is also introduced first in this paper. We also discuss the existing industrial engagement and research that could leverage this view for practical application.
Results:
There are still several open issues, so we highlight the most urgent perspectives for future work.
Conclusions:
A disruptive view on how to engineer Industry 4.0 systems must be established to pave the way for the realization of the fourth industrial revolution.",Information and Software Technology,18 Mar 2025,7.0,Addresses a significant issue in Industry 4.0 systems development with practical applications and future perspectives.
https://www.sciencedirect.com/science/article/pii/S0950584921000458,Continuous Systems and Software Engineering for Industry 4.0: A disruptive view,July 2021,Not Found,Elisa Yumi=Nakagawa: elisa@icmc.usp.br; Pablo Oliveira=Antonino: pablo.antonino@iese.fraunhofer.de; Frank=Schnicke: Frank.Schnicke@iese.fraunhofer.de; Peter=Liggesmeyer: peter.liggesmeyer@iese.fraunhofer.de,"Abstract
Context:
Industry 4.0
 has substantially changed the manufacturing processes, leading to smart factories with full 
digitalization
, intelligence, and dynamic production. The need for rigorous and continuous development of highly networked software-intensive 
Industry 4.0
 systems entails great challenges. Hence, Industry 4.0 requires new ways to develop, operate, and evolve these systems accordingly.
Objective:
We introduce the view of 
Continuous Systems
 and 
Software Engineering
 for Industry 4.0 (CSSE I4.0).
Method:
Based on our research and industrial projects, we propose this novel view and its core elements, including continuous twinning, which is also introduced first in this paper. We also discuss the existing industrial engagement and research that could leverage this view for practical application.
Results:
There are still several open issues, so we highlight the most urgent perspectives for future work.
Conclusions:
A disruptive view on how to engineer Industry 4.0 systems must be established to pave the way for the realization of the fourth industrial revolution.",Information and Software Technology,18 Mar 2025,7.0,Addresses a significant issue in Industry 4.0 systems development with practical applications and future perspectives.
https://www.sciencedirect.com/science/article/pii/S0950584921000501,Test case generation for agent-based models: A systematic literature review,July 2021,Not Found,Andrew G.=Clark: agclark2@sheffield.ac.uk; Neil=Walkinshaw: n.walkinshaw@sheffield.ac.uk; Robert M.=Hierons: r.hierons@sheffield.ac.uk,"Abstract
Context:
Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a 
software fault
 may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential.
Objective:
Our objective is to summarise the state-of-the-art techniques for test case generation in agent-based models and identify future research directions.
Method:
We have conducted a systematic literature review in which we pose five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test?
Results:
Out of the 464 
initial search results
, we identified 24 primary publications. Based on these primary publications, we formed a taxonomy to summarise the state-of-the-art techniques for test case generation in agent-based models. Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Furthermore, the majority of techniques cannot test non-functional requirements or “soft goals”.
Conclusions:
This paper reports insights into the key developments and open challenges concerning test case generation in agent-based models that may be of interest to both researchers and practitioners. In particular, we identify the need for test case generation techniques that focus on societal and non-functional behaviour, and a more thorough evaluation using realistic 
case studies
 that feature challenging properties associated with a typical agent-based model.",Information and Software Technology,18 Mar 2025,5.0,"Although important for agent-based models, the impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000379,On the generalizability of Neural Program Models with respect to semantic-preserving program transformations,July 2021,Not Found,Md Rafiqul Islam=Rabin: mrabin@uh.edu; Nghi D.Q.=Bui: Not Found; Ke=Wang: Not Found; Yijun=Yu: Not Found; Lingxiao=Jiang: Not Found; Mohammad Amin=Alipour: Not Found,"Abstract
Context:
With the prevalence of publicly available 
source code
 repositories to train 
deep neural network
 models, neural program models can do well in 
source code analysis
 tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen 
source code
 is largely unknown.
Objective:
Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the 
generalizability
 of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures.
Method:
We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art 
neural network
 models for code, namely 
code2vec
, 
code2seq
, and 
GGNN
, to build nine such neural program models for evaluation.
Results:
Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and 
control dependencies
 in programs generalize better than neural program models based only on 
abstract syntax trees
 (ASTs). On the positive side, we observe that as the size of the training dataset grows and diversifies the 
generalizability
 of correct predictions produced by the neural program models can be improved too.
Conclusion:
Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement.",Information and Software Technology,18 Mar 2025,6.0,"Relevant for neural program models, but the impact on startups may be moderate compared to the Industry 4.0 focus."
https://www.sciencedirect.com/science/article/pii/S0950584921000379,On the generalizability of Neural Program Models with respect to semantic-preserving program transformations,July 2021,Not Found,Md Rafiqul Islam=Rabin: mrabin@uh.edu; Nghi D.Q.=Bui: Not Found; Ke=Wang: Not Found; Yijun=Yu: Not Found; Lingxiao=Jiang: Not Found; Mohammad Amin=Alipour: Not Found,"Abstract
Context:
With the prevalence of publicly available 
source code
 repositories to train 
deep neural network
 models, neural program models can do well in 
source code analysis
 tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen 
source code
 is largely unknown.
Objective:
Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the 
generalizability
 of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures.
Method:
We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art 
neural network
 models for code, namely 
code2vec
, 
code2seq
, and 
GGNN
, to build nine such neural program models for evaluation.
Results:
Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and 
control dependencies
 in programs generalize better than neural program models based only on 
abstract syntax trees
 (ASTs). On the positive side, we observe that as the size of the training dataset grows and diversifies the 
generalizability
 of correct predictions produced by the neural program models can be improved too.
Conclusion:
Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement.",Information and Software Technology,18 Mar 2025,6.0,"Relevant for neural program models, but the impact on startups may be moderate compared to the Industry 4.0 focus."
https://www.sciencedirect.com/science/article/pii/S0950584921000422,iMER: Iterative process of entity relationship and business process model extraction from the requirements,July 2021,Not Found,Muhammad=Javed: m.javed@uon.edu.au; Yuqing=Lin: yuqing.lin@newcastle.edu.au,"Abstract
Context
Extracting conceptual models, e.g., 
entity relationship model
 or Business Process model, from software requirement document is an essential task in the 
software development life cycle
. Business process model presents a clear picture of required system's functionality. Operations in business process model together with the data entity consumed, help the software developers to understand the database design and operations to be implemented. Researchers have been aiming at automatic extraction of these artefacts from the requirement document.
Objective
In this paper, we present an automated approach to extract the entity relationship and business process models from requirements, which are possibly in different formats such as general requirements, use case specification and user stories. Our approach is based on the efficient 
natural language processing
 techniques.
Method
It is an iterative approach of Models Extraction from the Requirements (iMER). iMER has multiple iterations where each iteration is to address a sub-problem. In the first iteration, iMER extracts the data entities and attributes. Second iteration is to find the relationships between data entities, while extracting 
cardinalities
 is in the third step. Business process model is generated in the fourth iteration, containing the external (actors’) and internal (system's) operations.
Evaluation
To evaluate the performance and accuracy of iMER, experiments are conducted on various formats of the requirement documents. Additionally, we have also evaluated our approaches using the requirement documents which been modified by shuffling the sentences and by merging with other requirements. Comparative study is also performed. The preliminary results show a noticeable improvement.
Conclusion
The iMER is an efficient automated iterative approach that is able to extract the conceptual models from the various formats of requirements.",Information and Software Technology,18 Mar 2025,7.0,"The automated extraction of entity relationship and business process models from requirements can greatly benefit software developers, leading to improved database design and system understanding for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000495,Augmenting commit classification by using fine-grained source code changes and a pre-trained deep neural language model,July 2021,Not Found,Lobna=Ghadhab: lobnaghadhab@gmail.com; Ilyes=Jenhani: ijenhani@pmu.edu.sa; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Montassar=Ben Messaoud: montassar.benmassaoud@isgs.u-sousse.tn,"Abstract
Context:
Analyzing software maintenance activities is very helpful in ensuring cost-effective evolution and development activities. The categorization of commits into maintenance tasks supports practitioners in making decisions about 
resource allocation
 and managing technical debt.
Objective:
In this paper, we propose to use a pre-trained language neural model, namely BERT (Bidirectional Encoder Representations from Transformers) for the classification of commits into three categories of maintenance tasks — corrective, perfective and adaptive. The proposed commit 
classification approach
 will help the classifier better understand the context of each word in the commit message.
Methods:
We built a balanced dataset of 1793 labeled commits that we collected from publicly available datasets. We used several popular code change distillers to extract fine-grained code changes that we have incorporated into our dataset as additional features to BERT’s word representation features. In our study, a 
deep neural network
 (DNN) classifier has been used as an additional layer to fine-tune the BERT model on the task of commit classification. Several models have been evaluated to come up with a deep analysis of the impact of code changes on the classification performance of each commit category.
Results and conclusions:
Experimental results have shown that the 
DNN model
 trained on BERT’s word representations and Fixminer code changes (DNN@BERT+Fix_cc) provided the best performance and achieved 79.66% accuracy and a macro-average f1 score of 0.8. Comparison with the state-of-the-art model that combines keywords and code changes (RF@KW+CD_cc) has shown that our model achieved approximately 8% improvement in accuracy. Results have also shown that a 
DNN model
 using only BERT’s word representation features achieved an improvement of 5% in accuracy compared to the RF@KW+CD_cc model.",Information and Software Technology,18 Mar 2025,9.0,"The use of a pre-trained language neural model for classifying commits into maintenance tasks shows a high level of accuracy and improvement over state-of-the-art models, which can greatly assist startups in efficient resource allocation and technical debt management."
https://www.sciencedirect.com/science/article/pii/S0950584921000525,MEGDroid: A model-driven event generation framework for dynamic android malware analysis,July 2021,Not Found,Hayyan=Hasan: Not Found; Behrouz=Tork Ladani: Ladani@eng.ui.ac.ir; Bahman=Zamani: Not Found,"Abstract
Context
The tremendous growth of 
Android malware
 in recent years is a strong motivation for the vast endeavor in detection and analysis of 
malware
 apps. A prominent approach for this purpose is dynamic analysis in which providing complex interactions with the samples under analysis is a need. Event generation tools are almost used to provide such interactions, but they have deficiencies for effective 
malware analysis
. For example, anti-static and anti-dynamic analysis techniques employed by the 
malware
 prevent event generators to extract sufficient information for generating appropriate events. As a result, they fail to trigger 
malicious payloads
 or obtain high code coverage in most cases.
Objective
In this paper, we aim to present a new framework to improve the event generation process for dynamic analysis of 
Android malware
.
Method
We propose MEGDroid, a 
Model Driven Engineering
 (MDE) framework in which malware-related information is automatically extracted and represented as a domain-specific model. This model, then is used to generate appropriate events for 
malware analysis
 using model-to-model and model-to-code transformations. The proposed model-driven artifacts also provide required facilities to put the 
human in the loop
 for properly taking his/her knowledge into account.
Results
The proposed framework has been realized as an Eclipse plugin and we performed extensive practical analysis on a set of 
malware samples
 selected from the AMD dataset. The experimental results showed that MEGDroid considerably increases the number of triggered 
malicious payloads
 as well as the execution code coverage compared with Monkey and DroidBot, as two state of the art general-purpose and malware specific event generators respectively.
Conclusion
The proposed MDE approach, enhances the event generation process through both automatic event generation and analyzer user involvement who can efficiently direct the process to increase the effectiveness of the generated events considering small amount of information that is extractable from the malware code.",Information and Software Technology,18 Mar 2025,8.0,"The proposed framework for improving event generation for dynamic analysis of Android malware shows significant improvement in triggering malicious payloads and code coverage, which can be crucial for startups in enhancing cybersecurity measures."
https://www.sciencedirect.com/science/article/pii/S0950584921000483,Multi-objective software performance optimisation at the architecture level using randomised search rules,July 2021,Not Found,Youcong=Ni: youcongni@foxmail.com; Xin=Du: xindu79@126.com; Peng=Ye: whuyp@126.com; Leandro L.=Minku: Not Found; Mark=Harman: Not Found; Ruliang=Xiao: Not Found,"Abstract
Architecture-based software 
performance optimisation
 can help to find potential performance problems and mitigate their negative effects at an early stage. To automate this optimisation process, rule-based and metaheuristic-based 
performance optimisation
 methods have been proposed. However, existing rule-based methods explore a limited 
search space
, potentially excluding optimal or near-optimal solutions. Most of current metaheuristic-based methods ignore existing practical knowledge of 
performance improvement
, and lead to solutions that are not easily explicable to humans. To address these problems, we propose a novel approach for performance optimisation at the software architecture level named Multiobjective performance Optimisation based on 
Randomised search
 rulEs (MORE). First, we 
design randomised
 search rules (MORE-R) to provide explanation without parameters while benefiting from existing practical knowledge of 
performance improvement
. Second, based on all possible 
composite applications
 of MORE-R, an explicable multi-objective 
optimisation problem
 (MORE-P) is defined to enlarge 
search space
 and enable solutions explicable to architectural stakeholder. Third, a multi-objective evolutionary algorithm (MORE-EA) with an introduced do-nothing rule, innovative encoding and repair mechanism is designed to effectively solve MORE-P. The experiments show that MORE is able to achieve more explicable and higher quality solutions than two state-of-the-art techniques. They also demonstrate the benefits of integrating search-based 
software engineering
 approaches with practical knowledge.",Information and Software Technology,18 Mar 2025,10.0,"The MORE approach for performance optimization at the software architecture level offers explicable and high-quality solutions, integrating practical knowledge with evolutionary algorithms, which can greatly benefit early-stage ventures in mitigating performance problems and improving software architecture."
https://www.sciencedirect.com/science/article/pii/S0950584921000513,Analyzing the sensitivity of multi-objective software architecture refactoring to configuration characteristics,July 2021,"Search-based software engineering, Automated refactoring, Software quality, Multi-objective optimization, Genetic algorithms, Software performance engineering, Software performance antipatterns",Vittorio=Cortellessa: vittorio.cortellessa@univaq.it; Daniele=Di Pompeo: daniele.dipompeo@univaq.it,"Abstract
Context:
Software architecture refactoring can be induced by multiple reasons, such as satisfying new functional requirements or improving non-functional properties. Multi-objective optimization approaches have been widely used in the last few years to introduce automation in the 
refactoring process
, and they have revealed their potential especially when quantifiable attributes are targeted. However, the effectiveness of such approaches can be heavily affected by configuration characteristics of the 
optimization algorithm
, such as the composition of solutions.
Objective:
In this paper, we analyze the behavior of 
, which is an Evolutionary Approach for Software archItecturE Refactoring, while varying its configuration characteristics, with the objective of studying its potential to find near-optimal solutions under different configurations.
Method:
In particular, we use two different 
solution space
 inspection algorithms (i.e., 
 and 
) while varying the genome length and the solution composition.
Results:
We have conducted our experiments on a specific 
case study
 modeled in 
Æ
 ADL, on which we have shown the ability of 
 to identify performance-critical elements in the software architecture where refactoring is worth to be applied. Beside this, from the comparison of multi-objective algorithms, 
 has revealed to outperform 
 in most of cases, although the latter one is able to induce more diversity in the proposed solutions.
Conclusion:
Our results show that the 
 thoroughly automated process for software architecture refactoring allows to identify configuration contexts of the 
evolutionary algorithm
 in which multi-objective optimization more effectively finds near-optimal Pareto solutions.",Information and Software Technology,18 Mar 2025,6.0,"The analysis of the Evolutionary Approach for Software archItecturE Refactoring shows potential in identifying performance-critical elements, but the impact on startups may vary depending on the configuration characteristics."
https://www.sciencedirect.com/science/article/pii/S0950584921000434,A field experiment on trialsourcing and the effect of contract types on outsourced software development,June 2021,Not Found,Magne=Jørgensen: magnej@simula.no; Jon=Grov: jon.grov@langs.no,"ABSTRACT
Context
To ensure the success of software projects, it is essential to select skilled developers and to use suitable work contracts.
Objective
This study tests two hypotheses: (i) the use of work-sample testing (trialsourcing) improves the selection of skilled software developers; and (ii) the use of contracts based on hourly payment leads to better software project outcomes than fixed-price contracts.
Method
Fifty-seven software freelancers with relevant experience and good evaluation scores from previous clients were invited to complete a two-hour long trialsourcing task to qualify for a software development project. Thirty-six developers completed the trialsourcing task with acceptable performance, and, based on a 
stratified random
 allocation process, were asked to give a proposal based on an hourly payment or a fixed-price contract. Eight hourly payment-based and eight fixed-priced proposals were accepted. The process and product characteristics of the completion of these 16 projects were collected and analysed.
Results and Conclusion
While the use of trialsourcing may have prevented the selection of developers with insufficient skills, the performance on the trialsourcing task of the selected developers did, to a large extent, not predict their performance on the projects. The use of hourly payments seems to have led to lower costs than fixed-price contracts, but not to improved processes or products. We plan to follow up these results with research on how to design more skill-predictive trialsourcing tasks, and when and why different project contexts give different contract consequences.",Information and Software Technology,18 Mar 2025,5.0,"While the study addresses the selection of skilled developers and work contracts in software projects, the impact on European early-stage ventures is limited as it focuses more on individual freelancers. The results suggest areas for further research but do not provide immediate practical value for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000276,A decentralized blockchain oriented framework for automated bug assignment,June 2021,Not Found,Chetna=Gupta: chetna.gupta@ubi.pt; Mário M=Freire: mario@di.ubi.pt,"ABSTRACT
Context
In large software projects bug fixing is a time-bound, time-consuming, mind-numbing, and challenging task that requires a collaborative effort with multiple developers, separated geographically.
Objective
The objective of this paper is to propose a decentralized automated bug assignment approach to improve the quality of bug assignments to minimize backlogs and overall bug fixing time.
Method
To the best of our knowledge, the literature lacks in studies focusing on how to increase software developer's motivation for efficient bug resolution. It is a novel decentralized 
blockchain
 oriented, transparent auction-based bug assignment framework which uses incentive mechanism as reward and penalty backed by 
blockchain
 technology using 
smart contracts
 for developers motivation. The process allows individual developers to select 
bug reports
, matching their preferences and schedule for which they shall we able to provide 
robust solutions
 with reduced overhead in cost and time of bug fixing.
Results
Results of experimentation and surveys conclude that the proposed method is transparent and effective in bug assignment minimizing overall bug fixing time. The low cost of contract execution demonstrates that it can be used quantitatively and without ambiguity.
Conclusion
The work presented is novel to improve (i) bug assignment (ii) allow individual developers to choose what they like to provide 
robust solutions
 (iii) handles two major issues of differentiating between active and inactive developers and confusion over the assignment of bugs (iv) will further reduce bug-fixing delays and will prevent reassignment problem.",Information and Software Technology,18 Mar 2025,7.0,The proposed decentralized automated bug assignment approach using blockchain technology could have a significant impact on improving bug resolution efficiency for European early-stage ventures. The transparent and effective method with low cost implications could be valuable for startups with limited resources.
https://www.sciencedirect.com/science/article/pii/S0950584921000252,Game industry problems: An extensive analysis of the gray literature,June 2021,Not Found,Cristiano=Politowski: c_polito@encs.concordia.ca; Fabio=Petrillo: fabio@petrillo.com; Gabriel C.=Ullmann: gabriel.cavalheiro@sou.unijui.edu.br; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@concordia.ca,"Abstract
Context:
Given its competitiveness, the video-game 
industry
 has a closed-source culture. Hence, little is known about the problems faced by game developers. However, game developers do 
share information
 about their game projects through postmortems, which describe informally what happened during the projects.
Objective:
The software-engineering research community and game developers would benefit from a state of the problems of the 
video game
 
industry
, in particular the problems faced by game developers, their evolution in time, and their root causes. This state of the practice would allow researchers and practitioners to work towards solving these problems.
Method:
We analyzed 200 postmortems from 1997 to 2019, resulting in 927 problems divided into 20 types. Through our analysis, we described the overall landscape of game industry problems in the past 23 years and how these problems evolved over the years. We also give details on the most common problems, their root causes, and possible solutions. We finally discuss suggestions for future projects.
Results:
We observe that (1) the game industry suffers from management and production problems in the same proportion; (2) management problems decreased over the years, giving space to 
business problems
, while production problems remained constant; (3a) technical and game design problems are decreasing over the years, the latter only after the last decade; (3b) problems related to the team increase over the last decade; (3c) marketing problems are the ones that had the biggest increase over the 23 years compared to other problem types; (4) finally, the majority of the main root causes are related to people, not technologies. Conclusions: In this paper, we provide a state of the practice for researchers to understand and study video-game development problems. We also offer suggestions to help practitioners to avoid the most common problems in future projects.",Information and Software Technology,18 Mar 2025,8.0,"The analysis of postmortems in the video game industry provides valuable insights into the problems faced by game developers over the years. Understanding the evolving landscape of industry challenges can guide European startups in the gaming sector to anticipate and address common issues, enhancing project outcomes."
https://www.sciencedirect.com/science/article/pii/S095058492100029X,Efilter: An effective fault localization based on information entropy with unlabelled test cases,June 2021,Not Found,Yan=Xiaobo: yxbbuaa@buaa.edu.cn; Liu=Bin: liubin@buaa.edu.cn; Wang=Shihai: wangshihai@buaa.edu.cn; An=Dong: ad14011099@buaa.edu.cn; Zhu=Feng: fenix_zh@126.com; Yang=Yelin: ylyang05@163.com,"Abstract
Context:
Automatic 
fault localization
 is essential to intelligent software system. Most 
fault localization
 techniques assume the test oracle is perfect before debugging, which is hard to exist in practice. In fact, the test suite would contain a number of unlabelled test cases which have been proved to be useful in fault localization. However, due to the execution diversity, not all unlabelled test cases are suitable for fault localization. Selecting inappropriate unlabelled test cases can even weaken the fault localization efficiency.
Objective:
To solve the problem of filtering unlabelled test cases, this work aims to construct a feasible framework to select suitable unlabelled test cases for better fault localization.
Method:
To address this issue, an entropy-based framework Efilter is constructed to filter unlabelled test cases. In Efilter, a Statement-based entropy and Testsuite-based entropy are constructed to measure the localization uncertainty of given test suite. The unlabelled test case with less Statement-based entropy or Testsuite-based entropy compared with its threshold would be selected. Further, the feature integration strategies for both Statement-based entropy and Testsuite-based entropy are given to calculate the 
suspiciousness
 of statements.
Results:
The Efilter efficiency is evaluated across 6 open-source programs and 3 spectrum-based fault localizations. The results reveal that Efilter can improve fault localization efficiency by 18.8% and 16.5% with the Statement-based entropy and the Testsuite-based entropy respectively compared with the strategy without Efilter from the perspective of 
EXAM
 score on average.
Conclusion:
Our results indicate that the Efilter with both the Statement-based entropy and the Testsuite-based entropy can improve the fault localization in the scenario lack of test oracles, serving as an enhancement for fault localization in practice.",Information and Software Technology,18 Mar 2025,6.0,"The framework for selecting suitable unlabelled test cases for fault localization presents a promising approach to improve software system intelligence. While the results show efficiency improvements, the direct impact on European early-stage ventures may not be immediate but could be valuable for startups focusing on software quality."
https://www.sciencedirect.com/science/article/pii/S0950584921000446,A process for analysing the energy efficiency of software,June 2021,Not Found,Javier=Mancebo: Javier.Mancebo@uclm.es; Félix=García: Felix.Garcia@uclm.es; Coral=Calero: Coral.Calero@uclm.es,"Abstract
Context
It is essential to be aware of the energy efficiency of software when it is running, so that it can be improved; to that end, energy consumption measurements need to be carried out. To ensure that these measurements are as reliable as possible, it is recommended that a well-defined process be followed.
Objective
To identify how the process for analysing the energy efficiency of software should be carried out (including the definition of the software to be evaluated, the selection of measuring instruments, the analysis and the presentation of results, etc.), in an endeavour to improve the reliability and consistency of the information obtained regarding energy efficiency.
Method
An analysis of related work was carried out, to extract some good practices in measuring energy consumption; based on our experience, a process to analyse the energy efficiency of the software has been defined.
Results
We have defined a process to analyse the energy efficiency of the software. We describe this process through a set of phases that covers all the steps needed to carry out a correct analysis of the energy consumption of the software executed. Moreover, this process was validated with two different studies using different measurement instruments (one with a hardware-based approach and one with a software-based approach) to ensure its applicability to all types of studies with software energy consumption measurement.
Conclusion
The steps to be followed to analyse the energy efficiency of the software need to be established. A new process has hence been defined to improve the reliability and consistency of the measurements. Furthermore, this process facilitates the replicability and comparison of the studies carried out.",Information and Software Technology,18 Mar 2025,7.0,"The defined process for analyzing the energy efficiency of software provides practical guidance for conducting reliable energy consumption measurements. European early-stage ventures, including startups, can benefit from improved energy efficiency analysis to optimize software performance and resource usage."
https://www.sciencedirect.com/science/article/pii/S0950584921000367,Controlled experimentation in continuous experimentation: Knowledge and challenges,June 2021,"Continuous experimentation, Online controlled experiments, A/B testing, Systematic literature review",Florian=Auer: florian.auer@uibk.ac.at; Rasmus=Ros: rasmus.ros@cs.lth.se; Lukas=Kaltenbrunner: lukas.kaltenbrunner@uibk.ac.at; Per=Runeson: per.runeson@cs.lth.se,"Abstract
Context:
Continuous experimentation and A/B testing is an established industry practice that has been researched for more than 10 years. Our aim is to synthesize the conducted research.
Objective:
We wanted to find the core constituents of a framework for continuous experimentation and the solutions that are applied within the field. Finally, we were interested in the challenges and benefits reported of continuous experimentation.
Methods:
We applied forward snowballing on a known set of papers and identified a total of 128 relevant papers. Based on this set of papers we performed two qualitative narrative syntheses and a thematic synthesis to answer the research questions.
Results:
The framework constituents for continuous experimentation include experimentation processes as well as supportive technical and organizational infrastructure. The solutions found in the literature were synthesized to nine themes, e.g. experiment design, automated experiments, or metric specification. Concerning the challenges of continuous experimentation, the analysis identified cultural, organizational, business, technical, statistical, ethical, and domain-specific challenges. Further, the study concludes that the benefits of experimentation are mostly implicit in the studies.
Conclusion:
The research on continuous experimentation has yielded a large body of knowledge on experimentation. The synthesis of published research presented within include recommended infrastructure and experimentation process models, guidelines to mitigate the identified challenges, and what problems the various published solutions solve.",Information and Software Technology,18 Mar 2025,7.0,The synthesis of research on continuous experimentation provides valuable insights and recommendations for infrastructure and experimental processes.
https://www.sciencedirect.com/science/article/pii/S0950584921000264,On the practitioners’ understanding of coupling smells — A grey literature based Grounded-Theory study,June 2021,"Grey literature, Grounded theory, Design smells, Code smells, Coupling smells, Software design quality, Code quality",Apitchaka=Singjai: apitchaka.singjai@univie.ac.at; Georg=Simhandl: georg.simhandl@univie.ac.at; Uwe=Zdun: uwe.zdun@univie.ac.at,"Abstract
Context:
Code and design smells, such as the coupling smells examined in this article, are widely studied. Existing empirical studies reveal gaps between the scientific theory and practice, not yet explained by the scientific literature. Only basic coupling smell detection approaches and metrics seem to have been transferred to practice so far.
Objective:
This article aims to study the current practitioner’s understanding of coupling smells.
Method:
Based on grey literature sources containing practitioner views on coupling smells, we performed a Grounded Theory (GT) study. We used UML-based modeling to precisely encode our findings and performed a 
rigorous analysis
 of our codes and models.
Results:
Our results are defining factors of coupling smells, as well as smell impacts, trade-offs, relationships to other smells, relationships to practices and patterns, and fix options as perceived by practitioners. We further identified gaps in the understanding of coupling smells between science and practice, and derived opportunities and challenges for future scientific work.
Conclusions:
Five lessons are presented as opportunities and challenges for future research. Our results can help scientists to get a better understanding of practitioner concerns, and practitioners to get an overview of the current perception of other practitioners on coupling smells.",Information and Software Technology,18 Mar 2025,8.0,Studying the understanding of coupling smells by practitioners is important for bridging the gap between theory and practice in software engineering.
https://www.sciencedirect.com/science/article/pii/S0950584921000033,"Case Study Research in Software Engineering—It is a Case, and it is a Study, but is it a Case Study?",May 2021,"Case study, Empirical, Misuse, Software engineering",Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Background:
Case studies
 are regularly published in the 
software engineering
 literature, and guidelines for conducting case studies are available. Based on a perception that the label “case study” is assigned to studies that are not case studies, an investigation has been conducted.
Objective:
The aim was to investigate whether or not the label “case study” is correctly used in software engineering research.
Method:
To address the objective, 100 recent articles found through Scopus when searching for case studies in software engineering have been investigated and classified.
Results:
Unfortunately, the perception of misuse of the label “case study” is correct. Close to 50% of the articles investigated were judged as not being case studies according to the definition of a case study.
Conclusions:
We either need to ensure correct use of the label “case study”, or we need another label for its definition. Given that “case study” is a well-established label, it is probably impossible to change the label. Thus, we introduce an alternative definition of case study emphasising its real-life context, and urge researchers to carefully follow the definition of different research methods when presenting their research.",Information and Software Technology,18 Mar 2025,5.0,Investigating the correct use of the label 'case study' in software engineering research is informative but may have limited practical impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000033,"Case Study Research in Software Engineering—It is a Case, and it is a Study, but is it a Case Study?",May 2021,"Case study, Empirical, Misuse, Software engineering",Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Background:
Case studies
 are regularly published in the 
software engineering
 literature, and guidelines for conducting case studies are available. Based on a perception that the label “case study” is assigned to studies that are not case studies, an investigation has been conducted.
Objective:
The aim was to investigate whether or not the label “case study” is correctly used in software engineering research.
Method:
To address the objective, 100 recent articles found through Scopus when searching for case studies in software engineering have been investigated and classified.
Results:
Unfortunately, the perception of misuse of the label “case study” is correct. Close to 50% of the articles investigated were judged as not being case studies according to the definition of a case study.
Conclusions:
We either need to ensure correct use of the label “case study”, or we need another label for its definition. Given that “case study” is a well-established label, it is probably impossible to change the label. Thus, we introduce an alternative definition of case study emphasising its real-life context, and urge researchers to carefully follow the definition of different research methods when presenting their research.",Information and Software Technology,18 Mar 2025,5.0,Investigating the correct use of the label 'case study' in software engineering research is informative but may have limited practical impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584920302123,Mobile app privacy in software engineering research: A systematic mapping study,May 2021,Not Found,Fahimeh=Ebrahimi: Not Found; Miroslav=Tushev: Not Found; Anas=Mahmoud: mahmoud@csc.lsu.edu,"Abstract
Context:
 Mobile applications (apps) have become deeply personal, constantly demanding access to privacy-sensitive information in exchange for more personalized 
user experiences
. Such privacy-invading practices have generated major multidimensional privacy concerns among app users.
Objective:
 The research on mobile app privacy has experienced rapid growth over the past decade. This line of research is aimed at systematically exposing the privacy practices of apps and proposing solutions to protect the privacy of mobile app users. In this paper, we conduct a 
systematic mapping study
 of this body of research. Our objectives are to 
a)
 explore trends in 
SE
 app privacy research, 
b)
 categorize existing evidence, and 
c)
 identify potential directions for future research.
Method:
 A 
systematic mapping study
 of 59 Software Engineering (SE) primary studies on mobile app privacy. Our scope is studies published in software engineering venues between 2008 and 2018.
Results:
 Our results show that existing literature can be divided into four main categories: privacy policy, requirements, user perspective, and leak detection. Furthermore, our survey reveals an imbalance between these categories—the majority of existing research focuses on proposing tools for detecting privacy leaks, with fewer studies targeting privacy requirements and policy and even fewer on user perspective.
Conclusions:
 Our survey exposes several gaps in existing research and suggests areas for improvement.",Information and Software Technology,18 Mar 2025,8.0,"The systematic mapping study on mobile app privacy research provides insights into current trends, categories of research, and gaps for improvement in protecting app users' privacy."
https://www.sciencedirect.com/science/article/pii/S0950584920302123,Mobile app privacy in software engineering research: A systematic mapping study,May 2021,Not Found,Fahimeh=Ebrahimi: Not Found; Miroslav=Tushev: Not Found; Anas=Mahmoud: mahmoud@csc.lsu.edu,"Abstract
Context:
 Mobile applications (apps) have become deeply personal, constantly demanding access to privacy-sensitive information in exchange for more personalized 
user experiences
. Such privacy-invading practices have generated major multidimensional privacy concerns among app users.
Objective:
 The research on mobile app privacy has experienced rapid growth over the past decade. This line of research is aimed at systematically exposing the privacy practices of apps and proposing solutions to protect the privacy of mobile app users. In this paper, we conduct a 
systematic mapping study
 of this body of research. Our objectives are to 
a)
 explore trends in 
SE
 app privacy research, 
b)
 categorize existing evidence, and 
c)
 identify potential directions for future research.
Method:
 A 
systematic mapping study
 of 59 Software Engineering (SE) primary studies on mobile app privacy. Our scope is studies published in software engineering venues between 2008 and 2018.
Results:
 Our results show that existing literature can be divided into four main categories: privacy policy, requirements, user perspective, and leak detection. Furthermore, our survey reveals an imbalance between these categories—the majority of existing research focuses on proposing tools for detecting privacy leaks, with fewer studies targeting privacy requirements and policy and even fewer on user perspective.
Conclusions:
 Our survey exposes several gaps in existing research and suggests areas for improvement.",Information and Software Technology,18 Mar 2025,7.0,"The research on mobile app privacy is crucial for early-stage ventures as privacy concerns impact user trust and adoption. The systematic mapping study provides valuable insights and directions for future research, which can be instrumental for startups developing mobile applications."
