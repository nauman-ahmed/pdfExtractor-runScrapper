link,title,published_year,keywords,author_email,abstract,publication_title,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S0950584920300604,Assessing the effectiveness of approximate functional sizing approaches for effort estimation,July 2020,Not Found,Sergio=Di Martino: sergio.dimartino@unina.it; Filomena=Ferrucci: fferrucci@unisa.it; Carmine=Gravino: gravino@unisa.it; Federica=Sarro: f.sarro@ucl.ac.uk,"Abstract
Context
: Functional Size Measurement (FSM) methods, like Function Points Analysis (FPA) or 
COSMIC
, are well-established approaches to estimate software size. Several 
approximations
 of these methods have been recently proposed as they require less time/information to be applied, however their effectiveness for effort prediction is not known.
Objective
: The effectiveness of approximated functional size measures for estimating the development effort is a key open question, since an approximate sizing approach may miss to capture factors affecting the effort. Therefore, we empirically investigated the use of approximate FPA and 
COSMIC
 sizing approaches, also compared with their standard versions, for effort estimation.
Method
: We measured 25 industrial software projects realised by a single company by using FPA, COSMIC, two approximate sizing approaches proposed by 
IFPUG
 for FPA (i.e. High Level and Indicative FPA), and three approximate sizing approaches proposed by the 
COSMIC organisation
 for COSMIC (i.e. Average Functional Process, Fixed Size Classification, and Equal Size Band). Then we investigated the quality of the regression models built using the obtained measures to estimate the development effort.
Results
: Models based on High Level FPA are effective, providing a prediction accuracy comparable to the one of the original FPA, while those based on the Indicative FPA method show poor estimation accuracy. Models based on COSMIC approximate 
sizing methods
 are also quite effective, in particular those based on the Equal Size Band approximation provided an accuracy similar to the one of standard COSMIC.
Conclusion
: Project managers should be aware that predictions based on High Level FPA and standard FPA can be similar, making this approximation very interesting and effective, while Indicative FPA should be avoided. COSMIC 
approximations
 can also provide accurate effort estimates, nevertheless, the Fixed Size Classification and Equal Size Band approaches introduce subjectivity in the measurement.",Information and Software Technology,18 Mar 2025,7.0,"The study provides practical insights into the effectiveness of approximated functional size measures for effort estimation in software projects, which can be valuable for startups in optimizing resource allocation."
https://www.sciencedirect.com/science/article/pii/S0950584920300495,Analyzing and documenting the systematic review results of software testing ontologies,July 2020,Not Found,Guido=Tebes: guido.tebes92@gmail.com; Denis=Peppino: denispeppino92@gmail.com; Pablo=Becker: beckerp@ing.unlpam.edu.ar; Gerardo=Matturro: matturro@ort.edu.uy; Martin=Solari: martin.solari@ort.edu.uy; Luis=Olsina: olsinal@ing.unlpam.edu.ar,"Abstract
Context
Software testing is a complex area since it has a large number of specific methods, processes and strategies, involving a lot of domain concepts. Therefore, it would be valuable to have a conceptualized software testing ontology that explicitly and unambiguously defines the concepts. Consequently, it is important to find out the available evidence in the literature on primary studies for software testing ontologies. In particular, we are looking for research that has a rich ontological coverage that includes Non-Functional Requirements (NFRs) and Functional Requirements (FRs) concepts in conjunction with static and dynamic testing concepts, which can be used in method and process specifications for a family of testing strategies.
Objective
The main goal for this secondary study is to identify, evaluate and synthesize the available primary studies on conceptualized software testing ontologies.
Method
To conduct this study, we use the Systematic Literature Review (SLR) approach, which follows our enhanced SLR process. We set three research questions. Additionally, to quantitatively evaluate the quality of the selected conceptualized ontologies, we designed a NFRs tree and its associated metrics and indicators.
Results
We obtained 12 primary studies documenting conceptualized testing ontologies by using three different 
retrieval methods
. In general, we noted that most of them have a lack of NFRs and static testing terminological coverage. Finally, we observe that none of them is directly linked with FRs and NFRs conceptual components.
Conclusion
A general benefit of having the suitable software testing ontology is to minimize the current heterogeneity, ambiguity and incompleteness problems in terms, properties and relationships. We have confirmed that exists heterogeneity, ambiguity, and incompleteness for concepts dealing with testing artifacts, roles, activities, and methods. Moreover, we did not find the suitable ontology for our aim since none of the conceptualized ontologies are directly linked with NFRs and FRs components.",Information and Software Technology,18 Mar 2025,5.0,"While the study on software testing ontologies is important for conceptual clarity, it may have limited immediate impact on early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492030063X,Demystifying the adoption of behavior-driven development in open source projects,July 2020,Not Found,Fiorella=Zampetti: Not Found; Andrea=Di Sorbo: Not Found; Corrado Aaron=Visaggio: Not Found; Gerardo=Canfora: Not Found; Massimiliano=Di Penta: dipenta@unisannio.it,"Abstract
Context:
Behavior-Driven Development (BDD) features the capability, through appropriate domain-specific languages, of specifying acceptance test cases and making them executable. The availability of frameworks such as Cucumber or RSpec makes the application of BDD possible in practice. However, it is unclear to what extent developers use such frameworks, and whether they use them for actually performing BDD, or, instead, for other purposes such as unit testing. 
Objective:
In this paper, we conduct an empirical investigation about the use of BDD tools in open source, and how, when a BDD tool is in place, BDD specifications co-evolve with source code. 
Method:
Our investigation includes three different phases: (i) a large-scale analysis to understand the extent to which BDD frameworks are used in 50,000 popular open-source projects written in five programming languages; (ii) a study on the co-evolution of scenarios, fixtures and production code in a sample of 20 Ruby projects, through the Granger’s causality test, and (iii) a survey with 31 developers to understand how they use BDD frameworks. 
Results:
Results of the study indicate that  ≃  27% of the sampled projects use BDD frameworks, with a prevalence in Ruby projects (68%). In about 37% of the cases, we found a co-evolution between scenarios/fixtures and production code. Specifically, changes to scenarios and fixtures often happen together or after changes to source code. Moreover, survey respondents indicate that, while they understand the intended purpose of BDD frameworks, most of them write tests while/after coding rather than strictly applying BDD. 
Conclusions:
Even if the BDD frameworks usage is widespread among 
open source projects
, in many cases they are used for different purposes such as unit testing activities. This mainly happens because developers felt BDD remains quite effort-prone, and its application goes beyond the simple adoption of a BDD framework.",Information and Software Technology,18 Mar 2025,6.0,"The investigation on the use of BDD tools in open source projects provides insights into real-world practices, which can be beneficial for startups in understanding industry trends."
https://www.sciencedirect.com/science/article/pii/S0950584920300598,An empirical comparison of predictive models for web page performance,July 2020,Not Found,Raghu=Ramakrishnan: raghuramakrishnan71@gmail.com; Arvinder=Kaur: Not Found,"Abstract
Context
The quality of 
user experience
 is the cornerstone of any organization’s successful digital transformation journey. Web pages are the main touchpoint for users to access services in a digital mode. Web page performance is a key determinant of the quality of 
user experience
. The 
negative impact
 of poor web page performance on the productivity, profits, and brand value of an organization is well-recognized. The use of realistic prediction models for predicting page load time at the early stages of development can help minimize the effort and cost arising out of fixing performance defects late in the lifecycle.
Objective
We present a comprehensive evaluation of models based on 18 widely used 
machine learning techniques
 on their capability to predict page load times. The models use only those metrics which relate to the form and structure of a page because such metrics are easy to ascertain during the early stages with minimal effort.
Method
The 
machine learning techniques
 are trained on more than 8,700 pages from HTTP Archive data, a database of web performance information widely used to conduct web performance research. The trained models are then validated using the 10-fold cross-validation method and accuracy measures like the 
Pearson
 correlation coefficient (r), 
Root Mean Square Error
 (RMSE), and Normalized 
Root Mean Square Error
 (NRMSE) are reported.
Results
Radial Basis Function
 regression and 
Random Forest
 outperform all other techniques. The value of r ranges from 0.69-0.92, indicating a high correlation between the observed and 
predicted values
. The NRMSE varies between 0.11-0.16, implying that RMSE is less than 16% of the range of actual value. The RMSE improves by 41%-54% compared to the best baseline prediction model.
Conclusion
It is possible to build realistic prediction models using 
machine learning techniques
 that can be used by practitioners during the early stages of development with minimal effort.",Information and Software Technology,18 Mar 2025,9.0,"The evaluation of machine learning techniques for predicting web page load times is highly relevant for startups aiming to optimize user experience and performance, providing practical tools for early-stage development."
https://www.sciencedirect.com/science/article/pii/S0950584920300483,State identification sequences from the splitting tree,July 2020,Not Found,Michal=Soucha: michal.soucha@gmail.com; Kirill=Bogdanov: k.bogdanov@sheffield.ac.uk,"Abstract
Context:
 Software testing based on finite-state machines.
Objective:
Improving the performance of existing 
testing methods
 by construction of more efficient separating sequences, so that states entered by a system under test can be identified in a much shorter span of time.
Method:
 This paper proposes an efficient way to construct separating sequences for subsets of states for any deterministic finite-state machine. It extends an existing algorithm that builds an adaptive distinguishing sequence (ADS) from a splitting tree to machines that do not possess an ADS. Our extension to this 
construction algorithm
 allows one not only to construct a separating sequence for any subset of states but also form sets of separating sequences, such as harmonized state identifiers (HSI) and incomplete adaptive distinguishing sequences, that are used by efficient testing and learning algorithms.
Results:
 The experiments confirm that the length and number of test sequences produced by 
testing methods
 that use HSIs constructed by our extension is significantly improved.
Conclusion:
By constructing more efficient separating sequences the performance of existing test methods significantly improves.",Information and Software Technology,18 Mar 2025,8.0,"The proposal of more efficient separating sequences for finite-state machines can greatly benefit startups by improving testing methods and reducing the time needed to identify system states, enhancing product development efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584919302137,Automatic prediction of the severity of bugs using stack traces and categorical features,July 2020,Not Found,Korosh Koochekian=Sabor: k_kooche@ece.concordia.ca; Mohammad=Hamdaqa: mhamdaqa@ru.is; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca,"Abstract
Context
The severity of a bug is often used as an indicator of how a bug negatively affects system functionality. It is used by developers to prioritize bugs which need to be fixed. The problem is that, for various reasons, bug submitters often enter the incorrect 
severity level
, delaying the bug resolution process. Techniques that can automatically predict the severity of a bug can significantly reduce the bug triaging overhead. In our previous work, we showed that the accuracy of description-based severity prediction techniques could be significantly improved by using stack traces as a source of information.
Objective
In this study, we expand our previous work by exploring the effect of using categorical features, in addition to stack traces, to predict the severity of bugs. These categorical features include faulty product, faulty component, and operating system. We experimented with other features and observed that they do not improve the severity prediction accuracy. A Software system is composed of many products; each has a set of components. Components interact with each to provide the functionality of the product. The operating 
system field
 refers to the operating system on which the software was running on during the crash.
Method
The proposed approach uses a 
linear combination
 of stack trace and categorical features similarity to predict the severity. We adopted a cost sensitive K Nearest Neighbor approach to overcome the unbalance label distribution problem and improve the classifier accuracy.
Results
Our experiments on 
bug reports
 of Eclipse submitted between 2001 and 2015 and Gnome submitted between 1999 and 2015 show that the accuracy of our severity prediction approach can be improved from 5% to 20% by considering categorical features, in addition to stack traces.
Conclusion
The accuracy of predicting the severity of bugs is higher when combining stack traces and three categorical features, product, component, and operating system.",Information and Software Technology,18 Mar 2025,8.0,"The study addresses a practical issue in bug triaging for software development, showing significant improvement in prediction accuracy, which can benefit early-stage ventures and startups by reducing bug resolution time and improving system functionality."
https://www.sciencedirect.com/science/article/pii/S0950584919302332,MAESTRO: Automated test generation framework for high test coverage and reduced human effort in automotive industry,July 2020,Not Found,Yunho=Kim: Not Found; Dongju=Lee: dongju.lee@mobis.co.kr; Junki=Baek: jk.baek@mobis.co.kr; Moonzoo=Kim: moonzoo@cs.kaist.ac.kr,"Abstract
Context
The importance of automotive software has been rapidly increasing because software controls many components of motor vehicles such as smart-key system, tire pressure 
monitoring system
, and 
advanced driver assistance system
. Consequently, the automotive industry spends a large amount of human effort to test automotive software and is interested in automated testing techniques to ensure high-quality automotive software with reduced human effort.
Objective
Applying automated test generation techniques to automotive software is technically challenging because of 
false alarms
 caused by imprecise test drivers/stubs and lack of tool supports for symbolic analysis of bit-fields and 
function pointers
 in C. To address such challenges, we have developed an automated testing framework MAESTRO.
Method
MAESTRO automatically builds a test driver and stubs for a target task (i.e., a software unit consisting of target functions). Then, it generates test inputs to a target task with the test driver and stubs by applying concolic testing and fuzzing together in an adaptive way. In addition, MAESTRO transforms a target program that uses bit-fields into a semantically equivalent one that does not use bit-fields. Also, MAESTRO supports symbolic 
function pointers
 by identifying the candidate functions of a symbolic function pointer through 
static analysis
.
Results
MAESTRO achieved 94.2% branch coverage and 82.3% MC/DC coverage on the four target modules (238 KLOC) developed by Hyundai Mobis. Furthermore, it significantly reduced the cost of coverage testing by reducing the manual effort for coverage testing by 58.8%.
Conclusion
By applying automated testing techniques, MAESTRO can achieve high test coverage for automotive software with significantly reduced manual testing effort.",Information and Software Technology,18 Mar 2025,7.0,"The development of an automated testing framework for automotive software can greatly benefit startups by reducing manual testing efforts and achieving high test coverage, leading to higher quality software products."
https://www.sciencedirect.com/science/article/pii/S0950584920300227,Test coverage criteria for software product line testing: Systematic literature review,June 2020,Not Found,Jihyun=Lee: jihyun30@jbnu.ac.kr; Sungwon=Kang: sungwon.kang@kaist.ac.kr; Pilsu=Jung: psjung@kaist.ac.kr,"Abstract
Context
In software product line testing (SPLT), test coverage criterion is an important concept, as it provides a means of measuring the extent to which domain testing has been performed and redundant application testing can be avoided based on the test coverage level achieved in domain testing. However, no previous literature reviews on SPLT have addressed test coverage criterion in SPLT.
Objective
The objectives of this paper are as follows: (1) to clarify the notions of test basis and test coverage criterion for SPLT; (2) to identify the test coverage criteria currently used for SPLT; (3) to investigate how various SPLT aspects, such as the 
SPLT method
, variability implementation mechanism, and variability management approach, affect the choice of test coverage criterion for SPLT; and (4) to analyze the limitations of test coverage criteria currently used for SPLT.
Method
This paper conducts a systematic review of test coverage criteria in SPLT with 78 selected studies.
Results
We have several findings that can guide the future research on SPLT. One important finding is that choice of test coverage criterion in SPLT is independent from variability implementation mechanism, variability management, SPL approach, and binding time but is dependent on the variability representation used in development artifacts. Another that is easily overlooked is that SPL test coverage criteria with the same test coverage criterion names of single system testing neither adequately convey what should be covered by the test methods applying them, nor can they be more generally regarded as extensions or generalizations for SPLT of their corresponding test coverage criteria of single system testing.
Conclusion
This study showed that SPL test coverage criteria should be defined or redefined so that they can clearly deliver the target properties to be satisfied by SPLT.",Information and Software Technology,18 Mar 2025,5.0,"While the systematic review on software product line testing is important, it may have limited immediate practical impact on early-stage ventures or startups in comparison to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300227,Test coverage criteria for software product line testing: Systematic literature review,June 2020,Not Found,Jihyun=Lee: jihyun30@jbnu.ac.kr; Sungwon=Kang: sungwon.kang@kaist.ac.kr; Pilsu=Jung: psjung@kaist.ac.kr,"Abstract
Context
In software product line testing (SPLT), test coverage criterion is an important concept, as it provides a means of measuring the extent to which domain testing has been performed and redundant application testing can be avoided based on the test coverage level achieved in domain testing. However, no previous literature reviews on SPLT have addressed test coverage criterion in SPLT.
Objective
The objectives of this paper are as follows: (1) to clarify the notions of test basis and test coverage criterion for SPLT; (2) to identify the test coverage criteria currently used for SPLT; (3) to investigate how various SPLT aspects, such as the 
SPLT method
, variability implementation mechanism, and variability management approach, affect the choice of test coverage criterion for SPLT; and (4) to analyze the limitations of test coverage criteria currently used for SPLT.
Method
This paper conducts a systematic review of test coverage criteria in SPLT with 78 selected studies.
Results
We have several findings that can guide the future research on SPLT. One important finding is that choice of test coverage criterion in SPLT is independent from variability implementation mechanism, variability management, SPL approach, and binding time but is dependent on the variability representation used in development artifacts. Another that is easily overlooked is that SPL test coverage criteria with the same test coverage criterion names of single system testing neither adequately convey what should be covered by the test methods applying them, nor can they be more generally regarded as extensions or generalizations for SPLT of their corresponding test coverage criteria of single system testing.
Conclusion
This study showed that SPL test coverage criteria should be defined or redefined so that they can clearly deliver the target properties to be satisfied by SPLT.",Information and Software Technology,18 Mar 2025,5.0,"Similar to abstract 8, the review on software product line testing, while valuable, may not have as direct practical implications for early-stage ventures or startups as the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300379,A systematic review of unsupervised learning techniques for software defect prediction,June 2020,Not Found,Ning=Li: Not Found; Martin=Shepperd: martin.shepperd@brunel.ac.uk; Yuchen=Guo: Not Found,"Abstract
Background
Unsupervised 
machine learners
 have been increasingly applied to software 
defect prediction
. It is an approach that may be valuable for software practitioners because it reduces the need for labeled 
training data
.
Objective
Investigate the use and performance of 
unsupervised learning
 techniques in 
software defect
 prediction.
Method
We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our 
inclusion criteria
 published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the 
confusion matrices
 and employed the Matthews 
Correlation Coefficient
 (MCC) as our main performance measure.
Results
Our meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check.
Conclusion
Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.",Information and Software Technology,18 Mar 2025,6.0,"The investigation on unsupervised learning techniques in software defect prediction is relevant and could be beneficial for startups in improving software quality, although the impact may be more indirect compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300215,Software architectures of the convergence of cloud computing and the Internet of Things: A systematic literature review,June 2020,Not Found,Ahmad=Banijamali: ahmad.banijamali@oulu.fi; Olli-Pekka=Pakanen: Not Found; Pasi=Kuvaja: Not Found; Markku=Oivo: Not Found,"Abstract
Context
Over the last few years, there has been an increasing interest in the convergence of 
cloud computing
 and the 
Internet of Things
 (IoT). Although software systems in this domain have attracted researchers to develop a large body of knowledge on 
software architecture designs
, there is no 
systematic analysis
 of this knowledge.
Objective
This study aims to identify and synthesise state-of-the-art 
architectural elements
 including the 
design patterns
, styles, views, 
quality attributes
, and evaluation methodologies in the convergence of 
cloud computing
 and IoT.
Method
We used systematic literature review (SLR) methodology for a detailed analysis of 82 primary studies of a total of 1618 studies.
Results
We extracted six 
architectural design
 patterns in this domain; among them, edge connectivity patterns stand out as the most popular choice. The service-oriented architecture is the most frequently applied style in this context. Among all applicable 
quality attributes
, scalability, timeliness, and security were the most investigated quality attributes. In addition, we included nine cross analyses to address the relationship between 
architectural patterns
, styles, views, and evaluation methodologies with respect to different quality attributes and 
application areas
.
Conclusions
Our findings indicate that research on software architectures in this domain is increasing. Although few studies were found in which industrial evaluations were presented, industry requires more scientific and empirically validated design frameworks to guide 
software engineering
 in this domain. This work provides an overview of the field while identifying areas for future research.",Information and Software Technology,18 Mar 2025,7.0,"The study provides valuable insights into architectural elements in the convergence of cloud computing and IoT, identifying popular patterns and styles. This can be beneficial for startups exploring this domain."
https://www.sciencedirect.com/science/article/pii/S0950584920300276,Mining API usage scenarios from stack overflow,June 2020,Not Found,Gias=Uddin: gias.uddin@mail.mcgill.ca; Foutse=Khomh: Not Found; Chanchal K=Roy: Not Found,"Abstract
Context
APIs
 play a central role in software development. The seminal research of Carroll et al. [15] on minimal manual and subsequent studies by Shull et al. [79] showed that developers prefer task-based API documentation instead of traditional hierarchical official documentation (e.g., Javadoc). The Q&A format in Stack Overflow offers developers an interface to ask and answer questions related to their development tasks.
Objective
With a view to produce API documentation, we study automated techniques to mine API 
usage scenarios
 from Stack Overflow.
Method
We propose a framework to mine API 
usage scenarios
 from Stack Overflow. Each task consists of a code example, the task description, and the reactions of developers towards the code example. First, we present an algorithm to automatically link a code example in a forum post to an API mentioned in the textual contents of the forum post. Second, we generate a natural language description of the task by summarizing the discussions around the code example. Third, we automatically associate developers reactions (i.e., positive and negative opinions) towards the code example to offer information about code quality.
Results
We evaluate the algorithms using three benchmarks. We compared the algorithms against seven baselines. Our algorithms outperformed each baseline. We developed an online tool by automatically mining API usage scenarios from Stack Overflow. A user study of 31 software developers shows that the participants preferred the mined usage scenarios in Opiner over API official documentation. The tool is available online at: 
http://opiner.polymtl.ca/
.
Conclusion
With a view to produce API documentation, we propose a framework to automatically mine API usage scenarios from Stack Overflow, supported by three novel algorithms. We evaluated the algorithms against a total of eight state of the art baselines. We implement and deploy the framework in our proof-of-concept online tool, Opiner.",Information and Software Technology,18 Mar 2025,9.0,"The framework proposed for mining API usage scenarios from Stack Overflow is innovative and outperforms existing baselines. The user study results indicate practical value for developers, especially in startups."
https://www.sciencedirect.com/science/article/pii/S0950584920300240,Is this GitHub project maintained? Measuring the level of maintenance activity of open-source projects,June 2020,Not Found,Jailton=Coelho: jailtoncoelho@dcc.ufmg.br; Marco Tulio=Valente: Not Found; Luciano=Milen: Not Found; Luciana L.=Silva: Not Found,"Abstract
Context
GitHub hosts an impressive number of high-quality OSS projects. However, selecting “the right tool for the job” is a challenging task, because we do not have precise information about those high-quality projects.
Objective
In this paper, we propose a data-driven approach to measure the level of maintenance activity of GitHub projects. Our goal is to alert users about the risks of using unmaintained projects and possibly motivate other developers to assume the maintenance of such projects.
Method
We train 
machine learning
 models to define a metric to express the level of maintenance activity of GitHub projects. Next, we analyze the historical evolution of 2927 active projects in the time frame of one year.
Results
From 2927 active projects, 16% become unmaintained in the interval of one year. We also found that Objective-C projects tend to have lower maintenance activity than projects implemented in other languages. Finally, software tools—such as compilers and editors—have the highest maintenance activity over time.
Conclusions
A metric about the level of maintenance activity of GitHub projects can help developers to select 
open source projects
.",Information and Software Technology,18 Mar 2025,8.0,The data-driven approach to measure maintenance activity of GitHub projects can help startups make informed decisions about using open source projects. The machine learning models and analysis provide practical insights.
https://www.sciencedirect.com/science/article/pii/S0950584920300288,A large scale empirical study of the impact of Spaghetti Code and Blob anti-patterns on program comprehension,June 2020,Not Found,Cristiano=Politowski: c_polito@encs.concordia.ca; Foutse=Khomh: foutse.khomh@polymtl.ca; Simone=Romano: simone.romano@uniba.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Fabio=Petrillo: fabio@petrillo.com; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@concordia.ca; Abdou=Maiga: ma_karim@yahoo.fr,"Abstract
Context
Several studies investigated the impact of anti-patterns (
i.e.,
 “poor” solutions to recurring design problems) during maintenance activities and reported that anti-patterns significantly affect the developers’ effort required to edit files. However, before developers edit files, they must understand the 
source code
 of the systems. This 
source code
 must be easy to understand by developers.
Objective
In this work, we provide a complete assessment of the impact of two instances of two anti-patterns, Blob or 
Spaghetti Code
, on 
program comprehension
.
Method
We analyze the impact of these two anti-patterns through three empirical studies conducted at Polytechnique Montré al (Canada) with 24 participants; at Carlton University (Canada) with 30 participants; and at University Basilicata (Italy) with 79 participants.
Results
We collect data from 372 tasks obtained thanks to 133 different participants from the three universities. We use three metrics to assess the developers’ comprehension of the source code: (1) the duration to complete each task; (2) their percentage of correct answers; and, (3) the NASA task load index for their effort.
Conclusions
We report that, although single occurrences of Blob or Spaghetti code anti-patterns have little effect on code comprehension, two occurrences of either Blob or 
Spaghetti Code
 significantly increases the developers’ time spent in their tasks, reduce their percentage of correct answers, and increase their effort. Hence, we recommend that developers act on both anti-patterns, which should be refactored out of the source code whenever possible. We also recommend further studies on combinations of anti-patterns rather than on single anti-patterns one at a time.",Information and Software Technology,18 Mar 2025,6.0,"The assessment of anti-patterns impact on program comprehension is useful for developers, but the focus on two specific anti-patterns may limit the applicability to a broader range of scenarios for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300380,WCA: A weighting local search for constrained combinatorial test optimization,June 2020,Not Found,Yingjie=Fu: fuyj@ios.ac.cn; Zhendong=Lei: leizd@ios.ac.cn; Shaowei=Cai: caisw@ios.ac.cn; Jinkun=Lin: jkunlin@gmail.com; Haoran=Wang: wanghr@ios.ac.cn,"Abstract
Context
Covering array generation (CAG) is the core task of Combinatorial interaction testing (CIT), which is widely used to discover interaction faults in real-world systems. Considering the universality, constrained covering array generation (CCAG) is more in line with the characteristics of applications, and has attracted a lot of researches in the past few years.
Objective
In CIT, a covering array (CA) with smaller size means lower cost of testing, particularly for the systems where the execution of a test suite is time consuming. As constraints between parameters are ubiquitous in real systems, this work is dedicated to more efficient algorithms for CCAG. Specifically, we aim to develop a 
heuristic search algorithm
 for CCAG, which allows generating CAs with smaller size in a limited time when compared with existing algorithms.
Method
We propose a weighting 
local search algorithm
 named 
WCA
, which makes use of weights associated with the tuples and dynamically adjusts them during the search, helping the algorithm to avoid search stagnation. As far as we know, this is the first weighting local search for solving CCAG.
Results
We apply 
WCA
 to a wide range of benchmarks, including real-world ones and synthetic ones. The results show that 
WCA
 achieves a significant improvement over three state-of-the-art competitors in 2-way and 3-way CCAG, in terms of both effectiveness and efficiency. The importance of weighting is also reflected by the experimental comparison between 
WCA
 and its alternative algorithm without the weighting mechanism.
Conclusion
WCA
 is an effective 
heuristic algorithm
 for CCAG to obtain smaller CAs efficiently, and the weighting mechanism plays a crucial role.",Information and Software Technology,18 Mar 2025,8.0,"The development of WCA heuristic search algorithm for CCAG shows significant improvement over existing algorithms, offering startups a more efficient way to generate covering arrays. The practical results demonstrate the algorithm's effectiveness."
https://www.sciencedirect.com/science/article/pii/S0950584920300409,Identifying security issues for mobile applications based on user review summarization,June 2020,Not Found,Chuanqi=Tao: taochuanqi@nuaa.edu.cn; Hongjing=Guo: Not Found; Zhiqiu=Huang: Not Found,"Abstract
Context
With the development of mobile apps, public concerns about security issues are continually rising. From the user’s perspective, it is crucial to be aware of the security issues of apps. Reviews serve as an important channel for users to discover the diverse issues of apps. However, previous works rarely rely on existing reviews to provide a detailed summarization of the app’s security issues.
Objective
To provide a detailed overview of apps’ security issues for users, this paper introduces SRR-Miner, a novel review summarization approach that automatically summarizes security issues and users’ sentiments.
Method
SRR-Miner follows a keyword-based approach to extracting security-related review sentences. It summarizes security issues and users’ sentiments with  <misbehavior-aspect-opinion>  triples, which makes full use of the deep analysis of sentence structures. SRR-Miner also provides visualized review summarization through a radar chart.
Results
The evaluation on 17 mobile apps shows that SRR-Miner achieves higher F1-score and 
MCC
 than Machine Learning-based 
classification approaches
 in extracting security-related review sentences. It also accurately identifies misbehaviors, aspects and opinions from review sentences. A qualitative study shows that SRR-Miner outperforms two state-of-the-art approaches (AR-Miner and SUR-Miner) in terms of summarizing security issues and users’ sentiments. A further user survey indicates the usefulness of the summarization of SRR-Miner.
Conclusion
SRR-Miner is capable of automatically extracting security-related review sentences based on keywords, and summarizing misbehaviors, aspects and opinions of review sentences with a deep analysis of the sentence structures.",Information and Software Technology,18 Mar 2025,7.0,"The paper introduces a novel approach, SRR-Miner, for summarizing security issues in mobile apps, outperforming state-of-the-art approaches. This can have practical value for early-stage ventures in ensuring app security."
https://www.sciencedirect.com/science/article/pii/S0950584920300410,A study of run-time behavioral evolution of benign versus malicious apps in android,June 2020,Not Found,Haipeng=Cai: haipeng.cai@wsu.edu; Xiaoqin=Fu: xiaoqin.fu@wsu.edu; Abdelwahab=Hamou-Lhadj: wahab.hamou-lhadj@concordia.ca,"Abstract
Context
The constant evolution of the Android platform and its applications have imposed significant challenges both to understanding and securing the Android ecosystem. Yet, despite the growing body of relevant research, it remains unclear how Android apps evolve in terms of their run-time behaviors in ways that impede our gaining consistent empirical knowledge about the workings of the ecosystem and developing effective 
technical solutions
 to defending it against security threats. Intuitively, an essential step towards addressing these challenges is to first understand the evolution itself. Among others, one avenue to examining a program’s run-time behavior is to dissect the program’s execution in terms of its 
syntactic
 and semantic structure.
Objective
In this paper, we study how benign Android apps execute differently from 
malware
 over time, in terms of their execution structures measured by the distribution and interaction among functionality scopes, app components, and callbacks. In doing so, we attempt to reveal how relevant app execution structure is to app security orientation (i.e., benign or malicious).
Method
By tracing the method calls and inter-component communications (ICCs) of 15,451 benign apps and 15,183 
malware
 developed during eight years (2010–2017), we systematically characterized the execution structure of malware versus benign apps and revealed similarities and disparities between them that are not previously known.
Results
Our results show, among other findings, that (1) despite their similarity in execution distribution over functionality scopes, malware accessed framework functionalities mainly through third-party libraries, while benign apps were dominated by calls within the framework; (2) use of 
Activity
 component had been rising in malware while benign apps saw continuous drop in such uses; (3) malware invoked significantly more 
Services
 but less 
Content Providers
 than benign apps during the evolution of both groups; (4) malware carried ICC data significantly less often via standard data fields than benign apps, albeit both groups did not carry any data in most ICCs; and (5) newer malware tended to have more even distribution of callbacks among event-handler categories, while the distribution remained constant in benign apps over time.
Conclusion
We discussed how these findings inform understanding app behaviors, optimizing static and dynamic code analysis of Android apps, and developing sustainable app security defense solutions.",Information and Software Technology,18 Mar 2025,8.0,The research on the evolution of Android apps in terms of run-time behaviors and security implications provides valuable insights that can impact the development and security strategies of European startups in the mobile app space.
https://www.sciencedirect.com/science/article/pii/S0950584920300185,Test Case Prioritization in Continuous Integration environments: A systematic mapping study,May 2020,Not Found,Jackson A.=Prado Lima: japlima@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br,"Abstract
Context:
 Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the 
regression test
 plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments.
Objective:
 This work presents the results of a 
systematic mapping study
 on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects.
Method:
 The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and 
testing problems
, and adopted evaluation measures.
Results:
 The results show a growing interest in this research subject. Most studies have been published in the last four years. 80% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI 
testing problems
 and characteristics, such as parallel execution and test case volatility.
Conclusions:
 We observed a growing number of studies in the field. Future work should explore other 
information sources
 such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry.",Information and Software Technology,18 Mar 2025,6.0,"The systematic mapping study on Test Case Prioritization in Continuous Integration environments contributes to improving software development practices, which can benefit early-stage ventures by offering cost-effective testing solutions."
https://www.sciencedirect.com/science/article/pii/S0950584920300185,Test Case Prioritization in Continuous Integration environments: A systematic mapping study,May 2020,Not Found,Jackson A.=Prado Lima: japlima@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br,"Abstract
Context:
 Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the 
regression test
 plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments.
Objective:
 This work presents the results of a 
systematic mapping study
 on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects.
Method:
 The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and 
testing problems
, and adopted evaluation measures.
Results:
 The results show a growing interest in this research subject. Most studies have been published in the last four years. 80% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI 
testing problems
 and characteristics, such as parallel execution and test case volatility.
Conclusions:
 We observed a growing number of studies in the field. Future work should explore other 
information sources
 such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry.",Information and Software Technology,18 Mar 2025,6.0,"Similar to Abstract 18, this study on Test Case Prioritization in CI environments is relevant for startups to enhance their testing processes. However, the findings may not be as groundbreaking as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302733,Web service design defects detection: A bi-level multi-objective approach,May 2020,Not Found,Soumaya=Rebai: srebal@umich.edu; Marouane=Kessentini: marouane@umich.edu; Hanzhang=Wang: hanzwang@ebay.com; Bruce=Maxim: bmaxim@umich.edu,"Abstract
Context:
 Web services frequently evolve to integrate new features, update existing operations and fix errors to meet the new requirements of subscribers. While this evolution is critical, it may have a 
negative impact
 on the quality of services (QoS) such as reduced cohesion, increased coupling, poor response time and availability, etc. Thus, the design of services could become hard to maintain and extend in future releases. Recent studies addressed the problem of web service design antipatterns detection, also called design defects, by either manually defining detection rules, as combination of quality metrics, or generating them automatically from a set of defect examples. The manual definition of these rules is time-consuming and difficult due to the subjective nature of design issues, especially to find the right thresholds value. The efficiency of the generated rules, using automated approaches, will depend on the quality of the training set since examples of web services antipatterns are limited. Furthermore, the majority of existing studies for design defects detection for web services are limited to structural information (interface/code static metrics) and they ignore the use of quality of services (QoS) or performance metrics, such as response time and availability, for this detection process or understanding the impact of antipatterns on these QoS attributes.
Objective:
 To address these challenges, we designed a bi-level multi-objective optimization approach to enable the generation of antipattern examples that can improve the efficiency of detection rules.
Method:
 The upper-level generates a set of detection rules as a combination of quality metrics with their threshold values maximizing the coverage of defect examples extracted from several existing web services and artificial ones generated by a lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules of the upper level and minimizes the similarity to well-designed web service examples. The generated detection rules, by our approach, are based on a combination of dynamic QoS attributes and structural information of web service (static interface/code metrics).
Results:
 The statistical analysis of our results, based on a data-set of 662 web services, confirms the efficiency of our approach in detecting web service antipatterns comparing to the current state of the art in terms of precision and recall.
Conclusion:
 The multi-objective search formulation at both levels helped to diversify the generated artificial web service defects which produced better quality of detection rules. Furthermore, the combination of dynamic QoS attributes and structural information of web services improved the efficiency of the generated detection rules.",Information and Software Technology,18 Mar 2025,8.0,"The approach designed for detecting web service design antipatterns using a bi-level multi-objective optimization can significantly improve the quality of services, which is crucial for startups aiming to maintain service quality and extendibility."
https://www.sciencedirect.com/science/article/pii/S0950584919302733,Web service design defects detection: A bi-level multi-objective approach,May 2020,Not Found,Soumaya=Rebai: srebal@umich.edu; Marouane=Kessentini: marouane@umich.edu; Hanzhang=Wang: hanzwang@ebay.com; Bruce=Maxim: bmaxim@umich.edu,"Abstract
Context:
 Web services frequently evolve to integrate new features, update existing operations and fix errors to meet the new requirements of subscribers. While this evolution is critical, it may have a 
negative impact
 on the quality of services (QoS) such as reduced cohesion, increased coupling, poor response time and availability, etc. Thus, the design of services could become hard to maintain and extend in future releases. Recent studies addressed the problem of web service design antipatterns detection, also called design defects, by either manually defining detection rules, as combination of quality metrics, or generating them automatically from a set of defect examples. The manual definition of these rules is time-consuming and difficult due to the subjective nature of design issues, especially to find the right thresholds value. The efficiency of the generated rules, using automated approaches, will depend on the quality of the training set since examples of web services antipatterns are limited. Furthermore, the majority of existing studies for design defects detection for web services are limited to structural information (interface/code static metrics) and they ignore the use of quality of services (QoS) or performance metrics, such as response time and availability, for this detection process or understanding the impact of antipatterns on these QoS attributes.
Objective:
 To address these challenges, we designed a bi-level multi-objective optimization approach to enable the generation of antipattern examples that can improve the efficiency of detection rules.
Method:
 The upper-level generates a set of detection rules as a combination of quality metrics with their threshold values maximizing the coverage of defect examples extracted from several existing web services and artificial ones generated by a lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules of the upper level and minimizes the similarity to well-designed web service examples. The generated detection rules, by our approach, are based on a combination of dynamic QoS attributes and structural information of web service (static interface/code metrics).
Results:
 The statistical analysis of our results, based on a data-set of 662 web services, confirms the efficiency of our approach in detecting web service antipatterns comparing to the current state of the art in terms of precision and recall.
Conclusion:
 The multi-objective search formulation at both levels helped to diversify the generated artificial web service defects which produced better quality of detection rules. Furthermore, the combination of dynamic QoS attributes and structural information of web services improved the efficiency of the generated detection rules.",Information and Software Technology,18 Mar 2025,8.0,"The research addresses a critical issue in web service design antipatterns detection, proposing a multi-objective optimization approach to improve efficiency. The results confirm the effectiveness of the approach in detecting web service antipatterns, which can have a positive impact on the quality of services for startups."
https://www.sciencedirect.com/science/article/pii/S095058492030001X,Understanding predictive factors for merge conflicts,May 2020,Not Found,Klissiomara=Dias: kld2@cin.ufpe.br; Paulo=Borba: phmb@cin.ufpe.br; Marcos=Barreto: msb5@cin.ufpe.br,"Abstract
Context:
 Merge conflicts often occur when developers change the same code artifacts. Such conflicts might be frequent in practice, and resolving them might be costly and is an error-prone activity.
Objective:
 To minimize these problems by reducing merge conflicts, it is important to better understand how conflict occurrence is affected by technical and organizational factors.
Method:
 With that aim, we investigate seven factors related to modularity, size, and timing of developers contributions. To do so, we reproduce and analyze 73504 merge scenarios in GitHub repositories of Ruby and Python MVC projects.
Results:
 We find evidence that the likelihood of merge conflict occurrence significantly increases when contributions to be merged are not modular in the sense that they involve files from the same MVC slice (related model, view, and controller files). We also find bigger contributions involving more developers, commits, and changed files are more likely associated with merge conflicts. Regarding the timing factors, we observe contributions developed over longer periods of time are more likely associated with conflicts. No evaluated factor shows 
predictive power
 concerning both the number of merge conflicts and the number of files with conflicts.
Conclusion:
 Our results could be used to derive recommendations for development teams and merge conflict prediction models. Project management and assistive tools could benefit from these models.",Information and Software Technology,18 Mar 2025,6.0,"The study on merge conflicts provides useful insights for development teams and merge conflict prediction models, which can help startups reduce costly and error-prone activities. However, the impact on early-stage ventures may not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300203,Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary,May 2020,Not Found,Mário André de Freitas=Farias: mario.andre@ifs.edu.br; Manoel Gomes de Mendonça=Neto: manoel.mendonca@ufba.br; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Rodrigo Oliveira=Spínola: rodrigo.spinola@unifacs.br,"Abstract
Context
Previous work has shown that one can explore code comments to detect Self-Admitted Technical Debt (SATD) using a contextualized vocabulary. However, current detection strategies still return a large number of 
false positives
 items. Moreover, those strategies do not allow the automatic identification of the type of debt of the identified items.
Objective
This work applies, evaluates, and improves a set of contextualized patterns we built to detect self-admitted technical debt using code comment analysis. We refer to this set of patterns as the self-admitted technical debt identification vocabulary.
Method
We carry out three empirical studies. Firstly, 23 participants analyze the patterns of a previously defined contextualized vocabulary and register their level of importance in identifying SATD items. Secondly, we perform a qualitative analysis to investigate the relation between each pattern and types of debt. Finally, we perform a feasibility study using a new vocabulary, improved based on the results of the previous empirical studies, to automatically identify self-admitted technical debt items, and types of debt, that exist in three 
open source projects
.
Results
More than half of the new patterns were considered decisive or very decisive to detect technical debt items. The new vocabulary was able to find items associated to code, design, defect, documentation, and requirement debt. Thus, the result of the work is an improved vocabulary that considers the level of importance of each pattern and the relationship between patterns and debt types to support the identification and classification of SATD items.
Conclusion
The studies allowed us to improve a vocabulary to identify self-admitted technical debt items through code comments analysis. The results show that the use of pattern-based code comment analysis can contribute to improve existing methods, or create new ones, for automatically identifying and classifying technical debt items.",Information and Software Technology,18 Mar 2025,7.0,"The work on identifying self-admitted technical debt through code comments analysis can benefit startups in improving existing methods for identifying and classifying technical debt items, leading to better software quality. The improved vocabulary provides practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300197,Incorporating fault-proneness estimations into coverage-based test case prioritization methods,May 2020,Not Found,Mostafa=Mahdieh: Not Found; Seyed-Hassan=Mirian-Hosseinabadi: hmirian@sharif.edu; Khashayar=Etemadi: Not Found; Ali=Nosrati: Not Found; Sajad=Jalali: Not Found,"Abstract
Context:
 During the 
development process
 of a software program, regression testing is used to ensure that the correct behavior of the software is retained after updates to the 
source code
. This regression testing becomes costly over time as the number of test cases increases and it makes sense to prioritize test cases in order to execute fault-detecting test cases as soon as possible. There are many coverage-based test case prioritization (TCP) methods that only use the code coverage data to prioritize test cases. By incorporating the fault-proneness estimations of code units into the coverage-based TCP methods, we can improve such techniques.
Objective:
 In this paper, we aim to propose an approach which improves coverage-based TCP methods by considering the fault-proneness distribution over code units. Further, we present the results of an empirical study that shows using our proposed approach significantly improves the additional strategy, which is a widely used coverage-based TCP method.
Method:
 The approach presented in this study uses the bug history of the software in order to introduce a 
defect prediction
 method to learn a 
neural network model
. This model is then used to estimate fault-proneness of each area of the 
source code
 and then the estimations are incorporated into coverage-based TCP methods. Our proposed approach is a general idea that can be applied to many coverage-based methods, such as the additional and total TCP methods.
Results:
 The proposed methods are evaluated on datasets collected from the development history of five real-world projects including 357 versions in total. The experiments show that using an appropriate bug history can improve coverage-based TCP methods.
Conclusion:
 The proposed approach can be applied to various coverage-based TCP methods and the experiments show that it can improve these methods by incorporating estimations of code units fault-proneness.",Information and Software Technology,18 Mar 2025,5.0,"The proposed approach for coverage-based TCP methods improvement is valuable for prioritizing test cases, but the direct impact on early-stage ventures may not be as significant compared to other abstracts. The approach may provide incremental benefits for startups in improving regression testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584920300239,Detection of malicious software by analyzing the behavioral artifacts using machine learning algorithms,May 2020,Not Found,Jagsir=Singh: erjagsirsingh18@gmail.com; Jaswinder=Singh: dr.jaswinder@pbi.ac.in,"Abstract
Malicious software
 deliberately affects the 
computer systems
. Malware are analyzed using static or dynamic analysis techniques. Using these techniques, unique patterns are extracted to 
detect malware
 correctly. In this paper, a behavior-based 
malware detection
 technique is proposed. Various runtime features are extracted by setting up a dynamic analysis environment using the Cuckoo sandbox. Three primary features are processed for developing malware classifier. Firstly, printable strings are processed word by word using text mining techniques which produced a very high dimension matrix of the string features. Then we apply the 
singular value
 decomposition technique for reducing dimensions of string features. Secondly, 
Shannon entropy
 is computed over the printable strings and API calls to consider the randomness of API and PSI features. In addition to these features, behavioral features regarding file operations, registry key modification and network activities are used in 
malware detection
. Finally, all features are integrated in the training feature set to develop the malware classifiers using the 
machine learning algorithms
. The proposed technique is validated with 16489 malware and 8422 benign files. Our experimental results show the accuracy of 99.54% in malware detection using ensemble 
machine learning algorithms
. Moreover, it aims to develop a behavior-based malware detection technique of high accuracy by processing the runtime features in a new way.",Information and Software Technology,18 Mar 2025,9.0,"The behavior-based malware detection technique proposed in this research has high accuracy in detecting malware, which is crucial for protecting computer systems of startups. The use of machine learning algorithms and innovative feature extraction methods provide practical value and impact for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300252,A survey on the practical use of UML for different software architecture viewpoints,May 2020,Not Found,Mert=Ozkaya: mozkaya@cse.yeditepe.edu.tr; Ferhat=Erata: Not Found,"Abstract
Context
Software 
architecture viewpoints
 modularize the software architectures in terms of different viewpoints that each address a different concern. 
Unified Modeling Language
 (UML) is so popular among practitioners for modeling software architectures from different viewpoints.
Objective
In this paper, we aimed at understanding the practitioners’ UML usage for the modeling of software architectures from different viewpoints.
Method
To this end, 109 practitioners with diverse profiles have been surveyed to understand practitioners’ UML usage for six different viewpoints: functional, information, concurrency, development, deployment, and operational. Each viewpoint has been considered in terms of a set of software models that can be created in that viewpoint.
Results
The survey includes 35 questions for different viewpoint models, and the results lead to interesting findings. While the top popular viewpoints for the UML-based software architecture modeling are the functional (96%) and information (99%) viewpoints, the least popular one is the operational viewpoint that is considered by 26% of the practitioners. The top popular 
UML modeling
 tool is Enterprise Architect regardless of the viewpoints considered. Concerning the software models that can be created in each viewpoint, UML’s 
class diagram
 is practitioners’ top choice for the functional structure (71%), 
data structure
 (85%), concurrency structure (75%), software code structure (34%), and system installation (39%), and system support (16%) models; UML’s 
sequence diagram
 is the top choice for the 
data lifecycle
 models (47%); UML’s deployment diagram for the physical structure (71%), mapping between the functional and physical components (53%), and system migration (21%) models; UML’s activity diagram for the data flow (65%), software build and release processes (20–22%), and system administration (36%) models; UML’s component diagram for the mapping between the functional and concurrent components (35%), software module structure (47%), and system configuration (21%) models; and UML’s 
package diagram
 for the software module structure (47%) models.",Information and Software Technology,18 Mar 2025,5.0,"The abstract provides insights into practitioners' UML usage for different software architecture viewpoints, but the practical value for early-stage ventures is limited as it focuses more on modeling techniques."
https://www.sciencedirect.com/science/article/pii/S0950584919302563,Generative software module development for domain-driven design with annotation-based domain specific language,April 2020,Not Found,Duc Minh=Le: duclm@hanu.edu.vn; Duc-Hanh=Dang: hanhdd@vnu.edu.vn; Viet-Ha=Nguyen: hanv@vnu.edu.vn,"Abstract
Context
Object-oriented domain-driven design (DDD) aims to iteratively develop software around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of recent work in DDD has been on using a form of annotation-based 
domain specific language
 (aDSL), internal to an object-oriented programming language, to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development.
Objective
In this paper, we tackle software module development with the DDD method by adopting a 
generative approach
 that uses aDSL. To achieve this, we first extend a previous work on module-based software architecture with three enhancements that make it amenable to generative development. We then treat module configurations as first-class objects and define an aDSL, named 
MCCL
, to express module configuration classes. To improve productivity, we define function 
MCCGen
 to automatically generate each configuration class from the module’s domain class.
Method
We define our method as a refinement of an aDSL-based 
software development method
 from a previous work. We apply meta-modelling with UML/OCL to define 
MCCL
 and implement 
MCCL
 in a Java software framework. We evaluate the applicability of our method using a 
case study
 and formally define an evaluation framework for module generativity. We also analyse the correctness and performance of function 
MCCGen
.
Results
MCCL
 is an aDSL for module configurations. Our evaluation shows 
MCCL
 is applicable to complex problem domains. Further, the MCCs and software modules can be generated with a high and quantifiable degree of automation.
Conclusion
Our method bridges an important gap in DDD with a software module development method that uses a novel aDSL with a module-based software architecture and a 
generative technique
 for module configuration.",Information and Software Technology,18 Mar 2025,9.0,"This abstract introduces a generative approach to software module development within the DDD method, addressing a gap in software architecture. The practical implications for early-stage ventures are significant."
https://www.sciencedirect.com/science/article/pii/S0950584919302563,Generative software module development for domain-driven design with annotation-based domain specific language,April 2020,Not Found,Duc Minh=Le: duclm@hanu.edu.vn; Duc-Hanh=Dang: hanhdd@vnu.edu.vn; Viet-Ha=Nguyen: hanv@vnu.edu.vn,"Abstract
Context
Object-oriented domain-driven design (DDD) aims to iteratively develop software around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of recent work in DDD has been on using a form of annotation-based 
domain specific language
 (aDSL), internal to an object-oriented programming language, to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development.
Objective
In this paper, we tackle software module development with the DDD method by adopting a 
generative approach
 that uses aDSL. To achieve this, we first extend a previous work on module-based software architecture with three enhancements that make it amenable to generative development. We then treat module configurations as first-class objects and define an aDSL, named 
MCCL
, to express module configuration classes. To improve productivity, we define function 
MCCGen
 to automatically generate each configuration class from the module’s domain class.
Method
We define our method as a refinement of an aDSL-based 
software development method
 from a previous work. We apply meta-modelling with UML/OCL to define 
MCCL
 and implement 
MCCL
 in a Java software framework. We evaluate the applicability of our method using a 
case study
 and formally define an evaluation framework for module generativity. We also analyse the correctness and performance of function 
MCCGen
.
Results
MCCL
 is an aDSL for module configurations. Our evaluation shows 
MCCL
 is applicable to complex problem domains. Further, the MCCs and software modules can be generated with a high and quantifiable degree of automation.
Conclusion
Our method bridges an important gap in DDD with a software module development method that uses a novel aDSL with a module-based software architecture and a 
generative technique
 for module configuration.",Information and Software Technology,18 Mar 2025,9.0,"Similar to abstract 27, this abstract also focuses on software module development within the DDD method using a generative approach, providing valuable insights for startups."
https://www.sciencedirect.com/science/article/pii/S0950584919302629,Patterns of user involvement in experiment-driven software development,April 2020,Not Found,Sezin=Yaman: sezin.yaman@helsinki.fi; Fabian=Fagerholm: fabian.fagerholm@helsinki.fi; Myriam=Munezero: myriam.munezero@helsinki.fi; Tomi=Männistö: tomi.mannisto@helsinki.fi; Tommi=Mikkonen: tommi.mikkonen@helsinki.fi,"Abstract
Background
Experiments are often used as a means to continuously validate user needs and to aid in making software development decisions. Involving users in the development of software products benefits both the users and companies. How software companies efficiently involve users in both general development and in experiments remains unclear; however, it is especially important to determine the perceptions and attitudes held by practitioners in different roles in these companies.
Objective
We seek to: 1) explore how software companies involve users in software development and experimentation; 2) understand how developer, manager and UX designer roles perceive and involve users in experimentation; and 3) uncover systematic patterns in practitioners’ views on user involvement in experimentation. The study aims to reveal behaviors and perceptions that could support or undermine experiment-driven development, point out what skills could enhance experiment-driven development, and raise awareness of such issues for companies that wish to adopt experiment-driven development.
Methods
We conducted a survey within four Nordic software companies, inviting practitioners in three major roles: developers, managers, and UX designers. We asked the respondents to indicate how they involve users in their job function, as well as their perspectives regarding software experiments and ethics.
Results and Conclusion
We identified six patterns describing experimentation and user involvement. For instance, managers were associated with a cautious user notification policy, that is, to always let users know of an experiment they are subject to, and they also believe that users have to be convinced before taking part in experiments. We discovered that, due to lack of clear processes for involving users and the lack of a common understanding of ethics in experimentation, practitioners tend to rationalize their perceptions based on their own experiences. Our patterns were based on empirical evidence and they can be evaluated in different populations and contexts.",Information and Software Technology,18 Mar 2025,7.0,"The abstract explores how software companies involve users in development and experimentation, which can be relevant for early-stage ventures looking to understand user involvement and perceptions."
https://www.sciencedirect.com/science/article/pii/S0950584919302605,Collaborative or individual identification of code smells? On the effectiveness of novice and professional developers,April 2020,Not Found,Roberto=Oliveira: roberto.oliveira@ueg.br; Rafael=de Mello: rmaiani@inf.puc-rio.br; Eduardo=Fernandes: emfernandes@inf.puc-rio.br; Alessandro=Garcia: afgarcia@inf.puc-rio.br; Carlos=Lucena: lucena@inf.puc-rio.br,"Abstract
Context
The code smell identification aims to reveal code structures that harm the software 
maintainability
. Such identification usually requires a 
deep understanding
 of multiple parts of a system. Unfortunately, developers in charge of identifying code smells individually can struggle to identify, confirm, and refute code smell suspects. Developers may reduce their struggle by identifying code smells in pairs through the collaborative smell identification.
Objective
The current knowledge on the effectiveness of collaborative smell identification remains limited. Some scenarios were not explored by previous work on effectiveness of collaborative versus individual smell identification. In this paper, we address a particular scenario that reflects various organizations worldwide. We also compare our study results with recent studies.
Method
We have carefully designed and conducted a controlled experiment with 34 developers. We exploited a particular scenario that reflects various organizations: novices and professionals inspecting systems they are unfamiliar with. We expect to minimize some critical threats to validity of previous work. Additionally, we interviewed 5 project leaders aimed to understand the potential adoption of the collaborative smell identification in practice.
Results
Statistical testing suggests 27% more precision and 36% more recall through the collaborative smell identification for both novices and professionals. These results partially confirm previous work in a not previously exploited scenario. Additionally, the interviews showed that leaders would strongly adopt the collaborative smell identification. However, some organization and tool constraints may limit such adoption. We derived recommendations to organizations concerned about adopting the collaborative smell identification in practice.
Conclusion
We recommend that organizations allocate novice developers for identifying code smells in collaboration. Thus, these organizations can promote the 
knowledge sharing
 and the correct smell identification. We also recommend the allocation of developers that are unfamiliar with the system for identifying smells. Thus, organizations can allocate more experience developers in more critical tasks.",Information and Software Technology,18 Mar 2025,8.0,"This abstract presents findings on collaborative code smell identification, offering insights that can improve software maintainability. The practical implications for startups are significant."
https://www.sciencedirect.com/science/article/pii/S0950584919302617,On the value of quality attributes for refactoring ATL model transformations: A multi-objective approach,April 2020,Not Found,Bader=Alkhazi: balkhazi@umich.edu; Chaima=Abid: cabid@umich.edu; Marouane=Kessentini: marouane@umich.edu; Manuel=Wimmer: manuel.wimmer@jku.at,"Abstract
Context
Model transformations play a fundamental role in Model-Driven Engineering (MDE) as they are used to manipulate models and to transform them between source and target metamodels. However, model transformation programs lack significant support to maintain good quality which is in contrast to established programming paradigms such as object-oriented programming. In order to improve the quality of model transformations, the majority of existing studies suggest manual support for the developers to execute a number of refactoring types on model transformation programs. Other recent studies aimed to automate the refactoring of model transformation programs, mostly focusing on the ATLAS Transformation Language (ATL), by improving mainly few quality metrics using a number of refactoring types.
Objective
In this paper, we propose a novel set of quality attributes to evaluate refactored 
ATL programs
 based on the hierarchical 
quality model
 QMOOD.
Method
We used the proposed quality attributes to guide the selection of the best refactorings to improve 
ATL programs
 using multi-objective search.
Results
We validate our approach on a comprehensive dataset of model transformations. The statistical analysis of our experiments on 30 runs shows that our automated approach recommended useful refactorings based on a benchmark of ATL transformations and compared to random search, mono-objective search formulation, a previous work based on a different formulation of multi-objective search with few quality metrics, and a semi-automated refactoring approach not based on 
heuristic search
.
Conclusion
All these existing studies did not use our QMOOD adaptation for ATL which confirms the relevance of our quality attributes to guide the search for good refactoring suggestions.",Information and Software Technology,18 Mar 2025,9.0,"The study proposes a novel set of quality attributes to evaluate and improve ATL programs, which can benefit early-stage ventures by enhancing the quality of model transformations."
https://www.sciencedirect.com/science/article/pii/S0950584919302721,Regression testing for large-scale embedded software development – Exploring the state of practice,April 2020,Not Found,Nasir Mehmood=Minhas: nasir.mehmood.minhas@bth.se; Kai=Petersen: kai.petersen@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Krzysztof=Wnuk: krzysztof.wnuk@bth.se,"Abstract
Context
A majority of the regression testing techniques proposed by academics have not been adopted in 
industry
. To increase adoption rates, we need to improve our understanding of the practitioners’ perspectives on regression testing.
Objective
This study aims at exploring the regression testing state of practice in the large-scale 
embedded software
 development. The study has two objectives: 1) to highlight the potential challenges in practice, and 2) to identify the industry-relevant research areas regarding regression testing.
Method
We conducted a qualitative study in two large-scale 
embedded software
 development companies, where we carried out semi-structured interviews with representatives from five software testing teams.
Results
The practitioners run regression testing mostly with limited scope based on the size, complexity, and location of the change. Test cases are prioritized on the basis of risk and critical functionality. The practitioners rely on their knowledge and experience for the decision making regarding selection and prioritization of test cases. The companies are using both automated and manual regression testing, and mainly rely on in-house developed tools for test automation. The challenges identified in the companies are: time to test, information management, test suite maintenance, lack of communication, test selection/prioritization, lack of assessment, etc. Regression testing goals identified in this study are customer satisfaction, critical defect detection, confidence, effectiveness, efficiency, and controlled slip through of faults.
Conclusions
Considering the current state of practice and the identified challenges we conclude that there is a need to reconsider the 
regression test
 strategy in the companies. Researchers need to analyze the 
industry
 perspective when proposing new regression testing techniques.",Information and Software Technology,18 Mar 2025,7.0,"The study explores regression testing practices in large-scale embedded software development, providing insights that can be valuable for startups in improving their testing strategies."
https://www.sciencedirect.com/science/article/pii/S0950584919302551,Comparison of development methodologies in web applications,March 2020,Not Found,Jimmy=Molina-Ríos: jmolina@utmachala.edu.ec; Nieves=Pedreira-Souto: nieves.pedreira@udc.es,"Abstract
Context
Web applications development is at its peak due to the advance of technological trends and the constant dependence of the Internet. As a result of the needs of developers, new development methodologies have emerged. However, that does not mean that companies always implement an optimal 
development process
; instead, there are several disadvantages presented by an inadequate and not versatile methodologies.
Objective
The aim is to compare web development methodologies based on dynamic features presented during the life cycle to identify their use, relevance, and characteristics. The process employing is an SLR and field research to Ecuadorian development companies.
Method
The method used 
is
 a systematic literature review (SLR) for the identification of characteristics and processes of development methodologies. Additionally, a survey of Ecuadorian web application developers was implemented to assess the importance of using a method during the project.
Results
The literature review exhibited as a result that UWE and OOHDM have greater flexibility than other methodologies before dynamic environments during the web 
development process
. On the other hand, within field research was obtained that companies use different 
software development methods
 than those assessed in the study (hybrid methodologies). However, within the range of companies using the compared methodologies, UWE is the most selected.
Conclusions
Each methodology holds particular features and employment environment, which makes them useful in specific conditions. Through the field research, it is possible to conclude that most of the companies use different methodologies than the evaluated ones; thus, the process is guided by hybrids methods or models based on experience. On the other hand, through the SLR, we identified UWE as the most suitable methodology for web development under dynamic environments, such as the size of the company, the need to modify the requirements, or the knowledge that the development team has about the process.",Information and Software Technology,18 Mar 2025,8.0,"The comparison of web development methodologies based on dynamic features offers useful insights for companies looking to optimize their development processes, potentially benefiting early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919302551,Comparison of development methodologies in web applications,March 2020,Not Found,Jimmy=Molina-Ríos: jmolina@utmachala.edu.ec; Nieves=Pedreira-Souto: nieves.pedreira@udc.es,"Abstract
Context
Web applications development is at its peak due to the advance of technological trends and the constant dependence of the Internet. As a result of the needs of developers, new development methodologies have emerged. However, that does not mean that companies always implement an optimal 
development process
; instead, there are several disadvantages presented by an inadequate and not versatile methodologies.
Objective
The aim is to compare web development methodologies based on dynamic features presented during the life cycle to identify their use, relevance, and characteristics. The process employing is an SLR and field research to Ecuadorian development companies.
Method
The method used 
is
 a systematic literature review (SLR) for the identification of characteristics and processes of development methodologies. Additionally, a survey of Ecuadorian web application developers was implemented to assess the importance of using a method during the project.
Results
The literature review exhibited as a result that UWE and OOHDM have greater flexibility than other methodologies before dynamic environments during the web 
development process
. On the other hand, within field research was obtained that companies use different 
software development methods
 than those assessed in the study (hybrid methodologies). However, within the range of companies using the compared methodologies, UWE is the most selected.
Conclusions
Each methodology holds particular features and employment environment, which makes them useful in specific conditions. Through the field research, it is possible to conclude that most of the companies use different methodologies than the evaluated ones; thus, the process is guided by hybrids methods or models based on experience. On the other hand, through the SLR, we identified UWE as the most suitable methodology for web development under dynamic environments, such as the size of the company, the need to modify the requirements, or the knowledge that the development team has about the process.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 33, the comparison of web development methodologies can provide valuable information for startups aiming to enhance their development processes and methodologies."
https://www.sciencedirect.com/science/article/pii/S0950584919302484,Software trustworthiness evaluation model based on a behaviour trajectory matrix,March 2020,Not Found,Junfeng=Tian: Not Found; Yuhui=Guo: tjf@hbu.edu.cn,"Abstract
Context
Software trustworthiness is a highly important 
research topic
. 
Trustworthiness evaluation
 based on factors that affect software behaviour is conducted mainly according to the influence degrees of these factors on the software behaviour to evaluate trustworthiness. As a result, minimization of the interference of human factors is considered.
Objective
In this study, to ensure the objectivity of evaluating the trustworthiness of software behaviour, a software trustworthiness evaluation model based on a behaviour trajectory matrix, namely, BTBM-TM was proposed.
Method
Checkpoints were set up in the trajectory of the software behaviour, and binary code was introduced to express the software behaviour trajectory tree. The scenario information of the checkpoints was acquired, and used to construct behaviour trajectory matrices, which were used to represent the behaviour trajectory. The behaviour trajectory matrices were transformed into grayscale images, which were used to train the 
deep residual network
 (ResNet) to classify the software behaviour. The trained 
deep residual network
 was used to categorize the current software behaviour, and the 
cosine similarity
 algorithm was used to calculate the deviation degree of the software behaviour trajectory; to perform a dual evaluation of the trustworthiness of software behaviour.
Results
The behaviour trajectory information of the Model class of 300 cycles was used to evaluate the trustworthiness of the mine-sweeping game. The trustworthiness evaluation results of the software behaviour of the scheme proposed in this paper (BTBM-TM) were compared with those of the schemes from references [6] and [10]. The accuracies of the schemes from [6] and [10] are lower than that of the BTBM-TM scheme.
Conclusions
The trajectory of software behaviour is represented by a matrix and converted into a grayscale image, whose 
processing method
 is used to evaluate the trustworthiness of software behaviour more objectively and accurately.",Information and Software Technology,18 Mar 2025,7.0,"The proposed software trustworthiness evaluation model based on behaviour trajectory matrix can contribute to enhancing the reliability of software products, which could be beneficial for startups seeking to build trust with their users."
https://www.sciencedirect.com/science/article/pii/S0950584919302125,Better together: Comparing vulnerability prediction models,March 2020,Not Found,Christopher=Theisen: http://www.theisencr.github.io/; Laurie=Williams: lawilli3@ncsu.edu,"Abstract
Context
Vulnerability Prediction Models (VPMs) are an approach for prioritizing security inspection and testing to find and fix vulnerabilities. VPMs have been created based on a variety of metrics and approaches, yet widespread adoption of VPM usage in practice has not occurred. Knowing which VPMs have strong prediction and which VPMs have low data requirements and resources usage would be useful for practitioners to match VPMs to their project’s needs. The low density of vulnerabilities compared to defects is also an obstacle for practical VPMs.
Objective
The goal of the paper is to help security practitioners and researchers choose appropriate features for vulnerability prediction through a comparison of Vulnerability Prediction Models.
Method
We performed replications of VPMs on Mozilla Firefox with 28,750 
source code files
 featuring 271 vulnerabilities using software metrics, text mining, and crash data. We then combined features from each VPM and reran our classifiers.
Results
We improved the F-score of the best VPM (.20 to 0.28) by combining features from three types of VPMs and using Naive Bayes as the classifier. The strongest features in the combined model were the number of times a file was involved in a crash, the number of outgoing calls from a file, and the string “nullptr”.
Conclusion
Our results indicate that further work is needed to develop new features for input into classifiers. In addition, new analytic approaches for VPMs are needed for VPMs to be useful in practical situations, due to the low density of vulnerabilities in software (less than 1% for our dataset).",Information and Software Technology,18 Mar 2025,7.0,"The study provides insights into improving Vulnerability Prediction Models, which can be crucial for security practitioners and researchers in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919302496,How are distributed bugs diagnosed and fixed through system logs?,March 2020,Not Found,Wei=Yuan: cindyyuanwei@buaa.edu.cn; Shan=Lu: shanlu@uchicago.edu; Hailong=Sun: sunhl@buaa.edu.cn; Xudong=Liu: liuxd@act.buaa.edu.cn,"Abstract
Context
Distributed systems are the backbone of today’s computing ecosystems. Debugging distributed bugs is crucial and challenging. There are still many unknowns about debugging real-world distributed bugs, especially through system logs.
Objective
This paper aims to provide a comprehensive study of how system logs can help diagnose and fix distributed bugs in practice.
Method
The study was carried out with three core research questions (RQs): How to identify failures in distributed bugs through logs? How to find and utilize bug-related log entries to figure out the root causes? How are distributed bugs fixed and how are logs and patches related? To answer these questions, we studied 106 real-world distributed bugs randomly sampled from five widely used distributed systems, and manually checked the 
bug report
, the log, the patch, the source code and other related information for each of these bugs.
Results
Seven findings are observed and the main findings include: (1) For only about half of the distributed bugs, the failures are indicated by FATAL or ERROR log entries. FATAL are not always fatal, and INFO could be fatal. (2) For more than half of the studied bugs, root-cause diagnosis relies on log entries that are not part of the failure symptoms. (3) One third of the studied bugs are fixed by eliminating end symptoms instead of root causes. Finally, a distributed bug dataset with the in-depth analysis has been released to the research community.
Conclusion
The findings in our study reveal the characteristics of distributed bugs, the differences from debugging single-machine system bugs, and the usages and limitations of existing logs. Our study also provides guidance and opportunities for future research on distributed bug diagnosis, fixing, and log analysis and enhancement.",Information and Software Technology,18 Mar 2025,8.0,"Understanding and diagnosing distributed bugs through system logs can greatly benefit startups dealing with distributed systems, making this study highly valuable."
https://www.sciencedirect.com/science/article/pii/S0950584919302393,Adequate vs. inadequate test suite reduction approaches,March 2020,Not Found,Carmen=Coviello: carmen.coviello@unibas.it; Simone=Romano: simone.romano@uniba.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Alessandro=Marchetto: alex.marchetto@gmail.com; Anna=Corazza: anna.corazza@unina.it; Giuliano=Antoniol: antoniol@ieee.org,"Abstract
Context:
 Regression testing is an important activity that allows ensuring the correct behavior of a system after changes. As the system grows, the time and resources to perform regression testing increase. Test Suite Reduction (TSR) approaches aim to speed up regression testing by removing obsolete or redundant test cases. These approaches can be classified as adequate or inadequate. Adequate TSR approaches reduce test suites and completely preserve test requirements (
e.g.,
 covered statements) of the original test suites. Inadequate TSR approaches do not preserve test requirements. The percentage of satisfied test requirements indicates the inadequacy level.
Objective:
 We compare some state-of-the-art adequate and inadequate TSR approaches with respect to the size of reduced test suites and their fault-detection capability. We aim to increase our body of knowledge on TSR approaches by comparing: 
(i)
 well-known traditional adequate TSR approaches; 
(ii)
 their inadequate variants; and 
(iii)
 several variants of a novel Clustering-Based (CB) approach for (adequate and inadequate) TSR.
Method:
 We conducted an experiment to compare adequate and inadequate TSR approaches. This comparison is founded on a public dataset containing information on real faults.
Results:
 The most important findings from our experiment can be summarized as follows: 
(i)
 there is not an inadequate TSR approach that outperforms the others;
(ii)
 some inadequate variants of the CB approach, and few traditional inadequate approaches, outperform the adequate ones in terms of reduction in test suite size with a 
negligible effect
 on fault-detection capability; and 
(iii)
 the CB approach is less sensitive than the other inadequate approaches, that is, variations in the inadequacy level have small effect on reduction in test suite size and on loss in fault-detection capability.
Conclusions:
 These findings imply that inadequate TSR approaches and especially the CB approach might be appealing because they lead to a greater reduction in test suite size (with respect to the adequate ones) at the expense of a small loss in fault-detection capability.",Information and Software Technology,18 Mar 2025,6.0,"While the study on Test Suite Reduction approaches is important, the practical implications for early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302228,A systematic literature review of machine learning techniques for software maintainability prediction,March 2020,Not Found,Hadeel=Alsolai: hadeel.alsolai@strath.ac.uk; Marc=Roper: marc.roper@strath.ac.uk,"Abstract
Context
Software 
maintainability
 is one of the fundamental 
quality attributes
 of 
software engineering
. The accurate prediction of software 
maintainability
 is a significant challenge for the effective management of the software maintenance process.
Objective
The major aim of this paper is to present a systematic review of studies related to the prediction of 
maintainability
 of object-oriented software systems using 
machine learning techniques
. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software 
maintainability
 measurements, metrics, datasets, evaluation measures, individual models and ensemble models.
Method
The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.
Results
We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other 
software quality attributes
. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level 
product metrics
 as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on 
regression problems
 and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.
Conclusion
Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.",Information and Software Technology,18 Mar 2025,9.0,"Predicting maintainability of software systems using machine learning techniques is highly relevant for startups, making this systematic review valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919302587,Intelligent software engineering in the context of agile software development: A systematic literature review,March 2020,Not Found,Mirko=Perkusich: mirko@embedded.ufcg.edu.br; Lenardo=Chaves e Silva: lenardo.silva@embedded.ufcg.edu.br; Alexandre=Costa: alexandre.costa@embedded.ufcg.edu.br; Felipe=Ramos: felipe.ramos@embedded.ufcg.edu.br; Renata=Saraiva: renata.saraiva@embedded.ufcg.edu.br; Arthur=Freire: arthur.freire@embedded.ufcg.edu.br; Ednaldo=Dilorenzo: ednaldo.dilorenzo@virtus.ufcg.edu.br; Emanuel=Dantas: emanuel.dantas@embedded.ufcg.edu.br; Danilo=Santos: danilo.santos@embedded.ufcg.edu.br; Kyller=Gorgônio: kyller@embedded.ufcg.edu.br; Hyggo=Almeida: hyggo@embedded.ufcg.edu.br; Angelo=Perkusich: perkusic@virtus.ufcg.edu.br,"Abstract
CONTEXT
: Intelligent 
Software Engineering
 (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, 
natural language processing
, perception or supporting decision-making.
OBJECTIVE
: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to 
Agile Software Development
 (ASD). Furthermore, we assess its maturity and identify adoption risks.
METHOD
: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. 
RESULTS
: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and 
machine learning
 are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, 
resource allocation
, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques.
CONCLUSION
: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.",Information and Software Technology,18 Mar 2025,8.0,"Analyzing the state of applying intelligent techniques to Agile Software Development can provide valuable insights for startups looking to adopt these techniques, making this study highly practical and impactful."
https://www.sciencedirect.com/science/article/pii/S0950584918301642,Towards assisting developers in API usage by automated recovery of complex temporal patterns,March 2020,Not Found,Mohamed Aymen=Saied: m_saied@encs.concordia.ca; Erick=Raelijohn: erick.raelijohn@umontreal.ca; Edouard=Batot: batotedo@iro.umontreal.ca; Michalis=Famelis: famelis@iro.umontreal.ca; Houari=Sahraoui: sahraouh@iro.umontreal.ca,"Abstract
Context
Despite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers use those libraries. However, most of these techniques produce patterns that generally do not involve 
temporal properties
.
Objective
In this paper, we discuss the problem of temporal usage patterns recovery and propose an algorithm to solve it. We also discuss how the obtained patterns can be used at different stages of client development.
Method
We address the recovery of temporal API usage patterns as an 
optimization problem
 and solve it using a genetic-programming algorithm.
Results
Our evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage that are useful and generalizable to new API clients.
Conclusion
Recovering API usage temporal patterns helps client developers to use APIs in an appropriate way. In addition to potentially improve productivity, such patterns also helps preventing errors that result from an incorrect use of the APIs.",Information and Software Technology,18 Mar 2025,7.0,"The proposed algorithm for recovering temporal API usage patterns can potentially improve productivity and prevent errors in client development, which could be valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918301642,Towards assisting developers in API usage by automated recovery of complex temporal patterns,March 2020,Not Found,Mohamed Aymen=Saied: m_saied@encs.concordia.ca; Erick=Raelijohn: erick.raelijohn@umontreal.ca; Edouard=Batot: batotedo@iro.umontreal.ca; Michalis=Famelis: famelis@iro.umontreal.ca; Houari=Sahraoui: sahraouh@iro.umontreal.ca,"Abstract
Context
Despite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers use those libraries. However, most of these techniques produce patterns that generally do not involve 
temporal properties
.
Objective
In this paper, we discuss the problem of temporal usage patterns recovery and propose an algorithm to solve it. We also discuss how the obtained patterns can be used at different stages of client development.
Method
We address the recovery of temporal API usage patterns as an 
optimization problem
 and solve it using a genetic-programming algorithm.
Results
Our evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage that are useful and generalizable to new API clients.
Conclusion
Recovering API usage temporal patterns helps client developers to use APIs in an appropriate way. In addition to potentially improve productivity, such patterns also helps preventing errors that result from an incorrect use of the APIs.",Information and Software Technology,18 Mar 2025,7.0,The investigation of research on System of Systems (SoS) architecting and the identified research gaps could provide insights for startups working on complex systems and software quality attributes.
https://www.sciencedirect.com/science/article/pii/S0950584919302083,Architecting systems of systems: A tertiary study,February 2020,Not Found,Héctor=Cadavid: h.f.cadavid.rengifo@rug.nl; Vasilios=Andrikopoulos: v.andrikopoulos@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
 The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent 
constituent systems
 that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the 
Internet of Things
.
Objective:
 This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and 
software quality attributes
 by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. 
Method:
 We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. 
Results:
 The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and 
quality attributes
. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. 
Conclusions:
 Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.",Information and Software Technology,18 Mar 2025,5.0,"The study on the Overall Path Complexity (OPC) metric for software complexity evaluation provides a new perspective, but the practical implications for startups might be limited compared to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302083,Architecting systems of systems: A tertiary study,February 2020,Not Found,Héctor=Cadavid: h.f.cadavid.rengifo@rug.nl; Vasilios=Andrikopoulos: v.andrikopoulos@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
 The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent 
constituent systems
 that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the 
Internet of Things
.
Objective:
 This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and 
software quality attributes
 by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. 
Method:
 We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. 
Results:
 The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and 
quality attributes
. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. 
Conclusions:
 Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.",Information and Software Technology,18 Mar 2025,5.0,"The study on the Overall Path Complexity (OPC) metric for software complexity evaluation provides a new perspective, but the practical implications for startups might be limited compared to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302095,Toward recursion aware complexity metrics,February 2020,Not Found,Gordana=Rakić: gordana.rakic@dmi.uns.ac.rs; Melinda=Tóth: tothmelinda@elte.hu; Zoran=Budimac: zoran.budimac@dmi.uns.ac.rs,"Abstract
Context
: Software developers spend a significant amount of time on reading, comprehending, and debugging of 
source code
. Numerous software metrics can give us awareness of incomprehensible functions or of flaws in their collaboration. Invocation chains, especially recursive ones, affect solution complexity, readability, and 
understandability
. Even though decomposed and recursive solutions are characterized as short and clear in comparison with iterative ones, they hide the complexity of the observed problem and solution. As the collaboration between functions can strongly depend on context, difficulties are usually detected in debugging, testing or by 
static analysis
, while metrics support is still very weak.
Objective
: We introduce a new complexity metric, called Overall Path Complexity (OPC), which is aware of (recursive) call chains in the observed 
source code
. As invocations are basic collaboration mechanism and recursions are broadly accepted, the OPC metric is intended to be applicable independently on programming language and paradigm.
Method
: We propose four different versions of the OPC calculation algorithm and explore and discuss their suitability. We have validated proposed metrics based on a Framework specially designed for evaluation and validation of software complexity metrics and accordingly performed theoretical, empirical and practical validation. Practical validation was performed on toy examples and industrial cases (47012 LOCs, 2899 functions, and 758 recursive paths) written in Erlang.
Result
: Based on our analysis we selected the most suitable (of 4 proposed) OPC calculation formula, and showed that the new metric expresses advanced properties of the software in comparison with other available metrics that was confirmed by 
low correlation
.
Conclusion
: We introduced the OPC metric calculated on the Overall Control 
Flow Graph
 as an extension of 
Cyclomatic Complexity
 by adding awareness of (recursive) invocations. The values of the new metric can lead us to find the problematic fragments of the code or of the execution paths.",Information and Software Technology,18 Mar 2025,,
https://www.sciencedirect.com/science/article/pii/S0950584919302289,A generic methodology for early identification of non-maintainable source code components through analysis of software releases,February 2020,Not Found,Michail D.=Papamichail: mpapamic@issel.ee.auth.gr; Andreas L.=Symeonidis: Not Found,"Abstract
Context
Contemporary development approaches consider that time-to-market is of 
utmost importance
 and assume that software projects are constantly evolving, driven by the continuously changing requirements of end-users. This practically requires an iterative process where software is changing by introducing new or updating existing software/user features, while at the same time continuing to support the stable ones. In order to ensure efficient software evolution, the need to produce maintainable software is evident.
Objective
In this work, we argue that non-maintainable software is not the outcome of a single change, but the consequence of a series of changes throughout the 
development lifecycle
. To that end, we define a 
maintainability
 evaluation methodology across releases and employ various information residing in software repositories, so as to decide on the maintainability of software.
Method
Upon using the dropping of packages as a non-maintainability indicator (accompanied by a series of quality-related criteria), the proposed methodology involves using one-class-classification techniques for evaluating maintainability at a package level, on four different axes each targeting a primary 
source code
 property: complexity, cohesion, coupling, and inheritance.
Results
Given the qualitative and 
quantitative evaluation
 of our methodology, we argue that apart from providing accurate and interpretable maintainability evaluation at package level, we can also identify non-maintainable components at an early stage. This early stage is in many cases around 50% of the software package lifecycle.
Conclusion
Based on our findings, we conclude that modeling the trending behavior of certain 
static analysis
 metrics enables the effective identification of non-maintainable software components and thus can be a valuable tool for the software engineers.",Information and Software Technology,18 Mar 2025,5.0,The evaluation methodology for maintainability of software can be useful for early-stage startups to ensure efficient software evolution.
https://www.sciencedirect.com/science/article/pii/S0950584919302216,Practical detection of CMS plugin conflicts in large plugin sets,February 2020,Not Found,Igor=Lima: isol2@cin.ufpe.br; Jeanderson=Cândido: j.barroscandido@tudelft.nl; Marcelo=d’Amorim: damorim@cin.ufpe.br,"Abstract
Context
Content Management Systems
 (CMS), such as WordPress, are a very popular category of software for creating web sites and blogs. These systems typically build on top of plugin architectures. Unfortunately, it is not uncommon that the combined activation of multiple plugins in a CMS web site will produce unexpected behavior. Conflict-detection techniques exist but they do not scale.
Objective
This paper proposes 
Pena
, a technique to detect conflicts in large sets of plugins as those present in plugin market places.
Method
Pena
 takes on input a configuration, consisting of a potentially large set of plugins, and reports on output the offending plugin combinations. 
Pena
 uses an iterative divide-and-conquer search to explore the large space of plugin combinations and a staged filtering process to eliminate 
false alarms
.
Results
We evaluated 
Pena
 with plugins selected from the WordPress official repository and compared its efficiency and accuracy against the technique that checks conflicts in all pairs of plugins. Results show that 
Pena
 is 12.4x to 19.6x more efficient than the comparison baseline and can find as many conflicts as it.",Information and Software Technology,18 Mar 2025,7.0,"The conflict-detection technique for CMS plugins can significantly improve the efficiency and accuracy of detecting conflicts, which can benefit startups using these systems."
https://www.sciencedirect.com/science/article/pii/S0950584919302307,Energy efficient adaptation engines for android applications,February 2020,Not Found,Angel=Cañete: angelcv@lcc.uma.es; Jose-Miguel=Horcas: Not Found; Inmaculada=Ayala: Not Found; Lidia=Fuentes: Not Found,"Abstract
Context
 The energy consumption of mobile devices is increasing due to the improvement in their components (e.g., better processors, larger screens). Although the hardware consumes the energy, the software is responsible for managing hardware resources such as the camera software and its functionality, and therefore, affects the energy consumption. Energy consumption not only depends on the installed code, but also on the execution context (environment, devices status) and how the user interacts with the application.
Objective
 In order to reduce the energy consumption based on 
user behavior
, it is necessary to dynamically adapt the application. However, the adaptation mechanism also consumes a certain amount of energy in itself, which may lead to an important increase in the energy expenditure of the application in comparison with the benefits of the adaptation. Therefore, this footprint must be measured and compared with the benefit obtained.
Method
 In this paper, we (1) determine the benefits, in terms of energy consumption, of dynamically adapting mobile applications, based on 
user behavior
; and (2) advocate the most energy-efficient adaptation mechanism. We provide four different implementations of a proposed adaptation model and measure their energy consumption.
Results
 The proposed adaptation engines do not increase the energy consumption when compared to the benefits of the adaptation, which can reduce the energy consumption by up to 20%.
Conclusion
 The adaptation engines proposed in this paper can decrease the energy consumption of the mobile devices based on user behavior. The overhead introduced by the adaptation engines is negligible in comparison with the benefits obtained by the adaptation.",Information and Software Technology,18 Mar 2025,8.0,The energy-efficient adaptation mechanism for mobile applications based on user behavior can help startups reduce energy consumption and improve overall performance.
https://www.sciencedirect.com/science/article/pii/S0950584918302258,Energy aware simulation and testing of smart-spaces,February 2020,Not Found,Khaled=El-Fakih: kelfakih@aus.edu; Teruhiro=Mizumoto: mizumoto@ist.osaka-u.ac.jp; Keiichi=Yasumoto: yasumoto@is.naist.jp; Teruo=Higashino: higashino@ist.osaka-u.ac.jp,"Abstract
Context
A smart-space SS typically consists of many rooms, with temperature and humidity environment attributes, devices, and software components that communicate with each other to satisfy certain test purposes that need to be checked over various realistic exterior environment weather conditions.
Objective
We present a novel energy-aware approach for the validation of smart-spaces while minimizing the energy consumption encountered during testing.
Method
A framework for deriving minimal (energy) cost tests is provided. It includes SS, a controlled environment 
Env
 that depicts the exterior conditions, and a 
Tester
 that can control SS and 
Env
, derive and runs tests, and observe relevant SS attributes in order to release a success verdict whenever a test purpose is met. A simulator is proposed for deriving tests by appropriately exploring part of the SS behavior employing several 
cost functions
 for computing the estimated cost and duration of test events.
Results
The framework is deployed in a real SS environment which is used to assess the actual energy consumption of derived tests in practice. Experiments show that the actual 
power consumption
 of the derived tests is close to the ones estimated by the simulator. A 
case study
 that assesses the gains in using energy aware tests in comparison to non energy-aware alternatives is also provided.
Conclusions
The obtained results highlight the importance of considering 
power consumption
 in the development and testing of smart-spaces.",Information and Software Technology,18 Mar 2025,10.0,The energy-aware approach for validating smart-spaces and minimizing energy consumption during testing is highly valuable for startups working with smart-space technologies.
https://www.sciencedirect.com/science/article/pii/S0950584918302258,Energy aware simulation and testing of smart-spaces,February 2020,Not Found,Khaled=El-Fakih: kelfakih@aus.edu; Teruhiro=Mizumoto: mizumoto@ist.osaka-u.ac.jp; Keiichi=Yasumoto: yasumoto@is.naist.jp; Teruo=Higashino: higashino@ist.osaka-u.ac.jp,"Abstract
Context
A smart-space SS typically consists of many rooms, with temperature and humidity environment attributes, devices, and software components that communicate with each other to satisfy certain test purposes that need to be checked over various realistic exterior environment weather conditions.
Objective
We present a novel energy-aware approach for the validation of smart-spaces while minimizing the energy consumption encountered during testing.
Method
A framework for deriving minimal (energy) cost tests is provided. It includes SS, a controlled environment 
Env
 that depicts the exterior conditions, and a 
Tester
 that can control SS and 
Env
, derive and runs tests, and observe relevant SS attributes in order to release a success verdict whenever a test purpose is met. A simulator is proposed for deriving tests by appropriately exploring part of the SS behavior employing several 
cost functions
 for computing the estimated cost and duration of test events.
Results
The framework is deployed in a real SS environment which is used to assess the actual energy consumption of derived tests in practice. Experiments show that the actual 
power consumption
 of the derived tests is close to the ones estimated by the simulator. A 
case study
 that assesses the gains in using energy aware tests in comparison to non energy-aware alternatives is also provided.
Conclusions
The obtained results highlight the importance of considering 
power consumption
 in the development and testing of smart-spaces.",Information and Software Technology,18 Mar 2025,10.0,The energy-aware approach for validating smart-spaces and minimizing energy consumption during testing is highly valuable for startups working with smart-space technologies.
https://www.sciencedirect.com/science/article/pii/S0950584919302022,"Impact of usability mechanisms: An experiment on efficiency, effectiveness and user satisfaction",January 2020,Not Found,Juan M.=Ferreira: jmferreira1978@fpuna.edu.py; Silvia T.=Acuña: silvia.acunna@uam.es; Oscar=Dieste: odieste@fi.upm.es; Sira=Vegas: svegas@fi.upm.es; Adrián=Santos: adrian.santos.parrilla@oulu.fi; Francy=Rodríguez: francy.rodriguez@uam.es; Natalia=Juristo: natalia@fi.upm.es,"Abstract
Context
As a software quality characteristic, usability includes the attributes of efficiency, effectiveness and user satisfaction. There are several recommendations in the literature on how to build usable software systems, but there are not very many empirical studies that provide evidence about their impact.
Objective
We report an experiment carried out with users to understand the effect of three usability mechanisms —Abort Operation, Progress Feedback and Preferences— on efficiency, effectiveness and user satisfaction. Usability mechanisms are functionalities that should, according to the HCI community, be implemented within a software system to increase its usability.
Method
The experiment was conducted with 168 users divided into 24 experimental groups. Each group performs three online shopping tasks. We measure efficiency variables (number of clicks and time taken), effectiveness (percentage of task completion) and user satisfaction gathered from a questionnaire.
Results
The adoption of Abort Operation has a significantly positive effect on efficiency (time taken), effectiveness and user satisfaction. The adoption of Progress Feedback does not appear to have any impact on any of the variables. The adoption of Preferences has a significantly positive effect on effectiveness and user satisfaction but no influence on efficiency.
Conclusions
We provide relevant evidence of the impact of the three usability mechanisms on efficiency, effectiveness and user satisfaction. In no case do the usability mechanisms degrade user performance. The effort to adopt Abort Operation and Preferences appears to be justified by the benefits in terms of effectiveness and user satisfaction. Also Abort Operation enables the user to be more productive. We believe that the effects on efficiency, effectiveness and satisfaction depend not only on mechanism functionality but also on the problem domain. The impact of a mechanism in other contexts could differ. Therefore, we need to conduct further experiments to gather more evidence and confirm these results.",Information and Software Technology,18 Mar 2025,8.0,"The study provides relevant evidence of the impact of usability mechanisms on efficiency, effectiveness, and user satisfaction, which can be valuable for European early-stage ventures in improving user experience and product performance."
https://www.sciencedirect.com/science/article/pii/S0950584919302022,"Impact of usability mechanisms: An experiment on efficiency, effectiveness and user satisfaction",January 2020,Not Found,Juan M.=Ferreira: jmferreira1978@fpuna.edu.py; Silvia T.=Acuña: silvia.acunna@uam.es; Oscar=Dieste: odieste@fi.upm.es; Sira=Vegas: svegas@fi.upm.es; Adrián=Santos: adrian.santos.parrilla@oulu.fi; Francy=Rodríguez: francy.rodriguez@uam.es; Natalia=Juristo: natalia@fi.upm.es,"Abstract
Context
As a software quality characteristic, usability includes the attributes of efficiency, effectiveness and user satisfaction. There are several recommendations in the literature on how to build usable software systems, but there are not very many empirical studies that provide evidence about their impact.
Objective
We report an experiment carried out with users to understand the effect of three usability mechanisms —Abort Operation, Progress Feedback and Preferences— on efficiency, effectiveness and user satisfaction. Usability mechanisms are functionalities that should, according to the HCI community, be implemented within a software system to increase its usability.
Method
The experiment was conducted with 168 users divided into 24 experimental groups. Each group performs three online shopping tasks. We measure efficiency variables (number of clicks and time taken), effectiveness (percentage of task completion) and user satisfaction gathered from a questionnaire.
Results
The adoption of Abort Operation has a significantly positive effect on efficiency (time taken), effectiveness and user satisfaction. The adoption of Progress Feedback does not appear to have any impact on any of the variables. The adoption of Preferences has a significantly positive effect on effectiveness and user satisfaction but no influence on efficiency.
Conclusions
We provide relevant evidence of the impact of the three usability mechanisms on efficiency, effectiveness and user satisfaction. In no case do the usability mechanisms degrade user performance. The effort to adopt Abort Operation and Preferences appears to be justified by the benefits in terms of effectiveness and user satisfaction. Also Abort Operation enables the user to be more productive. We believe that the effects on efficiency, effectiveness and satisfaction depend not only on mechanism functionality but also on the problem domain. The impact of a mechanism in other contexts could differ. Therefore, we need to conduct further experiments to gather more evidence and confirm these results.",Information and Software Technology,18 Mar 2025,8.0,"The research presents evidence of the impact of usability mechanisms on efficiency, effectiveness, and user satisfaction, which can be beneficial for startups in enhancing the usability of their software products and user engagement."
https://www.sciencedirect.com/science/article/pii/S0950584919302046,The missing link – A semantic web based approach for integrating screencasts with security advisories,January 2020,Not Found,Ellis E.=Eghan: e_eghan@encs.concordia.ca; Parisa=Moslehi: p_mosleh@encs.concordia.ca; Juergen=Rilling: juergen.rilling@concordia.ca; Bram=Adams: bram.adams@polymtl.ca,"Abstract
Context
Collaborative tools and repositories have been introduced to facilitate 
open source software development
, allowing projects, developers, and users to share their knowledge and expertise through formal and informal channels such as repositories, Q&A websites, blogs and screencasts. While significant progress has been made in mining and cross-linking traditional software repositories, limited work exists in making 
multimedia content
 in the form of screencasts or 
audio recordings
 an integrated part of 
software engineering
 processes.
Objective
The objective of this research is to provide a standardized ontological representation that allows for a seamless knowledge integration of screencasts with other software artifacts across knowledge resource boundaries.
Method
In this paper, we propose a modeling approach that takes advantage of the Semantic Web and its inference services to capture and establish 
traceability links
 between knowledge extracted from different resources such as 
vulnerability information
 in 
NVD
, project dependency information from Maven Central, and YouTube screencasts.
Results
We performed a 
case study
 on 48 videos that illustrate attacks on vulnerable systems and show that our approach can successfully link relevant vulnerabilities and screencasts with an average precision of 98% and an average recall of 54% when vulnerability identifiers (CVE ID) are explicitly mentioned in the metadata (title and description) of videos. When no CVE ID is present, our initial results show that for a reduced search space (for one vulnerability), using only the textual content of the image frames, our approach is still able to link video-vulnerability pairs and rank the correct result within the top two positions of the result set.
Conclusion
Our approach not only establishes bi-directional, direct, and indirect 
traceability links
 from screencasts to these other software artifacts; these links can also be used to guide practitioners in comprehending the potential security impact of vulnerable components in their projects.",Information and Software Technology,18 Mar 2025,3.0,"While the study explores linking screencasts with other software artifacts, the direct practical value for early-stage ventures or startups in Europe may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302058,Automatic extraction of product line architecture and feature models from UML class diagram variants,January 2020,Not Found,Wesley K.G.=Assunção: wesleyk@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br; Roberto E.=Lopez-Herrejon: roberto.lopez@etsmtl.ca,"Abstract
Context
Software Product Lines (SPLs) are families of 
related products
 developed for specific domains. SPLs commonly emerge from existing variants when their individual maintenance and/or evolution become complex. Even though there exists a vast research literature on SPL extraction, the majority of the approaches have only focused on 
source code
, are partially automated, or do not reflect domain constraints. Such limitations can make more difficult the extraction, management, documentation and generation of some important SPL artifacts such as the 
product line architecture
, a fact that can impact negatively the evolution and maintenance of SPLs.
Objective
To tackle these limitations, this work presents ModelVars2SPL (
Model Variants to SPL Core Assets
), an automated approach to aid the development of SPLs from existing system variants.
Method
The input for ModelVars2SPL is a set of 
Unified Modeling Language
 (UML) 
class diagrams
 and the list of features they implement. The approach extracts two main assets: (i) Feature Model (FM), which represents the combinations of features, and (ii) a Product Line Architecture (PLA), which represents a global structure of the variants. ModelVars2SPL is composed of four automated steps. We conducted a thorough evaluation of ModelVars2SPL to analyze the artefacts it generates and its performance.
Results
The results show that the FMs well-represent the features organization, providing useful information to define and manage commonalities and variabilities. The PLAs show a global structure of current variants, facilitating the understanding of existing implementations of all variants.
Conclusions
An advantage of ModelVars2SPL is to exploit the use of UML design models, that is, it is independent of the programming language, and supports the re-engineering process in the design level, allowing practitioners to have a broader view of the SPL.",Information and Software Technology,18 Mar 2025,4.0,"The approach presented aids in developing software product lines from existing variants, which may have limited immediate practical impact on early-stage ventures, thus receiving a moderate score."
https://www.sciencedirect.com/science/article/pii/S095058491930206X,Utilising CI environment for efficient and effective testing of NFRs,January 2020,Not Found,Liang=Yu: liang.yu@bth.se; Emil=Alégroth: emil.alegroth@bth.se; Panagiota=Chatzipetrou: panagiota.chatzipetrou@oru.se; Tony=Gorschek: tony.gorschek@bth.se,"Abstract
Context
Continuous integration (CI) is a practice that aims to continuously verify 
quality aspects
 of a software intensive system both for functional and non-functional requirements (NFRs). Functional requirements are the inputs of development and can be tested in isolation, utilising either manual or automated tests. In contrast, some NFRs are difficult to test without functionality, for NFRs are often aspects of functionality and express 
quality aspects
. Lacking this 
testability
 attribute makes NFR testing complicated and, therefore, underrepresented in industrial practice. However, the emergence of CI has radically affected software development and created new avenues for software quality evaluation and quality 
information acquisition
. Research has, consequently, been devoted to the utilisation of this additional information for more efficient and effective NFR verification.
Objective
We aim to identify the state-of-the-art of utilising the CI environment for NFR testing, hereinafter referred to as CI-NFR testing.
Method
Through rigorous selection, from an initial set of 747 papers, we identified 47 papers that describe how NFRs are tested in a CI environment. Evidence-based analysis, through coding, is performed on the identified papers in this SLR.
Results
Firstly, ten CI approaches are described by the papers selected, each describing different tools and nine different NFRs where reported to be tested. Secondly, although possible, CI-NFR testing is associated with eight challenges that adversely affect its adoption. Thirdly, the identified CI-NFR testing processes are tool-driven, but there is a lack of NFR testing tools that can be used in the CI environment. Finally, we proposed a CI framework for NFRs testing.
Conclusion
A synthesised CI framework is proposed for testing various NFRs, and associated CI tools are also mapped. This contribution is valuable as results of the study also show that CI-NFR testing can help improve the quality of NFR testing in practices.",Information and Software Technology,18 Mar 2025,5.0,"The study focuses on utilising the CI environment for NFR testing, which may provide some insights for startups in improving software quality evaluation, but the direct impact on European early-stage ventures is moderate."
https://www.sciencedirect.com/science/article/pii/S0950584918302271,Ontology-based test generation for automated and autonomous driving functions,January 2020,Not Found,Yihao=Li: yihao.li@ist.tugraz.at; Jianbo=Tao: Not Found; Franz=Wotawa: wotawa@ist.tugraz.at,"Abstract
Context:
 Ontologies are known as a formal and explicit conceptualization of entities, their interfaces, behaviors, and relationships. They have been applied in various application domains such as 
autonomous driving
 where ontologies are used for decision making, traffic description, auto-pilot etc. It has always been a challenge to test the corresponding safety-critical software systems in 
autonomous driving
 that have been playing an increasingly important role in our daily routines.
Objective:
 Failures in these systems potentially not only cause great financial loss but also the loss of lives. Therefore, it is vital to obtain and cover as many as critical driving scenarios during auto drive testing to ensure that the system can always reach a fail-safe state under different circumstances.
Method:
 We outline a general framework for testing, verification, and validation for automated and autonomous driving functions. The introduced method makes use of ontologies for describing the environment of autonomous vehicles and convert them to input models for combinatorial testing. The combinatorial test suite comprises abstract test cases that are mapped to concrete test cases that can be executed using simulation environments.
Results:
 We discuss in detail on how to automatically convert ontologies to the corresponding combinatorial testing input models. Specifically, we present two conversion algorithms and compare their applicability using ontologies with different sizes. We also carried out a 
case study
 to further demonstrate the practical value of applying ontology-based test generation in industrial settings.
Conclusion:
 The proposed approach for testing autonomous driving takes ontologies describing the environment of autonomous vehicles, and automatically converts it to test cases that are used in a simulation environment to verify automated driving functions. The conversion relies on combinatorial testing. The first experimental results relying on an example from the automotive industry indicates that the approach can be used in practice.",Information and Software Technology,18 Mar 2025,7.0,"The abstract presents a practical approach for testing autonomous driving functions using ontologies, which can have a significant impact on the safety and reliability of autonomous vehicles. The method outlined is robust and can contribute positively to the European early-stage ventures in the automotive industry."
https://www.sciencedirect.com/science/article/pii/S0950584918302271,Ontology-based test generation for automated and autonomous driving functions,January 2020,Not Found,Yihao=Li: yihao.li@ist.tugraz.at; Jianbo=Tao: Not Found; Franz=Wotawa: wotawa@ist.tugraz.at,"Abstract
Context:
 Ontologies are known as a formal and explicit conceptualization of entities, their interfaces, behaviors, and relationships. They have been applied in various application domains such as 
autonomous driving
 where ontologies are used for decision making, traffic description, auto-pilot etc. It has always been a challenge to test the corresponding safety-critical software systems in 
autonomous driving
 that have been playing an increasingly important role in our daily routines.
Objective:
 Failures in these systems potentially not only cause great financial loss but also the loss of lives. Therefore, it is vital to obtain and cover as many as critical driving scenarios during auto drive testing to ensure that the system can always reach a fail-safe state under different circumstances.
Method:
 We outline a general framework for testing, verification, and validation for automated and autonomous driving functions. The introduced method makes use of ontologies for describing the environment of autonomous vehicles and convert them to input models for combinatorial testing. The combinatorial test suite comprises abstract test cases that are mapped to concrete test cases that can be executed using simulation environments.
Results:
 We discuss in detail on how to automatically convert ontologies to the corresponding combinatorial testing input models. Specifically, we present two conversion algorithms and compare their applicability using ontologies with different sizes. We also carried out a 
case study
 to further demonstrate the practical value of applying ontology-based test generation in industrial settings.
Conclusion:
 The proposed approach for testing autonomous driving takes ontologies describing the environment of autonomous vehicles, and automatically converts it to test cases that are used in a simulation environment to verify automated driving functions. The conversion relies on combinatorial testing. The first experimental results relying on an example from the automotive industry indicates that the approach can be used in practice.",Information and Software Technology,18 Mar 2025,7.0,"Similar to abstract 56, this abstract presents a valuable approach for testing autonomous driving functions using ontologies. The practical application and demonstrated results can have a beneficial impact on early-stage ventures in the automotive industry."
https://www.sciencedirect.com/science/article/pii/S0950584919301843,Improving feature location accuracy via paragraph vector tuning,December 2019,Not Found,Allysson Costa e=Silva: allcostaes@ufu.br; Marcelo de Almeida=Maia: marcelo.maia@ufu.br,"Abstract
Context
Feature location techniques are still not highly accurate despite advances in the field.
Objective
This paper aims at investigating the impact of applying different tunings to paragraph vector to the feature location problem. It evaluates the influence of different 
artificial neural network
 (ANN) configurations for 
learning rate
 and negative sampling loss function in paragraph 
vectors training
.
Method
The suggested weight configuration relies on the search for an adequate ANN 
learning rate
 and an adequate calibration of negative sampling skip-gram mode of the Doc2vec (DV) algorithm. A dataset with 633 feature descriptions, extracted from six open-source Java projects, organized within method 
granularity
, is used for the empirical assessment.
Results
Our results suggest that feature location techniques benefit from the use of paragraph vector with systematic tuning. We show that an adequate update policy for 
ANN weights
 can increase feature location accuracy. An adequate calibration for negative sampling also improved accuracy. We got it with no default values of negative sampling pointed by literature. Moreover, an ensemble of learning rate policies and the use of a tuned DV negative sampling option had overcome state-of-the-art approaches.
Conclusions
We show evidence of a relationship between hyper-parameter settings and accuracy gain. Modern paragraph vector approaches require adequate calibration to produce better results, and we have improved the accuracy of feature location process with proper tuning.",Information and Software Technology,18 Mar 2025,5.0,"While the abstract discusses the impact of tuning paragraph vectors for feature location techniques, the practical implications for European early-stage ventures, especially startups, are not as direct or significant as the autonomous driving testing methods in the previous abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301843,Improving feature location accuracy via paragraph vector tuning,December 2019,Not Found,Allysson Costa e=Silva: allcostaes@ufu.br; Marcelo de Almeida=Maia: marcelo.maia@ufu.br,"Abstract
Context
Feature location techniques are still not highly accurate despite advances in the field.
Objective
This paper aims at investigating the impact of applying different tunings to paragraph vector to the feature location problem. It evaluates the influence of different 
artificial neural network
 (ANN) configurations for 
learning rate
 and negative sampling loss function in paragraph 
vectors training
.
Method
The suggested weight configuration relies on the search for an adequate ANN 
learning rate
 and an adequate calibration of negative sampling skip-gram mode of the Doc2vec (DV) algorithm. A dataset with 633 feature descriptions, extracted from six open-source Java projects, organized within method 
granularity
, is used for the empirical assessment.
Results
Our results suggest that feature location techniques benefit from the use of paragraph vector with systematic tuning. We show that an adequate update policy for 
ANN weights
 can increase feature location accuracy. An adequate calibration for negative sampling also improved accuracy. We got it with no default values of negative sampling pointed by literature. Moreover, an ensemble of learning rate policies and the use of a tuned DV negative sampling option had overcome state-of-the-art approaches.
Conclusions
We show evidence of a relationship between hyper-parameter settings and accuracy gain. Modern paragraph vector approaches require adequate calibration to produce better results, and we have improved the accuracy of feature location process with proper tuning.",Information and Software Technology,18 Mar 2025,5.0,"Similar to abstract 58, this abstract focuses on improving feature location techniques, but the practical value for European early-stage ventures may not be as high as the autonomous driving testing methods discussed in other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301715,Software process line as an approach to support software process reuse: A systematic literature review,December 2019,Not Found,Eldânae=Nogueira Teixeira: danny@cos.ufrj.br; Fellipe Araújo=Aleixo: Not Found; Francisco Dione de Sousa=Amâncio: Not Found; Edson=OliveiraJr: Not Found; Uirá=Kulesza: Not Found; Cláudia=Werner: Not Found,"Abstract
Context
Software 
Process Line
 (SPrL) aims at providing a systematic reuse technique to support reuse experiences and knowledge in the definition of software processes for new projects thus contributing to reduce effort and costs and to achieve improvements in quality. Although the research body in SPrL is expanding, it is still an immature area with results offering an overall view scattered with no consensus.
Objective
The goal of this work is to identify existing approaches for developing, using, managing and visualizing the evolution of SPrLs and to characterize their support, especially during the development of reusable process family artefacts, including an overview of existing SPrL supporting tools in their multiple stages; to 
analyse variability
 management and component-based aspects in SPrL; and, finally, to list practical examples and conducted evaluations. This research aims at reaching a broader and more consistent view of the research area and to provide perspectives and gaps for future research.
Method
We performed a systematic literature review according to well-established guidelines set. We used tools to partially support the process, which relies on a six-member research team.
Results
We report on 49 primary studies that deal mostly with conceptual or theoretical proposals and the domain engineering stage. Years 2014, 2015, and 2018 yielded the largest number of articles. This can indicate SPrL as a recent research theme and one that attracts ever-increasing interest.
Conclusion
Although this research area is growing, there is still a lack of practical experiences and approaches for actual applications or project-specific process derivations and decision-making support. The concept of an integrated reuse infrastructure is less discussed and explored; and the development of integrated tools to support all reuse stages is not fully addressed. Other topics for future research are discussed throughout the paper with gaps pointed as opportunities for improvements in the area.",Information and Software Technology,18 Mar 2025,4.0,"The abstract discusses the development and management of software process lines, but the direct impact on European early-stage ventures, especially startups, is not as clear or immediate as the autonomous driving testing methods presented in other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058491930165X,Impact of the conceptual model's representation format on identifying and understanding user stories,December 2019,Not Found,Marina=Trkman: marina.trkman@gmail.com; Jan=Mendling: Not Found; Peter=Trkman: Not Found; Marjan=Krisper: Not Found,"Abstract
Context
Eliciting user stories is a major challenge for 
agile development
 approaches. Conceptual models are used to support the identification of user stories and increase their understanding. In many companies, existing model documentation stored as either use cases or 
BPMN
 models is available. However, these two types of business process models might not be equally effective for 
elicitation
 tasks due to their formats.
Objective
We address the effectiveness of different 
elicitation
 tasks when supported either with visual or textual conceptual model. Since the agile literature shows little attention to reusing existing 
BPMN
 documentation, we propose several hypotheses to compare it to the use of textual use case models.
Method
We conducted an experiment to compare the effectiveness of the two business process formats: textual use cases and visual BPMN models. We studied their effects on three elicitation tasks: identifying user stories and understanding their execution-order and integration dependencies.
Results
The subjects better understood execution-order dependencies when visual input in the form of BPMN models was provided. The performance of the other two tasks showed no statistical differences.
Conclusion
We addressed an important problem of user story elicitation: which informationally equivalent model (visual BPMN or textual use case) is more effective when identifying and understanding user stories.",Information and Software Technology,18 Mar 2025,6.0,"The study addresses an important problem in agile development, but the impact on early-stage ventures may be limited to companies using agile methodologies."
https://www.sciencedirect.com/science/article/pii/S095058491930165X,Impact of the conceptual model's representation format on identifying and understanding user stories,December 2019,Not Found,Marina=Trkman: marina.trkman@gmail.com; Jan=Mendling: Not Found; Peter=Trkman: Not Found; Marjan=Krisper: Not Found,"Abstract
Context
Eliciting user stories is a major challenge for 
agile development
 approaches. Conceptual models are used to support the identification of user stories and increase their understanding. In many companies, existing model documentation stored as either use cases or 
BPMN
 models is available. However, these two types of business process models might not be equally effective for 
elicitation
 tasks due to their formats.
Objective
We address the effectiveness of different 
elicitation
 tasks when supported either with visual or textual conceptual model. Since the agile literature shows little attention to reusing existing 
BPMN
 documentation, we propose several hypotheses to compare it to the use of textual use case models.
Method
We conducted an experiment to compare the effectiveness of the two business process formats: textual use cases and visual BPMN models. We studied their effects on three elicitation tasks: identifying user stories and understanding their execution-order and integration dependencies.
Results
The subjects better understood execution-order dependencies when visual input in the form of BPMN models was provided. The performance of the other two tasks showed no statistical differences.
Conclusion
We addressed an important problem of user story elicitation: which informationally equivalent model (visual BPMN or textual use case) is more effective when identifying and understanding user stories.",Information and Software Technology,18 Mar 2025,6.0,"Similar to abstract 61, the study is relevant for agile development but may not have a direct impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301673,Assessing the effectiveness of goal-oriented modeling languages: A family of experiments,December 2019,Not Found,Silvia=Abrahão: sabrahao@dsic.upv.es; Emilio=Insfran: einsfran@dsic.upv.es; Fernando=González-Ladrón-de-Guevara: fgonzal@omp.upv.es; Marta=Fernández-Diego: marferdi@omp.upv.es; Carlos=Cano-Genoves: carcage1@dsic.upv.es; Raphael=Pereira de Oliveira: raphael.oliveira@ifs.edu.br,"Abstract
Context
Several goal-oriented languages focus on modeling stakeholders’ objectives, interests or wishes. However, these languages can be used for various purposes (e.g., exploring system solutions or evaluating alternatives), and there are few guidelines on how to use these models downstream to the software requirements and design artifacts. Moreover, little attention has been paid to the empirical evaluation of this kind of languages. In a previous work, we proposed value@GRL as a specialization of the Goal Requirements Language (GRL) to specify stakeholders’ goals when dealing with early requirements in the context of incremental software development.
Objective
This paper compares the value@GRL language with the i* language, with respect to the quality of goal models, the participants’ modeling time and productivity when creating the models, and their perceptions regarding ease of use and usefulness.
Method
A family of experiments was carried out with 184 students and practitioners in which the participants were asked to specify a goal model using each of the languages. The participants also filled in a questionnaire that allowed us to assess their perceptions.
Results
The results of the individual experiments and the meta-analysis indicate that the quality of goal models obtained with value@GRL is higher than that of i*, but that the participants required less time to create the goal models when using i*. The results also show that the participants perceived value@GRL to be easier to use and more useful than i* in at least two experiments of the family.
Conclusions
value@GRL makes it possible to obtain goal models with good quality when compared to i*, which is one of the most frequently used goal-oriented 
modeling languages
. It can, therefore, be considered as a promising emerging approach in this area. Several insights emerged from the study and opportunities for improving both languages are outlined.",Information and Software Technology,18 Mar 2025,8.0,"The comparison of goal-oriented modeling languages can provide valuable insights for early-stage ventures in software development, leading to potential improvements in their processes."
https://www.sciencedirect.com/science/article/pii/S0950584919301727,Finding key classes in object-oriented software systems by techniques based on static analysis,December 2019,Not Found,Ioana=Şora: ioana.sora@cs.upt.ro; Ciprian-Bogdan=Chirila: Not Found,"Abstract
Context
Software maintenance is burdened by 
program comprehension
 activities which consume a big part of project resources. Program comprehension is difficult because the code to be analyzed is very large and the documentation may not be well structured to help navigating through the code.
Objective
Tools should support the early stages of program comprehension. Our goal is to build tools that analyze the code and filter this large amount of information such that only the most important information is presented to the software 
maintenance team
. In the case of object-oriented systems, finding the important information means finding the most important classes, also called the key classes of the system.
Method
In this work, we formulate and explore several hypotheses regarding which are the class attributes that characterize important classes. By class attributes, we understand here different metrics that quantify properties of the class such as its connections and relationships with other classes. All the necessary input data for computing class attributes are extracted from code by 
static analysis
. We experimentally investigate which attributes are best suited to rank classes according to their importance, doing an extensive empirical study on fifteen software systems.
Result
Attributes from the categories of 
direct connections
 and network centrality are the best for finding key classes. We identified three class attributes which are best as class ranking criteria: PR-U2-W and CONN-TOTAL-W when the target set of key classes is small and CONN-TOTAL when the target set has a large and variable size. We show that the method of ranking classes based on these attributes outperforms known related work approaches of finding key classes.
Conclusions
Our method allows us to build easy-to-use fully automatic tools which find almost instantly the key classes of a software system starting from its code.",Information and Software Technology,18 Mar 2025,8.0,The development of tools for program comprehension and identifying key classes in software systems can have a significant impact on the efficiency and effectiveness of early-stage ventures in software development.
https://www.sciencedirect.com/science/article/pii/S0950584919301636,PrioriTTVs: A process aimed at supporting researchers to prioritize threats to validity and their mitigation actions when planning controlled experiments in SE,November 2019,Not Found,Eudis=Teixeira: eot@cin.ufpe.br; Liliane=Fonseca: lss4@cin.ufpe.br; Bruno=Cartaxo: email@brunocartaxo.com; Sergio=Soares: scbs@cin.ufpe.br,"Abstract
Context
Researchers argue that a critical component of any empirical study in 
Software Engineering
 (SE) is to identify, analyze, and mitigate threats to validity.
Objective
We propose PrioriTTVs, a process to support researchers in identifying and prioritizing threats to validity and their corresponding 
mitigation actions
 when planning controlled experiments in SE. We also introduce a tool to support the entire process.
Method
Empirical studies were conducted with six experts and 20 
postgraduate students
 to evaluate the ease of use, learning, and perceptions of satisfaction regarding PrioriTTVs.
Results
So far, participants have considered PrioriTTVs to be useful (83%), significantly contributing to learning (90%), and satisfaction (75%).
Conclusions
We believe both novice and expert users can benefit from the process we propose for addressing threats to validity when conducting SE experiments. We also intend to extend our approach to manage threats specific to different SE experiment contexts.",Information and Software Technology,18 Mar 2025,7.0,"The proposed process and tool for addressing threats to validity in SE experiments can benefit both novice and expert users in the field, including early-stage ventures conducting controlled experiments."
https://www.sciencedirect.com/science/article/pii/S0950584919301636,PrioriTTVs: A process aimed at supporting researchers to prioritize threats to validity and their mitigation actions when planning controlled experiments in SE,November 2019,Not Found,Eudis=Teixeira: eot@cin.ufpe.br; Liliane=Fonseca: lss4@cin.ufpe.br; Bruno=Cartaxo: email@brunocartaxo.com; Sergio=Soares: scbs@cin.ufpe.br,"Abstract
Context
Researchers argue that a critical component of any empirical study in 
Software Engineering
 (SE) is to identify, analyze, and mitigate threats to validity.
Objective
We propose PrioriTTVs, a process to support researchers in identifying and prioritizing threats to validity and their corresponding 
mitigation actions
 when planning controlled experiments in SE. We also introduce a tool to support the entire process.
Method
Empirical studies were conducted with six experts and 20 
postgraduate students
 to evaluate the ease of use, learning, and perceptions of satisfaction regarding PrioriTTVs.
Results
So far, participants have considered PrioriTTVs to be useful (83%), significantly contributing to learning (90%), and satisfaction (75%).
Conclusions
We believe both novice and expert users can benefit from the process we propose for addressing threats to validity when conducting SE experiments. We also intend to extend our approach to manage threats specific to different SE experiment contexts.",Information and Software Technology,18 Mar 2025,8.0,"The proposed PrioriTTVs process and tool provide a practical approach for addressing threats to validity in Software Engineering experiments, benefitting both novice and expert users."
https://www.sciencedirect.com/science/article/pii/S0950584919301661,Taking the emotional pulse of software engineering — A systematic literature review of empirical studies,November 2019,Not Found,Mary=Sánchez-Gordón: mary.sanchez-gordon@hiof.no; Ricardo=Colomo-Palacios: ricardo.colomo-palacios@hiof.no,"Abstract
Context
Over the past 50 years of 
Software Engineering
, numerous studies have acknowledged the importance of human factors. However, software developers’ emotions are still an area under investigation and debate that is gaining relevance in the software 
industry
.
Objective
In this study, a 
systematic literature review
 (SLR) was carried out to identify, evaluate, and synthesize research published concerning software developers’ emotions as well as the measures used to assess its existence.
Method
By searching five major 
bibliographic databases
, authors identified 7172 articles related to emotions in 
Software Engineering
. We selected 66 of these papers as primary studies. Then, they were analyzed in order to find empirical evidence of the intersection of emotions and software engineering.
Results
Studies report a total of 40 discrete emotions but the most frequent were: 
anger, fear, disgust, sadness, joy, love, and happiness
. There are also 2 different dimensional approaches and 10 datasets related to this topic which are publicly available on the Web. The findings also showed that self-reported 
mood
 instruments (e.g., 
SAM
, PANAS), 
physiological measures
 (e.g., heart rate, perspiration) or behavioral measures (e.g., keyboard use) are the least reported tools, although, there is a recognized intrinsic problem with the accuracy of current state of the art 
sentiment analysis
 tools. Moreover, most of the studies used software practitioners and/or datasets from industrial context as subjects.
Conclusions
The study of emotions has received a growing attention from the research community in the recent years, but the management of emotions has always been challenging in practice. Although it can be said that this field is not mature enough yet, our results provide a holistic view that will benefit researchers by providing the latest trends in this area and identifying the corresponding research gaps.",Information and Software Technology,18 Mar 2025,7.0,"The study of emotions in Software Engineering, although not mature yet, provides valuable insights and trends that can benefit researchers and identify research gaps in the field."
https://www.sciencedirect.com/science/article/pii/S0950584919301661,Taking the emotional pulse of software engineering — A systematic literature review of empirical studies,November 2019,Not Found,Mary=Sánchez-Gordón: mary.sanchez-gordon@hiof.no; Ricardo=Colomo-Palacios: ricardo.colomo-palacios@hiof.no,"Abstract
Context
Over the past 50 years of 
Software Engineering
, numerous studies have acknowledged the importance of human factors. However, software developers’ emotions are still an area under investigation and debate that is gaining relevance in the software 
industry
.
Objective
In this study, a 
systematic literature review
 (SLR) was carried out to identify, evaluate, and synthesize research published concerning software developers’ emotions as well as the measures used to assess its existence.
Method
By searching five major 
bibliographic databases
, authors identified 7172 articles related to emotions in 
Software Engineering
. We selected 66 of these papers as primary studies. Then, they were analyzed in order to find empirical evidence of the intersection of emotions and software engineering.
Results
Studies report a total of 40 discrete emotions but the most frequent were: 
anger, fear, disgust, sadness, joy, love, and happiness
. There are also 2 different dimensional approaches and 10 datasets related to this topic which are publicly available on the Web. The findings also showed that self-reported 
mood
 instruments (e.g., 
SAM
, PANAS), 
physiological measures
 (e.g., heart rate, perspiration) or behavioral measures (e.g., keyboard use) are the least reported tools, although, there is a recognized intrinsic problem with the accuracy of current state of the art 
sentiment analysis
 tools. Moreover, most of the studies used software practitioners and/or datasets from industrial context as subjects.
Conclusions
The study of emotions has received a growing attention from the research community in the recent years, but the management of emotions has always been challenging in practice. Although it can be said that this field is not mature enough yet, our results provide a holistic view that will benefit researchers by providing the latest trends in this area and identifying the corresponding research gaps.",Information and Software Technology,18 Mar 2025,7.0,"The study of emotions in Software Engineering, although not mature yet, provides valuable insights and trends that can benefit researchers and identify research gaps in the field."
https://www.sciencedirect.com/science/article/pii/S0950584919301648,Bug report severity level prediction in open source software: A survey and research opportunities,November 2019,Not Found,Luiz Alberto Ferreira=Gomes: luizgomes@pucpcaldas.br; Ricardo da Silva=Torres: rtorres@ic.unicamp.br; Mario Lúcio=Côrtes: cortes@ic.unicamp.br,"Abstract
Context:
 The 
severity level
 attribute of a 
bug report
 is considered one of the most critical variables for planning evolution and maintenance in Free/Libre 
Open Source Software
. This variable measures the impact the bug has on the successful execution of the software system and how soon a bug needs to be addressed by the development team. Both business and academic community have made an extensive investigation towards the proposal methods to automate the 
bug report
 severity prediction.
Objective:
 This paper aims to provide a comprehensive mapping study review of recent research efforts on automatically bug report severity prediction. To the best of our knowledge, this is the first review to categorize quantitatively more than ten aspects of the experiments reported in several papers on bug report severity prediction.
Method:
 The mapping study review was performed by searching four electronic databases. Studies published until December 2017 were considered. The initial resulting comprised of 54 papers. From this set, a total of 18 papers were selected. After performing snowballing, more nine papers were selected.
Results:
 From the mapping study, we identified 27 studies addressing bug report severity prediction on Free/Libre 
Open Source Software
. The gathered data confirm the relevance of this topic, reflects the scientific maturity of the research area, as well as, identify gaps, which can motivate new research initiatives.
Conclusion:
 The message drawn from this review is that unstructured text features along with traditional 
machine learning algorithms
 and text mining methods have been playing a central role in the most proposed methods in literature to predict bug 
severity level
. This scenario suggests that there is room for improving prediction results using state-of-the-art 
machine learning
 and text mining algorithms and techniques.",Information and Software Technology,18 Mar 2025,9.0,The mapping study review on bug report severity prediction in Free/Libre Open Source Software offers valuable quantitative insights that can motivate new research initiatives and improve prediction results.
https://www.sciencedirect.com/science/article/pii/S0950584919301478,Multi-reviewing pull-requests: An exploratory study on GitHub OSS projects,November 2019,Not Found,Dongyang=Hu: hudongyang17@163.com; Yang=Zhang: yangzhang15@nudt.edu.cn; Junsheng=Chang: Not Found; Gang=Yin: Not Found; Yue=Yu: Not Found; Tao=Wang: Not Found,"Abstract
Context:
GitHub has enabled developers to easily contribute their review comments on multiple pull-requests and switch their review focus 
between
 different pull-requests, 
i.e.
, multi-reviewing. Reviewing multiple pull-requests simultaneously may enhance work efficiency. However, multi-reviewing also relies on developers’ rationally allocating their focus, which may bring a different influence to the resolution of pull-requests.
Objective:
 In this paper, we present an ongoing study of the impact of multi-reviewing on pull-request resolution in GitHub 
open source projects
.
Method:
 We collected and analyzed 1,836,280 pull-requests from 760 GitHub projects to explore how multi-reviewing affects the resolution of a pull-request.
Results:
 We find that multi-reviewing is a common behavior in GitHub. However, more multi-reviewing behaviors tend to bring longer pull-request resolution latency.
Conclusion:
 Multi-reviewing is a complex behavior of developers, and has an important impact on the efficiency of pull-request resolution. Our study motivates the need for more research on multi-reviewing.",Information and Software Technology,18 Mar 2025,6.0,The ongoing study on the impact of multi-reviewing on pull-request resolution in GitHub open source projects is interesting but may need further exploration to determine practical implications for developers.
https://www.sciencedirect.com/science/article/pii/S0950584919301624,Using cognitive dimensions to evaluate the usability of security APIs: An empirical investigation,November 2019,Not Found,Chamila=Wijayarathna: z5122098@student.unsw.edu.au; Nalin Asanka Gamagedara=Arachchilage: n.arachchilage@latrobe.edu.au,"Abstract
Context
Usability issues
 of security 
Application Programming Interfaces
 (APIs) are a main factor for mistakes programmers make that could result in introducing 
security vulnerabilities
 into applications they develop. This has become a common problem as there is no methodology to evaluate the usability of security APIs. A 
usability evaluation
 methodology for security APIs would allow API developers to 
identify usability issues
 of security APIs and fix them. A Cognitive Dimensions Framework (CDF) based 
usability evaluation
 methodology has been proposed in previous research to empirically evaluate the usability of security APIs.
Objective
In this research, we evaluated the proposed CDF based methodology through four security APIs (Google 
Authentication
 API, Bouncy Castle light weight Crypto API, Java Secure Socket Extension API, OWASP Enterprise Security API).
Method
We conducted four experiments where in each experiment we recruited programmers and they completed a programming task using one of the four security APIs. Participants’ feedback on each cognitive dimension of the particular API was collected using the cognitive dimensions questionnaire. 
Usability issues
 of each API was identified based on this feedback.
Results
Results of the four experiments revealed that over 83% of the usability issues in a security API could be identified by this methodology with a considerably good validity and reliability.
Conclusion
The proposed CDF based usability evaluation methodology provides a good platform to conduct usability evaluation for security APIs.",Information and Software Technology,18 Mar 2025,7.0,The proposed methodology for evaluating usability of security APIs has practical value in improving the security of applications developed by early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584919301612,Automatic recall of software lessons learned for software project managers,November 2019,Not Found,Tamer Mohamed=Abdellatif: tmohame7@uwo.ca; Luiz Fernando=Capretz: lcapretz@uwo.ca; Danny=Ho: danny@nfa-estimation.com,"Abstract
Context
Lessons learned (LL) records constitute the software organization memory of successes and failures. LL are recorded within the organization repository for future reference to optimize planning, gain experience, and elevate market competitiveness. However, manually searching this repository is a daunting task, so it is often disregarded. This can lead to the repetition of previous mistakes or even missing potential opportunities. This, in turn, can negatively affect the organization's profitability and competitiveness.
Objective
We aim to present a novel solution that provides an automatic process to recall relevant LL and to push those LL to project managers. This will dramatically save the time and effort of manually searching the unstructured LL repositories and thus encourage the LL exploitation.
Method
We exploit existing project artifacts to build the LL search queries on-the-fly in order to bypass the tedious manual searching. An empirical 
case study
 is conducted to build the automatic LL recall solution and evaluate its effectiveness. The study employs three of the most popular information 
retrieval models
 to construct the solution. Furthermore, a real-world dataset of 212 LL records from 30 different software projects is used for validation. Top-k and MAP well-known accuracy metrics are used as well.
Results
Our case study results confirm the effectiveness of the automatic LL recall solution. Also, the results prove the success of using existing project artifacts to dynamically build the search query string. This is supported by a discerning accuracy of about 70% achieved in the case of top-k.
Conclusion
The automatic LL recall solution is valid with high accuracy. It will eliminate the effort needed to manually search the LL repository. Therefore, this will positively encourage project managers to reuse the available LL knowledge – which will avoid old pitfalls and unleash hidden business opportunities.",Information and Software Technology,18 Mar 2025,8.0,"The automatic LL recall solution presented has the potential to save time and effort for startups, improving planning and competitiveness."
https://www.sciencedirect.com/science/article/pii/S0950584919301697,The usefulness of software metric thresholds for detection of bad smells and fault prediction,November 2019,Not Found,Mariza A.S.=Bigonha: mariza@dcc.ufmg.br; Kecia=Ferreira: kecia@decom.cefetmg.br; Priscila=Souza: priscilinhapsouza@gmail.com; Bruno=Sousa: bruno.luan.sousa@dcc.ufmg.br; Marcela=Januário: marcelajanuario92@hotmail.com; Daniele=Lima: danieleddelima@gmail.com,"Abstract
Context
Software metrics may be an effective tool to assess the 
quality of software
, but to guide their use it is important to define their thresholds. Bad smells and fault also impact the 
quality of software
. Extracting metrics from software systems is relatively low cost since there are tools widely used for this purpose, which makes feasible applying software metrics to identify bad smells and to predict faults.
Objective
To inspect whether thresholds of object-oriented metrics may be used to aid bad smells detection and fault predictions.
Method
To direct this research, we have defined three research questions (RQ), two related to identification of bad smells, and one for identifying fault in software systems. To answer these RQs, we have proposed detection strategies for the bad smells: Large Class, Long Method, Data Class, Feature Envy, and Refused Bequest, based on metrics and their thresholds. To assess the quality of the derived thresholds, we have made two studies. The first one was conducted to evaluate their efficacy on detecting these bad smells on 12 systems. A second study was conducted to investigate for each of the class level software metrics: 
DIT
, LCOM, NOF, NOM, NORM, NSC, NSF, NSM, SIX, and WMC, if the ranges of values determined by thresholds are useful to identify fault in software systems.
Results
Both studies confirm that metric thresholds may support the prediction of faults in software and are significantly and effective in the detection of bad smells.
Conclusion
The results of this work suggest practical applications of metric thresholds to identify bad smells and predict faults and hence, support 
software quality assurance
 activities.Their use may help developers to focus their efforts on classes that tend to fail, thereby minimizing the occurrence of future problems.",Information and Software Technology,18 Mar 2025,6.0,The approach to using object-oriented metrics to identify bad smells and predict faults could provide useful insights for software quality assurance in startups.
https://www.sciencedirect.com/science/article/pii/S0950584919301259,Scaling-up domain-specific modelling languages through modularity services,November 2019,Not Found,Antonio=Garmendia: antonio.garmendia@uam.es; Esther=Guerra: Not Found; Juan=de Lara: Not Found; Antonio=García-Domínguez: Not Found; Dimitris=Kolovos: Not Found,"Abstract
Context
Model-driven engineering (MDE) promotes the active use of models in all phases of software development. Even though models are at a high level of abstraction, large or complex systems still require building monolithic models that prove to be too big for their processing by existing tools, and too difficult to comprehend by users. While modularization techniques are well-known in programming languages, they are not the norm in MDE.
Objective
Our goal is to ease the modularization of models to allow their efficient processing by tools and facilitate their management by users.
Method
We propose five patterns that can be used to extend a 
modelling language
 with services related to modularization and scalability. Specifically, the patterns allow defining model fragmentation strategies, scoping and visibility rules, model indexing services, and scoped constraints. Once the patterns have been applied to the meta-model of a 
modelling language
, we synthesize a customized modelling environment enriched with the defined services, which become applicable to both existing monolithic legacy models and new models.
Results
Our proposal is supported by a tool called EMF-Splitter, combined with the Hawk model indexer. Our experiments show that this tool improves the validation performance of large models. Moreover, the analysis of 224 meta-models from 
OMG
 standards, and a public repository with more than 300 meta-models, demonstrates the applicability of our patterns in practice.
Conclusions
Modularity mechanisms typically employed in programming IDEs can be successfully transferred to MDE, leading to more scalable and structured domain-specific modelling languages and environments.",Information and Software Technology,18 Mar 2025,6.0,The proposal for modularization of models in MDE could benefit startups by improving the efficiency and scalability of software development.
https://www.sciencedirect.com/science/article/pii/S095058491930134X,Test case selection-prioritization approach based on memoization dynamic programming algorithm,November 2019,Not Found,Ovidiu=Banias: ovidiu.banias@aut.upt.ro,"Abstract
Context
In the software industry, selection and prioritization techniques become a necessity in the regression and validation testing phases because a lot of test cases are available for reuse, yet time and project specific constraints must be respected.
Objective
In this paper we propose a 
dynamic programming
 approach in solving test case selection-prioritization problems. We focus on low memory consumption in pseudo-polynomial time complexity applicable in both selection and selection-prioritization problems over sets of test cases or test suites. In 
dynamic programming
 optimization solutions, huge amounts of memory are required and unfortunately the memory is limited. Therefore, lower memory consumption leads to a higher number of test cases to be involved in the selection process.
Method
Our approach is suited for medium to large projects where the required memory space is not higher than the order of tens of GBytes. We employed both objective methods as the 
dynamic programming algorithm
 and subjective and empiric human decision as defining the prioritization criteria. Furthermore, we propose a method of employing multiple project specific criteria in evaluating the importance of a test case in the project context.
Results
To evaluate the proposed solution relative to the classical dynamic programming 
knapsack
 solution, we developed a suite of 
comparative case studies
 based on 1000 generated scenarios as close as possible to real project scenarios. The results of the comparative study reported the proposed algorithm requires up to 400 times less memory in the best-case scenarios and about 40 times less memory in average.
Conclusion
The solution delivers optimal results in pseudo-polynomial time complexity, is effective for amounts of test cases up to the order of millions and compared with the classical 
dynamic programming methods
 leads to higher number of test cases to be involved in the selection process due to reduced memory consumption.",Information and Software Technology,18 Mar 2025,5.0,The dynamic programming approach for test case selection and prioritization could be helpful for startups in optimizing testing processes with lower memory consumption.
https://www.sciencedirect.com/science/article/pii/S095058491930134X,Test case selection-prioritization approach based on memoization dynamic programming algorithm,November 2019,Not Found,Ovidiu=Banias: ovidiu.banias@aut.upt.ro,"Abstract
Context
In the software industry, selection and prioritization techniques become a necessity in the regression and validation testing phases because a lot of test cases are available for reuse, yet time and project specific constraints must be respected.
Objective
In this paper we propose a 
dynamic programming
 approach in solving test case selection-prioritization problems. We focus on low memory consumption in pseudo-polynomial time complexity applicable in both selection and selection-prioritization problems over sets of test cases or test suites. In 
dynamic programming
 optimization solutions, huge amounts of memory are required and unfortunately the memory is limited. Therefore, lower memory consumption leads to a higher number of test cases to be involved in the selection process.
Method
Our approach is suited for medium to large projects where the required memory space is not higher than the order of tens of GBytes. We employed both objective methods as the 
dynamic programming algorithm
 and subjective and empiric human decision as defining the prioritization criteria. Furthermore, we propose a method of employing multiple project specific criteria in evaluating the importance of a test case in the project context.
Results
To evaluate the proposed solution relative to the classical dynamic programming 
knapsack
 solution, we developed a suite of 
comparative case studies
 based on 1000 generated scenarios as close as possible to real project scenarios. The results of the comparative study reported the proposed algorithm requires up to 400 times less memory in the best-case scenarios and about 40 times less memory in average.
Conclusion
The solution delivers optimal results in pseudo-polynomial time complexity, is effective for amounts of test cases up to the order of millions and compared with the classical 
dynamic programming methods
 leads to higher number of test cases to be involved in the selection process due to reduced memory consumption.",Information and Software Technology,18 Mar 2025,7.0,"The proposed dynamic programming approach addresses a common problem in software testing, optimizing memory consumption, and enabling more test cases to be involved in the selection process. This can have a significant impact on early-stage ventures by improving testing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584919301363,Simsax: A measure of project similarity based on symbolic approximation method and software defect inflow,November 2019,Not Found,Mirosław=Ochodek: mochodek@cs.put.poznan.pl; Miroslaw=Staron: miroslaw.staron@cse.gu.se; Wilhelm=Meding: wilhelm.meding@ericsson.com,"Abstract
Background
Profiling software development projects, in order to compare them, find similar sub-projects or sets of activities, helps to monitor changes in software processes. Since we lack 
objective measures
 for profiling or hashing, researchers often fall back on manual assessments.
Objective
The goal of our study is to define an objective and intuitive measure of similarity between software development projects based on software defect-inflow profiles.
Method
We defined a measure of project similarity called 
SimSAX
 which is based on segmentation of defect-inflow profiles, coding them into strings (sequences of symbols) and comparing these strings to find so-called motifs. We use simulations to find and calibrate the parameters of the measure. The objects in the simulations are two different large industry projects for which we know the similarity a priori, based on the input from industry experts. Finally, we apply the measure to find similarities between five industrial and six 
open source projects
.
Results
Our results show that the measure provides the most accurate simulated results when the compared motifs are long (32 or more weeks) and we use an alphabet of 5 or more symbols. The measure provides the possibility to calibrate for each industrial case, thus allowing to optimize the method for finding specific patterns in project similarity.
Conclusions
We conclude that our proposed measure provides a 
good approximation
 for project similarity. The industrial evaluation showed that it can provide a 
good starting point
 for finding similar periods in software development projects.",Information and Software Technology,18 Mar 2025,6.0,"The SimSAX measure provides a method for comparing software development projects based on defect-inflow profiles, offering a way to find similarities and patterns. While useful, it may not have as direct of an impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301363,Simsax: A measure of project similarity based on symbolic approximation method and software defect inflow,November 2019,Not Found,Mirosław=Ochodek: mochodek@cs.put.poznan.pl; Miroslaw=Staron: miroslaw.staron@cse.gu.se; Wilhelm=Meding: wilhelm.meding@ericsson.com,"Abstract
Background
Profiling software development projects, in order to compare them, find similar sub-projects or sets of activities, helps to monitor changes in software processes. Since we lack 
objective measures
 for profiling or hashing, researchers often fall back on manual assessments.
Objective
The goal of our study is to define an objective and intuitive measure of similarity between software development projects based on software defect-inflow profiles.
Method
We defined a measure of project similarity called 
SimSAX
 which is based on segmentation of defect-inflow profiles, coding them into strings (sequences of symbols) and comparing these strings to find so-called motifs. We use simulations to find and calibrate the parameters of the measure. The objects in the simulations are two different large industry projects for which we know the similarity a priori, based on the input from industry experts. Finally, we apply the measure to find similarities between five industrial and six 
open source projects
.
Results
Our results show that the measure provides the most accurate simulated results when the compared motifs are long (32 or more weeks) and we use an alphabet of 5 or more symbols. The measure provides the possibility to calibrate for each industrial case, thus allowing to optimize the method for finding specific patterns in project similarity.
Conclusions
We conclude that our proposed measure provides a 
good approximation
 for project similarity. The industrial evaluation showed that it can provide a 
good starting point
 for finding similar periods in software development projects.",Information and Software Technology,18 Mar 2025,6.0,"The SimSAX measure provides a method for comparing software development projects based on defect-inflow profiles, offering a way to find similarities and patterns. While useful, it may not have as direct of an impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301387,An empirical study of sentiments in code reviews,October 2019,Not Found,Ikram El=Asri: ikram.asri@um5s.net.ma; Noureddine=Kerzazi: Not Found; Gias=Uddin: Not Found; Foutse=Khomh: Not Found; M.A.=Janati Idrissi: Not Found,"Abstract
Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the 
code review process
.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (
e.g.,
 core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the 
code review process
 as well as the acceptance or rejection of the suggested changes.",Information and Software Technology,18 Mar 2025,8.0,Studying the impact of sentiments in developers' comments on the code review process can provide valuable insights for software development teams. Understanding how sentiments affect review time and outcomes can lead to improved collaboration and decision-making in early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584919301387,An empirical study of sentiments in code reviews,October 2019,Not Found,Ikram El=Asri: ikram.asri@um5s.net.ma; Noureddine=Kerzazi: Not Found; Gias=Uddin: Not Found; Foutse=Khomh: Not Found; M.A.=Janati Idrissi: Not Found,"Abstract
Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the 
code review process
.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (
e.g.,
 core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the 
code review process
 as well as the acceptance or rejection of the suggested changes.",Information and Software Technology,18 Mar 2025,8.0,Studying the impact of sentiments in developers' comments on the code review process can provide valuable insights for software development teams. Understanding how sentiments affect review time and outcomes can lead to improved collaboration and decision-making in early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584919301430,A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing,October 2019,Not Found,Juliana Marino=Balera: juliana.balera@inpe.br; Valdivino Alexandre de=Santiago Júnior: valdivino.santiago@inpe.br,"Abstract
Context
Search-based Software Testing
 (SBST) is a research field where testing a software product is formulated as an 
optimization problem
. It is an active sub-area of 
Search-based 
Software Engineering
 (SBSE) where many studies have been published and some reviews have been carried out. The majority of studies in SBST has been adopted meta-heuristics while hyper-heuristics have a long way to go. Moreover, there is still a lack of studies to perceive the state-of-the-art of the use of hyper-heuristics within SBST.
Objective
The objective of this work is to investigate the adoption of hyper-heuristics for Software Testing highlighting the current efforts and identifying new research directions.
Method
A 
Systematic mapping
 study was carried out with 5 research questions considering papers published up to may/2019, and 4 different bases. The research questions aims to find out, among other things, what are the hyper-heuristics used in the context of Software Testing, for what problems hyper-heuristics have been applied, and what are the objective functions in the scope of Software Testing.
Results
A total of 734 studies were found via the search strings and 164 articles were related to Software Testing. However, from these, only 26 papers were actually in accordance with the scope of this research and 3 more papers were considered due to snowballing or expert’s suggestion, totalizing 29 selected papers. Few different problems and application domains where hyper-heuristics have been considered were identified.
Conclusion
Differently from other communities (Operational Research, Artificial Intelligence), SBST has little explored the benefits of hyper-heuristics which include generalization and less difficulty in parameterization. Hence, it is important to further investigate this area in order to alleviate the effort of practitioners to use such an approach in their testing activities.",Information and Software Technology,18 Mar 2025,8.0,"The abstract focuses on an important research area related to software testing and highlights the need for further investigation into hyper-heuristics, which could benefit European early-stage ventures by improving testing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584919301399,Employment of multiple algorithms for optimal path-based test selection strategy,October 2019,Not Found,Miroslav=Bures: miroslav.bures@fel.cvut.cz; Bestoun S.=Ahmed: bestoun@kau.se,"Abstract
Context
Executing various sequences of 
system functions
 in a system under test represents one of the primary techniques in software testing. The natural method for creating effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate tests that satisfy a defined test coverage criterion. Several criteria for preferred test set properties can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined.
Objective
Using this prioritization, the test cases exercise the high-priority parts of the system under test by more path combinations than those with low priority (this prioritization can be combined with the test coverage criterion that determines how many path combinations of the individual parts of the system are tested). Evidence from the literature and our observations confirm that finding a universal algorithm that produces a test set with preferred properties for all test coverage criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide results with the best value of a preferred property. In this paper, we present a portfolio-based strategy to perform the best test selection.
Method
The proposed strategy first employs a set of current algorithms to generate test sets; then, a preferred property of each test set is assessed in terms of the selected criterion, and finally, the test set with the best value of a preferred property is chosen.
Results
The experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided results having the best preferred property value; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set preferred property criteria.
Conclusion
In addition to the used algorithms, the proposed strategy can be used to assess the 
optimality
 of different path-based testing algorithms and choose a 
suitable algorithm
 for the testing.",Information and Software Technology,18 Mar 2025,6.0,"The abstract presents a strategy for test selection which can be useful for startups in optimizing their testing processes, but the applicability may be limited to certain contexts."
https://www.sciencedirect.com/science/article/pii/S0950584919301399,Employment of multiple algorithms for optimal path-based test selection strategy,October 2019,Not Found,Miroslav=Bures: miroslav.bures@fel.cvut.cz; Bestoun S.=Ahmed: bestoun@kau.se,"Abstract
Context
Executing various sequences of 
system functions
 in a system under test represents one of the primary techniques in software testing. The natural method for creating effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate tests that satisfy a defined test coverage criterion. Several criteria for preferred test set properties can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined.
Objective
Using this prioritization, the test cases exercise the high-priority parts of the system under test by more path combinations than those with low priority (this prioritization can be combined with the test coverage criterion that determines how many path combinations of the individual parts of the system are tested). Evidence from the literature and our observations confirm that finding a universal algorithm that produces a test set with preferred properties for all test coverage criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide results with the best value of a preferred property. In this paper, we present a portfolio-based strategy to perform the best test selection.
Method
The proposed strategy first employs a set of current algorithms to generate test sets; then, a preferred property of each test set is assessed in terms of the selected criterion, and finally, the test set with the best value of a preferred property is chosen.
Results
The experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided results having the best preferred property value; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set preferred property criteria.
Conclusion
In addition to the used algorithms, the proposed strategy can be used to assess the 
optimality
 of different path-based testing algorithms and choose a 
suitable algorithm
 for the testing.",Information and Software Technology,18 Mar 2025,6.0,"Similar to abstract 82, this abstract also presents a strategy for test selection which can be valuable for startups, but the novelty and impact may be lower due to similarity in content."
https://www.sciencedirect.com/science/article/pii/S0950584919301417,Startup ecosystem effect on minimum viable product development in software startups,October 2019,Not Found,Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Markku=Oivo: Not Found; Kari=Liukkunen: Not Found; Jouni=Markkula: Not Found,"Abstract
Context
Software startups develop innovative products through which they scale their business rapidly, and thus, provide value to the economy, including job generation. However, most startups fail within two years of their launch because of a poor problem-solution fit and negligence of the learning process during 
minimum viable product
 (MVP) development. An ideal startup ecosystem can assist in MVP development by providing the necessary entrepreneurial education and technical skills to founding team members for identifying problem-solution fit for their product idea, allowing them to find the right product-market fit. However, existing knowledge on the effect of the startup ecosystem elements on the MVP development is limited.
Objective
The empirical study presented in this article aims to identify the effect of the six ecosystem elements (entrepreneurs, technology, market, support factors, finance, and human capital) on MVP development.
Method
We conducted a study with 13 software startups and five supporting organizations (accelerators, 
incubator
, co-working space, and investment firm) in the startup ecosystem of the city of Oulu in Finland. Data were collected through semi-structured interviews, observation, and materials.
Results
The study results showed that 
internal sources
 are most common for identifying requirements for the product idea for MVP development. The findings indicate that supporting factors, such as incubators and accelerators, can influence MVP development by providing young founders with the necessary entrepreneurship skills and education needed to create the right product-market fit.
Conclusions
We conclude from this study of a regional startup ecosystem that the 
MVP development process
 is most affected by founding team members’ experiences and skill sets and by advanced technologies. Furthermore, a constructive startup ecosystem around software startups can boost up the creation of an effective MVP to test product ideas and find a product-market fit.",Information and Software Technology,18 Mar 2025,9.0,"This abstract addresses a critical issue faced by software startups regarding MVP development and the role of ecosystem elements, providing practical insights that can directly benefit European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301405,On the use of virtual reality in software visualization: The case of the city metaphor,October 2019,Not Found,Simone=Romano: simone.romano@uniba.it; Nicola=Capece: nicola.capece@unibas.it; Ugo=Erra: ugo.erra@unibas.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Michele=Lanza: michele.lanza@usi.ch,"Abstract
Background:
 Researchers have been exploring 3D representations for visualizing software. Among these representations, one of the most popular is the 
city metaphor
, which represents a target object-oriented system as a virtual city. Recently, this metaphor has been also implemented in interactive software visualization tools that use virtual reality in an immersive 3D environment medium.
Aims:
 We assessed the city metaphor displayed on a standard 
computer screen
 and in an 
immersive virtual reality
 with respect to the support provided in the comprehension of Java software systems.
Method:
 We conducted a controlled experiment where we asked the participants to fulfill 
program comprehension
 tasks with the support of 
(i)
 an 
integrated development environment
 (Eclipse) with a plugin for gathering 
code metrics
 and identifying bad smells; and 
(ii)
 a visualization tool of the city metaphor displayed on a standard 
computer screen
 and in an 
immersive virtual reality
.
Results:
 The use of the city metaphor displayed on a standard computer screen and in an immersive virtual reality significantly improved the correctness of the solutions to 
program comprehension
 tasks with respect to Eclipse. Moreover, when carrying out these tasks, the participants using the city metaphor displayed in an immersive virtual reality were significantly faster than those visualizing with the city metaphor on a standard computer screen.
Conclusions:
 Virtual reality is a viable means for software visualization.",Information and Software Technology,18 Mar 2025,7.0,"The abstract discusses the use of virtual reality for software visualization, which could have potential applications for startups in enhancing their development processes, but the direct impact may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301351,A novel approach for automatic remodularization of software systems using extended ant colony optimization algorithm,October 2019,Not Found,Bright Gee=Varghese R: brightachus@karunya.edu; Kumudha=Raimond: kraimond@karunya.edu; Jeno=Lovesum: jenolovesum@karunya.edu,"Abstract
Context
Software modularization is extremely important to streamline the inner structure of the program modules without influencing its 
core functionality
. As the framework advances during the upkeep stage, the pristine design of the software package gets disintegrated and hence it is arduous to understand and maintain. There are many existing approaches being carried out to automatically remodularize using optimization techniques to ease the maintenance and improve the quality of the system. The outcomes are rather insufficiently optimal and depend on problem-specific operators, which in turn expands the time multifaceted nature to land at an answer. Apart from these limitations, the issues, such as time complexity, scalability and performance need to be addressed.
Objective
In this paper, an efficient automatic software remodularization using extended 
Ant Colony Optimization
 (ACO) has been proposed to remodularize the software systems.
Method
The proposed approach mainly includes two phases: optimised traversal of software system using ACO for finding the order of software files to be processed and remodularization of software system using the proposed approach of extended ACO.
Results
We experimented our proposed approach on seven software systems. The performance is evaluated by using Turbo modularization quality (MQ) which supports 
Module dependency
 graph (MDG) that have edge weights. The time complexity of remodularized software system is evaluated based on number of Turbo MQ.
Conclusion
It can be concluded that when the performance has been compared with the subsisting methodologies, for example, 
Genetic algorithm
 (GA), Hill climbing (HC) and Interactive genetic algorithms (I-GAs), the proposed approach has higher Turbo MQ value with lesser time complexity in the evaluated software systems.",Information and Software Technology,18 Mar 2025,7.0,The proposed approach of automatic software remodularization using extended Ant Colony Optimization can have a significant impact on improving the quality of software systems. The experimentation on seven software systems and comparison with existing methodologies show promising results.
https://www.sciencedirect.com/science/article/pii/S0950584919301454,Scheduling sequence selection for generating test data to cover paths of MPI programs,October 2019,Not Found,Baicai=Sun: Not Found; Jinxin=Wang: Not Found; Dunwei=Gong: dwgong@vip.163.com; Tian=Tian: Not Found,"Abstract
Context: As one of key tasks in software testing, test data generation has been receiving widespread attention in recent years. Message-passing Interface (MPI) programs, which are one representative type of parallel programs, have the characteristic of non-determinism, which is reflected by the non-deterministic execution under different scheduling sequences against the same program input. Previous studies have shown that different difficulties are raised in generating test data under different scheduling sequences, suggesting that selecting appropriate scheduling sequences is beneficial to a high efficiency.
Objective: We propose a method of selecting a superior and feasible scheduling sequence for generating test data in the criterion of path coverage against each target path of an MPI program.
Method: In the proposed method, a number of program inputs are first sampled by Latin 
hypercube
 sampling. Then, the program is executed against each sample under each scheduling sequence, and all the scheduling sequences are sorted according to the similarities between the paths traversed by these samples and the target one. Finally, the feasibility of a scheduling sequence with the best quality is investigated based on the symbolic execution.
Results: We apply the proposed method to seven typical MPI programs and compare it with the random one. The experimental results show that test data covering the target path can be generated under the selected scheduling sequence with high success rate and low time consumption.
Conclusion: The proposed method takes the influence of scheduling sequences on generating test data into consideration, thus providing a competent way to test parallel programs.",Information and Software Technology,18 Mar 2025,5.0,"The method of selecting a scheduling sequence for test data generation in MPI programs addresses an important issue, but the impact may not be as significant for a wider range of European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584919301466,Improving defect prediction with deep forest,October 2019,Not Found,Tianchi=Zhou: Not Found; Xiaobing=Sun: xbsun@yzu.edu.cn; Xin=Xia: Not Found; Bin=Li: Not Found; Xiang=Chen: Not Found,"Abstract
Context
Software defect
 prediction is important to ensure the 
quality of software
. Nowadays, many supervised learning techniques have been applied to identify defective instances (
e.g.
, methods, classes, and modules).
Objective
However, the performance of these supervised learning techniques are still far from satisfactory, and it will be important to design more advanced techniques to improve the performance of 
defect prediction
 models.
Method
We propose a new deep forest model to build the defect prediction model (
DPDF
). This model can identify more important defect features by using a new cascade strategy, which transforms 
random forest classifiers
 into a layer-by-layer structure. This design takes full advantage of 
ensemble learning
 and 
deep learning
.
Results
We evaluate our approach on 25 
open source projects
 from four public datasets (
i.e.
, NASA, PROMISE, AEEEM and Relink). Experimental results show that our approach increases AUC value by 5% compared with the best traditional 
machine learning algorithms
.
Conclusion
The deep strategy in 
DPDF
 is effective for software defect prediction.",Information and Software Technology,18 Mar 2025,9.0,The proposed deep forest model for software defect prediction introduces a new innovative technique that shows significant improvement in performance compared to traditional machine learning algorithms. The evaluation on 25 open source projects demonstrates strong practical value.
https://www.sciencedirect.com/science/article/pii/S0950584917302793,DevOps in practice: A multiple case study of five companies,October 2019,Not Found,Lucy Ellen=Lwakatare: lucylwakatare@yahoo.com; Terhi=Kilamo: Not Found; Teemu=Karvonen: Not Found; Tanja=Sauvola: Not Found; Ville=Heikkilä: Not Found; Juha=Itkonen: Not Found; Pasi=Kuvaja: Not Found; Tommi=Mikkonen: Not Found; Markku=Oivo: Not Found; Casper=Lassenius: Not Found,"Abstract
Context:
 
DevOps
 is considered important in the ability to frequently and reliably update a system in operational state. 
DevOps
 presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects.
Objectives:
 This 
exploratory study
 presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in 
small and medium sized companies
.
Method:
 A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis.
Results:
 Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep 
learning curve
 for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure.
Conclusion:
 Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in 
small and medium sized companies
. We discuss two practical implications of the results.",Information and Software Technology,18 Mar 2025,8.0,The detailed exploration of DevOps implementation in small and medium sized companies provides valuable insights into the practical challenges and benefits. The study contributes to the understanding of DevOps impact on software development processes.
https://www.sciencedirect.com/science/article/pii/S0950584918301216,"A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem",October 2019,Not Found,Fabio=Calefato: fabio.calefato@uniba.it; Filippo=Lanubile: filippo.lanubile@uniba.it; Bogdan=Vasilescu: vasilescu@cmu.edu,"Abstract
Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global 
software engineering
 — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in 
software engineering
 to use psychometric tools to control for differences in developers’ personalities.",Information and Software Technology,18 Mar 2025,6.0,"The analysis of developers' personalities in large scale-distributed projects is interesting, but the practical implications for European early-stage ventures and startups may not be as direct or immediately impactful."
https://www.sciencedirect.com/science/article/pii/S0950584918301216,"A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem",October 2019,Not Found,Fabio=Calefato: fabio.calefato@uniba.it; Filippo=Lanubile: filippo.lanubile@uniba.it; Bogdan=Vasilescu: vasilescu@cmu.edu,"Abstract
Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global 
software engineering
 — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in 
software engineering
 to use psychometric tools to control for differences in developers’ personalities.",Information and Software Technology,18 Mar 2025,7.0,"The study provides insights into the influence of developers' personalities on project outcomes in large-scale distributed projects, which can be valuable for startups working in such environments."
https://www.sciencedirect.com/science/article/pii/S0950584918301228,Resilience of distributed student teams to stress factors: A longitudinal case-study,October 2019,Not Found,Igor=Čavrak: igor.cavrak@fer.hr; Ivana=Bosnić: Not Found; Federico=Ciccozzi: Not Found; Raffaela=Mirandola: Not Found,"Abstract
Context:
 Teaching global 
software engineering
 is continuously evolving and improving to prepare future software engineers adequately. Geographically distributed work in project-oriented software development courses is both demanding and rewarding for student teams, who are susceptible to various risks stemming from different internal and external factors, being the sources of stress and impacting team performance.
Objective:
 In this paper, we analyze the resilience of teams of students working in a geographically fully distributed setting. Resilience is analyzed in relation to two representative stress factors: non-contributing team members and changes to customer project requirements. We also reason on team collaboration patterns and analyze potential dependencies among these collaboration patterns, team resilience and stress factors.
Method:
 We conduct a longitudinal case-study over five years on our Distributed Software Development (DSD) course. Based on empirical data, we study team resilience to two stress factors by observing their impact on process and product 
quality aspects
 of team performance. The same performance aspects are studied for identified collaboration patterns, and bidirectional influence between patterns and resilience is investigated.
Results:
 Teams with up to two non-contributing members experience 
graceful degradation
 of performance indicators. A large number of non-contributing students almost guarantees the occurrence of educationally undesirable collaboration patterns. Exposed to requirements change stress, less resilient teams tend to focus on delivering the functional product rather than retaining a proper 
development process
.
Conclusions:
 Practical recommendations to be applied in contexts similar to our case have been provided at the end of the study. They include suggestions to mitigate the sources of stress, for example, by careful planning the team organization and balancing the number of regular and exchange students, or by discussing the issue of changing requirements with the external customers before the start of the project.",Information and Software Technology,18 Mar 2025,8.0,"The analysis of team resilience in geographically distributed settings can offer practical recommendations for startups facing similar challenges, thereby enhancing their team performance and project outcomes."
https://www.sciencedirect.com/science/article/pii/S0950584918301721,Pareto efficient multi-objective black-box test case selection for simulation-based testing,October 2019,Not Found,Aitor=Arrieta: aarrieta@mondragon.edu; Shuai=Wang: shuai.wang@testify.no; Urtzi=Markiegi: umarkiegi@mondragon.edu; Ainhoa=Arruabarrena: eibek03@mondragon.edu; Leire=Etxeberria: letxeberria@mondragon.edu; Goiuria=Sagardui: gsagardui@mondragon.edu,"Abstract
Context:
 In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the 
historical data
 related to failures might not be available.
Objective:
 The objective of the approach presented in this paper is to cost-effectively select test cases without making use of white-box coverage information or 
historical data
 related to fault detection.
Method:
 We propose a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total six effectiveness measures and one cost measure followed by deriving in total 21 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed six effectiveness metrics are specific to simulation models and are based on anti-patterns and similarity measures.
Results:
 We empirically evaluated our approach with these 21 combinations using six 
case studies
 by employing mutation testing to assess the fault revealing capability. We compared our approach with Random Search (RS), two many-objective algorithm, as well as three white-box metrics. The results demonstrated that our approach managed to improve Random Search by up to around 28% in terms of the Hypervolume quality indicator. Similarly, black-box metrics-based test case selection also significantly outperformed those of white-box metrics.
Conclusion:
 We demonstrate that test case selection is a non-trivial problem in the context of simulation models. We also show that the proposed effectiveness metrics performed significantly better than traditional white-box metrics. Thus, we show that black-box test selection approaches are appropriate to solve the test case selection problem within simulation models.",Information and Software Technology,18 Mar 2025,9.0,"The cost-effective approach for test case selection in simulation models can significantly benefit startups by improving testing efficiency and fault detection, thus optimizing software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584918301721,Pareto efficient multi-objective black-box test case selection for simulation-based testing,October 2019,Not Found,Aitor=Arrieta: aarrieta@mondragon.edu; Shuai=Wang: shuai.wang@testify.no; Urtzi=Markiegi: umarkiegi@mondragon.edu; Ainhoa=Arruabarrena: eibek03@mondragon.edu; Leire=Etxeberria: letxeberria@mondragon.edu; Goiuria=Sagardui: gsagardui@mondragon.edu,"Abstract
Context:
 In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the 
historical data
 related to failures might not be available.
Objective:
 The objective of the approach presented in this paper is to cost-effectively select test cases without making use of white-box coverage information or 
historical data
 related to fault detection.
Method:
 We propose a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total six effectiveness measures and one cost measure followed by deriving in total 21 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed six effectiveness metrics are specific to simulation models and are based on anti-patterns and similarity measures.
Results:
 We empirically evaluated our approach with these 21 combinations using six 
case studies
 by employing mutation testing to assess the fault revealing capability. We compared our approach with Random Search (RS), two many-objective algorithm, as well as three white-box metrics. The results demonstrated that our approach managed to improve Random Search by up to around 28% in terms of the Hypervolume quality indicator. Similarly, black-box metrics-based test case selection also significantly outperformed those of white-box metrics.
Conclusion:
 We demonstrate that test case selection is a non-trivial problem in the context of simulation models. We also show that the proposed effectiveness metrics performed significantly better than traditional white-box metrics. Thus, we show that black-box test selection approaches are appropriate to solve the test case selection problem within simulation models.",Information and Software Technology,18 Mar 2025,9.0,"Similar to Abstract 93, the cost-effective test case selection approach can have a high practical value for startups, leading to improved quality and efficiency in software testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584918301630,Standing on the shoulders of giants: Seeding search-based multi-objective optimization with prior knowledge for software service composition,October 2019,Not Found,Tao=Chen: txc919@gmail.com; Miqing=Li: m.li.8@cs.bham.ac.uk,"Abstract
Context
Search-Based 
Software Engineering
, in particular multi-objective 
evolutionary algorithm
, is a promising approach to engineering software service composition while simultaneously optimizing multiple conflicting Quality-of-Service (QoS) objectives. Yet, existing applications of 
evolutionary algorithms
 have failed to consider domain knowledge about the problem into the optimization, which is a perhaps obvious but challenging task.
Objective
This paper aims to investigate different strategies of exploring and injecting knowledge about the problem into the Multi-Objective Evolutionary Algorithm (MOEA) by 
seeding
. Further, we investigate various factors that contribute to the effectiveness of seeding, including the number of seeds, the importance of crossover operation and the similarity of historical problems.
Method
We conduced empirical evaluations with NSGA-II, MOEA/D and IBEA based on a wide spectrum of problem instances, including 10 different workflow structures, from 5 to 100 abstract services and 510 to 5.529  × 10
203
 candidate concrete services with diverse QoS on latency, throughput and cost, which was chosen from the real-world WS-DREAM dataset that contains 4500 QoS values.
Results
We found that, (i) all seeding strategies generally outperform their non-seeded counterparts under the same search budget with large statistical significance. Yet, they may involve relatively smaller compromise on one or two of the 
quality aspects
 among convergence, uniformity and spread. (ii) The implication of the number of seeds on the service 
composition problems
 is minimal in general (except for IBEA). (iii) In contrast to the non-seeded counterparts, the seeding strategies suffer much less implications by the crossover operation. (iv) The differences of historical problems, which are important for two proposed seeding strategies, can indeed affect the results in a non-linear manner; however, the results are still greatly better than the non-seeded counterparts even with up to 90% difference of the problem settings.
Conclusion
The paper concludes that (i) When applying the seeding strategies, the number of seeds to be placed in is less important in general, except for the pre-optimization based strategies under IBEA. (ii) Eliminating or having less crossover is harmful for multi-objective service composition optimization, but the seeding strategies are much less sensitive to this operator than their non-seeded counterparts. (iii) For the history based seeding strategies, the seeds do not have to come from the most similar historical composition problem to achieve the best HV value, but a largely different historical problem should usually be avoided, unless they are the only available seeds.",Information and Software Technology,18 Mar 2025,8.0,Investigating strategies for injecting domain knowledge into evolutionary algorithms for software composition optimization can be highly beneficial for startups aiming to enhance the quality of their software services while optimizing multiple QoS objectives.
https://www.sciencedirect.com/science/article/pii/S0950584919300990,A systematic literature review of test breakage prevention and repair techniques,September 2019,Not Found,Javaria=Imtiaz: javaria.imtiaz@questlab.pk; Salman=Sherin: salman.sherin@questlab.pk; Muhammad Uzair=Khan: uzair.khan@questlab.pk; Muhammad Zohaib=Iqbal: zohaib.iqbal@questlab.pk,"Abstract
Context
When an application evolves, some of the developed test cases break. Discarding broken test cases causes a significant waste of effort and leads to test suites that are less effective and have lower coverage. Test repair approaches evolve test suites along with applications by repairing the broken test cases.
Objective
Numerous studies are published on test repair approaches every year. It is important to summarise and consolidate the existing knowledge in the area to provide directions to researchers and practitioners. This research work provides a systematic literature review in the area of test case repair and breakage prevention, aiming to guide researchers and practitioners in the field of software testing.
Method
We followed the standard protocol for conducting a systematic literature review. First, research goals were defined using the Goal Question Metric (GQM). Then we formulate research questions corresponding to each goal. Finally, metrics are extracted from the included papers. Based on the defined selection criteria a final set of 41 primary studies are included for analysis.
Results
The selection process resulted in 5 journal papers, and 36 
conference papers
. We present a taxonomy that lists the causes of test case breakages extracted from the literature. We found that only four proposed test repair tools are publicly available. Most studies evaluated their approaches on open-source 
case studies
.
Conclusion
There is significant room for future research on test repair techniques. Despite the positive trend of evaluating approaches on large scale open source studies, there is a clear lack of results from studies done in a real industrial context. Few tools are publicly available which lowers the potential of adaption by industry practitioners.",Information and Software Technology,18 Mar 2025,6.0,"The systematic literature review on test case repair and breakage prevention provides valuable insights for researchers and practitioners in the field of software testing. However, the lack of real industrial context studies and limited availability of test repair tools slightly limit the immediate practical impact."
https://www.sciencedirect.com/science/article/pii/S095058491930103X,Adopting configuration management principles for managing experiment materials in families of experiments,September 2019,Not Found,Edison=Espinosa: egespinosa1@espe.edu.ec; Silvia Teresita=Acuña: silvia.acunna@uam.es; Sira=Vegas: svegas@fi.upm.es; Natalia=Juristo: natalia@fi.upm.es,"Abstract
Context
Replication is a key component of experimentation for verifying previous results and findings. Experiment replication requires products like documentation describing the baseline experiment and a version of the experimental material. When replicating an experiment, changes may have to be made to some of the products, leading to new or modified versions of materials. After the replication has been conducted, part of or all the materials should be added to the family history or to the baseline experiment documentation. As the number of replications increases, more versions of the materials are generated. This can lead to product management chaos in replications sharing the same protocol.
Objective
The aim of this paper is to adopt 
configuration management
 principles to manage experimental materials. We apply and validate these principles in a code inspection technique comparison experiment and a personality quasi-experiment.
Method
The study was conducted within a research group with lengthy experience in experiment replication. This research group has had trouble with the management of the materials used to run some of the experiments replicated by other colleagues. This is a suitable context for applying action research. We used action research to adopt the 
configuration management
 principles and build a materials management framework.
Result
We generated the instances of an experiment and a quasi-experiment, identifying the status and traceability of the materials. Additionally, we documented the workload required for 
instantiation
 in person-hours. We also checked the ease of use and understanding of the framework for instantiating the personality quasi-experiment configuration plan executed by researchers who did not develop the framework, as well as its usefulness for managing the experimental materials.
Conclusion
The experimental materials management framework is useful for establishing the status and traceability of the experimental materials. Additionally, it improves the storage, search, location and retrieval of the experimental material versions.",Information and Software Technology,18 Mar 2025,4.0,"The experimental materials management framework is useful for establishing the status and traceability of experimental materials, improving storage and retrieval. However, the focus on a specific research group's experience may limit generalizability and widespread practical applicability."
https://www.sciencedirect.com/science/article/pii/S0950584919301053,Multi-armed bandits in the wild: Pitfalls and strategies in online experiments,September 2019,Not Found,David=Issa Mattos: davidis@chalmers.se; Jan=Bosch: jan.bosch@chalmers.se; Helena Holmström=Olsson: helena.holmstrom.olsson@mah.se,"Abstract
Context
Delivering faster value to customers with online experimentation is an emerging practice in industry. Multi-Armed Bandit (MAB) based experiments have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, the incorrect use of MAB-based experiments can lead to incorrect conclusions that can potentially hurt the company's business.
Objective
The objective of this study is to understand the pitfalls and restrictions of using MABs in online experiments, as well as the strategies that are used to overcome them.
Method
This research uses a multiple 
case study method
 with eleven experts across five software companies and simulations to triangulate the data of some of the identified limitations.
Results
This study analyzes some limitations faced by companies using MAB and discusses strategies used to overcome them. The results are summarized into practitioners’ guidelines with criteria to select an appropriated 
experimental design
.
Conclusion
MAB algorithms have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, potential mistakes can occur and hinder the 
potential benefits
 of such approach. Together with the provided guidelines, we aim for this paper to be used as reference material for practitioners during the design of an online experiment.",Information and Software Technology,18 Mar 2025,8.0,Understanding the pitfalls and restrictions of using Multi-Armed Bandit (MAB) experiments and providing guidelines for selecting experimental designs can significantly impact companies engaging in online experiments. The practical implications of avoiding potential mistakes and improving resource allocation make this study highly valuable.
https://www.sciencedirect.com/science/article/pii/S0950584919301156,Specifying quantities in software models,September 2019,Not Found,Loli=Burgueño: lburguenoc@uoc.edu; Tanja=Mayerhofer: Not Found; Manuel=Wimmer: Not Found; Antonio=Vallecillo: Not Found,"Abstract
Context
An essential requirement for the design and development of any 
engineering application
 that deals with real-world 
physical systems
 is the formal representation and processing of 
physical quantities
, comprising both measurement uncertainty and units. Although solutions exist for several programming languages and simulation frameworks, this problem has not yet been fully solved for software models.
Objective
This paper shows how both measurement uncertainty and units can be effectively incorporated into software models, becoming part of their basic type systems.
Method
We introduce the main concepts and mechanisms needed for representing and handling 
physical quantities
 in software models. More precisely, we describe an extension of basic type Real, called Quantity, and a set of operations defined for the values of that type, together with a ready-to-use library of 
dimensions and units
, which can be added to any modeling project.
Results
We show how our approach permits modelers to safely represent and operate with physical quantities, statically ensuring type- and unit-safe assignments and operations, prior to any simulation of the system or implementation in any programming language.
Conclusion
Our approach improves the 
expressiveness
 and type-safety of software models with respect to measurement uncertainty and units of physical quantities, and its effective use in modeling projects of 
physical systems
.",Information and Software Technology,18 Mar 2025,7.0,"Incorporating measurement uncertainty and units into software models can enhance type-safety and expressiveness in representing physical quantities, benefiting engineering applications. The proposed Quantity type and operations provide a practical solution, although the application in real-world projects needs further validation."
https://www.sciencedirect.com/science/article/pii/S095058491930117X,An HMM-based approach for automatic detection and classification of duplicate bug reports,September 2019,Not Found,Neda=Ebrahimi: n_ebr@ece.concordia.ca; Abdelaziz=Trabelsi: trabelsi@ece.concordia.ca; Md. Shariful=Islam: mdsha_i@ece.concordia.ca; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca; Kobra=Khanmohammadi: k_khanm@ece.concordia.ca,"Abstract
Context
Software projects rely on their issue tracking systems to guide maintenance activities of software developers. 
Bug reports
 submitted to the issue tracking systems carry crucial information about the nature of the crash (such as texts from users or developers and execution information about the running functions before the occurrence of a crash). Typically, big software projects receive thousands of reports every day.
Objective
The aim is to reduce the time and effort required to fix bugs while improving software quality overall. Previous studies have shown that a large amount of bug reports are duplicates of previously reported ones. For example, as many as 30% of all reports in for Firefox are duplicates.
Method
While there exist a wide variety of approaches to automatically detect duplicate bug reports by 
natural language processing
, only a few approaches have considered execution information (the so-called stack traces) inside bug reports. In this paper, we propose a novel approach that automatically detects duplicate bug reports using stack traces and Hidden Markov Models.
Results
When applying our approach to Firefox and GNOME datasets, we show that, for Firefox, the average recall for Rank 
k
 = 1 is 59%, for Rank 
k
 = 2 is 75.55%. We start reaching the 90% recall from 
k
 = 10. The 
Mean Average Precision
 (MAP) value is up to 76.5%. For GNOME, The recall at 
k
 = 1 is around 63%, while this value increases by about 10% for 
k
 = 2. The recall increases to 97% for 
k
 = 11. A MAP value of up to 73% is achieved.
Conclusion
We show that HMM and stack traces are a powerful combination for detecting and classifying duplicate bug reports in large bug repositories.",Information and Software Technology,18 Mar 2025,9.0,The novel approach of automatically detecting duplicate bug reports using stack traces and Hidden Markov Models shows promising results with high recall rates and Mean Average Precision values for large bug repositories like Firefox and GNOME. The potential to significantly reduce time and effort required to fix bugs and improve software quality makes this study highly impactful.
https://www.sciencedirect.com/science/article/pii/S0950584919301223,On the use of usage patterns from telemetry data for test case prioritization,September 2019,Not Found,Jeff=Anderson: jeffand@microsoft.com; Maral=Azizi: maraazizi@my.unt.edu; Saeed=Salem: saeed.salem@ndsu.edu; Hyunsook=Do: hyunsook.do@unt.edu,"Abstract
Context
Modern applications contain pervasive telemetry to ensure reliability and enable monitoring and diagnosis. This presents a new opportunity in the area of regression testing techniques, as we now have the ability to consider usage profiles of the software when making decisions on test execution.
Objective
The results of our prior work on test prioritization using telemetry data showed improvement rate on test suite reduction, and test 
execution time
. The objective of this paper is to further investigate this approach and apply prioritization based on multiple prioritization algorithms in an enterprise level cloud application as well as 
open source projects
. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of this technique factors with code coverage-based prioritization approaches, which is the most commonly used test prioritization technique.
Method
We introduce a method for identifying usage patterns based on telemetry, which we refer to as “telemetry fingerprinting.” Through the use of various algorithms to compute fingerprints, we conduct empirical studies on multiple software products to show that telemetry fingerprinting can be used to more effectively prioritize 
regression tests
.
Results
Our experimental results show that the proposed techniques were able to reduce over 30% in regression test suite run times compared to the coverage-based prioritization technique in detecting discoverable faults. Further, the results indicate that fingerprints are effective in identifying usage patterns, and that the fingerprints can be applied to improve regression testing techniques.
Conclusion
In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms to compute fingerprints and conduct empirical studies that show that fingerprints are effective in identifying distinct usage patterns. By applying these techniques, we believe that regression testing techniques can be improved beyond the current state-of-the-art, yielding additional cost and quality benefits.",Information and Software Technology,18 Mar 2025,8.0,"The paper introduces a novel approach in regression testing techniques using telemetry data, showing significant improvements in test suite reduction and execution time, which can benefit early-stage ventures by improving testing efficiency and quality."
https://www.sciencedirect.com/science/article/pii/S0950584919300709,Ranking of software developers based on expertise score for bug triaging,August 2019,Not Found,Asmita=Yadav: asmita.yadav85@gmail.com; Sandeep Kumar=Singh: sandeepk.singh@jiit.ac.in,"Abstract
Context
Existing bug triage approaches for developer recommendation systems are mainly based on 
machine learning
 (ML) techniques. These approaches have shown low prediction accuracy and high bug tossing length (BTL).
Objective
The objective of this paper is to develop a robust algorithm for reducing BTL based on the concept of developer expertise score (DES).
Method
None of the existing approaches to the best of our knowledge have utilized metrics to build developer expertise score. The novel strategy of DES is consisted of two stages: Stage-I consisted of an offline process for detecting the developers based on DES which computes the score using priority, versatility and average fix-time for his individual contributions. The online system process consisted of finding the capable developers using three kinds of similarity measures (feature-based, cosine-similarity and Jaccard). Stage-II of the online process consisted of simply ranking the developers. Hit-ratio and reassignment accuracy were used for performance evaluation. We compared our system against the ML-based bug triaging approaches using three types of classifiers: Navies Bayes, Support Vector Machine and C4.5 paradigms.
Results
By adapting the five open source databases, namely: Mozilla, Eclipse, Netbeans, Firefox, and Freedesktop, covering 41,622 
bug reports
, our novel DES system yielded a mean accuracy, precision, recall rate and F-score of 
89.49%, 89.53%, 89.42%
 and 
89.49%
, respectively, reduced BTLs of up to 
88.55%
. This demonstrates an improvement of up to 
20%
 over existing strategies.
Conclusion
This work presented a novel developer recommendation algorithm to rank the developers based on a metric-based integrated score for bug triaging. This integrated score was based on the developer's expertise with an objective to improve (i) bug assignment and (ii) reduce the bug tossing length. Such architecture has an application in software bug triaging frameworks.",Information and Software Technology,18 Mar 2025,9.0,"The paper presents a robust algorithm for bug triage based on developer expertise score, showing a high level of accuracy and significantly reduced bug tossing length, which could have a substantial impact on startups by improving bug resolution efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584919300709,Ranking of software developers based on expertise score for bug triaging,August 2019,Not Found,Asmita=Yadav: asmita.yadav85@gmail.com; Sandeep Kumar=Singh: sandeepk.singh@jiit.ac.in,"Abstract
Context
Existing bug triage approaches for developer recommendation systems are mainly based on 
machine learning
 (ML) techniques. These approaches have shown low prediction accuracy and high bug tossing length (BTL).
Objective
The objective of this paper is to develop a robust algorithm for reducing BTL based on the concept of developer expertise score (DES).
Method
None of the existing approaches to the best of our knowledge have utilized metrics to build developer expertise score. The novel strategy of DES is consisted of two stages: Stage-I consisted of an offline process for detecting the developers based on DES which computes the score using priority, versatility and average fix-time for his individual contributions. The online system process consisted of finding the capable developers using three kinds of similarity measures (feature-based, cosine-similarity and Jaccard). Stage-II of the online process consisted of simply ranking the developers. Hit-ratio and reassignment accuracy were used for performance evaluation. We compared our system against the ML-based bug triaging approaches using three types of classifiers: Navies Bayes, Support Vector Machine and C4.5 paradigms.
Results
By adapting the five open source databases, namely: Mozilla, Eclipse, Netbeans, Firefox, and Freedesktop, covering 41,622 
bug reports
, our novel DES system yielded a mean accuracy, precision, recall rate and F-score of 
89.49%, 89.53%, 89.42%
 and 
89.49%
, respectively, reduced BTLs of up to 
88.55%
. This demonstrates an improvement of up to 
20%
 over existing strategies.
Conclusion
This work presented a novel developer recommendation algorithm to rank the developers based on a metric-based integrated score for bug triaging. This integrated score was based on the developer's expertise with an objective to improve (i) bug assignment and (ii) reduce the bug tossing length. Such architecture has an application in software bug triaging frameworks.",Information and Software Technology,18 Mar 2025,9.0,"The paper presents a robust algorithm for bug triage based on developer expertise score, showing a high level of accuracy and significantly reduced bug tossing length, which could have a substantial impact on startups by improving bug resolution efficiency."
https://www.sciencedirect.com/science/article/pii/S095058491930076X,“Bad smells” in software analytics papers,August 2019,Not Found,Tim=Menzies: timm@ieee.org; Martin=Shepperd: Not Found,"Abstract
Context
There has been a rapid growth in the use of 
data analytics
 to underpin evidence-based 
software engineering
. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies.
Objective
Our goal is to provide guidance for producers and consumers of 
software analytics
 studies (computational experiments and correlation studies).
Method
We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in 
software analytics
 studies.
Results
We list 12 “bad smells” in software analytics papers (and show their impact by examples).
Conclusions
We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).",Information and Software Technology,18 Mar 2025,5.0,"The paper addresses concerns about the reliability of software analytics studies but does not provide a concrete solution or improvement for early-stage ventures, hence the score is lower."
https://www.sciencedirect.com/science/article/pii/S0950584919300771,A critical appraisal tool for systematic literature reviews in software engineering,August 2019,"Systematic literature reviews, Quality assessment, Software engineering, Critical appraisal tools, AMSTAR",Nauman bin=Ali: nauman.ali@bth.se; Muhammad=Usman: muu@bth.se,"Abstract
Context:
 Methodological research on 
systematic literature reviews
 (SLRs) in 
Software Engineering
 (SE) has so far focused on developing and evaluating guidelines for conducting 
systematic reviews
. However, the support for quality assessment of completed SLRs has not received the same level of attention.
Objective:
 To raise awareness of the need for a critical appraisal tool (CAT) for assessing the quality of SLRs in SE. To initiate a community-based effort towards the development of such a tool.
Method:
 We reviewed the literature on the quality assessment of SLRs to identify the frequently used CATs in SE and other fields. 
Results:
 We identified that the CATs currently used is SE were borrowed from medicine, but have not kept pace with substantial advancements in the field of medicine.
Conclusion:
 In this paper, we have argued the need for a CAT for quality appraisal of SLRs in SE. We have also identified a tool that has the potential for application in SE. Furthermore, we have presented our approach for adapting this state-of-the-art CAT for assessing SLRs in SE.",Information and Software Technology,18 Mar 2025,6.0,"The paper highlights the need for a critical appraisal tool for assessing the quality of systematic literature reviews in Software Engineering, which can be beneficial for startups in understanding and evaluating existing research but does not directly impact their operations."
https://www.sciencedirect.com/science/article/pii/S0950584919300904,Dynamic selection of fitness function for software change prediction using Particle Swarm Optimization,August 2019,Not Found,Ruchika=Malhotra: ruchikamalhotra@dtu.ac.in; Megha=Khanna: Not Found,"Abstract
Context
Over the past few years, researchers have been actively searching for an effective classifier which correctly predicts change prone classes. Though, few researchers have ascertained the predictive capability of search-based algorithms in this domain, their effectiveness is highly dependent on the selection of an optimum fitness function. The criteria for selecting one fitness function over the other is the improved predictive capability of the developed model on the entire dataset. However, it may be the case that various subsets of instances of a dataset may give best results with a different fitness function.
Objective
The aim of this study is to choose the best fitness function for each instance rather than the entire dataset so as to create models which correctly ascertain the change prone nature of majority of instances. Therefore, we propose a novel framework for the adaptive selection of a dynamic optimum fitness function for each instance of the dataset, which would correctly determine its change prone nature.
Method
The 
predictive models
 in this study are developed using seven different fitness variants of Particle Swarm Optimization (PSO) algorithm. The proposed framework predicts the best suited fitness variant amongst the seven investigated fitness variants on the basis of structural characteristics of a corresponding instance.
Results
The results of the study are empirically validated on fifteen datasets collected from popular open-source software. The proposed adaptive framework was found efficient in determination of change prone classes as it yielded improved results when compared with models developed using individual fitness variants and fitness-based voting 
ensemble classifiers
.
Conclusion
The performance of the models developed using the proposed adaptive framework were statistically better than the models developed using individual fitness variants of PSO algorithm and competent to models developed using 
machine learning
 
ensemble classifiers
.",Information and Software Technology,18 Mar 2025,8.0,"The study proposes a novel framework for adaptive selection of fitness functions in predictive models, which could greatly impact the effectiveness of change prone class determination in software development."
https://www.sciencedirect.com/science/article/pii/S0950584919300898,Towards a reduction in architectural knowledge vaporization during agile global software development,August 2019,Not Found,Gilberto=Borrego: gilberto.borrego@uabc.edu.mx; Alberto L.=Morán: Not Found; Ramón R.=Palacio: Not Found; Aurora=Vizcaíno: Not Found; Félix O.=García: Not Found,"Abstract
Context
The adoption of agile methods is a trend in 
global software development
 (GSD), but may result in many challenges. One important challenge is architectural knowledge (AK) management, since agile developers prefer 
sharing knowledge
 through face-to-face interactions, while in GSD the preferred manner is documents. Agile knowledge-sharing practices tend to predominate in GSD companies that practice 
agile development
 (AGSD), leading to a lack of documents, such as 
architectural designs
, data models, deployment specifications, etc., resulting in the loss of AK over time, i.e., it vaporizes.
Objective
In a previous study, we found that there is important AK in the log files of unstructured textual electronic media (UTEM), such as instant messengers, emails, forums, etc., which are the preferred means employed in AGSD to contact remote teammates. The objective of this paper is to present and evaluate a proposal with which to recover AK from UTEM logs. We developed and evaluated a prototype that implements our proposal in order to determine its feasibility.
Method
The evaluation was performed by conducting a study with agile/global developers and students, who used the prototype and different UTEM to execute tasks that emulate common situations concerning AGSD teams’ lack of documentation during development phases.
Results
Our prototype was considered a useful, usable and unobtrusive tool when retrieving AK from UTEM logs. The participants also preferred our prototype when searching for AK and found AK faster with the prototype than with UTEM when the origin of the AK required was unknown.
Conclusion
The participants’ performance and perceptions when using our prototype provided evidence that our proposal could reduce AK vaporization in AGSD environments. These results encourage us to evaluate our proposal in a long-term test as future work.",Information and Software Technology,18 Mar 2025,7.0,The prototype developed to recover architectural knowledge from unstructured textual electronic media was considered useful and could potentially reduce knowledge loss in agile/global software development environments.
https://www.sciencedirect.com/science/article/pii/S0950584919300928,A Community Strategy Framework – How to obtain influence on requirements in meritocratic open source software communities?,August 2019,Not Found,J.=Linåker: johan.linaker@cs.lth.se; B.=Regnell: bjorn.regnell@cs.lth.se; D.=Damian: damian.daniela@gmail.com,"Abstract
Context:
 In the 
Requirements Engineering
 (RE) process of an 
Open Source Software
 (OSS) community, an involved firm is a stakeholder among many. Conflicting agendas may create miss-alignment with the firm’s internal requirements strategy. In communities with meritocratic governance or with aspects thereof, a firm has the opportunity to affect the RE process in line with their own agenda by gaining influence through active and symbiotic engagements.
Objective:
 The focus of this study has been to identify what aspects that firms should consider when they assess their need of influencing the RE process in an OSS community, as well as what engagement practices that should be considered in order to gain this influence.
Method:
 Using a design science approach, 21 interviews with 18 industry professionals from 12 different software-intensive firms were conducted to explore, design and validate an artifact for the problem context.
Results:
 A Community Strategy Framework (CSF) is presented to help firms create community strategies that describe if and why they need influence on the RE process in a specific (meritocratic) OSS community, and how the firm could gain it. The framework consists of aspects and engagement practices. The aspects help determine how important an 
OSS project
 and its community is from business and technical perspectives. A community perspective is used when considering the feasibility and potential in gaining influence. The engagement practices are intended as a tool-box for how a firm can engage with a community in order to build influence needed.
Conclusion:
 It is concluded from interview-based validation that the proposed CSF may provide support for firms in creating and tailoring community strategies and help them to focus resources on communities that matter and gain the influence needed on their respective RE processes.",Information and Software Technology,18 Mar 2025,6.0,"The Community Strategy Framework presented in the study could assist firms in gaining influence on the Requirements Engineering process in Open Source Software communities, but its practical application and impact may vary."
https://www.sciencedirect.com/science/article/pii/S0950584919300941,Enhancing context specifications for dependable adaptive systems: A data mining approach,August 2019,Not Found,Arthur=Rodrigues: arthy.rf@gmail.com; Genaína Nunes=Rodrigues: genaina@cic.unb.br; Alessia=Knauss: alessia.knauss@chalmers.se; Raian=Ali: rali@bournemouth.ac.uk; Hugo=Andrade: sica@chalmers.se,"Abstract
Context:
 Adaptive systems are expected to cater for various 
operational contexts
 by having multiple strategies in achieving their objectives and the logic for matching strategies to an actual context. The prediction of relevant contexts at design time is paramount for dependability. With the current trend on using data mining to support the 
requirements engineering
 process, this task of understanding context for adaptive system at design time can benefit from such techniques as well.
Objective:
 The objective is to provide a method to refine the specification of contextual variables and their relation to strategies for dependability. This refinement shall detect dependencies between such variables, priorities in monitoring them, and decide on their relevance in choosing the right strategy in a 
decision tree
.
Method:
 Our requirements-driven approach adopts the contextual goal modelling structure in addition to the operationalization values of sensed information to map contexts to the system’s behaviour. We propose a design time analysis process using a subset of 
data mining algorithms
 to extract a list of relevant contexts and their related variables, tasks, and/or goals.
Results:
 We experimentally evaluated our proposal on a Body Sensor Network system (BSN), simulating 12 resources that could lead to a variability space of 4096 possible context conditions. Our approach was able to elicit subtle contexts that would significantly affect the service provided to assisted patients and relations between contexts, assisting the decision on their need, and priority in monitoring.
Conclusion:
 The use of some 
data mining techniques
 can mitigate the lack of precise definition of contexts and their relation to system strategies for dependability. Our method is practical and supportive to traditional requirements specification methods, which typically require intense human intervention.",Information and Software Technology,18 Mar 2025,5.0,"The method proposed for refining contextual variables in adaptive systems using data mining techniques shows promise in improving system dependability, but the practical implications and real-world impact need further validation."
https://www.sciencedirect.com/science/article/pii/S0950584919300953,Using Squeeziness to test component-based systems defined as Finite State Machines,August 2019,Not Found,Alfredo=Ibias: aibias@ucm.es; Robert M.=Hierons: r.hierons@sheffield.ac.uk; Manuel=Núñez: manuelnu@ucm.es,"Abstract
Context:
Testing is the main validation technique used to increase the reliability of software systems. The effectiveness of testing can be strongly reduced by 
Failed 
Error Propagation
. This situation happens when the System Under Test executes a faulty statement, the state of the system is affected by this fault, but the expected output is observed. Squeeziness is an information 
theoretic measure
 designed to quantify the likelihood of Failed Error Propagation and previous work has shown that Squeeziness correlates strongly with Failed Error Propagation in white-box scenarios. Despite its usefulness, this measure, in its current formulation, cannot be used in a black-box scenario where we do not have access to the 
source code
 of the components.
Objective:
The main goal of this paper is to adapt Squeeziness to a black-box scenario and evaluate whether it can be used to estimate the likelihood that a component of a software system introduces Failed Error Propagation.
Method:
 First, we defined our black-box scenario. Specifically, we considered the Failed Error Propagation that a component introduces when it receives its input from another component. We were interested in this since such fault masking makes it more difficult to find faults in the 
previous
 component when testing. Second, we defined our notion of Squeeziness in this framework. Finally, we carried out experiments in order to evaluate our measure.
Results:
 Our experiments showed a strong correlation between the likelihood of Failed Error Propagation and Squeeziness.
Conclusion:
 We can conclude that our new notion of Squeeziness can be used as a measure that estimates the probability of Failed Error Propagation being introduced by a component. As a result, it has the potential to be used as a measure of 
testability
, allowing testers to assess how easy it is to test either the whole system or a single component. We considered a simple model (Finite State Machines) but the notions and results can be extended/adapted to deal with more complex state-based models, in particular, those containing data.",Information and Software Technology,18 Mar 2025,4.0,"Adapting Squeeziness to estimate the likelihood of Failed Error Propagation in software components has potential for improving testability, but the applicability and effectiveness in real-world scenarios require more extensive testing and validation."
https://www.sciencedirect.com/science/article/pii/S0950584919300965,Source code properties of defective infrastructure as code scripts,August 2019,Not Found,Akond=Rahman: aarahman@ncsu.edu; Laurie=Williams: Not Found,"Abstract
Context
In continuous deployment, software and services are rapidly deployed to end-users using an automated deployment pipeline. Defects in infrastructure as code (IaC) scripts can hinder the reliability of the automated deployment pipeline. We hypothesize that certain properties of IaC 
source code
 such as lines of code and hard-coded strings used as configuration values, show correlation with defective IaC scripts.
Objective
The objective of this paper is to help practitioners in increasing the quality of infrastructure as code (IaC) scripts through an empirical study that identifies 
source code
 properties of defective IaC scripts.
Methodology
We apply qualitative analysis on defect-related commits mined from 
open source software
 repositories to identify source code properties that correlate with defective IaC scripts. Next, we survey practitioners to assess the practitioner’s agreement level with the identified properties. We also construct 
defect prediction
 models using the identified properties for 2439 scripts collected from four datasets.
Results
We identify 10 source code properties that correlate with defective IaC scripts. Of the identified 10 properties we observe lines of code and hard-coded string i.e. putting strings as configuration values, to show the strongest correlation with defective IaC scripts. According to our survey analysis, majority of the practitioners show agreement for two properties: include, the property of executing external modules or scripts, and hard-coded string. Using the identified properties, our constructed 
defect prediction
 models show a precision of 0.70
∼
0.78, and a recall of 0.54
∼
0.67.
Conclusion
Based on our findings, we recommend practitioners to allocate sufficient inspection and testing efforts on IaC scripts that include any of the identified 10 source code properties of IaC scripts.",Information and Software Technology,18 Mar 2025,8.0,"The study provides valuable insights on improving the quality of infrastructure as code, which is crucial for early-stage ventures relying on automated deployment pipelines."
https://www.sciencedirect.com/science/article/pii/S0950584918301290,Internal and external quality in the evolution of mobile software: An exploratory study in open-source market,August 2019,Not Found,Bahar=Gezici: bahargezici@hacettepe.edu.tr; Ayça=Tarhan: atarhan@hacettepe.edu.tr; Oumout=Chouseinoglou: uhus@hacettepe.edu.tr,"Abstract
Context
Mobile applications evolve rapidly and grow constantly to meet user requirements. Satisfying these requirements may lead to poor design choices that can degrade internal quality and performance, and consequently external quality and quality in use. Therefore, monitoring the characteristics of mobile applications through their evolution is important to facilitate maintenance and development.
Objective
This study aims to explore internal quality, external quality and the relation between these two by carrying out an embedded, multiple 
case study
 that includes two cases in different functional domains. In each 
case study
, the evolution of three open-source mobile applications having similar features in the same domain and platform is investigated with the analysis of a number of code-based and community-based metrics, to understand whether they are significantly related to 
quality characteristics
.
Method
A total of 105 releases of the six mobile applications are analyzed to understand internal quality, where code-based characteristics are employed in the light of Lehman’s Increasing Complexity, Continuous Growth, and Decreasing Quality laws. External quality is explored by adapting DeLone and McLean model of 
information system
 success and using community-based metrics, when data is available for the included releases, to derive a corresponding success index. Finally, internal and external quality relationship is investigated by applying Spearman’s correlation analysis on metrics data from 91 corresponding releases.
Results
The analysis of Lehman’s laws shows that only the law of Continuous Growth is validated for the selected mobile applications in both case studies. Spearman’s analysis results indicate that the internal 
quality attribute
 of ‘Understandability’ is negatively related to ‘Success Index’ for Case Study A and ‘LCOM’ is negatively related to ‘Success Index’ for Case Study B. No other significant relationship between the internal quality attributes and the Success Index is observed; but specific to community-based metrics, some significant relationships with code-based attributes were determined.
Conclusion
Our 
exploratory study
 is unique for the method it employs for exploring the relationship between internal and external quality in the evolution of mobile applications. Yet, our findings should be used with caution as they are derived from a limited number of applications. Therefore, this study should be considered to provide initial evidence for applicability of the method and a degree of confidence for repeating similar studies in wider contexts.",Information and Software Technology,18 Mar 2025,6.0,"The study's findings on internal and external quality in the evolution of mobile applications can offer some insights for startups, but the limited application of the findings may reduce its impact."
https://www.sciencedirect.com/science/article/pii/S0950584919300539,Enactment of adaptation in data stream processing with latency implications—A systematic literature review,July 2019,Not Found,Cui=Qin: qin@sse.uni-hildesheim.de; Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Klaus=Schmid: schmid@sse.uni-hildesheim.de,"Abstract
Context
Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data 
processing tasks
. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established.
Objective
This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension.
Method
We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, 
evaluation metrics
 as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation.
Results
We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 
evaluation metrics
 and 12 evaluation parameters according to the extracted data properties.
Conclusion
We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.",Information and Software Technology,18 Mar 2025,9.0,The categorization of runtime adaptation approaches in stream processing can have significant practical implications for startups dealing with huge amounts of data and real-time processing.
https://www.sciencedirect.com/science/article/pii/S0950584919300539,Enactment of adaptation in data stream processing with latency implications—A systematic literature review,July 2019,Not Found,Cui=Qin: qin@sse.uni-hildesheim.de; Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Klaus=Schmid: schmid@sse.uni-hildesheim.de,"Abstract
Context
Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data 
processing tasks
. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established.
Objective
This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension.
Method
We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, 
evaluation metrics
 as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation.
Results
We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 
evaluation metrics
 and 12 evaluation parameters according to the extracted data properties.
Conclusion
We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.",Information and Software Technology,18 Mar 2025,9.0,The identification and characterization of different runtime adaptation approaches in stream processing with a focus on latency can provide valuable guidance for startups in optimizing data processing tasks.
https://www.sciencedirect.com/science/article/pii/S0950584919300564,Bootstrapping cookbooks for APIs from crowd knowledge on Stack Overflow,July 2019,Not Found,Lucas B.L.=Souza: Not Found; Eduardo C.=Campos: eccampos@ufu.br; Fernanda=Madeiral: fernanda.madeiral@ufu.br; Klérisson=Paixão: klerisson@ufu.br; Adriano M.=Rocha: Not Found; Marcelo de Almeida=Maia: marcelo.maia@ufu.br,"Abstract
Context
Well established libraries typically have 
API
 documentation. However, they frequently lack examples and explanations, possibly making difficult their effective reuse. Stack Overflow is a question-and-answer website oriented to issues related to software development. Despite the increasing adoption of Stack Overflow, the information related to a particular topic (e.g., an API) is spread across the website. Thus, Stack Overflow still lacks organization of the 
crowd knowledge
 available on it.
Objective
Our target goal is to address the problem of the poor quality documentation for APIs by providing an alternative artifact to document them based on the crowd knowledge available on Stack Overflow, called 
crowd cookbook
. A 
cookbook
 is a recipe-oriented book, and we refer to our cookbook as 
crowd cookbook
 since it contains 
content generated
 by a crowd. The cookbooks are meant to be used through an exploration process, i.e. browsing.
Method
In this paper, we present a semi-automatic approach that organizes the crowd knowledge available on Stack Overflow to build cookbooks for APIs. We have generated cookbooks for three APIs widely used by the software development community: SWT, LINQ and QT. We have also defined 
desired properties
 that crowd cookbooks must meet, and we conducted an evaluation of the cookbooks against these properties with 
human subjects
.
Results
The results showed that the cookbooks built using our approach, in general, meet those properties. As a highlight, most of the recipes were considered appropriate to be in the cookbooks and have self-contained information.
Conclusion
We concluded that our approach is capable to produce adequate cookbooks automatically, which can be as useful as manually produced cookbooks. This opens an opportunity for 
API designers
 to enrich existent cookbooks with the different points of view from the crowd, or even to generate initial versions of new cookbooks.",Information and Software Technology,18 Mar 2025,7.0,"The creation of crowd cookbooks based on crowd knowledge from Stack Overflow can help in addressing the issue of poor quality documentation for APIs, which can benefit startups in utilizing APIs effectively."
https://www.sciencedirect.com/science/article/pii/S0950584919300576,The relationship between personality and decision-making: A Systematic literature review,July 2019,Not Found,Fabiana Freitas=Mendes: fabianamendes@unb.br; Emilia=Mendes: emilia.mendes@bth.se; Norsaremah=Salleh: norsaremah@iium.edu.my,"Abstract
Context
From a point of view, software development is a set of decisions that need to be made while the software is developed. Many alternatives should be considered, such as the technology to employ, or the most important features to implement. However, many factors can influence one’s decision-making, such as the decision maker’s personality.
Objective
This paper reports the state of the art with regard to the relationship between decision-makers’ personality and decision-making aspects.
Method
We conducted a 
Systematic Literature Review
 to search and analyze published primary studies that discuss the abovementioned relationship in the context of companies that develop any kind of product or service.
Results
Despite the recognized influence of personality in decision-making activities, we were not able to find any study in 
Software Engineering
 field that discusses this relationship. We included 15 studies and most of them are from Management field, excluding one from 
Information System
 field. From these studies, we identified 75 reported relationships between 28 different personality aspects and 30 different decision-making aspects.
Conclusion
The interest in this topic born on 80’s and it has grown after 2002. However, despite the number of reported relationships, and the number of personalities and decision-making aspects investigated, more research on this topic is necessary. In particular, it is important to verify how someone’s personality influences the decision-making considering the software development context. This can help in improving how a decision is made in software engineering context.",Information and Software Technology,18 Mar 2025,5.0,"The paper addresses the relationship between decision-makers' personality and decision-making aspects in the context of software engineering, which can potentially impact decision-making processes in early-stage ventures. However, the practical application and impact of this research on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S095058491930059X,A reference model-based user requirements elicitation process: Toward operational business-IT alignment in a co-creation value network,July 2019,Not Found,Samaneh=Bagheri: s.bagheri@tue.nl; R.J.=Kusters: r.j.kusters@tue.nl; J.J.M.=Trienekens: j.j.m.trienekens@tue.nl; P.W.P.J.=Grefen: p.w.p.j.grefen@tue.nl,"Abstract
Context
To improve operational business-IT alignment (BITA), the development of IT-based systems should be derived from business requirements. However, the 
requirements elicitation
 process is challenging and encounters several problems which might lead to acquiring low-quality user requirements and failure of systems development projects. Many of 
elicitation
 problems are also identified as being relevant in the BITA literature. We focus on one category of well-known 
elicitation
 problems, such as communication flaws.
Until now, the majority of 
requirements elicitation
 studies with the aim of addressing operational BITA are based on an asking strategy. This elicitation strategy is suitable for relatively stable situations. To compensate for the limitation of this strategy in a more complex situation, e.g., a co-creation value network (VN) setting, using it in conjunction with other elicitation strategies is more likely to yield satisfactory results.
Objective
To contribute to operational BITA improvement in a VN setting by addressing one category of elicitation problems. For this purpose, we design and evaluate a reference model-based approach to facilitate the user requirements 
elicitation process
.
Method
Two-phase research according to the design science approach is followed. In the design phase, a reference model-based user requirements 
elicitation process
 is designed. Also, as a proof of concept, two instances of this artifact are designed. Two reference models, respectively, describing customer 
knowledge management
 processes and customer 
knowledge management
 challenges in a VN setting are used separately in designing these two instances. In the evaluation phase, the applicability and usefulness of these instances are evaluated in two separate studies.
Results
A reference model supports asking-based user requirements elicitation process via a Delphi method in a complex context of a VN. It improves the user requirements elicitation process by addressing a set of recognized elicitation problems.
Conclusions
The reference model-based approach, by addressing the elicitation problems, contributes to user requirements elicitation process improvement in general and to a better operational BITA in the complex situation of a VN in particular.",Information and Software Technology,18 Mar 2025,7.0,"The paper focuses on improving operational business-IT alignment by addressing requirements elicitation problems. This can have a direct impact on the development of IT-based systems in early-stage ventures, enhancing their operational efficiency and overall success. The reference model-based approach presented in the paper offers practical insights for startups."
https://www.sciencedirect.com/science/article/pii/S0950584919300734,Leveraging keyword-guided exploration to build test models for web applications,July 2019,Not Found,Xiao-Fang=Qi: xfqi@seu.edu.cn; Yun-Long=Hua: Not Found; Peng=Wang: pwang@seu.edu.cn; Zi-Yuan=Wang: wangziyuan@njupt.edu.cn,"Abstract
Context
Dynamic exploration techniques, which automatically exercise possible 
user interface elements
, have been used to explore user interface state flow graphs as test models for web applications. An exhaustive exploration may incur the well-known state explosion problem. In a limited amount of time, most existing dynamic exploration techniques tend to become mired in local or irrelevant regions of the web application due to not considering functionality semantics information. Hence, generated test models have often inadequate functionality coverage for deriving effective test cases.
Objective
This paper proposes a keyword-guided exploration strategy for automatic construction of web application test models. The goal is to generate incomplete test models with adequate functionality coverage in a given time budget for deriving test cases w.r.t. specified functionalities.
Method
Given very few keywords that describe specified functionalities, our strategy guides the exploration to discover user interface states and transitions among them that are relevant to the specified functionalities by computing similarity scores between text contents in web pages and given keywords. We use nine representative web applications to perform dynamic explorations in a given time budget and empirically evaluate functionality coverage, and other metrics, e.g., code coverage, the size of test model, the number of the test suite, path diversity, and 
DOM
 diversity.
Results
Our keyword-guided exploration strategy achieves a higher functionality coverage as compared with the generic and feedback-directed exploration strategies. Yet the significant improvement of functionality coverage achieved by our strategy is not exchanged at the cost of other metrics.
Conclusion
Our keyword-guided exploration strategy is more effective than the generic and feedback-directed exploration strategies in terms of functionality coverage. In a limited amount of time, test models generated with our strategy can be used to derive effective web application test cases.",Information and Software Technology,18 Mar 2025,8.0,"The keyword-guided exploration strategy proposed in the paper for automatic construction of web application test models can significantly benefit early-stage ventures by improving functionality coverage in a limited time frame. This practical approach can enhance the testing process for startups, ultimately leading to better product quality and user experience."
https://www.sciencedirect.com/science/article/pii/S095058491830106X,State of the art in hybrid strategies for context reasoning: A systematic literature review,July 2019,Not Found,Roger S.=Machado: rdsmachado@inf.ufpel.edu.br; Ricardo B.=Almeida: rbalmeida@inf.ufpel.edu.br; Ana Marilza=Pernas: marilza@inf.ufpel.edu.br; Adenauer C.=Yamin: adenauer@inf.ufpel.edu.br,"Abstract
Context
Several strategies have been used to implement context reasoning, and a strategy that can be applied satisfactorily in different smart systems applications has not yet been found. Because of this, hybrid proposals for context reasoning are gaining prominence. These proposals allow the combination of two or more strategies.
Objective
This work aims to identify the state of the art in the 
context awareness
 field, considering papers that use 
hybrid strategies
 for context reasoning.
Method
A Systematic Literature Review was explored, contributing to the identification of relevant works in the field, as well as the specification of criteria for its selection. In this review, we analyzed papers published between 2004 and 2018.
Results
During the process, we identified 3241 papers. After applying filtering and conditioning processes, ten papers about 
hybrid strategies
 for context reasoning were selected. We described, discussed, and compared the selected papers.
Conclusion
The Systematic Literature Review showed that some researchers explore hybrid proposals, but these proposals do not offer flexibility regarding the reasoning strategies used. Thus, we noted that research efforts related to the topic are still necessary, mainly focusing on the development of dynamic approaches that allow the applications to choose how they want to use the different resources available.",Information and Software Technology,18 Mar 2025,6.0,"The paper explores hybrid strategies for context reasoning, which can be relevant for smart systems applications, including those used in early-stage ventures. While the research contributes to the state of the art in context awareness, the direct practical implications for European startups may require further exploration."
https://www.sciencedirect.com/science/article/pii/S0950584918301113,An overview of a novel analysis approach for enhancing context awareness in smart environments,July 2019,Not Found,Nesrine=Khabou: nesrine.khabou@redcad.org; Ismael=Bouassida Rodriguez: bouassida@redcad.org; Mohamed=Jmaiel: mohamed.jmaiel@enis.rnu.tn,"Abstract
Context
This work is part of 
context aware applications
 design and development, and smart environments in which context changes frequently.
Objective
The objective of the work is to facilitate the design and the development of 
context aware applications
 able to detect context changes and to predict context.
Method
In the paper, two analysis tasks are proposed. An analysis task for detection aiming at supporting application designers to conceive easily context aware applications able to detect context changes and an analysis task for prediction aiming at helping the application designers to conceive context aware applications able to predict context. The paper details also an analysis module that implements the functionalities of the analysis tasks. The analysis module helps the application developers to develop context aware applications. Finally, the paper introduces a 
case study
 related to smart buildings in order to show the usefulness of the analysis tasks.
Results
The paper shows an application scenario related to smart buildings and particularly water consumption prediction. Also the paper presents experiments related to memory consumption introduced by the use of our analysis module.
Conclusions
The application scenario illustrates the usefulness of the analysis approach. The overhead introduced by the analysis module is negligeable.",Information and Software Technology,18 Mar 2025,7.0,"The paper provides analysis tasks for detection and prediction of context changes in context-aware applications, with a focus on smart environments like smart buildings. The practical application of these analysis tasks can benefit European early-stage ventures by improving the design and development of context-aware applications for enhanced user experiences."
https://www.sciencedirect.com/science/article/pii/S0950584918301137,A distributed event-driven architectural model based on situational awareness applied on internet of things,July 2019,Not Found,Ricardo Borges=Almeida: rbalmeida@inf.ufpel.edu.br; Victor Renan Covalski=Junes: vrcjunes@inf.ufpel.edu.br; Roger da Silva=Machado: rdsmachado@inf.ufpel.edu.br; Diórgenes Yuri Leal da=Rosa: diorgenes@inf.ufpel.edu.br; Lucas Medeiros=Donato: lucas.donato@my365.dmu.ac.uk; Adenauer Corrêa=Yamin: adenauer@inf.ufpel.edu.br; Ana Marilza=Pernas: marilza@inf.ufpel.edu.br,"Abstract
Context
The 
IoT
 network is comprised of numerous and heterogeneous devices that are capable of generating large amounts of events. To enable the 
IoT
 paradigm, it is necessary to integrate, process, and react to events on the fly.
Objective
The goal of this paper is to support the increased demands of scalability, flexibility, autonomy, and heterogeneity for IoT event processing. A distributed 
architectural model
 based on 
Situational Awareness
, named EXEHDA-SA, was designed to provide event collection, hybrid processing, and customizable and dynamic reaction features.
Method
The conception of the model was based on a middleware for 
ubiquitous computing
 called EXEHDA, thus benefiting from its already defined strategies. The proposal follows a multi-level strategy and consists of three hierarchically interconnected modular components.
Results
Our main contribution is the conception and validation of a model for event collection, processing and reaction for modern distributed environments. The contribution is evidenced through experiments performed on a prototype implemented on consolidated free and 
open source technologies
. The experiments are made up of five 
case studies
 where each one evaluates a scenario for IoT demands.
Conclusion
Through these 
case studies
 which were proposed in information security area, we demonstrated the feasibility of this proposal for deployment in IoT production environments. Furthermore, EXEHDA-SA is able to operate on different scenarios due to each component modularity and its consequent extensibility.",Information and Software Technology,18 Mar 2025,7.0,"The abstract presents a model for event processing in IoT environments, which is relevant for early-stage ventures working with IoT technologies. The use of open source technologies and case studies add practical value."
https://www.sciencedirect.com/science/article/pii/S0950584918300715,Detecting terminological ambiguity in user stories: Tool and experimentation,June 2019,Not Found,Fabiano=Dalpiaz: f.dalpiaz@uu.nl; Ivor=van der Schalk: Not Found; Sjaak=Brinkkemper: Not Found; Fatma Başak=Aydemir: Not Found; Garm=Lucassen: Not Found,"Abstract
Context.
 Defects such as ambiguity and incompleteness are pervasive in software requirements, often due to the limited time that practitioners devote to writing good requirements. 
Objective.
We study whether a synergy between humans’ analytic capabilities and 
natural language processing
 is an effective approach for quickly identifying near-synonyms, a possible source of terminological ambiguity. 
Method.
We propose a tool-supported approach that blends 
information visualization
 with two 
natural language processing
 techniques: conceptual model extraction and semantic similarity. We evaluate the precision and recall of our approach compared to a pen-and-paper manual inspection session through a controlled quasi-experiment that involves 57 participants organized into 28 groups, each group working on one real-world requirements data set. 
Results.
The experimental results indicate that manual inspection delivers higher recall (statistically significant with 
p
 ≤ 0.01) and non-significantly higher precision. Based on qualitative observations, we analyze the quantitative results and suggest interpretations that explain the advantages and disadvantages of each approach. 
Conclusions.
Our experiment confirms conventional wisdom in 
requirements engineering
: identifying terminological ambiguities is time consuming, even when with tool support; and it is hard to determine whether a near-synonym may challenge the correct development of a software system. The results suggest that the most effective approach may be a combination of manual inspection with an improved version of our tool.",Information and Software Technology,18 Mar 2025,5.0,"The abstract addresses a common issue in software requirements, but the focus on terminological ambiguity may have limited impact on early-stage ventures. The experimental results provide some insights but may not have significant practical implications for startups."
https://www.sciencedirect.com/science/article/pii/S0950584918301599,GuideGen: An approach for keeping requirements and acceptance tests aligned via automatically generated guidance,June 2019,Not Found,Sofija=Hotomski: hotomski@ifi.uzh.ch; Martin=Glinz: glinz@ifi.uzh.ch,"Abstract
Context
When software-based systems evolve, their requirements change. The changes in requirements affect the associated 
acceptance tests
, which should be adapted accordingly. In practice, however, requirements and their 
acceptance tests
 are not always kept up-to-date nor aligned. Such inconsistencies may introduce software quality problems, unintended costs and project delays.
Objective
In order to keep evolving requirements and their acceptance tests aligned, we are developing an approach called GuideGen. GuideGen automatically generates guidance in natural language about how to adapt the impacted acceptance tests when their requirements change.
Method
We have implemented GuideGen as a prototype tool and evaluated it in two studies: first, by assessing the correctness, completeness, 
understandability
 and relevance of the generated guidance using three data sets from industry and second, by assessing the applicability and usefulness of the approach and the tool with 23 practitioners from ten companies.
When a requirement having more than one associated acceptance test is changed, GuideGen currently generates guidance for all of them together. As a first step towards overcoming this limitation, we assessed how well existing methods for change impact analysis can identify the tests actually impacted by the changes in a requirement.
Results
In the first study, we found that GuideGen produced correct guidance in about 67 to 89 percent of all changes. Our approach performed better for agile requirements than for traditional ones. The results of the second study show that GuideGen is perceived to be useful, but that the practitioners would prefer a GuideGen plug-in for commercial tools instead of a standalone tool. Further, in our experiment we could correctly identify the affected acceptance tests for 63% to 91% of the changes in the requirements.
Conclusion
Our approach facilitates the alignment of acceptance tests with the actual requirements and can improve the communication between requirements engineers and testers.",Information and Software Technology,18 Mar 2025,8.0,The development of GuideGen to keep requirements and acceptance tests aligned is a valuable contribution for startups aiming to improve software quality. The positive results from the studies and the preference for a plug-in tool enhance the practical relevance of the approach.
https://www.sciencedirect.com/science/article/pii/S0950584918300739,Quality requirements challenges in the context of large-scale distributed agile: An empirical study,June 2019,Not Found,Wasim=Alsaqaf: w.h.a.alsaqaf@utwente.nl; Maya=Daneva: m.daneva@utwente.nl; Roel=Wieringa: r.j.wieringa@utwente.nl,"Abstract
Context
Engineering quality requirements in agile projects does not fit organically with agile methods. Despite the agile community acknowledges this, little empirical evidence has been published on this topic.
Objective
This exploratory qualitative interview-based study explicates the challenging situations experienced by practitioners in engineering the quality requirements in the context of large-scale distributed agile projects. Moreover, this study describes the practices that agile distributed teams currently use which could contribute by dealing with the identified challenges.
Method
The challenging situations and possible mitigation practices were studied from the perspective of 17 practitioners from large distributed agile project teams in six organizations in The Netherlands. Qualitative data were collected using semi-structured, open-ended interviews. Qualitative coding techniques were used for data analysis, to identify the challenges of engineering quality requirements, the mechanisms behind the challenges and the practices used that could mitigate the impact of those challenges. Further, by using dialog mapping technique for qualitative data structuring, we have mapped the identified mechanisms and practices to the challenges.
Results
From the perspective of the participating practitioners, our 
exploratory study
 revealed 15 challenges classified in five categories: (1) team coordination and communication, (2) quality assurance, (3) quality 
requirements elicitation
, (4) conceptual challenges, and (5) software architecture. The study has also disclosed 13 mechanisms behind the challenges and 9 practices that could mitigate the impact of those challenges.
Conclusions
The main contributions of the paper are: (1) the explication of the challenges from practitioners’ perspective and the comparison of our findings with previously published results, (2) the description of the mechanisms behind the challenges, and (3) the identification of the practices currently used by agile teams that could mitigate the impact of the challenges. The findings of this study provide useful input into the process of designing possible solution approaches to overcome the challenges.",Information and Software Technology,18 Mar 2025,6.0,"The study on engineering quality requirements in agile projects offers insights into challenges faced by practitioners, but the practical implications for early-stage ventures may be limited. The identification of practices used by agile teams can be helpful, but more concrete recommendations could improve the score."
https://www.sciencedirect.com/science/article/pii/S095058491930031X,Investigation on test effort estimation of mobile applications: Systematic literature review and survey,June 2019,Not Found,Anureet=Kaur: anumahal@gmail.com; Kulwant=Kaur: kulwantkaur@apjimtc.org,"Abstract
Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of 
agile methodology
, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.",Information and Software Technology,18 Mar 2025,7.0,The focus on test estimation for mobile applications is relevant given the importance of mobile technology for startups. The use of systematic literature review and survey results to improve test estimation techniques provides practical value for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584919300321,Towards functional change decision support based on COSMIC FSM method,June 2019,Not Found,Mariem=Haoues: mariem.haoues@isims.usf.tn; Asma=Sellami: asma.sellami@isims.usf.tn; Hanêne=Ben-Abdallah: hbenabdallah@hct.ac.ae,"Abstract
Context:
 Managing requirements change is a central issue in the software development industry. In fact, inappropriate decisions about a change request may jeopardize the project development progress by going over budget/time or delivering a software with functional requirements that do not fully meet the user’s needs. Hence, a change decision support is required for the success of the software development.
Objective:
 This paper has a three-fold objective: (i) explore the applicability of the ISO standard COSMIC FSM method to evaluate a change request; (ii) investigate the use of estimation models to predict the effort required to handle a functional change and its impact on the initially estimated software development effort; and (iii) propose a decision support method that offers the appropriate information for the 
change advisory board
 members to decide whether to accept, deny or defer a functional change request.
Method:
 To guide the decision on a change request, the method proposed in this paper accounts for the most important factors when evaluating a change request, namely the functional change status, the preference of the change requester, and the effort required to handle the change. The functional change status is identified based on the sensitivity of the changed functionality and the functional size of the functional change. The functional change effort can be estimated using several ways including the COCOMO II model, the 
Simple Linear Regression
 Model and 
expert judgment
. Furthermore, this paper proposes a prototype to determine automatically the functional change status and offers pertinent information that the 
change advisory board
 can use to determine how to handle a change request. The use of the decision support method and tool is illustrated through three 
case studies
.
Results:
 A decision support method to help decision-makers respond to a functional change request is provided. This method takes into account the functional change status, the preference of the change requester and the functional change effort. The empirical evaluation of the proposed method is illustrated through three 
case studies
. The role of experiments here is primarily to provide a proof-of-concept rather than an exhaustive evaluation.
Conclusion:
 Using COSMIC FSM method, it is possible to identify functional changes leading to a potential impact on the software development progress. Based on the evaluation of the functional change, the change advisory board members can make judicious decisions about whether to accept, defer or deny a functional change request.",Information and Software Technology,18 Mar 2025,8.0,"The paper provides a decision support method for managing requirements change in software development, which can significantly impact the success of projects. The proposed method and tool offer practical guidance for decision-makers to handle functional change requests."
https://www.sciencedirect.com/science/article/pii/S0950584919300400,CaMeLOT: An educational framework for conceptual data modelling,June 2019,Not Found,Daria=Bogdanova: daria.bogdanova@kuleuven.be; Monique=Snoeck: monique.snoeck@kuleuven.be,"Abstract
Context
Teaching 
conceptual data modelling
 (CDM) remains a challenging task for educators. Despite the fact that CDM is an integral part of 
software engineering
 curricula, there is no generally accepted educational framework for the subject. Moreover, the existing educational literature shows significant gaps when it comes to pursued 
learning outcomes
 and their assessment.
Objective
In this paper, we propose an educational framework for 
conceptual data modelling
, based on the revised Bloom's taxonomy of educational objectives, and provide necessary examples of systemized 
learning outcomes
.
Method
We utilized the revised Bloom's taxonomy to develop an adapted framework specifically for learning outcomes related to CDM. We validated the framework by mapping learning outcomes distilled from the existing course material to the framework, by presenting the framework for feedback to the experts in the field and further elaborating and refining it based on the feedback and experiences from these validation activities.
Results
CaMeLOT is an adaptation of the Bloom's taxonomy specifically for learning outcomes related to CDM. We identified different content areas and indicated the necessary scaffolding. Based on the framework, we worked out 17 example tables of learning outcomes related to content areas at different levels of scaffolding, exemplifying the different knowledge and cognitive levels. We clarify the differences in learning outcomes related to different knowledge and cognitive levels and thereby provide a domain specific clarification of the classification guidelines.
Conclusion
CaMeLOT gives educators an opportunity to enhance the CDM part of 
software engineering
 curricula with a systemized set of learning outcomes to be pursued, and open the path for creating more complete, useful and effective assessment packages. The adoption of our educational framework may reduce the time spent on designing educational material and, at the same time, improve its quality.",Information and Software Technology,18 Mar 2025,7.0,"The educational framework proposed for conceptual data modeling can enhance software engineering curricula, providing educators with a systematic set of learning outcomes. This framework may improve the quality of educational material and assessment packages."
https://www.sciencedirect.com/science/article/pii/S0950584919300424,Why is my code change abandoned?,June 2019,Not Found,Qingye=Wang: wqyy@zju.edu.cn; Xin=Xia: xin.xia@monash.edu; David=Lo: davidlo@smu.edu.sg; Shanping=Li: shan@zju.edu.cn,"Abstract
Context
: Software developers contribute numerous changes every day to the code review systems. However, not all submitted changes are merged into a codebase because they might not pass the 
code review process
. Some changes would be abandoned or be asked for resubmission after improvement, which results in more workload for developers and reviewers, and more delays to deliverables.
Objective
: To understand the underlying reasons why changes are abandoned, we conduct an empirical study on the code review of four 
open source projects
 (Eclipse, LibreOffice, OpenStack, and Qt).
Method
: First, we manually analyzed 1459 abandoned changes. Second, we leveraged the open card sorting method to label these changes with reasons why they were abandoned, and we identified 12 categories of reasons. Next, we further investigated the frequency distribution of the categories across projects. Finally, we studied the relationship between the categories and time-to-abandonment.
Results
: Our findings include the following: (1) 
Duplicate
 changes are the majority of the abandoned changes; (2) the frequency distribution of abandoned changes across the 12 categories is similar for the four 
open source projects
; (3) 98.39% of the changes are abandoned within a year.
Conclusion
: Our study concluded the root causes of abandoned changes, which will help developers submit high-quality code changes.",Information and Software Technology,18 Mar 2025,9.0,"The empirical study on abandoned code changes in open source projects addresses a common issue in software development. Understanding the root causes of abandoned changes can help developers submit higher quality code, reducing workload and delays."
https://www.sciencedirect.com/science/article/pii/S0950584919300461,Revisiting the refactoring mechanics,June 2019,Not Found,Jonhnanthan=Oliveira: jonhnanthan@copin.ufcg.edu.br; Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Melina=Mongiovi: melina@computacao.ufcg.edu.br; Gustavo=Soares: gustavo.soares@microsoft.com; Márcio=Ribeiro: marcio@ic.ufal.br; Alessandro=Garcia: afgarcia@inf.puc-rio.br,"Abstract
Context
Refactoring is a key practice in 
agile methodologies
 used by a number of developers, and available in popular IDEs. However, it is unclear whether the refactoring mechanics have the same meaning for developers.
Objective
In this article, we revisit the refactoring mechanics.
Method
We conduct a survey with 107 developers of popular Java projects on GitHub. We asked them about the output of seven refactoring types applied to small programs.
Results
Developers do not expect the same outputs in all questions. The refactoring mechanics is based on developers’ experience for a number of them (71.02%). Some developers (75.70%) use IDEs to apply refactorings. However, the output yielded by the preferred IDE is different from what they want.
Conclusion
Developers and IDE developers use different mechanics for most refactoring types considered in our survey, and this may impact developers’ communication.",Information and Software Technology,18 Mar 2025,6.0,"The survey on refactoring mechanics provides insights into developers' expectations and the discrepancies with IDE outputs. While the findings can impact developers' communication, the practical implications on early-stage European ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919300503,Images don’t lie: Duplicate crowdtesting reports detection with screenshot information,June 2019,Not Found,Junjie=Wang: wangjunjie@itechs.iscas.ac.cn; Mingyang=Li: limingyang@itechs.iscas.ac.cn; Song=Wang: song.wang@uwaterloo.ca; Tim=Menzies: tim@menzies.us; Qing=Wang: wq@itechs.iscas.ac.cn,"Abstract
Context
: Crowdtesting is effective especially when it comes to the feedback on GUI systems, or subjective opinions about features. Despite of this, we find crowdtesting reports are highly duplicated, i.e., 82% of them are duplicates of others. Most of the existing approaches mainly adopted textual information for 
duplicate detection
, and suffered from low accuracy because of the lexical gap. Our observation on real industrial crowdtesting data found that when dealing with crowdtesting reports of GUI systems, the reports would be accompanied with images, i.e., the screenshots of the tested app. We assume the screenshot to be valuable for duplicate crowdtesting report detection because it reflects the real context of the bug and is not affected by the variety of natural languages.
Objective
: We aim at automatically detecting duplicate crowdtesting reports that could help reduce triaging effort.
Method
: In this work, we propose SETU which combines information from the ScrEenshots and the TextUal descriptions to detect duplicate crowdtesting reports. We extract four types of features to characterize the screenshots (i.e., image structure feature and image color feature) and the textual descriptions (i.e., TF-IDF feature and 
word embedding
 feature), and design a hierarchical algorithm to detect duplicates based on the four similarity scores derived from the four features respectively.
Results
: We investigate the effectiveness of SETU on 12 projects with 3,689 reports from one of the Chinese largest crowdtesting platforms. Results show that recall@1 achieved by SETU is 0.44 to 0.79, recall@5 is 0.66 to 0.92, and 
MAP
 is 0.21 to 0.58 across all experimental projects. Furthermore, SETU can outperform existing state-of-the-art approaches significantly and substantially.
Conclusion
: Through combining the screenshots and textual descriptions, our proposed SETU can improve the duplicate crowdtesting reports detection performance.",Information and Software Technology,18 Mar 2025,8.0,"The proposed SETU method for detecting duplicate crowdtesting reports offers a practical solution to reduce triaging effort. The combination of screenshots and textual descriptions shows significant improvement in detection performance, which can benefit software testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584919300515,Reusability in goal modeling: A systematic literature review,June 2019,Not Found,Mustafa Berk=Duran: berk.duran@mail.mcgill.ca; Gunter=Mussbacher: gunter.mussbacher@mcgill.ca,"Abstract
Context:
 Goal modeling is an important instrument for the 
elicitation
, specification, analysis, and validation of early requirements. Goal models capture hierarchical representations of stakeholder objectives, requirements, possible solutions, and their relationships to help requirements engineers understand 
stakeholder goals
 and explore solutions based on their impact on these goals. To reuse a goal model and benefit from the strengths of goal modeling, we argue that it is necessary (i) to make sure that analysis and validation of goal models is possible through reuse hierarchies, (ii) to provide the means to delay decision making to a later point in the reuse hierarchy, (iii) to take constraints imposed by other 
modeling notations
 into account during analysis, (iv) to allow context dependent information to be modeled so that the goal model can be used in various reuse contexts, and (v) to provide an interface for reuse.
Objective:
 In this two-part systematic literature review, we (i) evaluate how well existing goal modeling approaches support reusability with our five desired characteristics of contextual and reusable goal models, (ii) categorize these approaches based on language constructs for context modeling and connection to other modeling formalisms, and then (iii) draw our conclusions on future research themes.
Method:
 Following guidelines by Kitchenham, the review is conducted on seven major academic search engines. Research questions, 
inclusion criteria
, and categorization criteria are specified, and threats to validity are discussed. A final list of 146 publications and 34 comparisons/assessments of goal modeling approaches is discussed in more detail.
Results:
 Five major research themes are derived to realize reusable goal models with context dependent information.
Conclusion:
 The results indicate that existing goal modeling approaches do not fully address the required capabilities for reusability in different contexts and that further research is needed to fill this gap in the landscape of goal modeling approaches.",Information and Software Technology,18 Mar 2025,6.0,"The research on goal modeling approaches is relevant for early-stage ventures in understanding stakeholder goals and exploring solutions, but the conclusion that existing approaches do not fully address reusability needs more research does not provide immediate practical value."
https://www.sciencedirect.com/science/article/pii/S0950584919300047,Is deep learning better than traditional approaches in tag recommendation for software information sites?,May 2019,Not Found,Pingyi=Zhou: zhou_pinyi@whu.edu.cn; Jin=Liu: jinliu@whu.edu.cn; Xiao=Liu: xiao.liu@deakin.edu.au; Zijiang=Yang: zijiang.yang@wmich.edu; John=Grundy: john.grundy@monash.edu,"Abstract
Context
Inspired by the success of 
deep learning
 in other domains, this new technique been gaining widespread recent interest in being applied to diverse data analysis problems in 
software engineering
. Many 
deep learning
 models, such as 
CNN
, 
DBN
, 
RNN
, 
LSTM
 and 
GAN
, have been proposed and recently applied to 
software engineering
 tasks including effort estimation, 
vulnerability analysis
, code clone detection, test case selection, requirements analysis and many others. However, there is a perception that applying 
deep learning
 is a ”silver bullet” if it can be applied to a software engineering data analysis problem.
Object
This motivated us to ask the question as to whether 
deep learning
 is better than traditional approaches in 
tag recommendation
 task for software information sites.
Method
In this paper we test this question by applying both the latest deep 
learning approaches
 and some traditional approaches on 
tag recommendation
 task for software information sites. This is a typical Software 
Engineering automation
 problem where intensive data processing is required to link disparate information to assist developers. Four different deep 
learning approaches
 – TagCNN, TagRNN, TagHAN and TagRCNN – are implemented and compared with three advanced traditional approaches – EnTagRec, TagMulRec, and FastTagRec.
Results
Our comprehensive experimental results show that the performance of these different deep learning approaches varies significantly. The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks.
Conclusion
Therefore, using appropriate deep learning approaches can indeed achieve better performance than traditional approaches in tag recommendation tasks for software information sites.",Information and Software Technology,18 Mar 2025,8.0,"The comparison between deep learning and traditional approaches in tag recommendation tasks for software information sites provides practical insights for early-stage ventures in software engineering, highlighting the potential of deep learning solutions."
https://www.sciencedirect.com/science/article/pii/S0950584919300059,"Formal Quality of Service assurances, ranking and verification of cloud deployment options with a probabilistic model checking method",May 2019,Not Found,Petar=Kochovski: Not Found; Pavel D.=Drobintsev: Not Found; Vlado=Stankovski: vlado.stankovski@fgg.uni-lj.si,"Abstract
Context
: Existing software workbenches allow for the deployment of cloud applications across a variety of Infrastructure-as-a-Service (IaaS) providers. The expected workload, 
Quality of Service
 (QoS) and Non-Functional Requirements (NFRs) must be considered before an appropriate infrastructure is selected. However, this decision-making process is complex and time-consuming. Moreover, the software engineer needs assurances that the selected infrastructure will lead to an adequate QoS of the application.
Objective
: The goal is to develop a new method for selection of an optimal cloud 
deployment option
, that is, an infrastructure and configuration for deployment and to verify that all hard and as many soft QoS requirements as possible will be met at runtime.
Method
: A new Formal QoS Assurances Method (FoQoSAM), which relies on stochastic Markov models is introduced to facilitate an automated decision-making process. For a given workload, it uses QoS 
monitoring data
 and a user-related metric in order to automatically generate a probabilistic model. The probabilistic model takes the form of a 
finite automaton
. It is further used to produce a rank list of cloud deployment options. As a result, any of the cloud deployment options can be verified by applying a probabilistic model checking approach.
Results
: Testing was performed by ranking deployment options for two cloud applications, File Upload and Video-conferencing. The FoQoSAM method was compared to a baseline 
Analytic Hierarchy Process
 (AHP). The results show that the first ranked cloud deployment options satisfy all hard and at least one of the soft requirements for both methods, however, the FoQoSAM method always satisfies at least an additional QoS requirement compared to the baseline AHP method.
Conclusions
: The proposed new FoQoSAM method is appropriate and can be used in decision-making when ranking and verifying cloud deployment options. Due to its practical utility it was integrated into the SWITCH workbench.",Information and Software Technology,18 Mar 2025,9.0,The development of a new method for optimal cloud deployment options with a focus on Quality of Service requirements and automation of decision-making processes is highly relevant and impactful for European early-stage ventures dealing with cloud applications.
https://www.sciencedirect.com/science/article/pii/S0950584919300072,On the need to update systematic literature reviews,May 2019,Not Found,Vilmar=Nepomuceno: vsn@cin.ufpe.br; Sergio=Soares: scbs@cin.ufpe.br,"Abstract
Context
Many 
Systematic Literature Reviews
 (SLRs) were performed in the recent past, but just a few are being updated. Keeping SLRs updated is essential to prolong their lifespan.
Objective
To give a picture about how SLRs are being updated and what researchers think about SLRs updates.
Method
In this work, we present a Systematic Mapping (SM) study about SLRs updates and a survey with 
EBSE
 researchers that published their SLRs between 2011 and 2015.
Results
We included 22 studies in the 
SM
, where 15 changed some artifact from the original study, including changes in research questions. We obtained 28 answers in our survey with SLRs authors that, in general, consolidate interpretations retrieved from the 
SM
, but some answers did not.
Conclusion
SLRs may lose their impact over the years. Identifying actions to keep them updated is of great importance to SLR research field.",Information and Software Technology,18 Mar 2025,4.0,"While the importance of updating Systematic Literature Reviews (SLRs) is acknowledged, the practical value and impact on early-stage ventures are not as direct as other abstracts that provide actionable insights."
https://www.sciencedirect.com/science/article/pii/S0950584919300096,"Euphoria: A Scalable, event-driven architecture for designing interactions across heterogeneous devices in smart environments",May 2019,Not Found,Ovidiu-Andrei=Schipor: schipor@eed.usv.ro; Radu-Daniel=Vatavu: http://www.eed.usv.ro/~vatavu; Jean=Vanderdonckt: jean.vanderdonckt@uclouvain.be,"Abstract
Context:
 From personal mobile and wearable devices to public ambient displays, our 
digital ecosystem
 has been growing with a large variety of smart sensors and devices that can capture and deliver insightful data to 
connected applications
, creating thus the need for new software architectures to enable fluent and flexible interactions in such smart environments.
Objective:
 We introduce 
Euphoria
, a new 
software architecture design
 and implementation that enables easy prototyping, deployment, and evaluation of adaptable and flexible interactions across heterogeneous devices in smart environments.
Method:
 We designed 
Euphoria
 by following the requirements of the ISO/IEC 25010:2011 standard on Software Quality Requirements and Evaluation applied to the specific context of smart environments.
Results:
 To demonstrate the adaptability and flexibility of 
Euphoria
, we describe three application scenarios for contexts of use involving multiple users, multiple input/output devices, and various types of smart environments, as follows: (1) wearable user interfaces and whole-body gesture input for interacting with public ambient displays, (2) multi-device interactions in physical-digital spaces, and (3) interactions on smartwatches for a connected car application scenario. We also perform a technical evaluation of 
Euphoria
 regarding the main factors responsible for the magnitudes of the request-response times for producing, broadcasting, and consuming messages inside the architecture. We deliver the source code of 
Euphoria
 free to download and use for research purposes.
Conclusion:
 By introducing 
Euphoria
 and discussing its applicability, we hope to foster advances and developments in new software architecture initiatives for our increasingly complex smart environments, but also to readily support implementations of novel interactive 
systems and applications
 for smart environments of all kinds.",Information and Software Technology,18 Mar 2025,7.0,"The introduction of Euphoria as a new software architecture design for smart environments and its practical implementation in various application scenarios demonstrate its potential to enable flexible interactions, which can be valuable for startups exploring smart environment technologies."
https://www.sciencedirect.com/science/article/pii/S0950584919300102,A new benchmark for evaluating pattern mining methods based on the automatic generation of testbeds,May 2019,Not Found,B.=Bafandeh Mayvan: Not Found; A.=Rasoolzadegan: rasoolzadegan@um.ac.ir; A.M.=Ebrahimi: Not Found,"Abstract
Context
Mining patterns is one of the most attractive topics in the field of software design. Knowledge about the number, type, and location of pattern instances is crucial to understand the original design decisions. Several techniques and tools have been presented in the literature for mining patterns in a software system. However, evaluating the quality of the detection results is usually done manually or subjectively. This can significantly affect the evaluation results. Therefore, a fair comparison of the quality of the various mining methods is not possible.
Objective
This paper describes a new benchmark to evaluate pattern mining methods in source code or design. Our work aims at overcoming the challenges faced in benchmarking in pattern detection. The proposed benchmark is comprehensive, fair, and objective, with a repeatable evaluation process.
Method
Our proposed benchmark is based on automatic generation of testbeds using graph theory. The generated testbeds are Java source codes and their corresponding class diagrams in which various types of patterns and their variants are inserted in different locations. The generated testbeds differ in their levels of complexity and full information is available on the utilized patterns.
Results
The results show that our proposed benchmark is able to evaluate the pattern mining methods quantitatively and objectively. Also, it can be used to compare pattern mining methods in a fair and repeatable manner.
Conclusions
Based on our findings, it can be argued that benchmarking in the pattern mining field is significantly less mature than topics such as presenting a new detection method. Therefore, special attention is needed in the pattern evaluation topic. Our proposed benchmark is a step towards achieving a comparative understanding of the effectiveness of detection methods and demonstrating their strengths and weaknesses.",Information and Software Technology,18 Mar 2025,7.0,"The benchmark proposed in this abstract can significantly impact early-stage ventures by providing a quantitative and objective way to compare pattern mining methods, a crucial aspect in software design."
https://www.sciencedirect.com/science/article/pii/S095058491930028X,A model of requirements engineering in software startups,May 2019,Not Found,Jorge=Melegati: jmelegatigoncalves@unibz.it; Alfredo=Goldman: Not Found; Fabio=Kon: Not Found; Xiaofeng=Wang: Not Found,"Abstract
Context
Over the past 20 years, software startups have created many products that have changed human life. Since these companies are creating brand-new products or services, requirements are difficult to gather and highly volatile. Although scientific interest in software development in this context has increased, the studies on 
requirements engineering
 in software startups are still scarce and mostly focused on elicitation activities.
Objective
This study overcomes this gap by answering how 
requirements engineering
 practices are performed in this context.
Method
We conducted a grounded theory study based on 17 interviews with software startups practitioners.
Results
We constructed a model to show that software startups do not follow a single set of practices but, instead, build a custom process, changed throughout the development of the company, combining different practices according to a set of influences (Founders, Software Development Manager, Developers, Market, Business Model and Startup Ecosystem).
Conclusion
Our findings show that requirements engineering activities in software startups are similar to those in agile teams, but some steps vary as a consequence of the lack of an accessible customer.",Information and Software Technology,18 Mar 2025,6.0,"This study on requirements engineering practices in software startups can be beneficial for early-stage ventures looking to understand and improve their development processes, but the findings are more specific to agile teams."
https://www.sciencedirect.com/science/article/pii/S0950584919300291,Empirical evaluation and proposals for bands-based COSMIC early estimation methods,May 2019,Not Found,Sandro=Morasca: Not Found,"Abstract
Background
. In the early phases of software development projects, thorough application of the 
COSMIC
 functional size measurement method may require more time and effort than available. Thus, early approximate methods have been proposed for estimating the 
COSMIC
 functional size of an application, instead of measuring it.
Objective
. The goal of this paper is to empirically evaluate the accuracy of the COSMIC early size estimation methods that are based on evaluations at the functional process level, for which 
historical data
 are available. The goal is to provide practitioners with empirical evidence on the accuracy of these methods.
Method
. We evaluated the Average Functional Process and the Equal Size Bands methods. We also proposed and evaluated two new approaches for defining bands in the Fixed Size Classification method. The estimation was performed by applying these methods to a set of 
software applications
 for which the data necessary to perform estimations were available, having been previously measured according to the standard COSMIC method.
Results
. Our analyses show that the Average Functional Process method generally provides estimates that are reasonable for early and quick sizing, but in some cases its 
estimation errors
 are too large to be acceptable. On the contrary, the methods using bands can provide quite accurate estimates. We determine the level of accuracy that can be obtained based on the type of method used, the number of bands used, and the quantitative characterization of the ability to classify each functional process in the correct band.
Conclusions
. The Average Functional Process method may be unreliable, as it occasionally yields quite large errors. Organizations using bands-based methods cannot just follow the prescribed estimation process: they need to properly train people in charge of classifying functional processes in the correct size band.",Information and Software Technology,18 Mar 2025,8.0,"The evaluation of early size estimation methods can directly benefit European early-stage ventures by providing empirical evidence on the accuracy of these methods, which can help in resource allocation and project planning."
https://www.sciencedirect.com/science/article/pii/S0950584918302477,Model-based test suite generation for graph transformation system using model simulation and search-based techniques,April 2019,Not Found,Akram=Kalaee: a-kalaee@arshad.araku.ac.ir; Vahid=Rafe: v-rafe@araku.ac.ir,"Abstract
Context
Test generation by model checking is a useful technique in model-based testing that allows automatic generation of test cases from models by utilizing the counter-examples/witnesses produced through a 
model checker
. However, generating redundant test cases and state space explosion problem are two major obstacles to transfer this technique into industrial practice.
Objective
An idea to cope with these challenges consists in an intelligent model checking for exploring only a portion of the state space according to the test objectives. Motivated by this idea, we propose an approach that exploits meta-heuristic algorithms to adapt a 
model checker
 when used for integration testing of systems formally specified by graph transformations.
Method
This method is not based on 
model checking algorithms
, but rather uses the 
modeling and simulation
 features of the underlying model checker. In the proposed approach, a population of test suites that each of which is a set of paths on the state space, is evolved towards satisfying the all def-use test objectives. Consequently, a test suite with high coverage is generated.
Results
To assess the efficiency of our approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Empirical results based on some 
case studies
, confirm a significant improvement in terms of coverage, speed and memory usage, in comparison with the 
state of the art techniques
.
Conclusion
Our analysis reveals that intelligent model checking can appropriately address the challenges of traditional model-checking-assisted testing. We further conclude that graph transformation specification is an efficient modeling solution to behavioral testing and graph transformation tools have a great potential for developing a model-based testing tool.",Information and Software Technology,18 Mar 2025,9.0,"The approach proposed in this abstract for intelligent model checking can greatly impact the efficiency of testing processes in software development, providing a practical solution to challenges faced in industrial practice."
https://www.sciencedirect.com/science/article/pii/S0950584918302490,A survey on software testability,April 2019,Not Found,Vahid=Garousi: vahid.garousi@wur.nl; Feyza Nur=Kılıçaslan: feyzanur@cs.hacettepe.edu.tr,"Abstract
Context
Software testability is the degree to which a software system or a unit under test supports its own testing. To predict and improve software testability, a large number of techniques and metrics have been proposed by both practitioners and researchers in the last several decades. Reviewing and getting an overview of the entire state-of-the-art and state-of-the-practice in this area is often challenging for a practitioner or a new researcher.
Objective
Our objective is to summarize the body of knowledge in this area and to benefit the readers (both practitioners and researchers) in preparing, measuring and improving software testability.
Method
To address the above need, the authors conducted a survey in the form of a systematic literature mapping (classification) to find out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our final pool included 208 papers (published between 1982 and 2017).
Results
The area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors affecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability.
Conclusion
This paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess 
potential benefits
 of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneficial in their testing activities. Our results can also benefit researchers in observing the trends in this area and identify the topics that require further investigation.",Information and Software Technology,18 Mar 2025,5.0,"While the survey on software testability provides an overview of the state-of-the-art, the practical value for European early-stage ventures may be limited as it is more focused on summarizing existing knowledge."
https://www.sciencedirect.com/science/article/pii/S0950584918302507,A systematic mapping study of infrastructure as code research,April 2019,Not Found,Akond=Rahman: aarahman@ncsu.edu; Rezvan=Mahdavi-Hezaveh: Not Found; Laurie=Williams: Not Found,"Abstract
Context:
 Infrastructure as code (IaC) is the practice to automatically configure system dependencies and to provision local and remote instances. Practitioners consider IaC as a fundamental pillar to implement 
DevOps
 practices, which helps them to rapidly deliver software and services to end-users. Information technology (IT) organizations, such as GitHub, Mozilla, Facebook, Google and Netflix have adopted IaC. A 
systematic mapping study
 on existing IaC research can help researchers to identify potential research areas related to IaC, for example defects and security flaws that may occur in IaC scripts.
Objective:
 The objective of this paper is to help researchers identify research areas related to infrastructure as code (IaC) by conducting a 
systematic mapping study
 of IaC-related research.
Method:
 We conduct our research study by searching five scholar databases. We collect a set of 31,498 publications by using seven search strings. By systematically applying inclusion and exclusion criteria, which includes removing duplicates and removing non-English and non peer-reviewed publications, we identify 32 publications related to IaC. We identify topics addressed in these publications by applying qualitative analysis.
Results:
 We identify four topics studied in IaC-related publications: (i) framework/tool for infrastructure as code; (ii) adoption of infrastructure as code; (iii) empirical study related to infrastructure as code; and (iv) testing in infrastructure as code. According to our analysis, 50.0% of the studied 32 publications propose a framework or tool to implement the practice of IaC or extend the functionality of an existing IaC tool.
Conclusion:
 Our findings suggest that framework or tools is a well-studied topic in IaC research. As defects and security flaws can have serious consequences for the deployment and development environments in 
DevOps
, we observe the need for research studies that will study defects and security flaws for IaC.",Information and Software Technology,18 Mar 2025,6.0,"The research on IaC can have a practical impact on improving software delivery processes in IT organizations, making it relevant for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302519,Impact of model notations on the productivity of domain modelling: An empirical study,April 2019,Not Found,Cristina=Cachero: ccachero@dlsi.ua.es; Santiago=Meliá: santi@dlsi.ua.es; Jesús M.=Hermida: jhermida@dlsi.ua.es,"Abstract
Context
The intensive use of models is a cornerstone of the Model-Driven Engineering (MDE) paradigm and its claimed gains in productivity. However, in order to maximize these productivity gains, it is important to adequately select the modeling formalism to be used. Unfortunately, the MDE community still lacks empirical data to support such choice.
Objective
This paper aims at contributing to filling this gap by reporting an empirical study in which two types of domain model notations, graphical vs. textual, are compared regarding their efficiency and effectiveness during the creation of domain models.
Method
A quasi-experiment was designed in which 127 participants were randomly classified in four groups. Then, each group was randomly assigned to a different combination of notation and application. All the participants were students enrolled in the 6th semester of the Computer Engineering degree at the University of Alicante. The statistical procedure applied was a two-factor multivariate analysis of variance (two-way MANOVA).
Results
The data shows a statistically significant effect of notation type on the efficiency and effectiveness of domain modelling activities, independently from the application being modelled.
Conclusion
The joint examination of our results and those of previous studies suggests that, in MDE, different tasks call for different types of notations. Therefore, MDE environments should offer both textual and graphical notations, and assist developers in selecting the most suitable one depending on the task being carried out. In particular, our data suggest that domain model creation tasks are better supported by graphical notations. To augment the validity of the conclusions of this paper, the experiment should be replicated with different subject profiles, notations, domain model sizes, tasks and application types.",Information and Software Technology,18 Mar 2025,8.0,"The study on selecting modeling formalism in MDE can provide valuable insights for startups looking to maximize productivity gains, hence scoring higher in practical value."
https://www.sciencedirect.com/science/article/pii/S0950584918302593,Live programming in practice: A controlled experiment on state machines for robotic behaviors,April 2019,Not Found,Miguel=Campusano: mcampusa@dcc.uchile.cl; Johan=Fabry: jfabry@gmail.com; Alexandre=Bergel: abergel@dcc.uchile.cl,"Abstract
Context
Live programming environments are gaining momentum across multiple programming languages. A tenet of live programming is a development feedback cycle, resulting in faster development practices. Although practitioners of live programming consider it a positive inclusion in their workflow, no in-depth investigations have yet been conducted on its benefits in a realistic scenario, nor using complex API.
Objective
This paper carefully studies the advantage of using live programming in defining nested state machines for robot behaviors. We analyzed two important aspects of developing robotic behaviors using these machines: 
program comprehension
 and program writing. We analyzed both development practices in terms of speed and accuracy.
Method
We conducted two controlled experiments, one for 
program comprehension
 and another for program writing. We measured the speed and accuracy of randomized assigned participants on completing programming tasks, against a baseline.
Results
In a robotic behavior context, we found that a live programming system for nested state machine programs does not significantly outperform a non-live language in program comprehension nor in program writing in terms of speed and accuracy. However, the feedback of test subjects indicates their preference for the live programming system.
Conclusions
The results of this work seem to contradict the studies of live programming in other areas, even while participants still favor using live programming techniques. We learned that the complex API chosen in this work has a strong negative influence on the results. To the best of our knowledge, this is the first in-depth live programming experiment in a complex domain.",Information and Software Technology,18 Mar 2025,4.0,"While the study on live programming in robot behaviors is interesting, the lack of significant performance improvement limits its practical applicability for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302623,Machine learning techniques for code smell detection: A systematic literature review and meta-analysis,April 2019,Not Found,Muhammad Ilyas=Azeem: azeem@itechs.iscas.ac.cn; Fabio=Palomba: palomba@ifi.uzh.ch; Lin=Shi: shilin@itechs.iscas.ac.cn; Qing=Wang: wq@itechs.iscas.ac.cn,"Abstract
Background
: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of 
machine learning techniques
 represents an ever increasing research area.
Objective
: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how 
machine learning approaches
 have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of 
machine learning approaches
 in the field of code smells.
Method
: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far.
Results
: The analyses performed show that 
God Class, Long Method, 
Functional Decomposition
, and 
Spaghetti Code
 have been heavily considered in the literature. 
Decision Trees
 and 
Support Vector Machines
 are the most commonly used 
machine learning algorithms
 for code smell detection. Models based on a large set of independent variables have performed well. 
JRip
 and 
Random Forest
 are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future.
Conclusion
: Based on our findings, we argue that there is still room for the improvement of 
machine learning techniques
 in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.",Information and Software Technology,18 Mar 2025,7.0,"The use of machine learning techniques for code smell detection can have a significant impact on software development practices, making it relevant and valuable for startups."
https://www.sciencedirect.com/science/article/pii/S0950584919300011,The current state of software license renewals in the I.T. industry,April 2019,Not Found,Aindrila=Ghosh: aindrila@ualberta.ca; Mona=Nashaat: nashaata@ualberta.ca; James=Miller: jimm@ualberta.ca,"Abstract
Context
The software industry has changed significantly in the 21st century; no longer is it dominated by organizations seeking to sell products directly to customers, instead most 
multinational organizations
 nowadays provide services via licensing agreements. These licenses are for a fixed-duration; and hence, the question of their renewal becomes of 
paramount importance
 for the selling organization’s revenue.
Objective
Despite its financial impact, the topic of license renewal strategies, processes, tools, and support receives very limited attention in the research literature. Hence, it is believed that an interesting research question is: What is the state of current industrial practice in this essential field?
Method
To initially explore the topic of license renewals, this paper implements the 
Grounded theory method
. To implement the method, semi-structured, cross-sectional, anonymous, selfreported interviews are carried out with 20 professionals from multiple organizations, later the Constant Comparative Method is used to analyse the 
collected data
.
Results
This paper presents a synthesized picture of the current industrial practice of the end-to-end software license 
renewal process
. Alongside, it also identifies a set of challenges and risk factors that impact on renewal decisions of customers, hence on the overall revenue of seller organizations. Finally, using structured brainstorming techniques, this paper identifies 11 future research directions, that can help organizations with the mitigation of the risks in the license 
renewal process
.
Conclusion
It is concluded that lack of effective communication among stakeholders, the absence of customer trust, and scarcity of value generated from purchased licenses are among the primary drivers that influence renewal decisions. Also, there is a need to invest in intelligent automation along with 
artificial intelligence
 enabled analytics in order to enhance customer satisfaction.",Information and Software Technology,18 Mar 2025,5.0,The research on software license renewal processes is important but might have limited immediate practical value for startups unless they are in the software licensing industry.
https://www.sciencedirect.com/science/article/pii/S095058491830209X,An extensible collaborative framework for monitoring software quality in critical systems,March 2019,Not Found,Marisol=García-Valls: mvalls@it.uc3m.es; Julio=Escribano-Barreno: jebarreno@indra.es; Javier=García-Muñoz: 100291551@alumnos.uc3m.es,"Abstract
Context
Current practices on software quality monitoring for critical software systems development rely on the manual integration of the information provided by a number of independent commercial tools for code analysis; some external tools for code analysis are mandatory in some critical software projects that must comply with specific norms. However, there are no approaches to providing an integrated view over the analysis results of independent external tools into a unified software quality framework.
Objective
This paper presents the design and development of ESQUF (Enhanced Software Quality Monitoring Framework) suitable for critical software systems. It provides the above enriched quality results presentation derived not only from multiple external tools but from the local analysis functions of the framework.
Method
An analysis of the norms and standards that apply to critical software systems is provided. The detailed and modular design of ESQUF adjusts to the integration requirements for external tools. UML is used for designing the framework, and Java is used to provide the detailed design. The framework is validated with a prototype implementation that integrates two different external tools and their respective quality results over a real software project 
source code
.
Results
The integration of results files and data from external tools as well as from internal analysis functions is enabled. The analysis of critical software projects is made posible yielding a 
collaborative space
 where 
verification engineers
 
share information
 about code analysis activities of specific projects; and single 
presentation space
 with rich static and dynamic analysis information of software projects that comply with the required development norms.",Information and Software Technology,18 Mar 2025,8.0,"The development of ESQUF framework for critical software systems could have a significant impact on improving software quality monitoring, which is crucial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058491830209X,An extensible collaborative framework for monitoring software quality in critical systems,March 2019,Not Found,Marisol=García-Valls: mvalls@it.uc3m.es; Julio=Escribano-Barreno: jebarreno@indra.es; Javier=García-Muñoz: 100291551@alumnos.uc3m.es,"Abstract
Context
Current practices on software quality monitoring for critical software systems development rely on the manual integration of the information provided by a number of independent commercial tools for code analysis; some external tools for code analysis are mandatory in some critical software projects that must comply with specific norms. However, there are no approaches to providing an integrated view over the analysis results of independent external tools into a unified software quality framework.
Objective
This paper presents the design and development of ESQUF (Enhanced Software Quality Monitoring Framework) suitable for critical software systems. It provides the above enriched quality results presentation derived not only from multiple external tools but from the local analysis functions of the framework.
Method
An analysis of the norms and standards that apply to critical software systems is provided. The detailed and modular design of ESQUF adjusts to the integration requirements for external tools. UML is used for designing the framework, and Java is used to provide the detailed design. The framework is validated with a prototype implementation that integrates two different external tools and their respective quality results over a real software project 
source code
.
Results
The integration of results files and data from external tools as well as from internal analysis functions is enabled. The analysis of critical software projects is made posible yielding a 
collaborative space
 where 
verification engineers
 
share information
 about code analysis activities of specific projects; and single 
presentation space
 with rich static and dynamic analysis information of software projects that comply with the required development norms.",Information and Software Technology,18 Mar 2025,8.0,"Similar to abstract 146, the ESQUF framework has practical value for critical software systems, potentially benefiting European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302210,Slimming javascript applications: An approach for removing unused functions from javascript libraries,March 2019,Not Found,H.C.=Vázquez: hvazquez@exa.unicen.edu.ar; A.=Bergel: abergel@dcc.uchile.cl; S.=Vidal: svidal@exa.unicen.edu.ar; J.A.=Díaz Pace: adiaz@exa.unicen.edu.ar; C.=Marcos: cmarcos@exa.unicen.edu.ar,"Abstract
Context
A 
common practice
 in JavaScript development is to ship and deploy an application as a large file, called 
bundle
, which is the result of combining the application code along with the code of all the libraries the application depends on. Despite the benefits of having a single bundle per application, this approach leads to applications being shipped with significant portions of code that are actually not used, which unnecessarily inflates the JavaScript bundles and could slow down website loading because of the extra unused code. Although some 
static analysis
 techniques exist for removing unused code, our investigations suggest that there is still room for improvements.
Objective
The goal of this paper is to address the problem of reducing the size of bundle files in JavaScript applications.
Method
In this context, we define the notion of Unused Foreign Function (UFF) to denote a JavaScript function contained in dependent libraries that is not needed at runtime. Furthermore, we propose an approach based on dynamic analysis that assists developers to identify and remove UFFs from JavaScript bundles.
Results
We report on a case-study performed over 22 JavaScript applications, showing evidence that our approach can produce size reductions of 26% on average (with reductions going up to 66% in some applications).
Conclusion
It is concluded that removing unused foreign functions from JavaScript bundles helps reduce their size, and thus, it can boost the results of existing 
static analysis
 techniques.",Information and Software Technology,18 Mar 2025,6.0,"Addressing the problem of reducing the size of bundle files in JavaScript applications is valuable, although the direct impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584918302234,Adapting usability techniques for application in open source Software: A multiple case study,March 2019,Not Found,Lucrecia=Llerena: lucrecia.llerena@estudiante.uam.es; Nancy=Rodriguez: nancy.rodriguez@estudiante.uam.es; John W.=Castro: john.castro@uda.cl; Silvia T.=Acuña: silvia.acunna@uam.es,"Abstract
Context
As a result of the growth of non-developer users of 
OSS applications
, usability has over the last ten years begun to attract the interest of the open source software (OSS) community. The 
OSS
 community has some special characteristics (such as worldwide geographical distribution of both users and developers and missing resources) which are an obstacle to the direct adoption of many usability techniques as specified in the human-computer interaction (HCI) field.
Objective
The aim of this research is to adapt and evaluate the feasibility of applying four usability techniques: user profiles, personas, direct observation and post-test information to four 
OSS projects
 from the viewpoint of the development team.
Method
The applied research method was a multiple 
case study
 of the following 
OSS projects
: Quite Universal Circuit Simulator, PSeInt, FreeMind and OpenOffice Writer.
Results
We formalized the application procedure of each of the adapted usability techniques. We found that either there were no procedures for adopting usability techniques in 
OSS
 or they were not fully systematized. Additionally, we identified the adverse conditions that are an obstacle to their adoption in OSS and propose the special adaptations required to overcome the obstacles. To avoid some of the adverse conditions, we created web artefacts (online survey, 
wiki
 and forum) that are very popular in the OSS field.
Conclusion
It is necessary to adapt usability techniques for application in OSS projects considering their idiosyncrasy. Additionally, we found that there are obstacles (for example, number of participant users, biased information provided by developers) to the application of the techniques. Despite these obstacles, it is feasible to apply the adapted techniques in OSS projects.",Information and Software Technology,18 Mar 2025,5.0,"Adapting usability techniques for OSS projects is important, but the practical impact on European early-stage ventures may require further validation."
https://www.sciencedirect.com/science/article/pii/S095058491830226X,On semantic detection of cloud API (anti)patterns,March 2019,Not Found,Hayet=Brabra: hayet.brabra@telecom-sudparis.eu; Achraf=Mtibaa: achraf.mtibaa@enetcom.usf.tn; Fabio=Petrillo: fabio@petrillo.com; Philippe=Merle: philippe.merle@inria.fr; Layth=Sliman: layth.sliman@efrei.fr; Naouel=Moha: moha.naouel@uqam.ca; Walid=Gaaloul: walid.gaaloul@telecom-sudparis.eu; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@polymtl.ca; Boualem=Benatallah: boualem@cse.unsw.edu.au; Faïez=Gargouri: faiez.gargouri@isims.usf.tn,"Abstract
Context
Open standards are urgently needed for enabling software interoperability in 
Cloud Computing
. Open 
Cloud Computing
 Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their 
understandability
 and 
reusability
.
Objective
We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles.
Method
First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)patterns and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility.
Results
We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an 
acceptable level
 of maturity regarding REST principles.
Conclusion
Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.",Information and Software Technology,18 Mar 2025,7.0,Enhancing REST management APIs in Cloud Computing through compliance evaluation and recommendations could have a moderate impact on European early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584918302313,Information quality requirements engineering with STS-IQ,March 2019,Not Found,Mohamad=Gharib: mohamad.gharib@unifi.it; Paolo=Giorgini: paolo.giorgini@unitn.it,"Abstract
Context
Information Quality (IQ) is particularly important for organizations: they depend on information for managing their daily tasks and relying on low-quality information may negatively influence their overall performance. Despite this, the literature shows that most software development approaches do not consider IQ requirements during the system design, which leaves the system open to different kinds of vulnerabilities.
Objective
The main objective of this research is proposing a framework for modeling and analyzing IQ requirements for Socio-Technical Systems (STS).
Method
We propose STS-IQ, a goal-oriented framework for modeling and analyzing IQ requirements in their social and organizational context since the early phases of the system design. The framework extends and refines our previous work, and it consists of: (i) a 
modeling language
 that provides concepts and constructs for modeling IQ requirements; (ii) a set of analysis techniques that support the verification of the correctness and consistency of the IQ requirements model; (iii) a mechanism for deriving the final IQ specifications in terms of IQ policies; (iv) a methodology to assist software engineers during the system design; and (v) a CASE tool, namely STS-IQ Tool.
Result
We demonstrated the applicability, usefulness, and scalability of the modeling and reasoning techniques within a stock market 
case study
, and we also evaluated the usability and utility of the framework with end-users.
Conclusion
We conclude that the STS-IQ framework supports the modeling and analysis of IQ requirements, and also the derivation of precise IQ specifications in terms of IQ policies. Therefore, we believe it has potential in practice.",Information and Software Technology,18 Mar 2025,7.0,"The proposed STS-IQ framework addresses an important issue in software development and has demonstrated applicability and utility in a case study, showing potential practical value."
https://www.sciencedirect.com/science/article/pii/S0950584918302325,Exploratory testing: Do contextual factors influence software fault identification?,March 2019,Not Found,Fredrik=Asplund: fasplund@kth.se,"Abstract
Context:
 Exploratory Testing (ET) is a manual approach to software testing in which learning, test design and test execution occurs simultaneously. Still a developing topic of interest to academia, although as yet insufficiently investigated, most studies focus on the skills and experience of the individual tester. However, contextual factors such as project processes, test scope and organisational boundaries are also likely to affect the approach.
Objective:
 This study explores contextual differences between teams of testers at a MedTec firm developing safety-critical products to ascertain whether contextual factors can influence the outcomes of ET, and what associated implications can be drawn for test management.
Method:
 A development project was studied in two iterations, each consisting of a quantitative phase testing hypotheses concerning when ET would identify faults in comparison to other testing approaches and a qualitative phase involving interviews.
Results:
 Influence on ET is traced to how the scope of tests focus learning on different types of knowledge and imply an asymmetry in the strength and number of information flows to test teams.
Conclusions:
 While test specialisation can be attractive to software development organisations, results suggest changes to processes and organisational structures might be required to maintain test efficiency throughout projects: the responsibility for test cases might need to be rotated late in projects, and asymmetries in information flows might require management to actively strengthen the presence and connections of test teams throughout the firm. However, further research is needed to investigate whether these results also hold for non safety-critical faults.",Information and Software Technology,18 Mar 2025,5.0,"The study on contextual differences in exploratory testing provides insights for test management, but the implications for practical application in startups are not clearly defined."
https://www.sciencedirect.com/science/article/pii/S0950584918302416,A two-phase transfer learning model for cross-project defect prediction,March 2019,Not Found,Chao=Liu: liu.chao@cqu.edu.cn; Dan=Yang: dyang@cqu.edu.cn; Xin=Xia: xin.xia@monash.edu; Meng=Yan: mengy@zju.edu.cn; Xiaohong=Zhang: xhongz@cqu.edu.cn,"Abstract
Context:
 Previous studies have shown that a 
transfer learning
 model, TCA+ proposed by Nam et al., can significantly improve the performance of cross-project 
defect prediction
 (CPDP). TCA+ achieves the improvement by reducing 
data distribution
 difference between source (training data) and target (testing data) projects. However, TCA+ is unstable, i.e., its performance varies largely when using different source projects to build prediction models. In practice, it is hard to choose a suitable source project to build the prediction model.
Objective:
 To address the limitation of TCA+, we propose a two-phase 
transfer learning
 model (TPTL) for CPDP.
Method:
 In the first phase, we propose a source project estimator (SPE) to automatically choose two source projects with the highest distribution similarity to a target project from candidates. Next, two source projects that are estimated to achieve the highest values of F1-score and cost-effectiveness are selected. In the second phase, we leverage TCA+ to build two prediction models based on the two selected projects and combine their prediction results to further improve the prediction performance.
Results:
 We evaluate TPTL on 42 defect datasets from PROMISE repository, and compare it with two versions of TCA+ (TCA+_Rnd, randomly selecting one source project; TCA+_All, using all alternative source projects), a related source project selection model TDS proposed by Herbold, a state-of-the-art CPDP model leveraging a log transformation (LT) method, and a 
transfer learning
 model Dycom with better form of TCA. Experiment results show that, on average across 42 datasets, TPTL respectively improves these 
baseline models
 by 19%, 5%, 36%, 27%, and 11% in terms of F1-score; by 64%, 92%, 71%, 11%, and 66% in terms of cost-effectiveness.
Conclusion:
 The proposed TPTL model can solve the instability problem of TCA+, showing substantial improvements over the state-of-the-art and related CPDP models.",Information and Software Technology,18 Mar 2025,9.0,"The TPTL model addresses the instability issue of an existing model with significant improvements in performance across various datasets, making it highly valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302428,FSCT: A new fuzzy search strategy in concolic testing,March 2019,Not Found,Arash=Sabbaghi: a.sabbaghi@qiau.ac.ir; Hamidreza=Rashidy Kanan: h.rashidykanan@sru.ac.ir; Mohammad Reza=Keyvanpour: keyvanpour@alzahra.ac.ir,"Abstract
Context
Concolic testing is a promising approach to automate structural test data generation. However, combinatorial explosion of the path space, known as path explosion, and also constrained testing budget, makes achieving high code coverage in concolic testing a challenging task.
Objective
All branches of the previously explored paths make up the 
search space
 of concolic testing and search strategy define the mechanism of choosing branches to be flipped to drive the execution toward testing goals. With regard to the large number of candidate branches, choosing the right branch to continue the search is so crucial and has a direct impact on coverage rate and effort. This paper aims to improve the effectiveness of branch testing by considering the characteristics of paths reaching uncovered branches and presenting a novel search strategy for effectively and efficiently exploring the 
search space
.
Method
We model the branch selection process in concolic testing as a decision making system and introduce a new Fuzzy Search Strategy in Concolic Testing (FSCT). FSCT chooses a branch to be filliped in which the most suitable path with respect to the proposed 
coverage factors
 reaches an uncovered branch with the highest priority and this priority is assigned by the designed fuzzy 
expert system
. The proposed 
coverage factors
 effectively help to determine the characteristics of paths.
Results
We implemented FSCT on top of CREST and evaluated it using several popular benchmarks. The experimental results show that FSCT outperforms the state-of-the-art techniques in terms of coverage rate and coverage effort.
Conclusion
FSCT helps concolic testing to better cope with path explosion problem and shows its capabilities to achieve higher code coverage while at the same time decreases testing efforts in terms of both runtime and number of iterations.",Information and Software Technology,18 Mar 2025,8.0,"The FSCT approach in concolic testing shows better coverage rates and effort reductions, highlighting its potential impact on efficiency and effectiveness for startups in software testing."
https://www.sciencedirect.com/science/article/pii/S0950584918302441,A survey on software coupling relations and tools,March 2019,Not Found,Enrico=Fregnan: fregnan@ifi.uzh.ch; Tobias=Baum: tobias.baum@inf.uni-hannover.de; Fabio=Palomba: palomba@ifi.uzh.ch; Alberto=Bacchelli: bacchelli@ifi.uzh.ch,"Abstract
Context
Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find.
Goals
The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the 
software engineering
 academic community to extract these metrics.
Method
This work constitutes a systematic literature review in 
software engineering
. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material.
Results
Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an 
independent group
 and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group.
Conclusion
This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. 
Public preprint
 [
https://doi.org/10.5281/zenodo.2002001
].",Information and Software Technology,18 Mar 2025,6.0,"The systematic literature review on software coupling metrics provides a comprehensive overview, but the direct practical implications for early-stage ventures are not explicitly stated."
https://www.sciencedirect.com/science/article/pii/S0950584918301873,Metrics for analyzing variability and its implementation in software product lines: A systematic literature review,February 2019,Not Found,Sascha=El-Sharkawy: elscha@sse.uni-hildesheim.de; Nozomi=Yamagishi-Eichler: Not Found; Klaus=Schmid: schmid@sse.uni-hildesheim.de,"Abstract
Context:
 Software Product Line (SPL) development requires at least concepts for variability implementation and 
variability modeling
 for deriving products from a product line. These variability implementation concepts are not required for the development of single systems and, thus, are not considered in traditional 
software engineering
. Metrics are well established in traditional 
software engineering
, but existing metrics are typically not applicable to SPLs as they do not address variability management. Over time, various specialized product line metrics have been described in literature, but no systematic description of these metrics and their characteristics is currently available.
Objective:
 This paper describes and analyzes variability-aware metrics, designed for the needs of software product lines. More precisely we restrict the scope of our study explicitly to metrics designed for variability models, code artifacts, and metrics taking both kinds of artifacts into account. Further, we categorize the purpose for which these metrics were developed. We also analyze to what extent these metrics were evaluated to provide a basis for researchers for selecting adequate metrics.
Method:
 We conducted a systematic literature review to identify variability-aware implementation metrics. We discovered 42 relevant papers reporting metrics intended to measure aspects of variability models or code artifacts.
Results:
 We identified 57 variability model metrics, 34 annotation-based 
code metrics
, 46 
code metrics
 specific to composition-based implementation techniques, and 10 metrics integrating information from variability model and code artifacts. For only 31 metrics, an evaluation was performed assessing their suitability to draw any qualitative conclusions.
Conclusions:
 We observed several problematic issues regarding the definition and the use of the metrics. Researchers and practitioners benefit from the catalog of variability-aware metrics, which is the first of its kind. Also, the research community benefits from the identified observations in order to avoid those problems when defining new metrics.",Information and Software Technology,18 Mar 2025,2.0,"While the paper provides insights into variability-aware metrics for software product lines, the practical value for early-stage ventures is limited as it is more focused on academia and research."
https://www.sciencedirect.com/science/article/pii/S0950584918301903,Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe,February 2019,Not Found,Loveleen=Kaur: loveleen.kaur@thapar.edu; Ashutosh=Mishra: ashutosh.mishra@thapar.edu,"Abstract
Context
It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
Objective
This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
Method
For multiple successive releases of two Java-based software projects, where the 
source code
 of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's 
source code
 Java files. We construct eight datasets and build 
predictive models
 using statistical analysis and 
machine learning techniques
.
Results
The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change.",Information and Software Technology,18 Mar 2025,5.0,The study on cognitive complexity in software maintenance has some relevance for startups as it addresses a common challenge that may impact software change practices.
https://www.sciencedirect.com/science/article/pii/S0950584917300770,Evaluating different i*-based approaches for selecting functional requirements while balancing and optimizing non-functional requirements: A controlled experiment,February 2019,Not Found,Jose=Zubcoff: jose.zubcoff@ua.es; Irene=Garrigós: Not Found; Jose-Norberto=Mazón: Not Found; Jose-Alfonso=Aguilar: Not Found; Francisco=Gomariz-Castillo: Not Found,"Abstract
Context
A relevant question in requirements engineering is which set of functional requirements (FR) to prioritize and implement, while keeping non-functional requirements (NFR) balanced and optimized.
Objective
We aim to provide empirical evidence that requirement engineers may perform better at the task of selecting FRs while optimizing and balancing NFRs using an alternative (automated) i* post-processed model, compared to the original i* model.
Method
We performed a controlled experiment, designed to compare the original i* graphical notation, with our post-processed i* visualizations based on Pareto efficiency (a tabular and a radar chart visualization). Our experiment consisted of solving different exercises of various complexity for selecting FRs while balancing NFR. We considered the efficiency (time spent to correctly answer exercises), and the effectiveness (regarding time: time spent to solve exercises, independent of correctness; and regarding correctness of the answer, independent of time).
Results
The efficiency analysis shows it is 3.51 times more likely to solve exercises correctly with our tabular and radar chart visualizations than with i*. Actually, i* was the most time-consuming (effectiveness regarding time), had a lower number of correct answers (effectiveness regarding correctness), and was affected by complexity. Visual or textual preference of the subjects had no effect on the score. Beginners took more time to solve exercises than experts if i* is used (no distinction if our Pareto-based visualizations are used).
Conclusion
For complex model instances, the Pareto front based tabular visualization results in more correct answers, compared to radar chart visualization. When we consider effectiveness regarding time, the i* graphical notation is the most time consuming visualization, independent of the complexity of the exercise. Finally, regarding efficiency, subjects consume less time when using radar chart visualization than tabular visualization, and even more so compared to the original i* graphical notation.",Information and Software Technology,18 Mar 2025,8.0,The experiment comparing i* visualizations for requirements selection provides valuable insights that can benefit early-stage ventures in optimizing functional and non-functional requirements prioritization.
https://www.sciencedirect.com/science/article/pii/S0950584918301927,Creative goal modeling for innovative requirements,February 2019,Not Found,J.=Horkoff: jenho@chalmers.se; N.A.=Maiden: neil.maiden.1@city.ac.uk; D.=Asboth: david.asboth.2@city.ac.uk,"Abstract
Context
 When determining the functions and qualities (a.k.a. requirements) for a system, creativity is key to drive innovation and foster business success. However, creative requirements must be practically operationalized, grounded in concrete functions and system interactions. 
Requirements Engineering
 (RE) has produced a wealth of methods centered around goal modeling, in order to graphically explore the space of alternative requirements, linking functions to goals and dependencies. In parallel work, 
creativity theories
 from the social sciences have been applied to the design of creative requirements workshops, pushing stakeholders to develop innovative systems. Goal models tend to focus on what is known, while creativity workshops are expensive, require a specific skill set to facilitate, and produce mainly paper-based, unstructured outputs. 
Objective
 Our aim in this work is to explore beneficial combinations of the two areas of work in order to overcome these and other limitations, facilitating creative 
requirements elicitation
, supported by a simple extension of a well-known and structured requirements modeling technique. 
Method
 We take a Design Science approach, iterating over 
exploratory studies
, design, and summative validation studies. 
Results
 The result is the Creative Leaf tool and method supporting creative goal modeling for RE. 
Conclusion
 We support creative RE by making creativity techniques more accessible, producing structured digital outputs which better match to existing RE methods with associated analysis procedures and transformations.",Information and Software Technology,18 Mar 2025,6.0,The Creative Leaf tool and method for creative requirements elicitation offers a practical approach that can be beneficial for startups in fostering innovation and structured digital outputs.
https://www.sciencedirect.com/science/article/pii/S0950584918301939,Guidelines for including grey literature and conducting multivocal literature reviews in software engineering,February 2019,Not Found,Vahid=Garousi: vahid.garousi@wur.nl; Mika V.=Mäntylä: mika.mantyla@oulu.fi,"Abstract
Context
A Multivocal Literature Review (MLR) is a form of a 
Systematic Literature Review
 (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in 
software engineering
 (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results.
Objective
There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE.
Method
To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example.
Results
The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations.
Conclusion
Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.",Information and Software Technology,18 Mar 2025,4.0,"The guidelines for conducting Multivocal Literature Reviews provide some value for researchers and practitioners in software engineering, but the direct impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584918302040,Exploiting Parts-of-Speech for effective automated requirements traceability,February 2019,Not Found,Nasir=Ali: cnali@memphis.edu; Haipeng=Cai: hcai@eecs.wsu.edu; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca; Jameleddine=Hassine: jhassine@kfupm.edu.sa,"Abstract
Context
Requirement traceability (RT) is defined as the ability to describe and follow the life of a requirement. RT helps developers ensure that relevant requirements are implemented and that the source code is consistent with its requirement with respect to a set of 
traceability links
 called 
trace links
. Previous work leverages Parts Of Speech (POS) tagging of software artifacts to recover trace links among them. These studies work on the premise that discarding one or more POS tags results in an improved accuracy of Information Retrieval (IR) techniques.
Objective
First, we show empirically that excluding one or more POS tags could negatively impact the accuracy of existing IR-based traceability approaches, namely the 
Vector Space Model
 (VSM) and the Jensen Shannon Model (JSM). Second, we propose a method that improves the accuracy of IR-based traceability approaches.
Method
We developed an approach, called 
ConPOS
, to recover 
trace links
 using constraint-based pruning. 
ConPOS
 uses major POS categories and applies constraints to the recovered trace links for pruning as a filtering process to significantly improve the effectiveness of IR-based techniques. We conducted an experiment to provide evidence that removing POSs does not improve the accuracy of IR techniques. Furthermore, we conducted two empirical studies to evaluate the effectiveness of 
ConPOS
 in recovering trace links compared to existing peer RT approaches.
Results
The results of the first empirical study show that removing one or more POS negatively impacts the accuracy of VSM and JSM. Furthermore, the results from the other empirical studies show that 
ConPOS
 provides 11%-107%, 8%-64%, and 15%-170% higher precision, recall, and 
mean average precision
 (MAP) than VSM and JSM.
Conclusion
We showed that 
ConPos
 outperforms existing IR-based RT approaches that discard some POS tags from the input documents.",Information and Software Technology,18 Mar 2025,9.0,"The proposed ConPos method shows significant improvement in trace links recovery compared to existing approaches, which can have a positive impact on early-stage ventures by enhancing requirement traceability."
https://www.sciencedirect.com/science/article/pii/S0950584918302052,Automatically identifying code features for software defect prediction: Using AST N-grams,February 2019,Not Found,Thomas=Shippey: t.shippey@herts.ac.uk; David=Bowes: dbowes@uclan.ac.uk; Tracy=Hall: t.hall3@lancaster.ac.uk,"Abstract
Context:
 Identifying defects in code early is important. A wide range of static 
code metrics
 have been evaluated as potential defect indicators. Most of these metrics offer only high level insights and focus on particular pre-selected features of the code. None of the currently used metrics clearly performs best in 
defect prediction
.
Objective:
 We use 
Abstract Syntax Tree
 (AST) n-grams to identify features of defective Java code that improve 
defect prediction
 performance.
Method:
 Our approach is bottom-up and does not rely on pre-selecting any specific features of code. We use non-parametric testing to determine relationships between AST n-grams and faults in both open source and commercial systems. We build defect prediction models using three 
machine learning techniques
.
Results:
 We show that AST n-grams are very significantly related to faults in some systems, with very large 
effect sizes
. The occurrence of some frequently occurring AST n-grams in a method can mean that the method is up to three times more likely to contain a fault. AST n-grams can have a large effect on the performance of defect prediction models.
Conclusions:
 We suggest that AST n-grams offer developers a promising approach to identifying potentially defective code.",Information and Software Technology,18 Mar 2025,8.0,The use of AST n-grams to identify features of defective code and improve defect prediction models can be valuable for startups to identify and address code defects early in the development process.
https://www.sciencedirect.com/science/article/pii/S0950584918302076,Software defect number prediction: Unsupervised vs supervised methods,February 2019,Not Found,Xiang=Chen: xchencs@ntu.edu.cn; Dun=Zhang: dunnzhang0@gmail.com; Yingquan=Zhao: enockchao@gmail.com; Zhanqi=Cui: czq@bistu.edu.cn; Chao=Ni: jacknichao920209@gmail.com,"Abstract
Context: 
Software defect
 number prediction (SDNP) can rank the program modules according to the prediction results and is helpful for the optimization of testing 
resource allocation
.
Objective: In previous studies, supervised methods vs 
unsupervised methods
 is an active issue for just-in-time 
defect prediction
 and file-level 
defect prediction
 based on effort-aware 
performance measures
. However, this issue has not been investigated for SDNP. To the best of our knowledge, we are the first to make a thorough comparison for these two different types of methods.
Method: In our empirical studies, we consider 7 real open-source projects with 24 versions in total, use 
FPA
 and 
Kendall
 as our effort-aware 
performance measures
, and consider three different performance evaluation scenarios (i.e., within-version scenario, cross-version scenario, and cross-project scenario).
Result: We first identify two 
unsupervised methods
 with best performance. These two methods simply rank modules according to the value of metric LOC and metric RFC from large to small respectively. Then we compare 9 state-of-the-art supervised methods incorporating SMOTEND, which is used for handling 
class imbalance problem
, with the unsupervised method based on LOC metric (i.e., LOC_D method). Final results show that LOC_D method can perform significantly better than or the same as these supervised methods. Later motivated by a recent study conducted by Agrawla and Menzies, we apply differential evolutionary (DE) to optimize parameter value of SMOTEND used by these supervised methods and find that using DE can effectively improve the performance of these supervised methods for SDNP too. Finally, we continue to compare LOC_D with these optimized supervised methods using DE, and LOC_D method still has advantages in the performance, especially in the cross-version and cross-project scenarios.
Conclusion: Based on these results, we suggest that researchers need to use the unsupervised method LOC_D as the 
baseline method
, which is used for comparing their proposed novel methods for SDNP problem in the future.",Information and Software Technology,18 Mar 2025,7.0,The comparison of supervised and unsupervised methods for software defect number prediction provides insights that can be beneficial for startups in optimizing testing resource allocation and improving performance evaluation in software development.
https://www.sciencedirect.com/science/article/pii/S0950584918302106,"Identifying, categorizing and mitigating threats to validity in software engineering secondary studies",February 2019,Not Found,Apostolos=Ampatzoglou: apostolos.ampatzoglou@gmail.com; Stamatia=Bibi: Not Found; Paris=Avgeriou: Not Found; Marijn=Verbeek: Not Found; Alexander=Chatzigeorgiou: Not Found,"Abstract
Context
Secondary studies are vulnerable to threats to validity. Although, mitigating these threats is crucial for the credibility of these studies, we currently lack a systematic approach to identify, categorize and mitigate threats to validity for secondary studies.
Objective
In this paper, we review the corpus of secondary studies, with the aim to identify: (a) the trend of reporting threats to validity, (b) the most common threats to validity and corresponding 
mitigation actions
, and (c) possible categories in which threats to validity can be classified.
Method
To achieve this goal we employ the tertiary study research method that is used for synthesizing knowledge from existing secondary studies. In particular, we 
collected data
 from more than 100 studies, published until December 2016 in top quality 
software engineering
 venues (both journals and conference).
Results
Our results suggest that in recent years, secondary studies are more likely to report their threats to validity. However, the presentation of such threats is rather ad hoc, e.g., the same threat may be presented with a different name, or under a different category. To alleviate this problem, we propose a classification schema for reporting threats to validity and possible 
mitigation actions
. Both the classification of threats and the associated mitigation actions have been validated by an empirical study, i.e., Delphi rounds with experts.
Conclusion
Based on the proposed schema, we provide a checklist, which authors of secondary studies can use for identifying and categorizing threats to validity and corresponding mitigation actions, while readers of secondary studies can use the checklist for assessing the validity of the reported results.",Information and Software Technology,18 Mar 2025,10.0,The systematic approach proposed to identify and mitigate threats to validity in secondary studies can greatly benefit startups by providing a structured way to ensure credibility and accuracy in research findings.
https://www.sciencedirect.com/science/article/pii/S0950584918302192,Heuristics for improving the rigour and relevance of grey literature searches for software engineering research,February 2019,Not Found,Austen=Rainer: austen.rainer@canterbury.ac.nz; Ashley=Williams: ashley.williams@pg.canterbury.ac.nz,"Abstract
Background:
 
Software engineering
 research has a growing interest in grey literature (GL). Aim: To improve the identification of relevant and rigorous GL. Method: We develop and demonstrate heuristics to find more relevant and rigorous GL. The heuristics generate stratified samples of search and post–search datasets using a formally structured set of 
search keywords
. Conclusion: The heuristics require further evaluation. We are developing a tool to implement the heuristics.",Information and Software Technology,18 Mar 2025,6.0,"The development of heuristics to identify relevant and rigorous grey literature can potentially assist startups in accessing valuable information, but further evaluation of the heuristics is needed to determine their effectiveness."
https://www.sciencedirect.com/science/article/pii/S0950584918302209,Challenges and recommended practices for software architecting in global software development,February 2019,Not Found,Outi=Sievi-Korte: outi.sievi-korte@tut.fi; Sarah=Beecham: sarah.beecham@lero.ie; Ita=Richardson: ita.richardson@lero.ie,"Abstract
Context
Global software development
 (GSD), although now a norm in the software industry, carries with it enormous challenges mostly regarding communication and coordination. Aforementioned challenges are highlighted when there is a need to transfer knowledge between sites, particularly when software artifacts assigned to different sites depend on each other. The design of the software architecture and associated 
task dependencies
 play a major role in reducing some of these challenges.
Objective
The current literature does not provide a cohesive picture of how the distributed nature of software development is taken into account during the design phase: what to avoid, and what works in practice. The objective of this paper is to gain an understanding of software architecting in the context of GSD, in order to develop a framework of challenges and solutions that can be applied in both research and practice.
Method
We conducted a systematic literature review (SLR) that synthesises (i) challenges which GSD imposes on 
software architecture design
, and (ii) recommended practices to alleviate these challenges.
Results
We produced a comprehensive set of guidelines for performing 
software architecture design
 in GSD based on 55 selected studies. Our framework comprises nine key challenges with 28 related concerns, and nine recommended practices, with 22 related concerns for software architecture design in GSD. These challenges and practices were mapped to a thematic conceptual model with the following concepts: Organization (Structure and Resources), Ways of Working (Architecture 
Knowledge Management
, Change Management and Quality Management), Design Practices, Modularity and Task Allocation.
Conclusion
The synthesis of findings resulted in a thematic conceptual model of the problem area, a mapping of the key challenges to practices, and a concern framework providing concrete questions to aid the design process in a distributed setting. This is a first step in creating more 
concrete architecture
 design practices and guidelines.",Information and Software Technology,18 Mar 2025,7.0,"The research on software architecture design in a distributed setting is highly relevant for early-stage ventures with global development teams, providing practical guidelines for addressing challenges in GSD."
https://www.sciencedirect.com/science/article/pii/S0950584918301617,What can violations of good practices tell about the relationship between GoF patterns and run-time quality attributes?,January 2019,Not Found,Daniel=Feitosa: d.feitosa@rug.nl; Apostolos=Ampatzoglou: Not Found; Paris=Avgeriou: Not Found; Alexander=Chatzigeorgiou: Not Found; Elisa.Y.=Nakagawa: Not Found,"Abstract
Context
GoF patterns have been extensively studied with respect to the benefit they provide as problem-solving, communication and quality improvement mechanisms. The latter has been mostly investigated through empirical studies, but some aspects of quality (esp. run-time ones) are still under-investigated.
Objective
In this paper, we study if the presence of patterns enforces the conformance to good coding practices. To achieve this goal, we explore the relationship between the presence of GoF 
design patterns
 and violations of good practices related to 
source code
 correctness, performance and security, via 
static analysis
.
Method
Specifically, we exploit 
static analysis
 so as to investigate whether the number of violations of good coding practices identified on classes is related to: (a) their participation in pattern occurrences, (b) the pattern category, (c) the pattern in which they participate, and (d) their role within the pattern occurrence. To answer these questions, we performed a 
case study
 on approximately 13,000 classes retrieved from five open-source projects.
Results
The obtained results suggest that classes not participating in patterns are more probable to violate good coding practices for correctness, performance and security. In a more fine-grained level of analysis, by focusing on specific patterns, we observed that patterns with more complex structure (e.g., Decorator) and pattern roles that are more change-prone (e.g., Subclasses) are more likely to be associated with a higher number of violations (up to 50 times more violations).
Conclusion
This finding implies that investing in a well-thought architecture based on best practices, such as patterns, is often accompanied with cleaner code with fewer violations.",Information and Software Technology,18 Mar 2025,8.0,"The study on the impact of design patterns on coding practices provides valuable insights for startups aiming to build clean and efficient code, which can be crucial for scalability and maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584918301666,An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study,January 2019,Not Found,Ivan=Švogor: isvogor@foi.hr; Ivica=Crnković: Not Found; Neven=Vrček: Not Found,"Abstract
Context:
 Application of 
component based software engineering
 methods to 
heterogeneous computing
 (HC) enables different 
software configurations
 to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task.
Objective:
 We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and 
average execution time
.
Method:
 We validated SCAF through its 
instantiation
 on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six 
software configurations
 to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with 
solution spaces
 ranging from 10
30
 to 30
70
) and benchmark the results against best performing random configurations.
Results:
 The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the 
genetic algorithm
 and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully).
Conclusion:
 SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model 
architectural constraints
 and preferences, acting as a multi–criterion software architecture decision 
support system
. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach.",Information and Software Technology,18 Mar 2025,9.0,"The Software Component Allocation Framework (SCAF) offers a practical solution for optimizing software configurations in heterogeneous computing systems, which can be beneficial for startups looking to enhance performance and efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584918301678,On the impact of code smells on the energy consumption of mobile applications,January 2019,Not Found,Fabio=Palomba: fpalomba@unisa.it; Dario=Di Nucci: Not Found; Annibale=Panichella: Not Found; Andy=Zaidman: Not Found; Andrea=De Lucia: Not Found,"Abstract
Context.
 The demand for green 
software design
 is steadily growing higher especially in the context of mobile devices, where the computation is often limited by battery life. Previous studies found how wrong programming solutions have a strong impact on the energy consumption. 
Objective.
 Despite the efforts spent so far, only a little knowledge on the influence of code smells, 
i.e.,
symptoms of poor design or implementation choices, on the energy consumption of mobile applications is available. 
Method.
 To provide a wider overview on the relationship between smells and energy efficiency, in this paper we conducted a large-scale empirical study on the influence of 9 Android-specific code smells on the energy consumption of 60 
Android
 apps. In particular, we focus our attention on the design flaws that are theoretically supposed to be related to non-functional attributes of 
source code
, such as performance and energy consumption. 
Results.
 The results of the study highlight that methods affected by four code smell types, 
i.e.,Internal Setter, Leaking Thread, Member Ignoring Method
, and 
Slow Loop
, consume up to 87 times more than methods affected by other code smells. Moreover, we found that refactoring these code smells reduces energy consumption in all of the situations. 
Conclusions.
 Based on our findings, we argue that more research aimed at designing automatic refactoring approaches and tools for mobile apps is needed.",Information and Software Technology,18 Mar 2025,8.0,"The investigation on code smells and energy consumption in mobile applications is highly relevant for startups focusing on developing energy-efficient software, offering insights for improving app performance."
https://www.sciencedirect.com/science/article/pii/S095058491830168X,Insights into startup ecosystems through exploration of multi-vocal literature,January 2019,Not Found,Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Pertti=Seppänen: Not Found; Ganesh=Boominathan: Not Found; Markku=Oivo: Not Found; Kari=Liukkunen: Not Found,"Abstract
Context: Successful startup firms have the ability to create jobs and contribute to 
economic welfare
. A suitable ecosystem developed around startups is important to form and support these firms. In this regard, it is crucial to understand the startup ecosystem, particularly from researchers’ and practitioners’ perspectives. However, a systematic literature research on the startup ecosystem is limited. Objective: In this study, our objective was to conduct a multi-vocal literature review and rigorously find existing studies on the startup ecosystem in order to organize and analyze them, know the definitions and major elements of this ecosystem, and determine the roles of such elements in startups’ product development. Method: We conducted a multi-vocal literature review to analyze relevant articles, which are published technical articles, white papers, and Internet articles that focused on the startup ecosystem. Our search generated 18,310 articles, of which 63 were considered primary candidates focusing on the startup ecosystem. Results: From our analysis of primary articles, we found four definitions of a startup ecosystem. These definitions used common terms, such as stakeholders, supporting organization, infrastructure, network, and region. Out of 63 articles, 34 belonged to the opinion type, with contributions in the form of reports, whereas over 50% had full relevance to the startup ecosystem. We identified eight major elements (finance, demography, market, education, 
human capital
, technology, entrepreneur, and support factors) of a startup ecosystem, which directly or indirectly affected startups. Conclusions: This study aims to provide the state of the art on the startup ecosystem through a multi-vocal literature review. The results indicate that current knowledge on the startup ecosystem is mainly shared by non-peer-reviewed literature, thus signifying the need for more systematic and empirical literature on the topic. Our study also provides some recommendations for future work.",Information and Software Technology,18 Mar 2025,6.0,"While the study on the startup ecosystem provides valuable insights, it focuses more on the understanding of the ecosystem rather than direct practical implications for early-stage ventures, hence the slightly lower score."
https://www.sciencedirect.com/science/article/pii/S0950584918301708,Combining Automated GUI Exploration of Android apps with Capture and Replay through Machine Learning,January 2019,Not Found,Domenico=Amalfitano: Not Found; Vincenzo=Riccio: Not Found; Nicola=Amatucci: Not Found; Vincenzo De=Simone: Not Found; Anna Rita=Fasolino: annarita.fasolino@unina.it,"Abstract
Context
Automated GUI Exploration Techniques have been widely adopted in the context of mobile apps for supporting critical engineering tasks such as reverse engineering, testing, and network traffic signature generation. Although several techniques have been proposed in the literature, most of them fail to guarantee the exploration of relevant parts of the applications when GUIs require to be exercised with particular and complex input event sequences. We refer to these GUIs as Gate GUIs and to the sequences required to effectively exercise them as Unlocking GUI Input Event Sequences.
Objective
In this paper, we aim at proposing a GUI exploration approach that exploits the human involvement in the automated process to solve the limitations introduced by Gate GUIs, without requiring the preliminary configuration of the technique or the user involvement for the entire duration of the exploration process.
Method
We propose juGULAR, a Hybrid GUI Exploration Technique combining Automated GUI Exploration with Capture and Replay. Our approach is able to automatically detect Gate GUIs during the app exploration by exploiting a 
Machine Learning approach
 and to unlock them by leveraging input event sequences provided by the user. We implement juGULAR in a modular software architecture that targets the 
Android
 mobile platform. We evaluate the performance of juGULAR by an experiment involving 14 real 
Android
 apps.
Results
The experiment shows that the hybridization introduced by juGULAR allows to improve the exploration capabilities in terms of Covered Activities, Covered Lines of Code, and generated Network Traffic Bytes at a reasonable manual intervention cost. The experimental results also prove that juGULAR is able to outperform the state-of-the-practice tool Monkey.
Conclusion
We conclude that the combination of Automated GUI Exploration approaches with Capture and Replay techniques is promising to achieve a thorough app exploration. Machine Learning approaches aid to pragmatically integrate these two techniques.",Information and Software Technology,18 Mar 2025,8.0,"The juGULAR approach proposed in this abstract addresses the limitations of current GUI exploration techniques in mobile apps, showcasing improved exploration capabilities with reasonable manual intervention cost. This has practical value for early-stage ventures focusing on app development."
https://www.sciencedirect.com/science/article/pii/S0950584917301118,CERSE - Catalog for empirical research in software engineering: A Systematic mapping study,January 2019,Not Found,Jefferson Seide=Molléri: jefferson.molleri@bth.se; Kai=Petersen: Not Found; Emilia=Mendes: Not Found,"Abstract
Context
 Empirical research in 
software engineering
 contributes towards developing 
scientific knowledge
 in this field, which in turn is relevant to inform decision-making in industry. A number of empirical studies have been carried out to date in 
software engineering
, and the need for guidelines for conducting and evaluating such research has been stressed.
Objective:
 The main goal of this mapping study is to identify and summarize the body of knowledge on research guidelines, assessment instruments and knowledge organization systems on how to conduct and evaluate empirical research in software engineering.
Method:
 A 
systematic mapping study
 employing manual search and snowballing techniques was carried out to identify the suitable papers. To build up the catalog, we extracted and categorized information provided by the identified papers.
Results:
 The mapping study comprises a list of 341 methodological papers, classified according to research methods, research phases covered, and type of instrument provided. Later, we derived a brief explanatory review of the instruments provided for each of the research methods.
Conclusion:
 We provide: an aggregated body of knowledge on the state of the art relating to guidelines, assessment instruments and knowledge organization systems for carrying out empirical software engineering research; an exemplary 
usage scenario
 that can be used to guide those carrying out such studies is also provided. Finally, we discuss the catalog’s implications for research practice and the needs for further research.",Information and Software Technology,18 Mar 2025,7.0,"This mapping study provides a comprehensive overview of research guidelines in software engineering, which can be valuable for startups looking to inform decision-making based on scientific knowledge. The implications and usage scenario provided can guide early-stage ventures in conducting empirical research."
https://www.sciencedirect.com/science/article/pii/S095058491830185X,A first look at unfollowing behavior on GitHub,January 2019,Not Found,Jing=Jiang: jiangjing@buaa.edu.cn; David=Lo: davidlo@smu.edu.sg; Yun=Yang: ayonel@qq.com; Jianfeng=Li: powerfaster@163.com; Li=Zhang: lily@buaa.edu.cn,"Abstract
Context
Many 
open source software projects
 rely on contributors to fix bugs and contribute new features. On GitHub, developers often broadcast their activities to followers, which may entice followers to be project contributors. It is important to understand unfollowing behavior, maintain current followers, and attract some followers to become contributors in OSS projects.
Objective
Our objective in this paper is to provide a comprehensive analysis of unfollowing behavior on GitHub.
Method
To the best of our knowledge, we present a first look at unfollowing behavior on GitHub. We collect a dataset containing 701,364 developers and their 4,602,440 following relationships in March 2016. We also crawl their following relationships in May 2013, August 2015 and November 2015. We conduct surveys, define potential impact factors, and analyze the correlation of factors with the likelihood of unfollowing behavior.
Results
Our main observations are: (1) Between May 2013 and August 2015, 19.8% of active developers ever unfollowed some users. (2) Developers are more likely to unfollow those who have fewer activities, lower programming language similarity, and asymmetric relationships.
Conclusion
Our results give suggestions for developers to reduce the likelihood of being unfollowed by their followers, and attract researchers’ attention on relationship dissolution.",Information and Software Technology,18 Mar 2025,5.0,"Analyzing unfollowing behavior on GitHub may not directly impact European early-stage ventures in the startup domain. While the study provides insights, the practical value for startups in Europe is relatively lower compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918301848,Software product line evolution: A systematic literature review,January 2019,Not Found,Maíra=Marques: mmarques@dcc.uchile.cl; Jocelyn=Simmonds: jsimmond@dcc.uchile.cl; Pedro O.=Rossel: prossel@ucsc.cl; María Cecilia=Bastarrica: cecilia@dcc.uchile.cl,"Abstract
Context:
 Software Product Lines (SPL) evolve when there are changes in the requirements, product structure or the technology being used. Different approaches have been proposed for managing SPL assets and some also address how evolution affects these assets. Existing mapping studies have focused on specific aspects of SPL evolution, but there is no cohesive body of work that gives an overview of the area as a whole.
Objective:
 The goals of this work are to review the characteristics of the approaches reported as supporting SPL evolution, and to synthesize the evidence provided by primary studies about the nature of their processes, as well as how they are reported and validated.
Method:
 We conducted a systematic literature review, considering six research questions formulated to evaluate evolution approaches for SPL. We considered journal, conference and workshop papers published up until March 2017 in leading digital libraries for computer science.
Results:
 After a thorough analysis of the papers retrieved from the digital libraries, we ended up with a set of 60 primary studies. Feature models are widely used to represent SPLs, so feature evolution is frequently addressed. Other assets are less frequently addressed. The area has matured over time: papers presenting more rigorous work are becoming more common. The processes used to support SPL evolution are systematic, but with a low level of automation.
Conclusions:
 Our research shows that there is no consensus about SPL formalization, what assets can evolve, nor how and when these evolve. 
Case studies
 are quite popular, but few industrial-sized case studies are publicly available. Also, few of the proposed techniques offer tool support. We believe that the SPL community needs to work together to improve the state of the art, creating methods and tools that support SPL evolution in a more comparable manner.",Information and Software Technology,18 Mar 2025,6.0,This systematic literature review on managing Software Product Lines (SPL) evolution offers insights but does not directly provide immediate practical value for European early-stage ventures. The need for more tool support and collaboration within the SPL community may have long-term benefits for startups.
https://www.sciencedirect.com/science/article/pii/S0950584916302178,Integration of feature models: A systematic mapping study,January 2019,Not Found,Vinicius=Bischoff: viniciusbischof@edu.unisinos.br; Kleinner=Farias: kleinnerfarias@unisinos.br; Lucian José=Gonçales: lucianj@edu.unisinos.br; Jorge Luis=Victória Barbosa: jbarbosa@unisinos.br,"Abstract
Context
The integration of feature models has been widely investigated in the last decades, given its 
pivotal role
 for supporting the evolution of software product lines. Unfortunately, academia and industry have overlooked the production of a thematic analysis of the current literature. Hence, a thorough understanding of the state-of-the-art works remains still limited.
Objective
This study seeks to create a panoramic view of the current literature to pinpoint gaps and supply insights of this research field.
Method
A 
systematic mapping study
 was performed based on well-established empirical guidelines for answering six research questions. In total, 47 primary studies were selected by applying a filtering process from a sample of 2874 studies.
Results
The main results obtained are: (1) most studies use a generic notation (68.09%, 32/47) for representing feature models; (2) only one study (2%, 1/47) compares feature models based on their 
syntactic
 and semantics; (3) there is no preponderant use of a particular integration technique in the selected studies; (4) most studies (70%, 33/47) provide a product-based strategy to evaluate the integrated feature models; (5) majority (70%, 33/47) automates the integration process; and (6) most studies (90%, 42/47) propose techniques, rather than focusing on producing practical knowledge derived from empirical studies.
Conclusion
The results were encouraging and suggest that integration of feature models is still an evolving research area. This study provides insightful information for the definition of a more ambitious 
research agenda
. Lastly, empirical studies exploring the required effort to apply the current integration techniques in real-world settings are highly recommended in future work.",Information and Software Technology,18 Mar 2025,7.0,"The systematic mapping study on the integration of feature models provides valuable insights for understanding the current literature in this research field. The study can help startups pinpoint gaps and define a research agenda, contributing to their understanding of feature model integration techniques."
https://www.sciencedirect.com/science/article/pii/S0950584918301885,Empirical research on concurrent software testing: A systematic mapping study,January 2019,Not Found,Silvana M.=Melo: morita@icmc.usp.br; Jeffrey C.=Carver: Not Found; Paulo S.L.=Souza: Not Found; Simone R.S.=Souza: Not Found,"Abstract
Background:
 
Concurrent software
 testing is a costly and difficult task, especially due to the exponential increase in the test sequences caused by non-determinism. Such an issue has motivated researchers to develop testing techniques that select a subset of the input domain that has a high probability of revealing faults. Academics and industrial practitioners rarely use most concurrent software testing techniques because of the lack of data about their applicability. Empirical evidence can provide an important scientific basis for the strengths and weaknesses of each technique to help researchers and practitioners choose concurrent testing techniques appropriate for their environments.
Aim:
 This paper gathers and synthesizes empirical research on concurrent software testing to characterize the field and the types of empirical studies performed.
Method:
 We performed a 
systematic mapping study
 to identify and analyze empirical research on concurrent software testing techniques. We provide a detailed analysis of the studies and their design choices.
Results:
 The primary findings are: (1) there is a general lack of empirical validation of concurrent software testing techniques, (2) the type of evaluation method varies with the type of technique, (3) there are some key challenges to empirical study design in concurrent software testing, and (4) there is a dearth of controlled experiments in concurrent software testing.
Conclusions:
 There is little empirical evidence available about some specific concurrent testing techniques like model-based testing and formal testing. Overall, researchers need to perform more empirical work, especially real-world 
case studies
 and controlled experiments, to validate properties of concurrent software testing techniques. In addition, researchers need to perform more analyses and synthesis of the existing evidence. This paper is a first step in that direction.",Information and Software Technology,18 Mar 2025,6.0,"The study on concurrent software testing techniques and the need for more empirical evidence can provide valuable insights for European early-stage ventures developing software products, but the practical applicability may vary depending on the specific startup's needs."
https://www.sciencedirect.com/science/article/pii/S0950584918301897,A new algorithm for software clustering considering the knowledge of dependency between artifacts in the source code,January 2019,Not Found,Sina=Mohammadi: Not Found; Habib=Izadkhah: izadkhah@tabrizu.ac.ir,"Abstract
Context:
 Software systems evolve over time to meet the new requirements of users. These new requirements, usually, are not reflected in the original documents of these software systems. Therefore, the new version of a software system deviates from the original and documented architecture. This way, it will be more difficult to understand it after a while and it will be difficult to make new changes conveniently. 
Clustering techniques
 are used to extract the architecture of a software system in order to understand it. An artifact 
dependency graph
 (ADG) is often used for clustering, which is extracted from a source code. In the literature, some hierarchical and search-based 
clustering methods
 have been presented to extract the software architecture. Hierarchical algorithms have reasonable search time; however, they are not able to find a good architecture. In contrast, search-based algorithms are often better in this regard; however, their time and space limitations make them useless in practice for large-scale software systems. Both hierarchical and search-based 
clustering methods
 overlook the existing knowledge in an ADG for clustering.
Objective:
 To overcome the limitations of the existing clustering methods, this paper presents a new deterministic 
clustering algorithm
 named Neighborhood tree algorithm.
Method:
 The new algorithm creates a neighborhood tree using available knowledge in an ADG and uses this tree for clustering.
Results:
 Our initial results indicate that the algorithm is better able to extract an acceptable architecture in a reasonable time, compared with hierarchical and search-based algorithms.
Conclusions:
 The proposed 
clustering algorithm
 is expected to greatly assist software engineers in extracting meaningful and understandable subsystems from a source code.",Information and Software Technology,18 Mar 2025,8.0,"The development of a new deterministic clustering algorithm that overcomes limitations of existing methods can have a significant impact on software engineering practices for European early-stage ventures, providing them with a more efficient way to extract software architecture. This innovation can greatly benefit startups seeking to improve their software systems."
