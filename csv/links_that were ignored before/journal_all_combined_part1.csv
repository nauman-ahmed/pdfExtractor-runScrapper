link,title,published_year,keywords,author_email,abstract,publication_title,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S2352673424000374,Gender effects and firm financial performance: A SUMAD meta-analysis of social responsibility and family-to-work conflict,November 2024,Not Found,Mark=Geiger: geigerm1@duq.edu,"Abstract
The current study uses 
SUMAD
 meta-analytic methods (Oh, 2020) to examine 
gender differences
 in social responsibility and family-to-work conflict. Synthesizing evidence from across social science literature, the results of this study provide an evidence-based foundation to support more theorizing and practical discourse regarding gender effects in entrepreneurship. As explained by theories of socialization and social roles, 
gender differences
 in (a) socially responsible attitudes and behaviors and (b) the balance between family and work responsibilities, are likely two of the more pervasive gender effects that influence entrepreneurial careers. The goal of this study is to motivate more research and practical discussion on these and related gender effects to improve our understanding of entrepreneurship phenomena. Using firm performance as an example, the results of the 
SUMAD
 meta-analysis suggest that gender effects related to social responsibility and family-to-work conflict have significant consequences for entrepreneurship outcomes. Based on the evidence and theory rooted in socialization and social roles, the current study calls for more theorizing and primary-level studies on these and related gender effects in entrepreneurship research.
Comment to Readers:
 
Does gender matter? Of course it does (depending on the issue).
 A simple search of “does gender matter” reveals ample discussion on this topic across a variety of gender issues. In this article I highlight 
gender
 regarding differences between women and men in social responsibility and family-to-work conflict. As the evidence suggests, gender does indeed matter as women – on average – are more socially responsible and have more family-to-work conflict than their men counterparts. The results of this study show that greater social responsibility is tied to better business performance whereas greater family-to-work conflict is tied to worse business performance. So, what should we do? First, acknowledge the fact that women and men are different in the contexts of social issues and family matters to clear the way for constructive discourse about these 
gender differences
. Second, embrace that women are higher than men in socially responsible attitudes and behaviors, and that more women in business could inherently result in more socially responsible business practices. Moreover, while this is a 
societal win
 in and of itself, the results suggest it could also carry over to improved financial and economic performance. Lastly, focus more on “why” there are differences between women and men regarding family-to-work conflict. Specifically, emphasize both societal-driven influences (e.g., stereotypes; biases) and individual-driven influences (e.g., individual differences; personal preferences). Understanding these influences, which are not mutually exclusive, is key for maximizing the personal and professional well-being of both women and men.",Business Venturing Insights,04 Mar 2025,6,"The abstract discusses significant gender differences in social responsibility and family-to-work conflict, impacting entrepreneurship outcomes. It calls for more research and practical discussion. The potential societal and economic impact of more socially responsible business practices is highlighted."
https://www.sciencedirect.com/science/article/pii/S2352673424000477,Extending behavioral theory of the firm to new ventures: Dispositional optimism as a moderating influence on new product introductions in high-tech ventures,November 2024,Not Found,R. Isil=Yavuz: ryavuz@bryant.edu; Dev K.=Dutta: dev.dutta@unh.edu; Mehmet A.=Soytas: mehmet.soytas@kfupm.edu.sa,"Abstract
Extant literature has typically drawn from the behavioral theory of the firm (BTOF) to examine new product introductions in the context of well-established companies. This paper extends the behavioral theory of the firm to entrepreneurial firms and argues that jointly considering founders' dispositional optimism together with the performance feedback promises to yield a better understanding of new product introductions in new ventures. We analyze a longitudinal dataset on the activities of 344 newly founded high technology ventures in the United States. The key insight of our study is that when BTOF is applied to the context of nascent, entrepreneurial ventures, the personality and dispositional characteristics of the entrepreneur must be considered. Specifically, we find that performance attainment discrepancy leads to new product introductions, but only when the entrepreneur's dispositional optimism level is high.",Business Venturing Insights,04 Mar 2025,8,"The abstract extends the behavioral theory of the firm to entrepreneurial ventures, emphasizing the importance of founders' dispositional optimism for new product introductions. The study provides insights into the impact of performance attainment discrepancy on new ventures, which can be valuable for early-stage startups."
https://www.sciencedirect.com/science/article/pii/S2352673424000519,Uncovering wellbeing: The complex realities of mompreneurs with additional needs children through Lego® Serious Play®,November 2024,"Lego® Serious Play®, Mompreneur, Wellbeing, Additional needs, Disability, Sensitive topics, Hermeneutic constructivist, Social constructionist, Caregiving, Entrepreneur",Regina=Casteleijn-Osorno: racaos@utu.fi,"Abstract
This paper explores the complexities of identifying the wellbeing of mompreneurs (mother-entrepreneurs) who are also caregivers to children with additional needs. A social constructionist perspective, Lego® Serious Play® was employed in individual interviews to uncover their complex wellbeing realities while pursuing entrepreneurship. A hermeneutic constructivist lens was applied to further conceptualize the language behind their experiences. The findings present three dimensions of wellbeing: 1) Internal conflict and self-neglect 2) Empowerment, Independence, Fulfilment 3) Resilient Control: Keeping the Balance. These dimensions provide an idiographic understanding, contributing to the broader knowledge of wellbeing as a component of entrepreneurship alongside caregiving responsibilities.",Business Venturing Insights,04 Mar 2025,4,"The abstract explores the wellbeing of mompreneurs caring for children with additional needs, using a social constructionist perspective. While the findings contribute to understanding wellbeing in entrepreneurship, the practical implications for European early-stage ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S2352673424000520,How sector fluidity (knowledge-intensiveness and innovation) shapes startups’ resilience during crises,November 2024,"Resilience, Value for customers, Digital transformation, Startups, Business models, COVID-19",Asif=Tanveer: muhammadasif.tanveer@hdr.qut.edu.au; Rui=Torres de Oliveira: rui.torresdeoliveira@deakin.edu.au; Shaheer=Rizvi: shaheer.rizvi@iub.edu.pk,"Abstract
In the context of the crises, this study sheds light on the varying impact of crises on startups in different sectors and outlines the specific resilience practices utilized in response, recovery, and growth phases. The research team conducted qualitative analysis on 18 public discussion interviews featuring key stakeholders in the Indian startup ecosystem, which included 51 chief executive officers, founders, cofounders, and venture capitalists who reflected on COVID-19. The study found that high-fluidity sectors (highly knowledge-intensive and innovative) leverage their agility through resourcefulness and customer value creation. In contrast, low-fluidity sectors (low in knowledge intensiveness) primarily focus on operational adjustments. As the transition goes from response to recovery, high-fluidity sectors prioritize digital transformation and strategic shifts, while low-fluidity sectors continue to cope. In the growth phase, high-fluidity startups exhibit growth aspirations, while low-fluidity ones emphasize business model innovation. This research provides valuable sector-specific insights into resilience and highlights the evolution of these strategies throughout a crisis, thereby enhancing our understanding of startup resilience in the context of the crisis.",Business Venturing Insights,04 Mar 2025,9,"The abstract examines the impact of crises on startups in different sectors, highlighting specific resilience practices. The sector-specific insights and strategies for response, recovery, and growth phases can be highly valuable for European early-stage ventures facing challenging circumstances."
https://www.sciencedirect.com/science/article/pii/S2352673424000507,Entrepreneurial finance and sustainability: Do institutional investors impact the ESG performance of SMEs?,November 2024,"Entrepreneurial finance, Venture capital, Private equity, Sustainability, Environmental, social, and governance (ESG), Corporate social responsibility (CSR)",Wolfgang=Drobetz: Not Found; Sadok=El Ghoul: Not Found; Omrane=Guedhami: Not Found; Jan P.=Hackmann: Not Found; Paul P.=Momtaz: momtaz@tum.de,"Abstract
Institutional investors improve the environmental, social, and governance (ESG) performance of small- and medium-sized enterprises (SMEs). Our difference-in-differences framework shows that the backing from private equity and venture capital funds leads to an increase in SMEs’ externally validated ESG scores compared to their matched non-investor-backed peers. Consistent with “ESG-as-insurance” theory, the ESG performance of SMEs with a higher probability of failure is more likely to benefit from the backing of institutional investors. This positive effect is heterogeneous; while SMEs with high ex-ante ESG performance further improve their ESG performance following institutional investor backing, SMEs with low ex-ante ESG performance are unlikely to implement any improvements. Entrepreneurial finance seems to help sustainable entrepreneurs transform into “sustainability champions,” while neglecting the betterment of non-sustainable SMEs.",Business Venturing Insights,04 Mar 2025,7,"The abstract shows how institutional investors can enhance the ESG performance of SMEs, with a focus on sustainability. The differential impact on SMEs based on their initial ESG performance levels and the transformation into 'sustainability champions' can have implications for European startups seeking growth and sustainability."
https://www.sciencedirect.com/science/article/pii/S2352673424000556,What’s the risk? It depends. Entrepreneurs’ and employees’ perceptions of domestic city-level institutional risk,November 2024,Not Found,Kaitlyn=DeGhetto: kdeghetto1@udayton.edu; Zachary A.=Russell: russellz1@xavier.edu,"Abstract
With a focus on entrepreneurs' decisions related to domestic location choices, this study draws from the international business institutional risk literature to evaluate city-level risk perceptions while accounting for individual-level political views. Specifically, we surveyed entrepreneurs and prospective employees in an effort to evaluate how important 1) safety risk, 2) political risk, and 3) social risk are when considering where to live, work, and start businesses. This process also included a comparison to ease of doing business, a previously studied driver of investment decisions. To identify low (and high) risk domestic investment locations, we had participants rate 25 large U.S. cities on the risk factors. Our findings indicate that entrepreneurs and prospective employees care about the city-level institutional risk factors. However, the focus and perceptions of entrepreneurs and prospective employees are greatly influenced by political views, perceptions do not always mirror objective data, and the two groups weight risk differently. The key insight of our study is that, in order to access and maintain valuable human capital, entrepreneurs should begin considering employees' perceptions related to city-level institutional risk. Importantly, these perceptions are biased by factors such as one's political views. Likewise, to attract business investment, city leaders should consider these risk perceptions.",Business Venturing Insights,04 Mar 2025,7,"The study provides valuable insights into the importance of city-level institutional risk factors for entrepreneurs and prospective employees, emphasizing the influence of political views on risk perceptions, which can impact decision-making for startups."
https://www.sciencedirect.com/science/article/pii/S2352673423000732,Pouring the Paycheck Protection Program into craft beer: PPP employment effects in service-intensive industries,June 2024,Not Found,Aaron J.=Staples: astaples@utk.edu; Kristopher=Deming: Not Found; Trey=Malone: Not Found; Craig W.=Carpenter: Not Found; Stephan=Weiler: Not Found,"Abstract
Small businesses in the food and beverage service 
industry
 are particularly vulnerable to crises such as the COVID-19 pandemic. One of the most salient vulnerabilities was the drastic decline in consumer spending at eating and drinking places, generating unprecedented swings in employment in this service-intensive sector. Governments across the globe implemented rapid response fiscal policies to mitigate these economic damages and improve small business crisis management. One such policy was the Paycheck Protection Program (PPP) in the United States. This study links restricted microdata from the Colorado Quarterly Census of Employment and Wages to microdata on PPP loan recipients to assess whether the loan program effectively reduced unemployment rates in Colorado's craft beer 
industry
. The results of a staggered difference-in-differences framework indicate immediate and longer-term positive and statistically significant effects of the loan program on employment outcomes, with employment effects ranging from 16.8 to 19.5%. These results emphasize the importance of understanding the loan program’s effectiveness among hard-hit industries comprised of small businesses.",Business Venturing Insights,04 Mar 2025,10,"This study is highly impactful as it evaluates the effectiveness of a government response program (PPP) on small businesses in a crisis, particularly in the food and beverage service industry, highlighting the positive employment outcomes for hard-hit businesses, which is crucial for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673424000040,How and why do social entrepreneurs experience goal conflict differently?,June 2024,Not Found,Rebecca=Pieniazek: R.Pieniazek@leeds.ac.uk; Kerrie L.=Unsworth: Not Found; Hannah=Dean: Not Found,"Abstract
It is well-known that the need for both social and financial missions creates tension within social enterprises. Less well-known are the specifics around how and why social entrepreneurs themselves construct and experience their situation. Given people vary in their psychological representations of their goals from concrete (i.e., tasks) to more abstract (i.e., values), we anticipated that goal conflict with engaging in financial activities could vary along these lines, leading to potentially different solutions for support. Through collecting interviews and focus group data using goal hierarchies from 37 social entrepreneurs, we find six constructed realities with different salient goals at different levels of cognitive abstraction which either dictate, conflict with, or are dissociated from financial activities. These can explain why social entrepreneurs perceive their financial activities differently – financial activities as out of sight out of mind, aversive, a ball to juggle, a necessary evil, part and parcel, and as king - which are associated with four experiences of goal conflict (i.e., goal conflict as continual questioning, inevitable, manageable, and irrelevant).",Business Venturing Insights,04 Mar 2025,6,"The research sheds light on the tension within social enterprises between social and financial missions, providing insights into how social entrepreneurs perceive and construct their financial activities, which could be relevant for startups navigating similar challenges."
https://www.sciencedirect.com/science/article/pii/S2352673424000143,"Upward, downward or steady: How social class experience shapes transnational social venturing",June 2024,"Social class, Social mobility, Venturing, Transnational, Entrepreneur",Nkosana=Mafico: nkosana.mafico@ed.ac.uk; Anna=Krzeminska: Not Found; Charmine=Härtel: Not Found; Josh=Keller: Not Found,"Abstract
Transnational social entrepreneurs leverage their cross-border knowledge and experiences to create and exploit opportunities in multiple markets. However, this knowledge and experience is not homogeneous or equally distributed among them. In this paper, we examine how the social class experiences of 18 transnational social entrepreneurs from the African diaspora living in the West influence their transnational social venturing. We identify four types of Transnational Social Class Experience (TSCE)—Grounded, Elite, Fallen and Elevated—each associated with a different approach to transnational social venturing. Our key contribution is introducing and unpacking the concept of Transnational Social Venturing Advantage (TSVA): the unique benefits that transnational social entrepreneurs can gain when their economic experiences across multiple countries intersect with the varied sociocultural environments they encounter. We also develop a framework that elucidates the connections between TSCE and social venturing approaches through TSVA. Taken together, our study advances the literature on transnational social venturing by unpacking the social class experience dynamics that enable transnational social entrepreneurs to access resources and understand their beneficiaries. It also advocates for a shift beyond a low versus high social class 
dichotomy
 in the broader (transnational) entrepreneurship discourse to a spectrum-based approach that accounts for social class experiences gained across borders.",Business Venturing Insights,04 Mar 2025,9,"This study offers valuable insights into how social class experiences influence transnational social venturing, introducing the concept of Transnational Social Venturing Advantage (TSVA) and providing a framework for understanding the dynamics of social venturing among transnational social entrepreneurs, which can be beneficial for European startups operating across borders."
https://www.sciencedirect.com/science/article/pii/S2352673424000209,Unbinding ideology: The impact of communist indoctrination revocation in polish schools on later life self-employment,June 2024,Not Found,Pankaj C.=Patel: pankaj.patel@villanova.edu,"Abstract
Given communist ideologies discourage individual 
enterprise
, this research investigates whether eliminating compulsory Marxist-Leninist indoctrination from schools influences later life self-employment. Focusing on a mid-1950s reform in Poland that revoked the Communist indoctrination curriculum while holding other aspects constant, the study leverages variation in exposure based on annual 
school enrollment
 cut-off birthdates. Contrary to expectation, the empirical analysis finds no discernible effect of indoctrination removal on later-life self-employment. Additionally, the study examines whether Polish immigrants exposed to reform and arriving in the US after 1960 exhibit increased self-employment propensity, but finds no significant differences. Overall, the study's findings highlight negligible impacts of the revocation of Communist indoctrination in Polish schools on self-employment.",Business Venturing Insights,04 Mar 2025,4,"While the study examines the impact of Communist indoctrination removal on later-life self-employment, the findings indicate negligible effects, which may have limited practical relevance or direct implications for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673424000179,Entrepreneurship after prison: It’s complicated,June 2024,Not Found,Fiona=Robinson: firobinson@mtroyal.ca; Stephanie A.=Fernhaber: sfernhab@butler.edu,"Abstract
Entrepreneurship is increasingly seen as a creative solution for individuals after they have been released from prison given the difficulties they face in finding viable employment. However, considering that entrepreneurship inherently involves maneuvering around and overcoming obstacles, it is likely an even more complicated endeavor for these individuals. A 
thick problem description
 of entrepreneurship after prison is needed to better understand the unique challenges associated with this unconventional entrepreneurial journey. Drawing on the existing literature coupled with semistructured interviews with five individuals who started businesses after being incarcerated, we utilize an empathy mapping tool to explicate our findings. We then outline key insights and offer recommendations on how to move forward.",Business Venturing Insights,04 Mar 2025,7,"The research focuses on a unique and important topic of entrepreneurship after prison, providing valuable insights that can potentially impact policies and support systems for individuals reentering society."
https://www.sciencedirect.com/science/article/pii/S2352673423000355,Bridging worlds: The intersection of religion and entrepreneurship as meaningful heterodoxy,November 2023,"Religion, Entrepreneurship, Heterodoxy, Subfield",Brett=Smith: smithbr2@miamioh.edu; Ali Aslan=Gümüsay: Not Found; David M.=Townsend: Not Found,"Abstract
There is a resurgence in both the advancement and critique of research at the intersection of religion and entrepreneurship. It is precisely because there are important conflicts, tensions, and paradoxes in religion and entrepreneurship that this stream of research is important to the field of entrepreneurship as a source of meaningful heterodoxy. Without grappling with these values and concerns, entrepreneurship scholars are left with an incomplete and possibly emaciated understanding of entrepreneurship. When properly harnessed to build bridges across social divides, the intersection of religion and entrepreneurship is an important source of fresh, new, and heterodox insights into the processes through which entrepreneurs strive to transform organizations and society through entrepreneurial action and an emerging subfield of entrepreneurship research.",Business Venturing Insights,04 Mar 2025,5,"While the intersection of religion and entrepreneurship is an interesting concept, the practical implications for early-stage ventures in Europe may be limited, resulting in a moderate score."
https://www.sciencedirect.com/science/article/pii/S2352673423000379,Startup grants and the development of academic startup projects during funding: Quasi-experimental evidence from the German ‘EXIST – Business startup grant’,November 2023,Not Found,Christoph E.=Mueller: chr.mueller@fz-juelich.de,"Abstract
Public support programs for (academic) startups are an important component of innovation and technology policy. There are many types of startup policy instrument, one of which is supporting prospective founders with startup grants. The present study contributes to the literature by examining the effectiveness of a specific startup grant entitled ‘EXIST – Business Startup Grant’. This measure is Germany’s largest public startup support program and aims to increase the number of technology-oriented and knowledge-based academic startups. The present research investigates the extent to which products of funded startups, the projects’ business planning, the founders’ skills, the degree of networking, and the uptake of external funding evolve over the duration of the grant. Evidence of the effects on these variables is generated by means of a pipeline-based comparison group design. Findings indicate that the startup grant contributes substantially to the development of the products and the business planning of the funded startups, moderately increases their degree of networking and their uptake of external funding, and slightly improves the skills of the founding team during the funding period. Several robustness checks – including a replication by another type of research design, namely a pre-post comparison – strongly support the findings. Hence it is very likely that the program advances startups in their development and thus contributes to later economic success.",Business Venturing Insights,04 Mar 2025,9,"The study on startup support programs in Germany provides concrete evidence of the effectiveness of a specific grant, offering valuable insights that can directly benefit European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S235267342300046X,Finding the sweet spot: Evaluating the role of structured idea-generation framework in generating high-quality new venture ideas,November 2023,Not Found,Puspa=Shah: shahp@uwosh.edu; Nischal=Thapa: thapan@uwgb.edu,"Abstract
Previous research has predominantly focused on cognitive antecedents related to the quality of new venture ideas (NVIs), resulting in a notable gap in understanding the influence of structural elements on NVIs. Drawing upon activation theory, we examine how the structure of the idea-generation framework, a dimension of routinization, influences NVI outcomes. To investigate this relationship, we conduct two separate studies: the first involving a student sample and the second involving practicing entrepreneurs. Our findings demonstrate an inverse U-shaped relationship between the structure in the idea-generation framework and the quality/quantity of NVIs. These findings contribute to the understanding of the antecedents that shape NVIs.",Business Venturing Insights,04 Mar 2025,6,The research on the influence of structural elements on new venture ideas contributes to theoretical knowledge but may have limited immediate practical implications for European early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S2352673423000549,Students' assumptions of Entrepreneurs’ performance: The paradox of excess entry and missed opportunity,November 2023,Not Found,Kaushik=Gala: kgala@iastate.edu; Carlos D.=Valladares: Not Found; Brandon A.=Mueller: Not Found,"Abstract
Most variables in entrepreneurship are not distributed normally. Instead, they are characterized by positive skew and heavy 
tails
 featuring influential outliers. Yet, this fundamental asymmetry in entrepreneurial endeavors is rarely discussed in 
entrepreneurship education
, which often oscillates between highlighting everyday entrepreneurs and high-growth ‘unicorn’ startups while overlooking the distributional context for these extremes. Therefore, this paper explores whether students accurately comprehend the non-normality that pervades entrepreneurship. We conducted two studies wherein undergraduate business students at a large, public university in the Midwest US estimated entrepreneurial performance. We elicited students' estimates of the range of performance exhibited by entrepreneurs using a real-world vignette and performance data for an online learning platform. By providing empirical evidence that students may carry largely 
in
accurate assumptions of performance distributions, this paper highlights the paradoxical risks of excess entrepreneurial entry on the one hand and missed opportunity on the other.",Business Venturing Insights,04 Mar 2025,4,"While the discussion on non-normality in entrepreneurship is intriguing, the practical impact on European early-stage ventures may be limited, resulting in a lower score."
https://www.sciencedirect.com/science/article/pii/S2352673423000616,Health resourcefulness behaviors: Implications of work-health resource trade-offs for the self-employed,November 2023,Not Found,Timothy L.=Michaelis: tmichaelis@niu.edu; Jon C.=Carr: jccarr@ncsu.edu; Alexander=McKelvie: mckelvie@syr.edu; April=Spivack: april.spivack@hanken.fi; Michael P.=Lerman: mlerman@iastate.edu,"Abstract
In this paper, we present an exploratory study to investigate why those who are self-employed in the United States may make more personal health-related trade-offs than adults working in traditional wage-employment jobs. A random sample of 10,663 working adults in the United States indicate that the self-employed engage in higher amounts of health-related resourcefulness behaviors than wage employees (e.g., skipping medication to save money). We find that concerns over healthcare access and 
affordability
 serve as antecedents to health resourcefulness behaviors among all working adults, but that age moderates these relationships differently for the self-employed. Specifically, younger self-employed adults engage in health resourcefulness behaviors due to healthcare 
affordability
 concerns while older self-employed adults engage in such behaviors due to healthcare 
access
 concerns. In sum, we contribute to the entrepreneurial resourcefulness literature and well-being literature by highlighting data on how the self-employed make trade-offs with their personal health resources. We offer multiple directions for theory development, future research, and implications for healthcare policy.",Business Venturing Insights,04 Mar 2025,7,"This study provides valuable insights into the health resourcefulness behaviors of self-employed individuals, which can impact early-stage ventures by highlighting the trade-offs with personal health resources."
https://www.sciencedirect.com/science/article/pii/S2352673423000367,The weaker sex? A tale of means and tails,November 2023,Not Found,Indu=Khurana: ikhurana@hsc.edu; Jagannadha Pawan=Tamvada: jp.tamvada@soton.ac.uk; David B.=Audretsch: daudrets@indiana.edu,"Abstract
One of the most commonly held beliefs prevalent in entrepreneurship research is that women-led ventures tend to generate lower earnings than men-led ventures. We contend that this thinking emanates from empirical analyses that obscure the variation in entrepreneurial performance across the earnings distribution. Relying solely on the mean as a measure of central tendency conceals the heterogeneity among so-called underperformers. Using density plots from a nationally representative database, we demonstrate that women-led ventures perform better at some quantiles of the earnings distribution, contrary to the common myth that men-led ventures consistently outperform them. Our study debunks this myth and contributes to entrepreneurship research that adopts a gendered perspective by showing that the reality experienced by women entrepreneurs is not as dismal as it appears when compared to focusing exclusively on the mean. Our study has implications for policymakers, who need to adjust their policy approach by designing targeted policies explicitly incorporating the heterogeneity inherent in entrepreneurship.",Business Venturing Insights,04 Mar 2025,8,"By debunking the myth of women-led ventures underperforming, this research contributes to entrepreneurship research with a gendered perspective, offering implications for policymakers and potentially influencing early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673423000215,The inefficiencies of venture capital funding,June 2023,Not Found,Chris=Welter: chriswelter@miamioh.edu; Tim R.=Holcomb: holcomtr@miamioh.edu; John=McIlwraith: john@allosventures.com,"Abstract
Despite plentiful research into venture capital (VC), the process of obtaining funding still frustrates both VC firms and startups. While uncertainty is necessary for the returns that VC investors desire, there are inefficiencies in the process that stem from 
information asymmetry
. We articulate those inefficiencies from both the founder and the VC perspective and offer some pathways toward reducing those inefficiencies to drive better outcomes for startups, VCs, and society as a whole.",Business Venturing Insights,04 Mar 2025,6,"While addressing inefficiencies in the process of obtaining funding for startups, this study may have a moderate impact on early-stage ventures by proposing pathways for better outcomes for entrepreneurs and VC firms."
https://www.sciencedirect.com/science/article/pii/S2352673422000592,A translational framework for entrepreneurship research,June 2023,"Translational research, Entrepreneurship research, Scholarly impact, Design science, Engaged scholarship",Pablo=Muñoz: pablo.munoz-roman@durham.ac.uk; Dimo=Dimov: dpd24@bath.ac.uk,"Abstract
In this paper, we put forward a new translational research framework for entrepreneurship, which leverages translational research from biomedical science and design science to lay the ground for a new research ecosystem of entrepreneurship. Instead of describing, explaining and predicting, our framework places emphasis on framing, experimenting and interacting. It comprises three modes of translational research, which allow for moving discoveries made in basic entrepreneurship research to entrepreneurial practice (T1), entrepreneurial communities (T2) and entrepreneurship policy (T3). These are alternative modes of research, marking different scientific domains, that can ensure the outcomes of our basic science are understood, adapted to and adopted by stakeholders in the best way possible. This new ecosystem can expand our scope of action as entrepreneurship researchers, open new pathways to materialize the elusive “scholarly impact” and advance the conversation and practice of engaged scholarship.",Business Venturing Insights,04 Mar 2025,9,"Introducing a new translational research framework for entrepreneurship can significantly impact early-stage ventures by expanding the scope of action for entrepreneurship researchers, advancing scholarly impact, and enhancing entrepreneurial practice, communities, and policy."
https://www.sciencedirect.com/science/article/pii/S2352673422000671,Whistleblowing in entrepreneurial ventures,June 2023,Not Found,Daniel R.=Clark: dclark@ivey.ca; Bradley R.=Skousen: bradleyskousen@gmail.com,"Abstract
Despite the occurrence of high-profile whistleblowing events in entrepreneurial firms (e.g., We Work, Uber, and Theranos), there is a dearth of understanding of when and why whistleblowing occurs 
outside
 the domain of traditional large firms. Indeed, we argue that new ventures represent a unique and meaningful heterodoxy as entrepreneurs, rebels with a cause, inspire others to join their cause who will ultimately betray the entrepreneur to protect that cause. We test and find support for our hypotheses regarding firm size and new venture status on whistleblowing on a unique dataset of 3113 reported frauds. From these findings we inductively theorize a new model of 
whistleblower
 motivation driven by the unique context of entrepreneurs drawing people to join a cause, and the price when fidelity is not maintained.",Business Venturing Insights,04 Mar 2025,5,"Exploring whistleblowing in entrepreneurial firms, this study offers insights on firm size and new venture status on whistleblowing behavior, which may have limited direct impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673423000070,You take after your father: Paternal grit and young adult self-employment,June 2023,Not Found,Pankaj C.=Patel: pankaj.patel@villanova.edu; Marcus T.=Wolfe: Marcus.Wolfe@unt.edu; Ryan C.=Bailey: Not Found,"Abstract
Considerable scholarly attention has been given to the influence parents have on the self-employment of their children. We further investigate this relationship by examining whether paternal grit, via a young adult's grit, could influence young adult self-employment. Using a sample of 1504 participants from the Cultural Pathways to Economic Self-Sufficiency and Entrepreneurship (CUPESSE) survey, our findings indicate that paternal grit is positively associated with young adult grit, and that paternal grit has a positive indirect relationship with young adult self-employment, as mediated by young adult grit. Our study has important implications to ongoing conversations regarding the generational transmission aspects of self-employment, as well as the influence that grit can have on the entrepreneurial process.",Business Venturing Insights,04 Mar 2025,7,"The study provides insights into the generational transmission aspects of self-employment and the influence of grit on the entrepreneurial process, which can be beneficial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673423000227,Keep on keeping on: A psychological approach to entrepreneurial persistence,June 2023,Not Found,Alan D.=Boss: adboss@ualr.edu; Jiaju=Yan: Justin_Yan@baylor.edu; Rhonda K.=Reger: Rhonda.Reger@unt.edu,"Abstract
Persistence typifies the behavior of most successful entrepreneurs. Yet systematic theory about entrepreneurial persistence is lacking. This paper theorizes about psychological differences that lead some entrepreneurs to persist appropriately while others quit too soon or persist excessively. Building on self-regulation literature, we develop a theory of entrepreneurial persistence, called entrepreneurial psychological resource approach. Our paper makes two primary contributions to guide future research: a research model expanding upon the resource-based perspective in entrepreneurship research, and an evaluative component relating psychological resources to entrepreneurial persistence.",Business Venturing Insights,04 Mar 2025,8,"The theoretical framework developed in this paper can guide future research on entrepreneurial persistence, offering valuable insights for startups looking to understand psychological differences affecting persistence."
https://www.sciencedirect.com/science/article/pii/S2352673422000439,Entrepreneurial miasma: Organizational miasma as a theoretical lens for increasing the odds of venture survival after the founder exits,November 2022,Not Found,James J.=Hoffman: jhoffman@nmsu.edu; Michaela=Driver: mdriver@nmsu.edu,"Abstract
The present study offers new insights on ventures undergoing founder exit. Specifically, it explores miasma as one potentially negative outcome. Miasma, a concept adapted from the organizational literature, refers to a state of contagion or pollution that affects all members of an organization causing potentially irreparable damage. This study develops a model of miasma in venture contexts when founders exit, a term we refer to as entrepreneurial miasma. This model includes the antecedents, moderating and mediating variables and outcomes of miasma. The purpose of this model is to develop insights into how miasma may be prevented and how ventures may work through it once it has occurred. Specifically, the study offers guidance for new management leading ventures on how to best understand, reestablish and build relationships with employees who are struggling with the exit of the founder to protect employee productivity and firm performance. The study also contributes to the organizational miasma literature by strengthening and clarifying the construct and its implications in both organizational and venture contexts.",Business Venturing Insights,04 Mar 2025,6,"The study on miasma in venture contexts offers guidance on preventing negative outcomes during founder exits, which can be relevant for early-stage ventures to maintain employee productivity and firm performance."
https://www.sciencedirect.com/science/article/pii/S235267342200049X,Linking passion to performance in the social commerce community: The role of collaborative information exchange,November 2022,"Entrepreneurial passion, Collaborative information exchange, Social commerce, Digital entrepreneurship",Yiwen=Chen: yiwenchen@sfsu.edu; Li=Chen: lchen28@suffolk.edu; Robert=Smith: rssmith@suffolk.edu,"Abstract
The proliferation of 
social commerce
 has enabled millions of passionate entrepreneurs to launch businesses. Yet, literature is sparse regarding whether and how passionate entrepreneurs succeed in this new context. Through an in-depth analysis of the social commerce community, this study articulates the role of 
collaborative information exchange
 as one behavioral mechanism through which entrepreneurial passion translates into performance. Using both primary survey data and secondary store traffic and trade volume data from a social commerce website, the authors find that collaborative information exchange partially mediates the effect of passion on entrepreneurs’ business satisfaction (subjective performance) and fully mediates the effect of passion on store traffic and trade volume (objective performance). This study deepens our understanding of entrepreneurial passion by identifying an important mechanism that is specific to the digital entrepreneurship domain and provides practical implications to both individual entrepreneurs and social commerce platforms.",Business Venturing Insights,04 Mar 2025,9,"The research on the role of collaborative information exchange in social commerce provides practical implications for entrepreneurial passion and performance in the digital entrepreneurship domain, which can benefit startups in this space."
https://www.sciencedirect.com/science/article/pii/S235267342100072X,Psychological well-being of hybrid entrepreneurs,June 2022,Not Found,Retno=Ardianti: retnoa@petra.ac.id; Martin=Obschonka: martin.obschonka@qut.edu.au; Per=Davidsson: Not Found,"Abstract
Although the phenomenon of hybrid entrepreneurs—individuals who work in paid and self-employment simultaneously—is prevalent, the psychological well-being of hybrid entrepreneurs has not been researched systematically to date. This is unlike research on paid employment and (assumed) full-time entrepreneurship, where psychological well-being has been researched as a key factor. Using data from the United Kingdom Household Longitudinal Survey, we address this void by studying whether hybrid entrepreneurs display distinct psychological well-being patterns (measured via mental strain, job satisfaction, and life satisfaction), utilizing a comparison with full-time paid employed, full-time self-employed and individuals working in two paid jobs. We further examine whether the specific work arrangements of hybrid entrepreneurs shape their well-being. To this end, we study the changes in well-being of hybrid entrepreneurs and other individuals in the comparison groups who switch to other jobs. For this purpose, we employed matching (entropy balancing approach) to account for self-selection effects. Our results suggest that the well-being of hybrid entrepreneurs is indeed distinct and can be explained by both self-selection effects and unique aspects of their work arrangements. Our study is thus the first to deliver evidence showing that hybrid entrepreneurs need to be studied as a separate group in entrepreneurship research concerned with well-being and psychological functioning. Our results have important implications not only for future research but also for practice.",Business Venturing Insights,04 Mar 2025,6,The study on psychological well-being of hybrid entrepreneurs fills a void in research and offers insights that can be valuable for early-stage ventures looking to understand the unique well-being patterns of this group.
https://www.sciencedirect.com/science/article/pii/S2352673421000767,Lassie shrugged: The premise and importance of considering non-human entrepreneurial action,June 2022,Not Found,Richard A.=Hunt: rickhunt@vt.edu; Daniel A.=Lerner: Daniel.Lerner@ie.edu; Avery=Ortiz-Hunt: avery_ortiz-hun1@baylor.edu,"Abstract
While management and entrepreneurship scholars have displayed comfort in and receptivity towards anthropomorphizing organizations, technologies, and even algorithms, our field has not yet grappled with a mountain of empirical evidence gathered over decades of research in the natural sciences that non-humans may behave entrepreneurially. For reflection and valuable perspective, our study relaxes the central assumption that entrepreneurial behaviors are the exclusive domain of human beings. Doing so invites fresh insights concerning the transversal nature of 
entrepreneurial action
, the biological origins of innovation and entrepreneurship, the categorical assumptions demarcating the field of entrepreneurship, and the persistent emphases on intendedly rational conceptions of 
entrepreneurial action
. The inspiration for our study involves “moving back from the species,” as E.O. Wilson advised. Through this “more distanced view” and by focusing on the reproducible benefits of entrepreneurship rather than narrower, human-centric conceptions of firm formation and profit generation, we find that the consideration of non-human behaviors contributes to the evolving definitions and future study of entrepreneurial action.",Business Venturing Insights,04 Mar 2025,7,"The study presents a fresh perspective on entrepreneurial action by considering non-human behaviors, contributing to evolving definitions and future study of entrepreneurship."
https://www.sciencedirect.com/science/article/pii/S235267342200004X,Informal competition and product innovation decisions of new ventures and incumbents across developing and transitioning countries,June 2022,Not Found,Punyashlok=Dwibedy: punyashlok.dwibedy@ahduni.edu.in,"Abstract
While existing research has found that informal competition positively impacts 
product innovation
 by formal firms in developing and transitioning countries, there is a lack of understanding of how this decision to innovate varies between incumbents and new ventures. Through replication and extension of the work by McCann and Bahl (2017), the article primarily tries to address this significant gap by analyzing the differential impact of informal competition on 
product innovation
 decisions of incumbents and new ventures. Using data from the latest round of the World Bank 
Enterprise
 Surveys, 2019–2020, conducted across multiple transitioning and developing countries and logistic regression as the method of analysis, this study finds that (a) informal competition positively impacts the likelihood of product innovation by formal firms, confirming the results of McCann and Bahl (2017) and (b) new ventures are less likely to engage in product innovation than incumbents when competing with informal firms. The article contributes to the literature by trying to empirically resolve the friction between Attention-Based View of the firm and liability of newness. This paper argues that when facing informal competition, new ventures are less likely to respond through innovation compared to incumbents due to competing resource requirements that limit their attention towards competition from informal firms.",Business Venturing Insights,04 Mar 2025,8,"The study addresses a significant gap in understanding how informal competition impacts product innovation decisions of incumbents and new ventures, contributing to the literature by empirically resolving important frictions."
https://www.sciencedirect.com/science/article/pii/S2352673421000366,What entrepreneurship is really “productive”? An alternative view on Baumol's typology,November 2021,Not Found,Dmitrii=Trubnikov: dtrubnikov@hse.ru,"Abstract
The paper aims to contribute to the discussion on “productive, unproductive, and destructive” entrepreneurship started by William Baumol. It supports the position that all three categories in Baumol's typology are important to understand how institutions affect entrepreneurship but advocates for an alternative approach to distinguish among them. While the traditional demarcation is often based on the net effect on productivity, the proposed approach focuses on the public choice reasoning that not only distinguishes between “rent-seeking” and “profit-seeking,” but also emphasizes that rent-seeking should not be considered a pure transfer. Rent-seeking not only affects regulatory decisions, it also generates activities in the sphere that is traditionally perceived in the productive side. It is important to take this sphere into account when welfare implications of regulatory initiatives are discussed and when policymakers aim to stimulate “productive” entrepreneurship.",Business Venturing Insights,04 Mar 2025,6,The paper contributes to the discussion on different categories of entrepreneurship and proposes an alternative approach to understand the impact of institutions on entrepreneurship.
https://www.sciencedirect.com/science/article/pii/S2352673421000433,The governance of entrepreneurial community ventures: How do conflicting community interests influence opportunity exploitation?,November 2021,Not Found,Helen M.=Haugh: h.haugh@jbs.cam.ac.uk,"Abstract
Participatory governance is upheld as a fundamental organizing principle in community entrepreneurship. This paper brings new insights from a 
case study
 that investigated how governance structures, processes, and practices, and divergent community interests influence community venture opportunity exploitation. We find that while community stakeholder governance enables 
community participation
 and accountability, the interests of the majority triumph over the minority in determining opportunity exploitation. Adopting supplementary governance mechanisms of multistakeholder advisory committees and community engagement process reporting would enable minority interests to be acknowledged and communicated, and increase accountability by acknowledging conflicting views about opportunity exploitation.",Business Venturing Insights,04 Mar 2025,7,"The case study provides insights into how governance structures influence community venture opportunity exploitation, highlighting the importance of supplementary governance mechanisms for better community participation and accountability."
https://www.sciencedirect.com/science/article/pii/S2352673421000469,Does gender matter? Evidence from crowdfunding,November 2021,Not Found,Ramy=Elitzur: ramy.elitzur@rotman.utoronto.ca; Eliran=Solodoha: solodoha@post.bgu.ac.il,"Abstract
The lack of resources and support for female entrepreneurs and their ventures lowers their chances of success and ultimately leads to the underrepresentation of female led ventures in the economy. Thus, to remedy this problem, it is crucial for female entrepreneurs to attract potential investors. The latter may use in their decision-making process cues such as the number of supporters, which provides social validation of the ventures. To test this, we analyze 2275 reward-based crowdfunding projects to investigate the effects of female entrepreneurs’ presence, and the consequences of social validation (i.e., number of supporters), on their crowdfunding success.",Business Venturing Insights,04 Mar 2025,8,"The study addresses the lack of support for female entrepreneurs by investigating the effects of social validation on crowdfunding success, offering valuable insights for potential investors and female entrepreneurs to increase their chances of success."
https://www.sciencedirect.com/science/article/pii/S2352673421000639,Occupy Wall Street ten years on: How its disruptive institutional entrepreneurship spread and why it fizzled,November 2021,Not Found,Thomas H.=Allison: t.allison@tcu.edu; Matthew=Grimes: m.grimes@jbs.cam.ac.uk; Aaron F.=McKenny: AMcKenny@iu.edu; Jeremy C.=Short: Jeremy.Short@unt.edu,"Abstract
How does media impact institutional entrepreneurs and their ability to create change? We draw from research on social movements and media frames to examine the paradox that media-informed discursive opportunities pose for institutional entrepreneurs engaged in efforts to transform or create social institutions. Through content analysis of 8473 newspaper articles covering the 2011 Occupy Wall Street movement, we highlight the paradox of discursive opportunities: the same types of media frames that initially encourage more disruptive tactics also subsequently increase the perceived threat of such disruption, thereby encouraging swifter counteraction. Our findings hold implications for the importance of media as a potential catalyst for 
entrepreneurial activity
 in the realm of social movements hoping to engage in reform.",Business Venturing Insights,04 Mar 2025,7,"This abstract provides insights into how media impacts institutional entrepreneurs, which can be valuable for startups trying to navigate social movements and engage in reform."
https://www.sciencedirect.com/science/article/pii/S2352673420300731,A xenophilic perspective of social entrepreneurship,June 2021,Not Found,Reginald=Tucker: regtucker@lsu.edu; Randall M.=Croom: Not Found,"Abstract
Social entrepreneurship has grown as an established field of inquiry, accompanied by growth in both academic and practice. In this paper, we offer a novel perspective on why some social entrepreneurs venture for foreigners. We employ xenophilia, a love of foreigners, to explain why some people care for foreigners by venturing for them. We conceptualize that religious and social class logics influence a xenophilic perspective of foreigners. We also analyze how xenophilia can have a darker side, particularly in social entrepreneurship. Our arguments and analysis allow us to provoke future research questions and offer practical implications.",Business Venturing Insights,04 Mar 2025,5,"While the perspective on social entrepreneurship and xenophilia is interesting, the practical implications for European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S2352673420300755,ADHD and entrepreneurship: Beyond person-entrepreneurship fit,June 2021,Not Found,Reginald=Tucker: regtucker@lsu.edu; Lu=Zuo: Not Found; Louis D.=Marino: Not Found; Graham H.=Lowman: Not Found; Alexander=Sleptsov: Not Found,"Abstract
Research examining 
mental health
 and entrepreneurship has found important links between 
mental health
 and entrepreneurship. These findings have led scholars to suggest a fit between some aspects of mental health, and in particular, mental dysfunction, and entrepreneurship. This paper complements extant studies in this area by examining the mental health and entrepreneurship relationship from a sociocognitive perspective. We examine to what extent does 
ADHD
 influence entrepreneurial self-efficacy and opportunity recognition tendency. Our findings are consistent with our hypotheses, suggesting that people with 
ADHD
 may not be efficacious in the entrepreneurial context, and specifically in recognizing opportunities. However, confidence in one’s ability regarding the entrepreneurship vocation can grow with education and experience. Our findings allow us to advance theory and offer practical implications.",Business Venturing Insights,04 Mar 2025,8,"The examination of the relationship between mental health, ADHD, and entrepreneurship from a sociocognitive perspective offers valuable insights for startups and entrepreneurs."
https://www.sciencedirect.com/science/article/pii/S2352673421000044,Relief and exploration after firm failure: Taking into account pre-failure experiences to understand post-failure responses,June 2021,Not Found,Anna S=Jenkins: a.jenkins@business.uq.edu.au,"Abstract
Conceptualizing firm failure as the loss of something important to the entrepreneur, the literature on emotional responses to firm failure has focused on the negative emotions experienced in response to this loss. We shift emphasis to introduce the positively valanced emotions relief and exploration as emotional responses to firm failure. These emotions reflect the inherently stressful nature of firm failure enabling an exploration into the timing of when stress is experienced during the failure process. Empirically we test our hypotheses, using a combination of a telephone and mail survey, on a sample of 114 entrepreneurs who had recently experienced the 
bankruptcy
 of their firm. We extend the literature on responses to firm failure by establishing relief and exploration as common emotional responses to firm failure and provide initial empirical support for the importance of considering pre-failure experiences in the process of entrepreneurial failure. Our findings also lend empirical support to the anticipatory grief argument put forward by Shepherd and colleagues (2009).",Business Venturing Insights,04 Mar 2025,6,"The exploration of emotional responses to firm failure is insightful, but the direct practical impact on European early-stage ventures may be moderate."
https://www.sciencedirect.com/science/article/pii/S2352673420300482,A dynamic analysis of the role of entrepreneurial ecosystems in reducing innovation obstacles for startups,November 2020,Not Found,Franco-Leal=Noelia: noelia.franco@uca.es; Diaz-Carrion=Rosalia: rosaliadiaz@us.es,"Abstract
Innovative startups 
face
 serious obstacles in their innovation processes because of the high costs of innovations, lack of commercial and managerial competencies, and difficulties in cooperating with industrial agents. Understanding the evolution of these obstacles, the dynamic nature of 
entrepreneurial ecosystems
, and how such ecosystems can reduce obstacles to innovation becomes crucial for managers and policymakers. This research examines the evolution of different types of obstacles innovative startups 
face
 and analyzes the effects market and research resources have on the entrepreneurial ecosystem in reducing these obstacles over time. Linear mixed models are used to analyze the evolution of the influence of the entrepreneurial ecosystem on the innovation obstacles of 911 Spanish innovative startups. The results indicate that different obstacles display different patterns over time. We found that the role played by market and/or research sources on the entrepreneurial ecosystem in reducing different obstacles to innovation could be linked to the tendency of these obstacles to diminish over time. The results point to the need for a set of agents to reinforce each other by creating an ecosystem in which innovation obstacles faced by startups are reduced over time.",Business Venturing Insights,04 Mar 2025,8,Understanding the evolution of obstacles faced by innovative startups and how entrepreneurial ecosystems can reduce these obstacles is highly relevant and valuable for European early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S235267342030055X,Classifying self-employed persons using segmentation criteria available in the Labour Force Survey (LFS) data,November 2020,Not Found,Ondřej=Dvouletý: ondrej.dvoulety@vse.cz,"Abstract
This paper responds to the call of researchers, business practitioners, and policymakers to treat different kinds of entrepreneurs separately by the empirical implementation of Cieślik and Dvouletý (2019) segmentation criteria for classifying self-employed persons. The article shows how the segmentation variables (i.e. work engagement; skills and job classification; growth aspirations and economic dependency) might be used when working with the European 
Labour Force Survey
 (LFS) data set. The paper exploits the Czech sample of the ad-hoc module 2017 data set, and it shows differences between various types of entrepreneurs by using tools of applied statistical techniques (Chi-Square tests of association, Cramer’s V and t-tests). The article contributes to the community by showing how to use the segmentation variables in their own empirical research. The study encourages all researchers to explore the diversity of self-employment to advance the entrepreneurship field further forward. The article also includes several recommendations and directions for future research at the individual, regional, country or cross-country levels.",Business Venturing Insights,04 Mar 2025,8,"This study provides practical insights on classifying self-employed persons and encourages further research in the field, which can be beneficial for early-stage ventures and startups in Europe."
https://www.sciencedirect.com/science/article/pii/S2352673420300585,The practice of “we”: A framework for balancing rigour and relevance in entrepreneurship scholarship,November 2020,Not Found,Isla=Kapasi: i.kapasi@leeds.ac.uk; Ainurul=Rosli: Ainurul.rosli@brunel.ac.uk,"Abstract
The rigour-relevance divide remains a longstanding concern for the entrepreneurship field. In this article we elucidate the practice of “we” in entrepreneurship scholarship and propose a means to encourage and realise it. Our contribution is in the combination of reflection (content reflection, process reflection, and premise reflection) and design science phases; thus, we develop and outline the concept and communal practice of entrepreneur
ial
 scholarship informed by a structured reflection framework. Our original model and related framework detail a series of overlapping phases of inquiry and questioning, demonstrating how can we work together with non-academics to collectively strengthen the relevance of entrepreneurship scholarship and, ultimately, be more accountable and relevant to those whom we research.",Business Venturing Insights,04 Mar 2025,6,"The article proposes a reflective framework for entrepreneurship scholarship, which could potentially enhance the relevance of research for European early-stage ventures, but may require further practical implementation."
https://www.sciencedirect.com/science/article/pii/S2352673419300587,Resilience as a moderator of government and family support in explaining entrepreneurial interest and readiness among single mothers,June 2020,Not Found,Yulita=Not Found: yulita@umt.edu.my; Suriyani=Muhamad: Not Found; Noorhaslinda=Kulub Abdul Rashid: Not Found; Nor Ermawati=Hussain: Not Found; Noor Haslina=Mohamad Akhir: Not Found; Nizam=Ahmat: Not Found,"Abstract
Resilience is the individual’s ability to cope with, adapt to and recover from stressful or 
traumatic experiences
. It is considered crucial in various fields, and particularly in entrepreneurship. In the current study, we sought to deepen our understanding of the role of an individual’s resilience in moderating the interaction between government support and family support to predict entrepreneurial-related outcomes. Specifically, using self-determination theory (SDT) and the social support perspective, we proposed that a positive beneficial three-way interaction effect, stated as resilience*government support*family support, would enhance the level of single mothers’ entrepreneurial interest and readiness for entrepreneurial challenge. Data collected from 519 Malaysian single mothers in Malaysia’s East Coast region were analysed using IBM 
SPSS Statistics
 software. The results showed that resilience moderated government support and family support interaction to predict both entrepreneurial interest and readiness for entrepreneurial challenge. Although both types of support were viewed as important, they were more effective for individuals who were highly resilient than for those who were not. These findings support the important role of resilience as ‘a moderator of the moderator’ and of government support as an external source of motivation, with both complementing the positive beneficial outcomes of family support.",Business Venturing Insights,04 Mar 2025,7,"The study on resilience, government support, and family support interaction provides valuable insights for predicting entrepreneurial outcomes, which could be applicable to European startups seeking external support."
https://www.sciencedirect.com/science/article/pii/S2352673420300202,Towards an integrative definition of scaling social impact in social enterprises,June 2020,Not Found,Syrus M.=Islam: syrus.islam@aut.ac.nz,"Abstract
Scaling social impact is a key concept in the social 
enterprise
 literature. While central, the wide range of meanings and the lack of conceptual uniformity detract from its usefulness. To tackle this issue, this paper conducts a 
systematic review
 to derive an integrative definition of scaling social impact: Scaling social impact is an ongoing process of increasing the magnitude of both quantitative and qualitative positive changes in society by addressing pressing social problems at individual and/or systemic levels through one or more scaling paths. Alongside improving conceptual clarity, the definition offers an operational structure with five underlying elements, setting the basis for new empirical work and theorising in social 
enterprise
 research.",Business Venturing Insights,04 Mar 2025,9,"The paper addresses the concept of scaling social impact in social enterprise literature, offering a clear definition and operational structure that can be highly beneficial for European early-stage ventures focused on social impact."
https://www.sciencedirect.com/science/article/pii/S2352673420300238,A meta-analysis of the gender gap(s) in venture funding: Funder- and entrepreneur-driven perspectives,June 2020,Not Found,Mark=Geiger: geigerm1@duq.edu,"Abstract
Using gender homophily and 
gender socialization
 as theoretical foundations, the current study takes the position that both funder-driven (supply-side) and entrepreneur-driven (demand-side) processes perpetuate the gender gap in venture funding. Using this positional anchor, I performed a meta-analysis on gender-funding associations. The results show that gender-funding associations are different across funding contexts, which is consistent with what gender homophily and a funder-driven perspective might suggest. However, the nature of the difference depends on whether the outcome is funding amount or funding success. In addition, business size and 
industry
 sector were found to fully mediate the relation between entrepreneur gender and funding needed. This finding is consistent with what 
gender socialization
 and an entrepreneur-driven perspective might suggest. The mediation results ultimately suggest that female entrepreneurs need less funding for their ventures, which in turn results in less funding amounts but greater funding success. As such, there is one gender gap to the disadvantage of female entrepreneurs (funding amount) and another gender gap to the advantage of female entrepreneurs (funding success). Together, the perspectives and findings presented in this paper provide insights for both research and practice on the gender 
gap(s)
 in venture funding.",Business Venturing Insights,04 Mar 2025,5,"While the study on gender gap in venture funding presents interesting findings, the practical implications for European startups are not as direct, warranting a slightly lower score."
https://www.sciencedirect.com/science/article/pii/S2352673419300344,Political climate and academic entrepreneurship: The case of strange bedfellows?,November 2019,Not Found,Peter T.=Gianiodis: gianiodisp@duq.edu; William R.=Meek: wmeek1@udayton.edu; Wendy=Chen: dchen16@gmu.edu,"Abstract
Universities have fully embraced academic entrepreneurship, transforming their structures, systems, and processes to generate licensing revenues and create new ventures. While prior research has mainly focused on the relationship between public policy and 
entrepreneurial activities
, this study examines a major gap – the performance implications of regional politics on academic entrepreneurship. We use a unique data set of U.S. universities and their regional governments to test how the influence of two elements of a region's political climate – consensus and stability – affects entrepreneurial and commercial performance. Our results suggest that political consensus and stability are positively associated with higher licensing revenues, while political stability is negatively associated with new venture creation. Our results reveal how regional politics influence university commercial outcomes, which suggests that entrepreneurship-enhancing public policy is intimately linked to the regional political process. We discuss the implications for theory and practice, and suggest possible future research directions.",Business Venturing Insights,04 Mar 2025,7,Examines the influence of regional politics on academic entrepreneurship with practical implications for entrepreneurial outcomes.
https://www.sciencedirect.com/science/article/pii/S2352673419300010,A chip off the old block? How parent-child interactions affect the intergenerational transmission of entrepreneurial intentions,June 2019,Not Found,Christian=Hopp: hopp@time.rwth-aachen.de; Dana=Minarikova: minarikova@time.rwth-aachen.de; Alexander=Speil: speil@time.rwth-aachen.de,"Abstract
Entrepreneurial parents can serve as sources of information and inspiration to transmit entrepreneurial intentions to their 
offspring
. Yet we find that role modelling is not purely observational, but it requires social interactions between parents and children to take effect. Our results using more than 2,500 child-parent dyads from the German socio-economic panel show that only if the socialization intensity between self-employed parents and their child is high entrepreneurial intentions will be transmitted. The intergenerational transmission of entrepreneurial intentions is dependent on the socialization intensity of the parents. The opportunity to share experiences with the children influences children's forming of 
entrepreneurial attitudes
 especially within same-sex parent-child dyads.",Business Venturing Insights,04 Mar 2025,6,"Explores the transmission of entrepreneurial intentions from self-employed parents to their children, providing insights into intergenerational entrepreneurship."
https://www.sciencedirect.com/science/article/pii/S235267341830132X,"The tacit knowledge of entrepreneurial design: Interrelating theory, practice and prescription in entrepreneurship research",June 2019,Not Found,Paul D.=Selden: paul.d.selden@gmail.com; Denise E.=Fletcher: denise.fletcher@uni.lu,"Abstract
An important challenge facing entrepreneurship researchers is the “three-body” knowledge problem of how to use “theoretical knowledge” to produce “prescriptive knowledge” that communicates the “practical knowledge” of situated practice to students and practitioners of entrepreneurship. We argue that a contribution can be made to solving this problem by theorizing practical knowledge as the “know-how” to do a situated entrepreneurial practice. “Know-how” is a cognitive “capacity to act” that 
prescribes
 for a practitioner how to produce a type of outcome in a range of circumstances. This “know-how” can potentially, therefore, be reconstructed theoretically as explicit micro-prescriptive guidelines for third-party practice. To exploit this connection between practical knowledge and prescriptive knowledge, however, we first need to overcome the problem that “know-how” is largely 
tacit
 in the moment of real-time forward-looking practice. In other words, the practitioner is not directly aware of their tacit “know-how”, or “tacit knowledge”, at the time of practice. In this article, we explore the contribution design theory can make to empirically eliciting, and conceptually inferring, the real-time “tacit knowledge” of entrepreneurial practice as a precursor to producing micro-prescriptive knowledge.",Business Venturing Insights,04 Mar 2025,5,"Addresses the challenge of translating theoretical knowledge into practical knowledge in entrepreneurship, offering potential solutions for practitioners."
https://www.sciencedirect.com/science/article/pii/S2352673418300933,Identifying design principles for business incubation in the European space sector,June 2019,Not Found,Daniel=Sagath: d.sagath@vu.nl; Elco=van Burg: j.c.van.burg@vu.nl; Joep P.=Cornelissen: cornelissen@rsm.nl; Christina=Giannopapa: Christina.giannopapa@esa.in,"Abstract
Organizations and policy makers seek to support business and entrepreneurship through facilitating new product and service development, for instance in business incubators. Taking stock of existing research and combining this with practitioner's insights, this study aims to identify a comprehensive set of design principles for incubation practices in a particular sector, the European space sector. We provide a synthesis of business incubation practices, resulting in a set of actionable design principles that also serves to tailor solutions for other contexts.",Business Venturing Insights,04 Mar 2025,6,"Identifies design principles for business incubation practices in the European space sector, providing actionable insights for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673418301112,Further exploring international entrepreneurial cognitions: The case of the Middle-East,June 2019,Not Found,Hamid=Vahidnia: hvahidnia@tulane.edu; J. Robert=Mitchell: rmitchell@ivey.uwo.ca; J. Brock=Smith: smithb@uvic.ca; Abdallah M.=Assaf: abdallah.assaf@ttu.edu; Ronald K.=Mitchell: ronald.mitchell@ttu.edu; Özlem=Araci: ozlem.araci@istanbul.edu.tr,"Abstract
Scholars argue that at least some entrepreneurial cognition is global; but there is little evidence to test this claim in the Middle East. For this region, composed of several countries with distinct socioeconomic contexts, even baseline descriptions are rare. Indeed, some have used the case of Middle East to challenge the global nature of entrepreneurial cognitions among individual entrepreneurs. Using data from 577 entrepreneurs and professionals in Egypt, Iran, 
Saudi Arabia
, and Turkey, four of the largest countries in the region, we study the extent to which entrepreneurial cognitions occur and explain an entrepreneurial 
mindset
 in this context.",Business Venturing Insights,04 Mar 2025,5,"Investigates entrepreneurial cognitions in the Middle East, contributing to our understanding of entrepreneurial mindset in diverse socioeconomic contexts."
https://www.sciencedirect.com/science/article/pii/S2352673418301021,Understanding the design of opportunities: Re-evaluating the agent-opportunity nexus through a design lens,June 2019,Not Found,Thomas=Ding: thomas.ding@strath.ac.uk,"Abstract
This paper reassesses the fundamental tenets of the opportunity construct from a design perspective. It acknowledges the incommensurability between the opportunity discovery and creation views, and uses a processual approach based on ontological pragmatism to conceptualize an opportunity. In essence, this paper outlines the associative nature of an opportunity and its implications to elucidate its ephemerality and its dependence on human agency for materialization.",Business Venturing Insights,04 Mar 2025,5,"The abstract provides insights into the nature of opportunities, which could be relevant to startups looking to innovate, but the impact on practical application is not clearly defined."
https://www.sciencedirect.com/science/article/pii/S0950584924002131,A multivocal literature review on the benefits and limitations of industry-leading AutoML tools,February 2025,Not Found,Luigi=Quaranta: Not Found; Kelly=Azevedo: Not Found; Fabio=Calefato: Not Found; Marcos=Kalinowski: kalinowski@inf.puc-rio.br,"Abstract
Context:
Rapid advancements in Artificial Intelligence (AI) and Machine Learning (ML) are revolutionizing software engineering in every application domain, driving unprecedented transformations and fostering innovation. However, despite these advances, several organizations are experiencing friction in the adoption of ML-based technologies, mainly due to the current shortage of ML professionals. In this context, Automated Machine Learning (AutoML) techniques have been presented as a promising solution to democratize ML adoption, even in the absence of specialized people.
Objective:
Our research aims to provide an overview of the evidence on the benefits and limitations of AutoML tools being adopted in industry.
Methods:
We conducted a Multivocal Literature Review, which allowed us to identify 54 sources from the academic literature and 108 sources from the grey literature reporting on AutoML benefits and limitations. We extracted explicitly reported benefits and limitations from the papers and applied the thematic analysis method for synthesis.
Results:
In general, we identified 18 reported benefits and 25 limitations. Concerning the benefits, we highlight that AutoML tools can help streamline the core steps of ML workflows, namely data preparation, feature engineering, model construction, and hyperparameter tuning—with concrete benefits on model performance, efficiency, and scalability. In addition, AutoML empowers both novice and experienced data scientists, promoting ML accessibility. However, we highlight several limitations that may represent obstacles to the widespread adoption of AutoML. For instance, AutoML tools may introduce barriers to transparency and interoperability, exhibit limited flexibility for complex scenarios, and offer inconsistent coverage of the ML workflow.
Conclusion:
The effectiveness of AutoML in facilitating the adoption of machine learning by users may vary depending on the specific tool and the context in which it is used. Today, AutoML tools are used to increase human expertise rather than replace it and, as such, require skilled users.",Information and Software Technology,04 Mar 2025,9,"The abstract discusses the benefits and limitations of AutoML tools, providing valuable information for startups in the AI and ML space. The research conducted and the results obtained are practical and useful for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924002106,FOBICS: Assessing project security level through a metrics framework that evaluates DevSecOps performance,February 2025,"DevSecOps, Metrics framework, Project security, Software engineering, Evaluating security, DevOps, Security assessment, Software metrics, Secure software development, Business metrics",Alessandro=Caniglia: alessandro.caniglia95@gmail.com; Vincenzo=Dentamaro: vincenzo.dentamaro@uniba.it; Stefano=Galantucci: stefano.galantucci@uniba.it; Donato=Impedovo: donato.impedovo@uniba.it,"Abstract
Context:
In today’s software development landscape, the DevSecOps approach has gained traction due to its focus on the software development process and bolstering security measures in projects, a task in light of the ever-evolving cybersecurity threats.
Objective:
This study aims to address the lack of metrics for quantitatively assessing its efficacy from both security and business logic perspectives.
Methods:
To tackle this issue, the research introduces the Framework of Business Index Concerning Security (FOBICS), a set of metrics designed to enable transparent evaluations of project security. FOBICS considers various perspectives relevant to DevSecOps practices. It includes factors such as project duration and financial outcomes, making it appealing for implementation in business settings.
Results:
The effectiveness of FOBICS is validated theoretically and empirically via its application in two real-world projects: the results from these implementations show a correlation between FOBICS metrics and the security strategies employed as the development methodologies adopted by diverse teams throughout the projects.
Conclusion:
Hence, FOBICS emerges as a tool for assessing and continuously monitoring project security, offering insights into areas of strength and areas that may require enhancement. FOBICS is shown to be effective in assessing the level of DevSecOps implementation. The ease of calculating FOBICS metrics makes them easily interpretable and continuously verifiable. Moreover, FOBICS summarizes most of the other quantitative and qualitative metrics in the literature.",Information and Software Technology,04 Mar 2025,8,"The abstract introduces FOBICS as a tool for assessing project security in the DevSecOps approach, which could be highly beneficial for startups in software development. The practical application and validation of FOBICS in real-world projects add significant value."
https://www.sciencedirect.com/science/article/pii/S0950584924002179,Impact of minimum viable product on software ecosystem failure,February 2025,Not Found,Kati=Saarni: katimarika.saarni@gmail.com; Marjo=Kauppinen: Not Found; Tomi=Männistö: Not Found,"Abstract
Context
Companies are interested in building successful value-producing ecosystems together to offer end users a broader digital service offering and better meet customer needs. However, most ecosystems fail in the early years.
Objective
We investigated one small software ecosystem from the planning phase to the operative phase, where the participating companies left one by one because the software ecosystem was unsuccessful, and the software ecosystem ended after four operative years. The software ecosystem provided a digital service offering based on the defined MVP (Minimum Viable Product). That is why we were interested in understanding the MVP's impact on the ecosystem's failure.
Method
We conducted a case study, the results of which are based on the semi-structured interviews of eight representatives of the software ecosystem.
Results
This study showed that the actors prioritized out functionalities from the MVP, and the MVP was no longer based on the defined value proposition, target customer groups, and customer paths. It was then difficult for the actors to achieve their objectives. The companies’ commitment depended on the set objectives, and when the objectives were not achieved, the actors left the ecosystem, and the software ecosystem failed.
Conclusion
The results show that the MVP can significantly affect the failure of the small software ecosystem, where all actors have a keystone role. The MVP largely defines what kind of digital service offering the software ecosystem provides and whether the actors can achieve the objectives, especially their sales goals. Thus, prioritizing the functionalities of the MVP is a critical activity.",Information and Software Technology,04 Mar 2025,3,"While the study on the impact of MVP on software ecosystem failure is relevant, the practical insights for startups are limited. The findings may not directly translate into actionable strategies for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924002143,Who uses personas in requirements engineering: The practitioners’ perspective,February 2025,"Requirements engineering, Personas, Human aspects, Survey, Interviews",Yi=Wang: xve@deakin.edu.au; Chetan=Arora: chetan.arora@monash.edu; Xiao=Liu: xiao.liu@deakin.edu.au; Thuong=Hoang: thuong.hoang@deakin.edu.au; Vasudha=Malhotra: vasu.malhotra@monash.edu; Ben=Cheng: chengye@deakin.edu.au; John=Grundy: john.grundy@monash.edu,"Abstract
Context:
Personas are commonly employed in software projects to better understand end-users needs. Despite their frequent usage, there is a limited understanding of their practical application and effectiveness.
Objective:
This paper aims to investigate the current practices, methods, and challenges associated with using personas in software development.
Methods:
A two-step investigation was conducted, comprising interviews with 26 software developers, UI/UX designers, business analysts, and product managers, along with a survey of 203 practitioners.
Results:
The findings reveal variations in the frequency and effectiveness of personas across different software projects and IT companies. Additionally, the study highlights the challenges practitioners face when using personas and the reasons for not using them. Notably, the research shows that some human aspects (e.g., the needs of users with disabilities), often assumed to be a key feature of personas, are frequently not considered for various reasons in requirements engineering.
Conclusions:
The study provides actionable insights for practitioners to overcome challenges in using personas during the requirements engineering stages. Furthermore, it identifies areas for future research to enhance the effectiveness of personas in software development.",Information and Software Technology,04 Mar 2025,7,The investigation into the usage of personas in software development provides useful insights for startups aiming to understand end-user needs. The challenges identified and actionable insights offered can be valuable for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584924001745,"A systematic literature review on Agile, Cloud, and DevOps integration: Challenges, benefits",January 2025,"DevOps, Cloud, Agile, Integration, Synergy",Fatiha El=Aouni: Not Found; Karima=Moumane: karima.moumane@ensias.um5.ac.ma; Ali=Idri: Not Found; Mehdi=Najib: Not Found; Saeed Ullah=Jan: Not Found,"Abstract
Context:
In today’s fast-paced digital landscape, integrating DevOps, cloud, and agile methodologies is crucial for meeting software demands. However, this integration remains under-researched.
Objective:
This study explores the integration of Agile, Cloud, and DevOps in today’s software development landscape. It aims to analyze the challenges and benefits associated with merging these three approaches, focusing on their impact on software testing and the role of mindset in successful implementation and identifying the most suitable Agile methodologies.
Methods:
This investigation utilizes a Systematic Literature Review(SLR) to enrich comprehension of this integration in current software development practices.
Results:
The analysis of 31 articles highlights benefits such as improved collaboration and accelerated development, despite challenges with tool proliferation. Platforms like Jenkins, GitLab, Kubernetes, and Docker show promise in addressing these complexities. Our study examines the advantages and challenges of this integration, focusing on its impact on software testing and the role of mindset in successful implementation and identifying the most suitable Agile methodologies.
Conclusion:
The integration of Agile, DevOps, and Cloud signifies a vital move towards collaborative, scalable, and automated methods, crucial for swift delivery, enhanced quality, and ongoing competitiveness. This unified approach is fundamental for organizational advancement and innovation in the ever-evolving software development realm. Further research should tackle challenges in merging these methods and delve into their interactions with emerging technologies to refine practices for increased efficiency.",Information and Software Technology,04 Mar 2025,7,"The study explores the integration of Agile, DevOps, and Cloud in software development, highlighting benefits and challenges. The findings can be valuable for early-stage ventures to enhance collaboration and accelerate development."
https://www.sciencedirect.com/science/article/pii/S0950584924001939,DeepMig: A transformer-based approach to support coupled library and code migrations,January 2025,"Recommender system, Library migration, Transformers",Juri=Di Rocco: juri.dirocco@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Claudio=Di Sipio: claudio.disipio@univaq.it; Riccardo=Rubei: riccardo.rubei@univaq.it; Davide=Di Ruscio: davide.diruscio@univaq.it; Massimiliano=Di Penta: dipenta@unisannio.it,"Abstract
Context:
While working on software projects, developers often replace third-party libraries (TPLs) with different ones offering similar functionalities. However, choosing a suitable TPL to migrate to is a complex task. As TPLs provide developers with Application Programming Interfaces (APIs) to allow for the invocation of their functionalities after adopting a new TPL, projects need to be migrated by the methods containing the affected API calls. Altogether, the coupled migration of TPLs and code is a strenuous process, requiring massive development effort. Most of the existing approaches either deal with library or API call migration but usually fail to solve both problems coherently simultaneously.
Objective:
This paper presents DeepMig, a novel approach to the coupled migration of TPLs and API calls. We aim to support developers in managing their projects, at the library and API level, allowing them to increase their productivity.
Methods:
DeepMig is based on a transformer architecture, accepts a set of libraries to predict a new set of libraries. Then, it looks for the changed API calls and recommends a migration plan for the affected methods. We evaluate DeepMig using datasets of Java projects collected from the Maven Central Repository, ensuring an assessment based on real-world dependency configurations.
Results:
Our evaluation reveals promising outcomes: DeepMig recommends both libraries and code; by several projects, it retrieves a perfect match for the recommended items, obtaining an accuracy of 1.0. Moreover, being fed with proper training data, DeepMig provides comparable code migration steps of a static API migrator, a baseline for the code migration task.
Conclusion:
We conclude that DeepMig is capable of recommending both TPL and API migration, providing developers with a practical tool to migrate the entire project.",Information and Software Technology,04 Mar 2025,8,"DeepMig presents a novel approach to coupled migration of TPLs and API calls, demonstrating promising outcomes in real-world projects. Early-stage ventures can benefit from increased productivity and ease of migration."
https://www.sciencedirect.com/science/article/pii/S0950584924001915,Constructing the graphical structure of expert-based Bayesian networks in the context of software engineering: A systematic mapping study,January 2025,"Bayesian networks, Bayesian network structure, Expert knowledge, Software engineering, Systematic mapping",Thiago=Rique: thiago.rique@ifpb.edu.br; Mirko=Perkusich: mirko@virtus.ufcg.edu.br; Kyller=Gorgônio: kyller@virtus.ufcg.edu.br; Hyggo=Almeida: hyggo@virtus.ufcg.edu.br; Angelo=Perkusich: perkusic@virtus.ufcg.edu.br,"Abstract
Context:
In scenarios where data availability issues hinder the applications of statistical causal modeling in software engineering (SE), Bayesian networks (BNs) have been widely used due to their flexibility in incorporating expert knowledge. However, the general understanding of how the graphical structure, i.e., the directed acyclic graph (DAG), of these models is built from domain experts is still insufficient.
Objective:
This study aims to characterize the SE landscape of constructing the graphical structure of BNs, including their potential for causal modeling.
Method:
We conducted a systematic mapping study employing a hybrid search strategy that combines a database search with parallel backward and forward snowballing.
Results:
Our mapping included a total of 106 studies. Different methods are commonly combined to construct expert-based BN structures. These methods span across data gathering & analysis (e.g., interviews, focus groups, literature research, grounded theory, and statistical analysis) and reasoning mechanisms (e.g., using idioms combined with the adoption of lifecycle models, risk-centric modeling, and other frameworks to guide BN construction). We found a lack of consensus regarding validation procedures, particularly critical when modeling cause–effect relationships from knowledge. Additionally, expert-based BNs are mainly applied at the tactical level to address problems related to software engineering management and software quality. Challenges in creating expert-based structures include validation procedures, experts’ availability, expertise level, and structure complexity handling. Key recommendations involve empirical validation, participatory involvement, and balance between adaptation to organizational constraints and model construction requirements.
Conclusion:
The construction of expert-based BN structures in SE varies in rigor, with some methods being systematic while others appear ad hoc. To enhance BN application, reducing expert knowledge subjectivity, enhancing methodological rigor, and clearly articulating the construction rationale is essential. Addressing these challenges is crucial for improving the reliability of causal inferences drawn from these models, ultimately leading to better-informed decisions in SE practices.",Information and Software Technology,04 Mar 2025,5,"The study characterizes the construction of BN structures in SE, highlighting challenges and recommendations. While informative, the practical impact on early-stage ventures may be more limited."
https://www.sciencedirect.com/science/article/pii/S0950584924001551,Bringing architecture-based adaption to the mainstream,December 2024,Not Found,Negar=Ghorbani: negargh@uci.edu; Joshua=Garcia: joshug4@uci.edu; Sam=Malek: malek@uci.edu,"Abstract
Software architecture has been shown to provide an appropriate level of granularity for representation of a managed software system and reasoning about the impact of adaptation choices on its properties. Software architecture-based adaptability is the ability to adapt a software system in terms of its architectural elements, such as its components and their interfaces. Despite its promise, architecture-based adaptation has remained largely elusive, mainly because it involves heavy engineering effort of making non-trivial changes to the manner in which a software system is implemented. In this paper, we present 
Acadia
—a framework that automatically enables architecture-based adaptation of practically any Java 9+ application without requiring any changes to the implementation of the application itself. 
Acadia
 builds on the 
Java Platform Module System (JPMS)
, which has brought extensive support for architecture-based development to Java 9 and subsequent versions. 
Acadia
 extends JPMS with the ability to provide and maintain a representation of an application’s architecture and make changes to it at runtime. The results of our experimental evaluation, conducted on three large open-source Java applications, indicate that 
Acadia
 is able to efficiently apply dynamic changes to the architecture of these applications without requiring any changes to their implementation.",Information and Software Technology,04 Mar 2025,6,Acadia offers a framework for architecture-based adaptation of Java applications without changes to implementation. This could be beneficial for startups looking to adapt their software systems efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584924001708,A3Test: Assertion-Augmented Automated Test case generation,December 2024,"Test case generation, Deep learning",Saranya=Alagarsamy: Not Found; Chakkrit=Tantithamthavorn: chakkrit@monash.edu; Aldeida=Aleti: Not Found,"Abstract
Context:
Test case generation is a critical yet challenging task in software development. Recently, AthenaTest – a Deep Learning (DL) approach for generating unit test cases has been proposed. However, our revisiting study reveals that AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification.
Objective:
This paper introduces A3Test, a novel DL-based approach to the generation of test cases, enhanced with assertion knowledge and a mechanism to verify consistency of the name and signatures of the tests. A3Test aims to adapt domain knowledge from assertion generation to test case generation.
Method:
A3Test employs domain adaptation principles and introduces a verification approach to name consistency and test signatures. We evaluate its effectiveness using 5,278 focal methods from the Defects4j dataset.
Results:
Our findings indicate that A3Test outperforms AthenaTest and ChatUniTest. A3Test generates 2.16% to 395.43% more correct test cases, achieves 2.17% to 34.29% higher method coverage, and 25.64% higher line coverage. A3Test achieves 2.13% to 12.20% higher branch coverage, 2.22% to 12.20% higher mutation scores, and 2.44% to 55.56% more correct assertions compared to both ChatUniTest and AthenaTest respectively for one iteration. When generating multiple test cases per method A3Test still shows improvements and comparable efficacy to ChatUnitTest. A survey of developers reveals that the majority of the participants 70.51% agree that test cases generated by A3Test are more readable than those generated by EvoSuite.
Conclusions:
A3Test significantly enhances test case generation through its incorporation of assertion knowledge and test signature verification, contributing to the generation of correct test cases.",Information and Software Technology,04 Mar 2025,7,"A3Test improves test case generation using DL and assertion knowledge, showing significant enhancements in correctness and coverage. Early-stage ventures can leverage this approach for better testing practices."
https://www.sciencedirect.com/science/article/pii/S0950584924001575,Perceived impact of agile principles: Insights from a survey-based study on agile software development project success,December 2024,Not Found,Yulianus=Palopak: ypalopak@unai.edu; Sun-Jen=Huang: huangsj@mail.ntust.edu.tw,"Abstract
Context
Agile methodology has emerged as a fundamental framework guiding software development projects, emphasizing values and principles for achieving successful project outcomes. Despite the widespread recognition of the importance of agile principles, there remains a gap in empirical research investigating their actual impact on agile project success.
Objective
This research aims to examine the relationship between agile principles and project outcomes and provide empirical evidence supporting the importance of agile principles in achieving success in agile software development (ASD) projects.
Method
A total of 298 Agile project practitioners participated in an online survey between August and September 2023 to test this study's research model using the partial least square structural equation modeling (PLS-SEM) method.
Results
We find a significant relationship between adopting agile principles and project success, with regular delivery, technical excellence, team member proactivity, and customer collaboration showing the highest impact on Agile project success. However, process simplicity was found not to be significant in the study.
Conclusions
Our analysis verifies the importance of Agile principles and suggests areas for further study to successfully understand their impact on Agile projects. The findings contribute to the ongoing discourse on agile principles and their impact on software development project success, opening avenues for future research and the refinement of agile methodologies. These insights could assist organizations in optimizing Agile practices and decision-making, leading to more successful and efficient software development projects.",Information and Software Technology,04 Mar 2025,8,"The research provides empirical evidence on the importance of agile principles in project success, contributing to the optimization of Agile practices and decision-making for startups."
https://www.sciencedirect.com/science/article/pii/S0950584924001496,Hidden code vulnerability detection: A study of the Graph-BiLSTM algorithm,November 2024,Not Found,Kao=Ge: Not Found; Qing-Bang=Han: 20111841@hhu.edu.cn,"Abstract
Context:
The accelerated growth of the Internet and the advent of artificial intelligence have led to a heightened interdependence of open source products, which has in turn resulted in a rise in the frequency of security incidents. Consequently, the cost-effective, fast and efficient detection of hidden code vulnerabilities in open source software products has become an urgent challenge for both academic and engineering communities.
Objectives:
In response to this pressing need, a novel and efficient code vulnerability detection model has been proposed: the Graph-Bi-Directional Long Short-Term Memory Network Algorithm (Graph-BiLSTM). The algorithm is designed to enable the detection of vulnerabilities in Github’s code commit records on a large scale, at low cost and in an efficient manner.
Methods:
In order to extract the most effective code vulnerability features, state-of-the-art vulnerability datasets were compared in order to identify the optimal training dataset. Initially, the Joern tool was employed to transform function-level code blocks into Code Property Graphs (CPGs). Thereafter, structural features (degree centrality, Katz centrality, and closeness centrality) of these CPGs were computed and combined with the embedding features of the node sequences to form a two-dimensional feature vector space for the function-level code blocks. Subsequently, the BiLSTM network algorithm was employed for the automated extraction and iterative model training of a substantial number of vulnerability code samples. Finally, the trained algorithmic model was applied to code commit records of open-source software products on GitHub, achieving effective detection of hidden code vulnerabilities.
Conclusion:
Experimental results indicate that the PrimeVul dataset represents the most optimal resource for vulnerability detection. Moreover, the Graph-BiLSTM model demonstrated superior performance in terms of accuracy, training cost, and inference time when compared to state-of-the-art algorithms for the detection of vulnerabilities in open-source software code on GitHub. This highlights the significant value of the model for engineering applications.",Information and Software Technology,04 Mar 2025,9,"The proposed Graph-BiLSTM model offers a novel solution to detecting vulnerabilities in open source software, addressing a critical challenge for startups to ensure software security."
https://www.sciencedirect.com/science/article/pii/S0950584924001344,Qubernetes: Towards a unified cloud-native execution platform for hybrid classic-quantum computing,November 2024,"Quantum software, Hybrid classical-quantum software, Containers, Quantum software development lifecycle, Cloud-native computing",Vlad=Stirbu: vlad.a.stirbu@jyu.fi; Otso=Kinanen: Not Found; Majid=Haghparast: Not Found; Tommi=Mikkonen: Not Found,"Abstract
Context:
The emergence of quantum computing proposes a revolutionary paradigm that can radically transform numerous scientific and industrial application domains. The ability of quantum computers to scale computations beyond what the current computers are capable of implies better performance and efficiency for certain algorithmic tasks.
Objective:
However, to benefit from such improvement, quantum computers must be integrated with existing software systems, a process that is not straightforward. In this paper, we propose a unified execution model that addresses the challenges that emerge from building hybrid classical-quantum applications at scale.
Method:
Following the Design Science Research methodology, we proposed a convention for mapping quantum resources and artifacts to Kubernetes concepts. Then, in an experimental Kubernetes cluster, we conducted experiments for scheduling and executing quantum tasks on both quantum simulators and hardware.
Results:
The experimental results demonstrate that the proposed platform Qubernetes (or Kubernetes for quantum) exposes the quantum computation tasks and hardware capabilities following established cloud-native principles, allowing seamless integration into the larger Kubernetes ecosystem.
Conclusion:
The quantum computing potential cannot be realized without seamless integration into classical computing. By validating that it is practical to execute quantum tasks in a Kubernetes infrastructure, we pave the way for leveraging the existing Kubernetes ecosystem as an enabler for hybrid classical-quantum computing.",Information and Software Technology,04 Mar 2025,7,The unified execution model for classical-quantum applications in Kubernetes offers a promising direction for startups to leverage quantum computing capabilities within existing software systems.
https://www.sciencedirect.com/science/article/pii/S095058492400106X,Reporting case studies in systematic literature studies—An evidential problem,October 2024,"Systematic mapping study, Systematic review, Systematic literature review, Case study, Credible evidence",Austen=Rainer: Not Found; Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Context:
The term and label, “case study”, is not used consistently by authors of primary studies in 
software engineering
 research. It is not clear whether this problem also occurs for systematic literature studies (SLSs).
Objective:
To investigate the extent to which SLSs in/correctly use the term and label, “case study”, when classifying primary studies.
Methods:
We systematically collect two sub-samples (2010–2021 & 2022) comprising a total of eleven SLSs and 79 primary studies. We examine the designs of these SLSs, and then analyse whether the SLS authors and the primary-study authors correctly label the respective primary study as a “case study”.
Results:
76% of the 79 primary studies are misclassified by SLSs (with the two sub-samples having 60% and 81% 
misclassification
, respectively). For 39% of the 79 studies, the SLSs propagate a mislabelling by the original authors, whilst for 37%, the SLSs introduce a new mislabel, thus making the problem worse. SLSs rarely present explicit definitions for “case study” and when they do, the definition is not consistent with established definitions.
Conclusions:
SLSs are both propagating and exacerbating the problem of the mislabelling of primary studies as “case studies”, rather than – as we should expect of SLSs – correcting the labelling of primary studies, and thus improving the body of credible evidence. Propagating and exacerbating mislabelling undermines the credibility of evidence in terms of its quantity, quality and relevance to both practice and research.",Information and Software Technology,04 Mar 2025,4,The investigation on misclassification of 'case studies' lacks direct practical impact on early-stage ventures or startups.
https://www.sciencedirect.com/science/article/pii/S0950584924001241,Towards antifragility of cloud systems: An adaptive chaos driven framework,October 2024,"Antifragility, Resilience, Chaos engineering, Self-adaptive software, Resilience testing, Cloud computing",Joseph S.=Botros: Not Found; Lamis F.=Al-Qora'n: Not Found; Amro=Al-Said Ahmad: a.m.al-said.ahmad@keele.ac.uk,"Abstract
Context
Unlike resilience, antifragility describes systems that get stronger rather than weaker under stress and chaos. Antifragile systems have the capacity to overcome stressors and come out stronger, whereas resilient systems are focused on their capacity to return to their previous state following a failure. As technology environments become increasingly complex, there is a great need for developing software systems that can benefit from failures while continuously improving. Most applications nowadays operate in cloud environments. Thus, with this increasing adoption of Cloud-Native Systems they require antifragility due to their distributed nature.
Objective
The paper proposes UNFRAGILE framework, which facilitates the transformation of existing systems into antifragile systems. The framework employs chaos engineering to introduce failures incrementally and assess the 
system's response
 under such perturbation and improves the quality of system response by removing fragilities and introducing adaptive 
fault tolerance
 strategies.
Method
The UNFRAGILE framework's feasibility has been validated by applying it to a cloud-native using a real-world architecture to enhance its antifragility towards long outbound service latencies. The empirical investigation of fragility is undertaken, and the results show how chaos affects application performance metrics and causes disturbances in them. To deal with chaotic 
network latency
, an adaptation phase is put into effect.
Results
The findings indicate that the steady stage's behaviour is like the antifragile stage's behaviour. This suggests that the system could self-stabilise during the chaos without the need to define a 
static configuration
 after determining from the context of the environment that the dependent system was experiencing difficulties.
Conclusion
Overall, this paper contributes to ongoing efforts to develop antifragile software capable of adapting to the rapidly changing complex environment. Overall, the research provides an operational framework for engineering software systems that learn and improve through exposure to failures rather than just surviving them.",Information and Software Technology,04 Mar 2025,8,"The UNFRAGILE framework introduces a practical approach for startups to enhance the antifragility of their cloud-native systems, aligning with the need for continuous improvement in complex technology environments."
https://www.sciencedirect.com/science/article/pii/S0950584924001265,Towards assessing the quality of knowledge graphs via differential testing,October 2024,Not Found,Jiajun=Tan: jjtan@smail.nju.edu.cn; Dong=Wang: juliawdd@henu.edu.cn; Jingyu=Sun: MF21320132@smail.nju.edu.cn; Zixi=Liu: zxliu@smail.nju.edu.cn; Xiaoruo=Li: lixiaoruo@henu.edu.cn; Yang=Feng: fengyang@nju.edu.cn,"Abstract
Knowledge graphs
 (KG) can aggregate data and make information resources easier to calculate and understand. With tremendous advancements in knowledge graphs, they have been incorporated into plenty of software systems to assist various tasks. However, while KGs determine the performance of downstream software systems, their quality is often measured by the accuracy of test data. Considering the limitation of accessible high-quality test data, an automated quality assessment technique could fundamentally improve the testing efficiency of KG-driven software systems and save plenty of manual labeling resources.
In this paper, we propose an automated approach to quantify the quality of KGs via differential testing. It first constructs multiple 
Knowledge Graph Embedding
 Models (KGEM) and conducts head prediction tasks on models. Then, it can produce a differential score that reflects the quality of KGs by comparing the proximity of output results. To validate the effectiveness of this approach, we experiment with four open-sourced knowledge graphs. The experiment results show that our approach is capable of accurately evaluating the quality of KGs and producing reliable results on different datasets. Moreover, we compared our method with existing methods and achieved certain advantages. The potential usefulness of our approach sheds light on the development of various KG-driven software systems.",Information and Software Technology,04 Mar 2025,7,"The proposed automated approach to quantify the quality of KGs can significantly improve testing efficiency and save manual labeling resources, which can benefit European startups using knowledge graphs."
https://www.sciencedirect.com/science/article/pii/S0950584924000934,Coverage-enhanced fault diagnosis for Deep Learning programs: A learning-based approach with hybrid metrics,September 2024,Not Found,Xiaofang=Qi: Not Found; Tiangang=Zhu: Not Found; Yanhui=Li: yanhuili@nju.edu.cn,"Abstract
Context:
Given the data-driven paradigm inherent to 
Deep Learning
 (DL), it is inevitable that DL software will exhibit incorrect behavior in real-world applications. DL programs have been identified as a primary source of DL faults. To tackle this, researchers have devised a unique framework that approaches fault diagnosis as a learning task, which leverages runtime data as metrics to construct predictive models, enabling effective fault diagnosis.
Object:
In this paper, we aim to propose new metrics, especially from the coverage view, to enhance the performance of fault diagnosis models.
Method:
We combine coverage criteria and statistical operators to propose 80 coverage metrics, which summarize the trend of coverage values in the model training procedure. We construct hybrid prediction models by combining our new coverage metrics and existing runtime metrics under four widely used classifiers.
Results:
To examine whether adding our new coverage metrics performs well in DL program fault diagnosis, we conduct our experiments on six widely used datasets under four indicators (i.e., accuracy, F1 score, AUC, and MCC). Through the experiments, we observe that (a) coverage metrics are 
not redundant
 with respect to the original runtime metrics, and (b) adding extra coverage metrics can 
significantly enhance
 the performance of fault diagnosis models.
Conclusions:
Our study shows that our proposed coverage metrics are helpful in constructing effective fault diagnosis models for DL programs.",Information and Software Technology,04 Mar 2025,5,"While the proposed framework for fault diagnosis in DL programs is interesting, it may have a moderate impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492400082X,Machine learning for requirements engineering (ML4RE): A systematic literature review complemented by practitioners’ voices from Stack Overflow,August 2024,"Requirements engineering, Machine learning, Systematic literature review",Tong=Li: litong@bjut.edu.cn; Xinran=Zhang: zhangxinran028@gmail.com; Yunduo=Wang: wangyunduo@buaa.edu.cn; Qixiang=Zhou: qixiang.zho@gmail.com; Yiting=Wang: wangyiting.official@gmail.com; Fangqi=Dong: dfq902@gmail.com,"Abstract
Context:
The research of 
machine learning
 for 
requirements engineering
 (ML4RE) has attracted more and more attention from researchers and practitioners. Although pioneering research has shown the potential of using 
ML techniques
 to improve RE practices, there lacks a systematic and comprehensive literature review in academia that integrates an industrial perspective. Specifically, none of the reviews available in ML4RE have considered the grey literature, which is primarily from practitioner origin and is more reflective of the real issues and challenges faced in practice.
Objective:
In this paper, we conduct a systematic survey of academic publications in ML4RE and complement it with the practitioners’ voices from Stack Overflow to complete a comprehensive literature review. Our research objective is to provide a comprehensive view of the current research progress in ML4RE, present the main questions and challenges faced in RE practice, understand the gap between research and practice, and provide our insights into how the RE academic domain can pragmatically develop in the future.
Method:
We systematically investigated 207 academic papers on ML4RE from 2010 to 2022, along with 375 questions related to RE practices on Stack Overflow and their corresponding answers. Our analysis encompassed their trends, focused RE activities and tasks, employed solutions, and associated data. Finally, we conducted a 
joint
 analysis, contrasting the outcomes of both parts.
Results:
Based on the statistical results from collected literature, we summarize an academic roadmap and analyse the disparities, offering research recommendations. Our suggestions include the development of intelligent question-answering assistants employing 
large language models
, the integration of machine learning into industrial tools, and the promotion of collaboration between academia and industry.
Conclusion:
This study contributes by providing a holistic view of ML4RE, delineating disparities between research and practice, and proposing pragmatic suggestions to bridge the academia-industry gap.",Information and Software Technology,04 Mar 2025,8,"The systematic survey of ML4RE literature and practitioners' voices provides valuable insights that can help European startups in requirements engineering, making it highly valuable."
https://www.sciencedirect.com/science/article/pii/S0950584924000831,Studying and recommending information highlighting in Stack Overflow answers,August 2024,Not Found,Shahla Shaan=Ahmed: ahmeds27@myumanitoba.ca; Shaowei=Wang: shaowei.wang@umanitoba.ca; Yuan=Tian: y.tian@queensu.ca; Tse-Hsun (Peter)=Chen: peterc@encs.concordia.ca; Haoxiang=Zhang: haoxiang.zhang@acm.org,"Abstract
Context:
Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or 
HTML
 so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information.
Objective:
We carried out the first large-scale 
exploratory study
 on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using 
neural network
 architectures initially designed for the 
Named Entity Recognition
 task.
Method:
In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN-based and BERT-based models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
Results:
Our models achieve a precision ranging from 0.50 to 0.72 for different formatting types. It is easier to build a model to recommend Code than other types. Models for text formatting types (i.e., Heading, Bold, and Italic) suffer low recall. Our analysis of failure cases indicates that the majority of the failure cases are due to missing identification. One explanation is that the models are easy to learn the frequent highlighted words while struggling to learn less frequent words (i.g., long-tail knowledge).
Conclusion:
Our findings suggest that it is possible to develop recommendation models for highlighting information for answers with different formatting styles on Stack Overflow.",Information and Software Technology,04 Mar 2025,6,The study on recommending highlighted content in Stack Overflow answers using neural network architectures is useful but may have limited immediate practical impact on European early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584924000764,Not yet another BPM lifecycle: A synthesis of existing approaches using BPMN,July 2024,Not Found,Nikolaos=Nousias: Not Found; George=Tsakalidis: Not Found; Kostas=Vergidis: kvergidis@uom.edu.gr,"Abstract
Context
Business Process Management (BPM) is considered an important management approach that encompasses a set of methods for managing the business processes of an organization. To maximize the benefits of BPM, scholars have conceptualized its steps in schematic diagrams with interrelated phases called BPM lifecycles. As this approach has been established, the phenomenon of perpetual proposition of BPM lifecycles has been observed in relevant literature. This practice obscures what should be relatively straightforward: a consensus among researchers and practitioners regarding the steps that a business process should flow through during its lifecycle.
Objective
The aim of this work is to investigate the existing BPM 
lifecycle models
 proposed in literature, identify convergences and variations in these models, analyze their core components and locate common patterns that will enable the synthesis of a BPM lifecycle that is conceptualized with 
Business Process Model and Notation
 (BPMN), the community de-facto business process modeling notation.
Method
To formalize the research problem and develop the design of a solution, the Design Science Research Process (DSRP) model was adopted. To investigate the perpetual proposition of BPM lifecycles in literature, the authors conducted a Systematic Literature Review (SLR). On whether recurring patterns can emerge from the BPM lifecycles, a normalization process was introduced to homogenize the data and four metrics were used to evaluate the results. The emerging patterns were assembled into a graph that formed the basis for proposing a synthetic BPM lifecycle.
Results
The outcome of the paper is three-fold: First, the identification of four major inefficiencies of existing BPM lifecycles, namely varying 
granularity
, inconsistent nomenclature, subjective polysemy, and lack of formal conceptualization approaches. Also, a standardized definition of the inclusive steps that exist in the lifecycles by clustering the existing ones in a conceptually systematic manner. Finally, a synthetic BPM lifecycle is conceptualized that systematizes the existing concepts and their interrelations based on a formalized BPMN model in two levels of 
granularity
: a basic version that illustrates the functional and control-flow aspects of the BPM lifecycle and an enhanced version incorporating additionally the resource and data perspectives.
Conclusion
This paper proposes a BPMN-based conceptualization of the BPM lifecycle that can facilitate the management of business processes by providing enhanced clarity, improved resource management, and predefined error handling in a BPM initiative. By systematizing the control-flow, data, and resource perspective of the BPM lifecycle, stakeholders can gain a clear understanding of the sequence of steps, the interrelated data flows, and the distribution of work.",Information and Software Technology,04 Mar 2025,9,"The investigation and synthesis of BPM lifecycle models using BPMN can greatly benefit European startups involved in Business Process Management, making it highly impactful."
https://www.sciencedirect.com/science/article/pii/S0950584924000600,Quantum aided efficient resource control for connected support in IRS assisted networks,July 2024,Not Found,Ashu=Taneja: ashu.taneja@chitkara.edu.in; Shalli=Rani: shalli.rani@chitkara.edu.in; Meshal=Alharbi: Mg.alharbi@psau.edu.sa; Muhammad=Zohaib: Muhammad.zohaib@lut.fi,"Abstract
Context:
To achieve the vision of all connected world with uninterrupted communication support, 6G technology plays an important role. But the scarce radio spectrum and limited network resources is the main challenge in delivering its promised performance.
Objectives:
This paper presents an IRS-aided cell free 
NOMA
 network model that aims to provide uniform network coverage. The future 6G technology envisions for serving billions of interconnected devices with seamless communication support, data handling capabilities and computational accuracy. But the scarcity of network resources is the main limitation. Thus, the need is to design quantum enabled intelligent and dynamic networks capable of offering extended network capabilities. Proposed work is on intelligent network framework that provides uniform network coverage through efficient resource management for a 6G enabled expanded 
IoT
 network.
Methods:
To enable efficient resource management, a quantum enabled 
resource control
 algorithm is proposed that creates user clusters and associates each AP-IRS pair to each cluster. Each 
AP
 transmits the superimposed signals of its intended cluster against all the user clusters as in conventional 
NOMA
 system. The nodes in each cluster have been assigned unique pilots so as to avoid intracluster interference. The use of 
IRS
 enables desired 
NOMA
 
beamforming
 such that the effect of unfavourable wireless environment is mitigated.
Results:
The performance of the IRS-aided cell-free 
NOMA
 network is evaluated for average sum rate with different 
AP
 transmit power, cluster sizes, 
IRS
 reflecting elements and IRS phase shifts. It is shown that at transmit power per AP of 30dBm, the average sum rate of the system improves by 7.52% with the proposed algorithm using equal power allocation scheme. Further, the comparative performance analysis of three different communication systems is carried out to validate the proposed communication model.
Conclusion:
It is observed that with more number of users per cluster, the average sum rate of the system initially increases for small cluster sizes and then it becomes constant for large cluster sizes. The proposed clustering method outperforms the random clustering approach achieving sum rate of 12.9 bits/s/Hz with 
N
 =300 and 
M
= 8. The comparison of different communication scenarios reveals that the maximum sum rate of 12.2 bits/s/Hz is achieved with the proposed model incorporating proposed clustering mechanism. Further, the energy efficiency analysis suggests that energy efficiency improves with 
N
 and 
P
c
 with proposed clustering approach. The use case scenarios for the integration of 
quantum computing
 with IRS technology are also presented.",Information and Software Technology,04 Mar 2025,8,"The research on IRS-aided cell free NOMA network model for 6G technology presents valuable insights for improving network capabilities and resource management, which can have a significant impact on European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000272,A method of multidimensional software aging prediction based on ensemble learning: A case of Android OS,June 2024,Not Found,Yuge=Nie: Not Found; Yulei=Chen: Not Found; Yujia=Jiang: Not Found; Huayao=Wu: Not Found; Beibei=Yin: yinbeibei@buaa.edu.cn; Kai-Yuan=Cai: Not Found,"Abstract
Context:
Software aging refers to the phenomenon of 
performance degradation
, increasing failure rate, or system crash due to resource consumption and error accumulation in software systems running for a long time. It has become the key factor affecting software systems’ 
sustainability
. Due to its complex formation reasons, precisely predicting the aging state in actual execution is hard but crucial for enabling proactive measures before a catastrophic situation. 
Machine learning
 (ML) has been employed on this issue.
Objective:
However, previous ML-based prediction methods are single-threaded in the whole process, posing challenges in delivering the desired performance facing diverse user scenarios. To alleviate this problem, we propose a multidimensional software aging prediction method based on 
ensemble learning
 (MSAP).
Method:
In the framework of MSAP, five dimensions, including datasets, labeling metrics, labeling thresholds, algorithms, and model decisions, are extracted and diversified according to aging characteristics and application situations.
Results:
Plenty of experiments have been conducted on 
Android
 devices from three distinct vendors. When subjected to identical workloads, MSAP demonstrates comparable performance to most unidimensional models. While under varied workloads, MSAP outperforms unidimensional models whose performance drops dramatically, demonstrating enhanced adaptability and 
predictive accuracy
.
Conclusion:
MSAP shows exceptional stability while concurrently upholding outstanding prediction precision across a spectrum of user scenarios. It has better generalization characteristics and application prospects.",Information and Software Technology,04 Mar 2025,7,"The multidimensional software aging prediction method based on ensemble learning offers a practical solution to predicting software aging, which can benefit European startups in enhancing their software sustainability and performance."
https://www.sciencedirect.com/science/article/pii/S0950584924000326,Evaluating the effectiveness of a security flaws prevention tool,June 2024,Not Found,Itzhak=Gershfeld: gershitz@post.bgu.ac.il; Arnon=Sturm: sturm@bgu.ac.il,"Abstract
Context:
Securing code is crucial for all software stakeholders. Nevertheless, state-of-the-art tools are imperfect and tend to miss critical errors, resulting in zero-day vulnerabilities. Thus, there is a need for alternatives to mitigate such issues.
Objective:
We aim to facilitate an effective identification mechanism of security flaws in the early stages of development.
Method:
Following our analysis of the root causes of vulnerabilities and examining existing code analyzers, we devise a new Rule-Based Security Flaws Prevention (RbSFP) tool. The tool is based on a set of allow-list rules and consists of the following stages: (1) 
AST
 creation based on the source code and marking critical code areas; (2) Context-based code analysis that further validates the code; (3) Results’ normalization to suggest alerts and warnings. To evaluate the RbSFP tool, we utilized two complementary evaluations. The first refers to the tool’s ability to detect security flaws compared to competing tools by executing them on open-source projects. The second refers to evaluating the tool’s usability and efficiency via a controlled experiment.
Results:
We found that the outcomes were of better quality when using the RbSFP tool, and the differences were statistically significant. Thus, utilizing the new approach and tool has a significant impact as it can eliminate root causes for security flaws at the early stages of development.
Conclusion:
Using an allow-list-based approach can reduce security flaws in the code. However, further analysis and evaluation are needed to provide a more comprehensive solution.",Information and Software Technology,04 Mar 2025,6,"The Rule-Based Security Flaws Prevention tool provides a helpful mechanism for identifying security flaws in the early stages of development, offering potential benefits to European startups in securing their code and preventing vulnerabilities."
https://www.sciencedirect.com/science/article/pii/S0950584924000405,Behaviour-driven development and metrics framework for enhanced agile practices in scrum teams,June 2024,Not Found,Thamizhiniyan=Natarajan: Not Found; Shanmugavadivu=Pichai: p.shanmugavadivu@ruraluniv.ac.in,"Abstract
Context
Agile methodologies
 highlight collaborative efforts among 
software engineering
 groups for iterative, high-quality product delivery within short timeframes. However, Scrum teams face persistent challenges in achieving these objectives, stemming from difficulties in seamless collaboration and effective communication among various roles, such as developers and testers. To address these issues, Scrum teams are increasingly adopting Behaviour-Driven Development (BDD), a testing technique fostering collaboration and shared understanding through test scenarios.
Objectives
This research investigates the adoption of BDD practices in Scrum teams and the formulation of a metrics framework tailored for optimizing Scrum practices and 
product quality
.
Methods
Employing action research, this study extends over two and half years, actively engaging Scrum team members and stakeholders to encompass their collaborative contributions, insights, and perspectives. It commences with defining a metrics framework through exploration within agile teams to measure and evaluate Scrum team performance. Subsequently, the focus shifts to implementing BDD practices systematically, employing training sessions, workshops, and iterative refinements.
Results
The results of the study emphasize the substantial role of Behaviour-Driven Development (BDD) in improving collaboration, communication, and the comprehension of requirements within the Scrum team. Concurrently, the tailored metrics framework bolsters quality assurance practices, enhancing software quality and customer satisfaction. BDD adoption expedites automation and product delivery, while the metrics framework enables informed decision-making.
Conclusions
Combining BDD practices with a custom metrics framework offers a holistic strategy for addressing Scrum challenges. Enhanced collaboration, communication, and requirements comprehension, resulting from BDD, synergize with the metrics framework to elevate Scrum teams' performance, software quality, and customer value. This research underlines the importance of adopting BDD as a testing methodology to achieve these improvements in Scrum teams.",Information and Software Technology,04 Mar 2025,9,"The research on adopting BDD practices in Scrum teams and formulating a metrics framework for optimizing Scrum practices and product quality is highly relevant to European startups, as it addresses common challenges and offers a practical solution for improved collaboration and software delivery."
https://www.sciencedirect.com/science/article/pii/S0950584924000594,Role of quantum computing in shaping the future of 6 G technology,June 2024,"6 G technology, Quantum computing, Challenges, Opportunities",Muhammad Azeem=Akbar: azeem.akbar@ymail.com; Arif Ali=Khan: Not Found; Sami=Hyrynsalmi: Not Found,"Abstract
Context
The emergence of 6 G technology heralds a groundbreaking era in digital connectivity, envisaging universal and seamless links. To address the intricate computational and security requirements of this revolution, the integration of 
quantum computing
 (QC) into these networks is perceived as a promising solution.
Objective
The objective this study presents a comprehensive investigation into the potential roles and implications of QC within the context of 6 G technology.
Methodology
To address the objectives of this study, firstly, we have conducted literature survey to identify the key applications of using QC in 6 G technology. Secondly, we performed interview study with industry experts to identify the best practices related to the key application of QC in 6 G technology.
Results
Our study unfolds in two distinct stages: firstly, we identify 15 key applications of QC in 6 G technology and segmented into 4 core areas. Secondly, the literature findings were empirically validated by conducting interview study and identified 49 best practices related to one of the identified key applications of QC in 6 G technology.
Conclusion
The outcomes of this research lay a solid foundation for understanding both the pivotal applications of QC in 6 G technology and the effective practices for its implementation, thus providing valuable insights to both academics and industry practitioners.",Information and Software Technology,04 Mar 2025,7,"The investigation into the potential roles of quantum computing in 6G technology provides valuable insights for future technological advancements, although the direct impact on European early-stage ventures may be more long-term."
https://www.sciencedirect.com/science/article/pii/S0950584924000284,Model driven engineering for machine learning components: A systematic literature review,May 2024,"Model driven engineering, Software engineering, Artificial intelligence, Machine learning, Systematic literature review",Hira=Naveed: hira.naveed@monash.edu; Chetan=Arora: chetan.arora@monash.edu; Hourieh=Khalajzadeh: hourieh.khalajzadeh@deakin.edu.au; John=Grundy: john.grundy@monash.edu; Omar=Haggag: omar.haggag@monash.edu,"Abstract
Context:
Machine Learning
 (ML) has become widely adopted as a component in many modern 
software applications
. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, 
anomaly detection
, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and 
software engineering
. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as 
modeling languages
, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.",Information and Software Technology,04 Mar 2025,7,"The study addresses a relevant intersection of MDE with ML, providing insights for researchers and practitioners in the field."
https://www.sciencedirect.com/science/article/pii/S0950584924000119,Context-based statement-level vulnerability localization,May 2024,Not Found,Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn,"Abstract
Context:
The number of attacks exploring software vulnerabilities has dramatically increased, which has caused various severe damages. Thus, early and accurately detecting vulnerabilities becomes essential to guarantee software quality and prevent the systems from 
malicious attacks
. Multiple automated 
vulnerability detection
 approaches have been proposed and obtained promising results. However, most studies detect vulnerabilities at a coarse-grained, i.e., file or method level. Thus, developers still have to spend significant investigation efforts on localizing vulnerable statements.
Objective:
In this paper, we introduce 
COSTA
, a novel context-based approach to localize vulnerable statements.
Method:
In particular, given a vulnerable function, 
COSTA
 identifies vulnerable statements based on their suspiciousness scores. Specifically, the suspiciousness of each statement is measured according to its semantics captured by four contexts, including 
operation context, dependence context, surrounding context
, and 
vulnerability type
.
Results:
Our experimental results on a large vulnerability dataset show that 
COSTA
 outperforms the state-of-the-art approaches up to 
96%
 in F1-score and 
167%
 in Accuracy. 
COSTA
 also surpasses these approaches up to 
two times
 in Top-1 Accuracy. Especially, 
COSTA
 obtains about 
80% at Top-3 Recall
. In other words, developers can find about 80% of the vulnerable statements by investigating only three first-ranked statements in each function.
Conclusion:
COSTA
 effectively addresses the challenge of statement-level vulnerability localization by leveraging multiple contextual features. Our experimental results show that 
COSTA
 outperforms existing state-of-the-art approaches. With the ability to accurately and efficiently identify vulnerable statements, developers can better allocate their investigation efforts, reduce the risk of potential security threats, and ensure software quality and security in real-world applications.",Information and Software Technology,04 Mar 2025,9,"The novel approach COSTA shows significant improvements in vulnerability detection, with practical implications for software quality and security."
https://www.sciencedirect.com/science/article/pii/S0950584924000132,UXH-GEDAPP: A set of user experience heuristics for evaluating generative design applications,April 2024,Not Found,Daniela=Quiñones: daniela.quinones@pucv.cl; Claudia=Ojeda: Not Found; Rodrigo F.=Herrera: Not Found; Luis Felipe=Rojas: Not Found,"Abstract
Context
Traditional building and infrastructure design methodologies are inflexible and inefficient, leading to high costs and environmental damage. Generative design, with an algorithm that provides multiple options, could be a potential solution. The challenge is creating an intuitive, user-friendly application that optimizes engineers’ time, reducing manual iterations and lead to a good 
user experience
 (UX). A method for evaluating the UX is 
heuristic evaluation
, in which heuristics are used to inspect a software product.
Objective
Since generative design applications have specific features, generic heuristics may not detect all problems related to UX. This article presents a novel set of 9 heuristics to evaluate UX in generative design applications: UXH-GEDAPP. This set is focused on evaluating both UX attributes and specific features of generative design applications.
Method
A formal methodology was used to develop the heuristics, through 7 stages: exploratory, descriptive, correlational, selection, specification, validation, and refinement. We performed 3 iterations and validated UXH-GEDAPP in 2 iterations through: 
heuristic evaluation
, 
expert judgment
, and user test. Since the methodology can be applied iteratively, we validated and refined the set to improve the proposal.
Results
The results obtained in the validation stage indicate that UXH-GEDAPP is useful and more effective than generic heuristics when evaluating generative design applications. UXH-GEDAPP allows to detect specific usability/UX problems as well as more severe problems related to generative design applications. Furthermore, 
evaluators
 made fewer errors associating the detected problems with the proposed heuristics, compared to generic sets.
Conclusion
UXH-GEDAPP is a new set of heuristics that encourages the creation and use of generative design applications with good UX. It can detect usability/UX problems and help correct them, as well as guide the development of new generative design applications for a pleasant and intuitive 
user experience
.",Information and Software Technology,04 Mar 2025,8,The development of specific heuristics for evaluating UX in generative design applications offers practical guidance for improving user experience.
https://www.sciencedirect.com/science/article/pii/S0950584923002306,Technical debt management automation: State of the art and future perspectives,March 2024,"Systematic mapping study, Technical debt, Technical debt management, Tools, Automation",João Paulo=Biazotto: j.p.biazotto@rug.nl; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl; Elisa Yumi=Nakagawa: elisa@icmc.usp.br,"Abstract
Context:
Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system’s maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most.
Objectives:
The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM.
Methods:
We conducted a 
systematic mapping study
 (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation.
Results:
We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges.
Conclusion:
The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts.",Information and Software Technology,04 Mar 2025,6,"The overview of TDM automation contributes to understanding the research landscape, but may have limited immediate practical impact for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923002197,Experiences from conducting rapid reviews in collaboration with practitioners — Two industrial cases,March 2024,"Literature reviews, Systematic review, Rapid reviews, Research relevance, Industry-academia collaboration",Sergio=Rico: Sergio.Rico@cs.lth.se; Nauman Bin=Ali: nauman.ali@bth.se; Emelie=Engström: emelie.engstrom@cs.lth.se; Martin=Höst: martin.host@cs.lth.se,"Abstract
Context:
Evidence-based 
software engineering
 (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice.
Objective:
This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and 
industry
. Through the lens of two 
case studies
, we delve into the practical application and experience of conducting RRs.
Methods:
We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We 
collected data
 through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis.
Results:
We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners.
Conclusion:
The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange.",Information and Software Technology,04 Mar 2025,7,The exploration of Rapid Reviews for fostering knowledge exchange between academia and industry provides insights for improving research utilization in practice.
https://www.sciencedirect.com/science/article/pii/S0950584923002318,Towards automating self-admitted technical debt repayment,March 2024,"Self-admitted technical debt, Software quality, Software maintenance, Software analytics, Deep learning, Technical debt repayment",Abdulaziz=Alhefdhi: aa043@uowmail.edu.au; Hoa Khanh=Dam: hoa@uow.edu.au; Aditya=Ghose: aditya@uow.edu.au,"Abstract
Context:
Self-Admitted Technical Debt (SATD) refers to the technical debt in software that is explicitly flagged, typically by the 
source code
 comment. The SATD literature has mainly focused on comprehending, describing, detecting, and recommending SATD. Most recently, there have been efforts to study the state of the code before and after removing the SATD comment. While these efforts serve as a preliminary step towards the repayment of SATD, actual attempts towards automating SATD repayment, to the best of our knowledge, are yet to be made.
Objective:
In this paper, we propose the first attempt towards direct, complete, and automated SATD repayment by providing two main contributions. The first contribution is an empirical study of how the SATD comment relates to repaying the debt. The second contribution is 
DLRepay
, our deep 
learning approach
 for SATD repayment.
Method:
We developed a SATD Repayment dataset, namely SATD-R, and established a taxonomy based on the relationship and helpfulness of the SATD comment to/in repaying the debt. In addition, we developed 
DLRepay
 which takes as an input a pair of SATD comment and code, and generates a new, TD-free code.
Results:
We found that there are five different categories in which the SATD comment relates to Technical Debt repayment. We also identify when the SATD comment has a positive and logical connection to repaying the debt, both generally and in every category. Furthermore, we illustrate the results of our SATD repayment approach across two datasets, three input types, two output types, and two 
neural networks
.
Conclusion:
The resulting taxonomy of our empirical study paves the way for research to tackle further in-depth questions concerning SATD repayment comprehension, identification, and automation. In addition, the various experimental setups we conduct provide multiple insights regarding the applicability of our SATD repayment approach.",Information and Software Technology,04 Mar 2025,8,"The automation of SATD repayment through DLRepay presents a significant advancement in software development and technical debt management, with potential impact on European startups."
https://www.sciencedirect.com/science/article/pii/S0950584923002082,Developer and End-User Perspectives on Addressing Human Aspects in Mobile eHealth Apps,February 2024,"eHealth App, Human Aspect, User Study, App Development, Stakeholders Perspectives",Md.=Shamsujjoha: md.shamsujjoha@monash.edu; John=Grundy: john.grundy@monash.edu; Hourieh=Khalajzadeh: hkhalajzadeh@deakin.edu.au; Qinghua=Lu: qinghua.lu@data61.csiro.au; Li=Li: lilicoding@ieee.org,"Abstract
Context:
eHealth apps are mobile apps that help in self-management of critical illnesses, provide home-based disease management, and help with personalized care. Users of eHealth apps are naturally very diverse in terms of their 
human aspects
, e.g., their age, gender, emotional reactions to the apps, cognitive style, physical and mental challenges. Unfortunately, many eHealth apps do not take these user differences sufficiently into account, making them ineffective or even unusable.
Objective:
This paper reports a study from eHealth app stakeholders’ – developers and end-users – perspectives on critical challenges and benefits of better incorporating 
human aspects
 into eHealth app development and usage. We also investigate how different 
human aspects
 are being addressed by developers, which ones are the most important for different user groups, and which ones are currently missing/poorly handled.
Method:
A mixed-method approach that integrates qualitative and quantitative research was used for this study. We gathered and analyzed data from 240 online survey responses and 25 detailed interviews within the same study and validated the results.
Results:
We report key issues encountered in eHealth app design, difficulty in addressing different 
human aspects
, areas requiring further research and practical assistance, and recommend our findings to best address these challenges. We found addressing 
human aspects
 throughout the app development life-cycle is beneficial for more effective eHealth apps. Our findings also suggest the need for improved standards and guidelines, better developer-user collaborative culture, and better 
human aspects
 education to produce more effective eHealth apps.
Conclusion:
This paper investigates current approaches used in the eHealth app domain that take into account the 
human aspects
 of app users. The paper guides eHealth app stakeholders, future researchers, academia and industry partners be aware of 
human aspects
 related challenges and improve produce apps.",Information and Software Technology,04 Mar 2025,6,"Addressing human aspects in eHealth app development is important, but the study, while valuable, may have limited direct impact on early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584923002239,To change or not to change? Modeling software system interactions using Temporal Graphs and Graph Neural Networks: A focus on change propagation,February 2024,Not Found,Manuella=Germanos: Not Found; Danielle=Azar: danielle.azar@lau.edu.lb; Eileen Marie=Hanna: Not Found,"Abstract
Context:
The world is quickly adopting new technologies and evolving to rely on software systems for the simplest tasks. This prompts developers to expand their software systems by adding new product features. However, this expansion should be cautiously tackled to prevent the degradation of the quality of the software product.
Objective:
One challenge when modifying code – whether to patch a bug or add a feature – is knowing which components will be affected by the change and amending possible misbehavior. In this context, the study of change propagation or the impact of introducing a change is needed. By investigating how changing one component may impact the functionality of a dependency (another component), developers can prevent unexpected behavior and maintain the quality of their system.
Methods:
In this work, we tackle the change propagation problem by modeling a software system as a 
temporal graph
 where nodes represent system files and edges co-changeability, i.e., the tendency of two files to change together. The 
graph representation
 is temporal so that nodes and edges can change with time, reflecting the addition of files in the system and changes in dependencies. We then employ a Temporal Graph Network and a Long Short-Term Memory model to predict which other files will be impacted by a modification performed on a file.
Results:
We test our model on software systems of different functionality, size, and nature. We compare our results to other published work, and our model shows a significantly higher ability to predict files impacted by a change.
Conclusion:
The proposed approach effectively predicts change propagation in software systems and can guide developers and software engineers in planning the change and estimating the cost in terms of time and money.",Information and Software Technology,04 Mar 2025,9,"Predicting change propagation in software systems can have a substantial positive impact on software quality and development efficiency, making it highly relevant for European startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001891,Microservice-based projects in agile world: A structured interview,January 2024,Not Found,Hüseyin=Ünlü: huseyinunlu@iyte.edu.tr; Dhia Eddine=Kennouche: Not Found; Görkem Kılınç=Soylu: Not Found; Onur=Demirörs: Not Found,"Abstract
Context
During the last decade
,
 Microservice-based software architecture (MSSA) has been a preferred design paradigm for a growing number of companies. MSSA, specifically in the form of reactive systems, has substantial differences from the more conventional design paradigms, such as object-oriented analysis and design. Therefore, adaptation demands software organizations to transform their culture. However, there is a lack of research studies that explore 
common practices
 utilized by software companies that implement MSSAs.
Objective
In this study
,
 our goal is to get an insight into how practices such as an 
agile methodology
, software analysis, design, test, size measurement, and effort estimation are performed in software projects which embrace the Microservice-based software architecture paradigm. Together with the identification of practices utilized for the MSSA paradigm, we aim to determine the challenges organizations face to adopt microservice-based software architectures.
Method
We performed a structured interview with participants coming from 20 different organizations over different roles, domains, and countries to collect information on their views, experience, and the challenges faced.
Results
Our results reveal that organizations find 
agile development
 compatible with microservices. In general, they continue to use traditional object-oriented 
modeling notations
 for analysis and design in an abstract way. They continue to use the same subjective size measurement and effort estimation approaches that they were using previously in traditional architectures. However, they face unique challenges in developing microservices.
Conclusion
Although organizations face challenges, practitioners continue to use familiar techniques that they have been using for traditional architectures. The results provide a snapshot of the software industry that utilizes microservices.",Information and Software Technology,04 Mar 2025,7,"Investigating common practices and challenges in implementing MSSA provides useful insights, but its practical value for early-stage European ventures may be somewhat limited."
https://www.sciencedirect.com/science/article/pii/S0950584923002045,QLSN: Quantum key distribution for large scale networks,January 2024,Not Found,Cherry=Mangla: Not Found; Shalli=Rani: shallir79@gmail.com; Ahmed=Abdelsalam: ahmed.abdelsalam@lut.fi,"Abstract
Context:
Key management among large-scale networks is still a challenging issue considering the limited information about adversaries (whether quantum or classical). One solution for such an issue is to use quantum-inspired to which refers to a technology, an algorithm, or a strategy that uses conventional computers to execute 
quantum physics
 concepts. These techniques make an effort to imitate particular 
quantum computing
 properties, such as superposition, 
entanglement
, or 
quantum parallelism
, in order to more effectively or creatively handle particular issues. It can provide security against all types of intruders.
Objectives:
In this article, inspired by 
Quantum Key Distribution
, we proposed a key-management protocol for enhancing the security and reliability of the network. Two major objectives have been taken into consideration during the proposal of QLSN (proposed protocol), (i) reduced risk if the network needs to transfer the long keys and (ii) a reliable network to transmit the information safely.
Methods:
(i) QLSN is proposed in the place of the classical Diffie–Hellman key exchange algorithm, which makes it secure against attacks during the transmission of data through the IPSec tunnel. (ii) 
Risk Analysis
 is performed on 1-bit, 2 bits, 15 bits, and 50 bits sizes of data. 
Data transmission
 is tested for risk analysis and the probability of attacks is checked under different scenarios.
Results:
The simulation results illustrated the better performance of the proposed protocol in different scenarios with large-scale networks compared to the existing Diffie–Hellman key exchange used in the IPSec protocol proposed by CISCO. Using 
quantum algorithms
 and processing advantages, quantum adversaries may be able to take advantage of weaknesses in conventional security systems.
Conclusion:
An essential step in determining and validating the viability of proposed protocol inside a 
quantum computing
 framework is the integration of a QLSN protocol into the IBM Qiskit simulator. By addressing pressing concerns, this validation approach clarifies the performance and security consequences of large-scale networks. Looking ahead, the fusion of 
artificial intelligence
 and 
quantum computing
 promises fresh methods for key management that will improve 
network security
. In addition, Turing machines–classical computing tools that bridge the gap between the classical and 
quantum computing
 paradigms–will continue to play crucial roles in the complex world of large-scale networks.",Information and Software Technology,04 Mar 2025,9,"The proposed QLSN protocol for key management in large-scale networks represents a significant advancement, leveraging quantum-inspired techniques, with potential benefits for European startups in the tech industry."
https://www.sciencedirect.com/science/article/pii/S0950584923001805,The role of Reinforcement Learning in software testing,December 2023,"Software testing, Machine learning, Reinforcement Learning, Artificial intelligence",Amr=Abo-eleneen: aa1405465@student.qu.edu.qa; Ahammed=Palliyali: ap1304567@student.qu.edu.qa; Cagatay=Catal: ccatal@qu.edu.qa,"Abstract
Context:
Software testing is applied to validate the behavior of the software system and identify flaws and bugs. Different 
machine learning technique
 types such as supervised and 
unsupervised learning
 were utilized in software testing. However, for some complex software testing scenarios, neither supervised nor unsupervised machine learning techniques were adequate. As such, researchers applied 
Reinforcement Learning
 (RL) techniques in some cases. However, a 
systematic overview
 of the state-of-the-art on the role of reinforcement learning in software testing is lacking.
Objective:
The objective of this study is to determine how and to what extent RL was used in software testing.
Methods:
In this study, a Systematic Literature Review (SLR) was conducted on the use of RL in software testing, and 40 primary studies were investigated.
Results:
This study highlights different software testing types to which RL has been applied, commonly used RL algorithms and architecture for learning, challenges faced, advantages and disadvantages of using RL, and the performance comparison of RL-based models against other techniques.
Conclusions:
RL has been widely used in software testing but has almost narrowed to two applications. There is a shortage of papers using advanced RL techniques in addition to multi-agent RL. Several challenges were presented in this study.",Information and Software Technology,04 Mar 2025,8,"The study provides a systematic overview of the role of reinforcement learning in software testing, highlighting challenges, advantages, and performance comparisons. This can have a significant impact on improving software testing techniques for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001702,Towards a successful secure software acquisition,December 2023,Not Found,Faisal=Alnaseef: Not Found; Mahmood=Niazi: mkniazi@kfupm.edu.sa; Sajjad=Mahmood: Not Found; Mohammad=Alshayeb: Not Found; Irfan=Ahmad: Not Found,"Abstract
Context
Security is a critical attribute 
of software quality
. Organizations invest considerable sums of money in protecting their assets. Despite investing in secure infrastructure, organizations remain prone to security risks and cyberattacks that exploit security flaws. Many factors contribute to the challenges related to software security, e.g., the exponential increase in Internet-enabled applications, threats from hackers, and the susceptibility of inexperienced Internet users. Moreover, organizations tend to procure off-the-shelf software from third-party suppliers. However, gaining a complete understanding of ways to assess suppliers’ readiness to provide secure software before selecting a supplier is imperative.
Objective
We have developed a readiness model for secure software acquisition (RMSSA) to help software organizations select suppliers who can provide secure software.
Method
We employed state-of-the-art techniques based on systematic literature review to determine the best practices undertaken by organizations in terms of acquiring secure software, which depends on six core security knowledge areas: confidentiality, integrity, availability, authorization, 
authentication
, and accountability.
Results
We evaluated the RMSSA theoretically and in a practical environment based on three 
case studies
 with software organizations. Our findings can guide software organizations in selecting the supplier who can develop secure software.
Conclusion
The proposed RMSSA can be used to evaluate suppliers’ readiness to provide secure software.",Information and Software Technology,04 Mar 2025,7,"The RMSSA model developed in this study can help software organizations select secure software suppliers, addressing the critical attribute of software security. This can be beneficial for early-stage ventures in ensuring the security of their products."
https://www.sciencedirect.com/science/article/pii/S0950584923001878,Towards accurate recommendations of merge conflicts resolution strategies,December 2023,Not Found,Paulo=Elias: pauloe@id.uff.br; Heleno de S.=Campos: helenocampos@id.uff.br; Eduardo=Ogasawara: eogasawara@ieee.org; Leonardo Gresta Paulino=Murta: leomurta@ic.uff.br,"Abstract
Context:
in 
software engineering
, developers working concurrently on a project frequently need to merge changes in the source code. The manual resolution of merge conflicts is a laborious and time-consuming task. Some studies have investigated the nature of merge conflicts and proposed methods to predict, mitigate, and resolve conflicts. However, the automatic resolution of conflicts is still an open problem.
Objective:
in this paper, we design and evaluate MESTRE (MErge STrategy REcommender), a conflict resolution strategy recommender that predicts the merge resolution strategy among version 1, version 2, concatenation of version 1 and 2, concatenation of version 2 and 1, combination of lines from version 1 and 2, and manually writing new code. For the first four strategies, MESTRE is able to not only recommend the strategy but also automatically resolve the conflict.
Methods:
we 
collected data
 from 20 open-source projects with more than 1000 merge conflicts each. Using this data, we trained and evaluated a separate classifier for each project to predict the conflict resolution strategy for a conflicting chunk.
Results:
MESTRE achieved an overall average accuracy of 80.8% among all projects. It represents a normalized improvement of 54.8% over the 
majority class
 baseline. Furthermore, since MESTRE can provide the exact conflict resolution for the most frequent conflict resolution strategies, it could automatically resolve 70.5% of the conflicts. We also found that 
attributes related
 to the conflicting chunk notably impact the 
classification accuracy
 more than those related to the merge and the file.
Conclusion:
This paper makes the following contributions: (1) MESTRE, a tool to predict merge conflict resolutions based on attributes of a Git repository; (2) an analysis of the relevance of attributes to the prediction of resolution strategies; and (3) an ablation study to find the contribution of each group of attributes to MESTRE’s performance.",Information and Software Technology,04 Mar 2025,9,"The MESTRE tool designed in this paper can predict and automatically resolve merge conflict resolutions with high accuracy. This can greatly benefit software development processes for early-stage ventures, saving time and effort in conflict resolution."
https://www.sciencedirect.com/science/article/pii/S0950584923001416,MDSSED: A safety and security enhanced model-driven development approach for smart home apps,November 2023,Not Found,Tong=Ye: yetong@nuaa.edu.cn; Yi=Zhuang: zy16@nuaa.edu.cn; Gongzhe=Qiao: qgz@nuaa.edu.cn,"Abstract
Context:
With the popularization of 
smart home devices
, people rely more on automation functions provided by 
smart home
 apps. This increases the attack surface for safety and security threats. Many of these threats are at the interaction level, caused by unintended or malicious interactions between apps.
Objective:
Most of the current studies focus on identifying unsafe interactions between 
smart home
 apps by code analysis. To the best of our knowledge, none of the existing studies focuses on enhancing the safety and security of smart home apps under interaction threats in the design phase. To fill this gap, this paper presents MDSSED, a safety and security enhanced model-driven development approach for smart home apps.
Method:
First, this paper identifies eleven types of interaction threats faced by smart home apps. Second, the MDSSED profile is proposed to support modeling smart home apps using 
UML
. Third, the MDSSED prototype tool is developed to generate threat models and corresponding safety and security properties automatically. Then, the safety and security properties are automatically verified by model checking. Finally, the MDSSED tool automatically converts the 
UML models
 to the Samsung SmartThings apps.
Results:
To evaluate the accuracy and effectiveness of MDSSED, this paper uses the benchmarks in existing state-of-the-art studies. The results show that MDSSED not only identified the safety and security problems in the existing benchmarks but also pointed out vulnerabilities of apps under other interaction threats identified in this paper.
Conclusion:
To the best of our knowledge, MDSSED is the first model-driven development approach that supports the automatic verification of the safety and security properties of smart home apps under interaction threats. The accuracy, practicality, and efficiency of MDSSED are corroborated by experiments. The 
source code
 of the MDSSED tool and the experimental data are available online.
1",Information and Software Technology,04 Mar 2025,6,"The MDSSED model-driven development approach presented in this paper addresses safety and security threats in smart home apps, which can be valuable for early-stage ventures developing IoT products. However, the impact may be more specific to ventures working in this domain."
https://www.sciencedirect.com/science/article/pii/S0950584923001532,Code review guidelines for GUI-based testing artifacts,November 2023,"GUI testing, GUI-based testing, Software testing, Code review, Modern code review, Guidelines, Practices",Andreas=Bauer: andreas.bauer@bth.se; Riccardo=Coppola: riccardo.coppola@polito.it; Emil=Alégroth: emil.alegroth@bth.se; Tony=Gorschek: tony.gorschek@bth.se,"Abstract
Context:
Review of software artifacts, such as source or test code, is a 
common practice
 in industrial practice. However, although review guidelines are available for source and low-level test code, for GUI-based testing artifacts, such guidelines are missing.
Objective:
The goal of this work is to define a set of guidelines from literature about production and test code, that can be mapped to GUI-based testing artifacts.
Method:
A systematic literature review is conducted, using white and gray literature to identify guidelines for source and test code. These synthesized guidelines are then mapped, through examples, to create actionable, and applicable, guidelines for GUI-based testing artifacts.
Results:
The results of the study are 33 guidelines, summarized in nine guideline categories, that are successfully mapped as applicable to GUI-based testing artifacts. Of the collected literature, only 10 sources contained test-specific code review guidelines. These guideline categories are: 
perform automated checks, use checklists, provide context information, utilize metrics, ensure readability, visualize changes, reduce complexity, check conformity with the requirements
 and 
follow design principles and patterns
.
Conclusion:
This pivotal set of guidelines provides an industrial contribution in filling the gap of general guidelines for review of GUI-based testing artifacts. Additionally, this work highlights, from an academic perspective, the need for future research in this area to also develop guidelines for other specific aspects of GUI-based testing practice, and to take into account other facets of the review process not covered by this work, such as reviewer selection.",Information and Software Technology,04 Mar 2025,5,"The set of guidelines provided in this work for review of GUI-based testing artifacts can be useful for software development teams, including early-stage ventures. While valuable, the impact may not be as significant as some other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001532,Code review guidelines for GUI-based testing artifacts,November 2023,"GUI testing, GUI-based testing, Software testing, Code review, Modern code review, Guidelines, Practices",Andreas=Bauer: andreas.bauer@bth.se; Riccardo=Coppola: riccardo.coppola@polito.it; Emil=Alégroth: emil.alegroth@bth.se; Tony=Gorschek: tony.gorschek@bth.se,"Abstract
Context:
Review of software artifacts, such as source or test code, is a 
common practice
 in industrial practice. However, although review guidelines are available for source and low-level test code, for GUI-based testing artifacts, such guidelines are missing.
Objective:
The goal of this work is to define a set of guidelines from literature about production and test code, that can be mapped to GUI-based testing artifacts.
Method:
A systematic literature review is conducted, using white and gray literature to identify guidelines for source and test code. These synthesized guidelines are then mapped, through examples, to create actionable, and applicable, guidelines for GUI-based testing artifacts.
Results:
The results of the study are 33 guidelines, summarized in nine guideline categories, that are successfully mapped as applicable to GUI-based testing artifacts. Of the collected literature, only 10 sources contained test-specific code review guidelines. These guideline categories are: 
perform automated checks, use checklists, provide context information, utilize metrics, ensure readability, visualize changes, reduce complexity, check conformity with the requirements
 and 
follow design principles and patterns
.
Conclusion:
This pivotal set of guidelines provides an industrial contribution in filling the gap of general guidelines for review of GUI-based testing artifacts. Additionally, this work highlights, from an academic perspective, the need for future research in this area to also develop guidelines for other specific aspects of GUI-based testing practice, and to take into account other facets of the review process not covered by this work, such as reviewer selection.",Information and Software Technology,04 Mar 2025,6,The guidelines for GUI-based testing artifacts can be beneficial for European early-stage ventures but the impact might be limited to a specific group of startups.
https://www.sciencedirect.com/science/article/pii/S0950584923001404,Reflections on Surrogate-Assisted Search-Based Testing: A Taxonomy and Two Replication Studies based on Industrial ADAS and Simulink Models,November 2023,Not Found,Shiva=Nejati: snejati@uottawa.ca; Lev=Sorokin: sorokin@fortiss.org; Damir=Safin: safin@fortiss.org; Federico=Formica: formicaf@mcmaster.ca; Mohammad Mahdi=Mahboob: mahbom2@mcmaster.ca; Claudio=Menghi: claudio.menghi@unibg.it,"Abstract
Context:
Surrogate-assisted search-based testing (SA-SBT) aims to reduce the 
computational time
 for testing compute-intensive systems. Surrogates enhance testing techniques by improving test case generation focusing the testing budget on the most critical portions of the input domain. In addition, they can serve as 
approximations
 of the system under test (SUT) to predict 
test results
 instead of executing the tests on compute-intensive SUTs.
Objective:
This article reflects on the existing SA-SBT techniques, particularly those applied to system-level testing and often facilitated using simulators or complex test beds. Recognizing the diversity of 
heuristic algorithms
 and evaluation methods employed in existing SA-SBT techniques, our objective is to synthesize these differences and present a comprehensive view of SA-SBT solutions. In addition, by critically reviewing our previous work on SA-SBT, we aim to identify the limitations in our proposed algorithms and evaluation methods and to propose potential improvements.
Method:
We present a taxonomy that categorizes and contrasts existing SA-SBT solutions and highlights key research gaps. To identify the evaluation challenges, we conduct two replication studies of our past SA-SBT solutions: One study uses industrial 
advanced driver assistance system
 (ADAS) and the other relies on a 
Simulink
 
model benchmark
. We compare our results with those of the original studies and identify the difficulties in evaluating SA-SBT techniques, including the impact of different contextual factors on results generalization and the validity of our 
evaluation metrics
.
Results:
Based on our taxonomy and replication studies, we propose future research directions, including re-considerations in the current 
evaluation metrics
 used for SA-SBT solutions, utilizing surrogates for 
fault localization
 and repair in addition to testing, and creating frameworks for large-scale experiments by applying SA-SBT to multiple SUTs and simulators.",Information and Software Technology,04 Mar 2025,8,The study on SA-SBT techniques can have a significant impact on European startups by reducing computational time and improving testing techniques for critical portions of the input domain.
https://www.sciencedirect.com/science/article/pii/S0950584923001192,Information needs and presentation in agile software development,October 2023,"Software engineering, Agile software development, DevOps, Information needs, Visualization",Henri=Bomström: henri.bomstrom@oulu.fi; Markus=Kelanti: markus.kelanti@oulu.fi; Elina=Annanperä: elina.annanpera@oulu.fi; Kari=Liukkunen: kari.liukkunen@oulu.fi; Terhi=Kilamo: terhi.kilamo@tuni.fi; Outi=Sievi-Korte: outi.sievi-korte@tuni.fi; Kari=Systä: kari.systa@tuni.fi,"Abstract
Context:
Agile software companies applying the 
DevOps
 approach require collaboration and information sharing between practitioners in various roles to produce value. Adopting new development practices affects how practitioners collaborate, requiring companies to form a closer connection between business strategy and software development. However, the types of information management, sales, and development needed to plan, evaluate features, and reconcile their expectations with each other need to be clarified.
Objective:
To support practitioners in collaborating and realizing changes to their practices, we investigated what information is needed and how it should be represented to support different stakeholders in their tasks. Compared to earlier research, we adopted a holistic approach – by including practitioners throughout the 
development process
 – to better understand the information needs from a broader viewpoint.
Method:
We conducted six workshops and 12 semi-structured interviews at three Finnish small and medium-sized enterprises from different software domains. Thematic analysis was used to identify information-related issues and information and visualization needs for daily tasks. Three themes were constructed as the result of our analysis.
Results:
Visual information representation catalyzes stakeholder discussion, and supporting information exchange between 
stakeholder groups
 is vital for efficient collaboration in software product development. Additionally, user-centric data collection practices are needed to understand how software products are used and to support practitioners’ daily information needs. We also found that a passive way of representing information, such as a dashboard that would disturb practitioners only when attention is needed, was preferred for daily information needs.
Conclusion:
The 
software engineering
 community should consider reviewing the information needs of practitioners from a more holistic view to better understand how tooling support can benefit information exchange between stakeholder groups when making product development decisions and how those tools should be built to accommodate different stakeholder views.",Information and Software Technology,04 Mar 2025,7,The research on information needs for collaboration in software development can provide valuable insights for European early-stage ventures in terms of better information exchange and collaboration between stakeholder groups.
https://www.sciencedirect.com/science/article/pii/S0950584923001222,A survey on dataset quality in machine learning,October 2023,"Dataset, Dataset quality, Machine Learning",Youdi=Gong: Not Found; Guangzhen=Liu: Not Found; Yunzhi=Xue: Not Found; Rui=Li: Not Found; Lingzhong=Meng: lingzhong@iscas.ac.cn,"Abstract
With the rise of big data, the quality of datasets has become a crucial factor affecting the performance of 
machine learning
 models. High-quality datasets are essential for the realization of data value. This survey article summarizes the research direction of dataset quality in machine learning, including the definition of related concepts, analysis of quality issues and risks, and a review of dataset quality dimensions and metrics throughout the dataset lifecycle and a review of dataset quality metrics analyzed from a dataset lifecycle perspective and summarized in literatures. Furthermore, this article introduces a comprehensive quality evaluation process, which includes a framework for dataset quality evaluation with dimensions and metrics, computation methods for quality metrics, and assessment models. These studies provide valuable guidance for evaluating dataset quality in the field of machine learning, which can help improve the accuracy, efficiency, and 
generalization ability
 of machine learning models, and promote the development and application of 
artificial intelligence
 technology.",Information and Software Technology,04 Mar 2025,9,"The survey on dataset quality in machine learning can be highly valuable for European startups working with AI and machine learning models as it provides guidance for evaluating dataset quality and improving accuracy, efficiency, and generalization ability."
https://www.sciencedirect.com/science/article/pii/S0950584923001052,"Retrieving arXiv, SocArXiv, and SSRN metadata for initial review screening",September 2023,Not Found,Rubia=Fatima: rubiafatima91@hotmail.com; Affan=Yasin: affan.yasin@tsinghua.edu.cn; Lin=Liu: linliu@tsinghua.edu.cn; Jianmin=Wang: jimwang@tsinghua.edu.cn; Wasif=Afzal: wasif.afzal@mdh.se,"Abstract
Context:
Researchers around the globe invest a lot of time searching the literature for performing reviews (Systematic Literature Review (SLR), Multivocal Literature Review (MLR)). The steps to performing the review includes inclusion of the grey literature, preprints, and quality assessed non-peer reviewed literature (the purpose is to minimize the publication bias). The 
initial screening
 of the papers takes time and 
bibliographic information
 is only available online for the researcher(s).
Objective:
Objective of our study is to propose, design, and develop a method that will help the research community to download the basic information of the papers (title, abstract, author) for the searched query from arxiv, SSRN, and SocArxiv (Social Science ArXiv).
Method:
We used Web scraping to extract data from the servers and save it in excel file. To retrieve the desired query from the databases, a Python code is used. Two methods have been discussed in the study to download the metadata of the searched query.
Results:
We have used different queries (such as “grey literature”, “testing software”, and “python” etc.) to see the results of our proposed method. Furthermore, we cross-verified the results with the online search results of the databases.
Conclusion:
Initial results from the preliminary pilot evaluations show that it is a viable method to search, download, and shortlist the research articles information (title, abstract etc.) from arXiv,
1
 SSRN,
2
 and SocArXiv.
3
 For external validity more evaluations are needed.",Information and Software Technology,04 Mar 2025,4,"While the proposed method for downloading paper information may be useful for researchers, its direct impact on European early-stage ventures or startups may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584923000988,A mixed method study of DevOps challenges,September 2023,Not Found,Minaoar Hossain=Tanzil: minaoar.tanzil@ucalgary.ca; Masud=Sarker: Not Found; Gias=Uddin: Not Found; Anindya=Iqbal: Not Found,"Abstract
Context:
DevOps
 practices combine software development and IT (Information Technology) operations. The continuous needs for rapid but quality software development requires the adoption of high-quality 
DevOps
 tools. There is a growing number of DevOps related posts in popular online developer forum Stack Overflow (SO). While previous research analyzed SO posts related to build/release engineering, we are aware of no research that specifically focused on DevOps related discussions.
Objective:
This paper aims to learn the challenges developers face while using the currently available DevOps tools and techniques along with the organizational challenges in DevOps practices.
Method:
We conduct an empirical study by applying 
topic modeling
 on 174K SO posts that contain DevOps discussions. We then validate and extend the empirical study findings with a survey of 21 professional DevOps practitioners.
Results:
We find that: (1) There are 23 DevOps topics grouped into four categories: Cloud & CI/CD Tools, Infrastructure as Code, Container & Orchestration, and Quality Assurance. (2) The topic category ‘Cloud & CI/CD Tools’ contains the highest number of topics (10) which cover 48.6% of all questions in our dataset, followed by the category Infrastructure as Code (28.9%). (3) The file management is the most popular topic followed by Jenkins Pipeline, while infrastructural Exception Handling and Jenkins Distributed Architecture are the most difficult topics (with least accepted answers). (4) In the survey, developers mention that it requires hands-on experience before current DevOps tools can be considered easy. They raised the needs for better documentation and learning resources to learn the rapidly changing DevOps tools and techniques. Practitioners also emphasized on the formal training approach by the organizations for DevOps skill development.
Conclusion:
Architects and managers can use the findings of this research to adopt appropriate DevOps technologies, and organizations can design tool or process specific DevOps training programs.",Information and Software Technology,04 Mar 2025,6,The research on challenges faced by developers in DevOps practices can provide valuable insights to early-stage ventures to adopt appropriate technologies and design training programs.
https://www.sciencedirect.com/science/article/pii/S0950584923000782,StartCards — A method for early-stage software startups,August 2023,"Software startups, Requirements engineering, Validation, Software engineering method, Action research",Kai-Kristian=Kemell: kai-kristian.kemell@helsinki.fi; Anh=Nguyen-Duc: Not Found; Mari=Suoranta: Not Found,"Abstract
Context:
Software startups are important drivers of economy on a global scale, and have become associated with innovation and high growth. However, the overwhelming majority of startups ends in failure. Many of these startup failures ultimately stem from 
software engineering
 issues, and 
requirements engineering
 (RE) ones in particular. Despite the emphasis placed on the importance of RE activities in the startup context, many startups continue to develop software without a clear market or customer, having never had meaningful contact with their would-be customer.
Objective:
We develop a method aimed at early-stage startups that is intended to help startups through the initial stages of the startup process: StartCards. The method emphasizes the importance of idea and product validation activities in particular in order to tackle anti-patterns related to (a lack of) RE in startups. This method is based on existing literature, both grey and academic literature.
Method:
StartCards was developed using the Canonical 
Action Research
 (CAR) approach, over the course of 4 AR cycles. During the AR process, the method was used by 44 student startup teams in a practical course setting. Data from the use of the method was collected through self-reporting in the form of modified learning diaries, mentoring meetings with the startup teams, and a qualitative survey.
Results:
We consider the current version of StartCards useful for early-stage startups based on the data we have collected. The method can also be used as a pedagogical tool in startup education.
Conclusions:
The paper presents the first published version of the method. While work on the method continues, the method is deemed ready for use.",Information and Software Technology,04 Mar 2025,8,"The StartCards method developed for early-stage startups can be a valuable tool to improve idea and product validation activities, potentially reducing failure rates in software startups."
https://www.sciencedirect.com/science/article/pii/S095058492300071X,BERT- and TF-IDF-based feature extraction for long-lived bug prediction in FLOSS: A comparative study,August 2023,Not Found,Luiz=Gomes: luizgomes@pucpcaldas.br; Ricardo=da Silva Torres: ricardo.torres@ntnu.no; Mario Lúcio=Côrtes: cortes@ic.unicamp.br,"Abstract
Context:
The correct prediction of long-lived bugs could help 
maintenance teams
 to build their plan and to fix more bugs that often adversely affect software quality and disturb the 
user experience
 across versions in Free/Libre Open-Source Software (FLOSS). 
Machine Learning
 and Text Mining methods have been applied to solve many real-world prediction problems, including 
bug report
 handling.
Objective:
Our research aims to compare the accuracy of ML classifiers on long-lived bug prediction in FLOSS using 
Bidirectional Encoder Representations from Transformers
 (BERT)- and Term Frequency - 
Inverse Document Frequency
 (TF-IDF)-based feature extraction. Besides that, we aim to investigate BERT variants on the same task.
Method:
We collected bug reports from six popular FLOSS and used the 
Machine Learning
 classifiers to predict long-lived bugs. Furthermore, we compare different feature extractors, based on BERT and TF-IDF methods, in long-lived bug prediction.
Results:
We found that long-lived bug prediction using BERT-based feature extraction systematically outperformed the TF-IDF. The 
SVM
 and 
Random Forest
 outperformed other classifiers in almost all datasets using BERT. Furthermore, smaller BERT architectures show themselves as competitive.
Conclusion:
Our results demonstrated a promising avenue to predict long-lived bugs based on BERT contextual embedding features and fine-tuning procedures.",Information and Software Technology,04 Mar 2025,7,"The research on predicting long-lived bugs using ML classifiers with BERT-based feature extraction can aid maintenance teams in improving software quality, which is crucial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000848,Migrating monoliths to cloud-native microservices for customizable SaaS,August 2023,"Microservices, Architecture, Cloud native, Migration, Multi-tenancy, Event-based, Customization",Espen Tønnessen=Nordli: espen.nordli@tietoevry.com; Sindre Grønstøl=Haugeland: sindre.haugeland@tietoevry.com; Phu H.=Nguyen: phu.nguyen@sintef.no; Hui=Song: hui.song@sintef.no; Franck=Chauvel: franck.chauvel@axbit.com,"Abstract
Context:
It was common that software vendors sell licenses to their clients to use software products, such as 
Enterprise Resource Planning
, which are deployed as a monolithic entity on clients’ premises. Moreover, many clients, especially big organizations, often require software products to be customized for their specific needs before deployment on premises.
Objective:
However, as software vendors are migrating their monolithic software products to Cloud-native Software-as-a-Service (SaaS), they face two big challenges that this paper aims at addressing: (1) How to migrate their exclusive monoliths to multi-tenant Cloud-native SaaS; and (2) How to enable tenant-specific 
customizations
 for multi-tenant Cloud-native SaaS.
Method:
This paper suggests an approach for migrating monoliths to microservice-based Cloud-native SaaS, providing customers with a flexible customization opportunity, while taking advantage of the economies of scale that the Cloud and multi-tenancy provide. We develop two proofs-of-concept to demonstrate our approach on migrating a reference application of Microsoft called SportStore to a customizable SaaS as well as customizing another Microsoft’s microservices reference application called eShopOnContainers.
Results:
We have shown not only the migration to microservices but also how to introduce the necessary infrastructure to support the new services and enable tenant-specific customization.
Conclusions:
Our customization-driven migration approach can guide a monolith to become SaaS having (synchronous and asynchronous) customization power for multi-tenant SaaS. Furthermore, our event-based customization approach can reduce the number of API calls to the main product while enabling different tenant-specific customization services for real-world scenarios.",Information and Software Technology,04 Mar 2025,9,The approach for migrating monoliths to microservice-based Cloud-native SaaS with customization capabilities can be highly beneficial for startups looking to scale their software products efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584923000538,Git command recommendations using crowd-sourced knowledge,July 2023,Not Found,Haitao=Jia: sz2116123@nuaa.edu.cn; Wenhua=Yang: ywh@nuaa.edu.cn; Chaochao=Shen: ccshen@nuaa.edu.cn; Minxue=Pan: mxp@nju.edu.cn; Yu=Zhou: zhouyu@nuaa.edu.cn,"Abstract
Context:
Git is a fast, scalable, distributed version control system with a rich command set that provides high-level operations and full access to the internals. It has been widely used by millions of developers worldwide. However, due to the flexibility of the usage of Git commands and the scarcity of Git documentation, many developers have experienced difficulties when using Git commands.
Objective:
This paper aims to propose an automatic approach to recommending Git commands for developers given a query described by natural language.
Method:
Our approach makes recommendations by mining the crowd-sourced knowledge related to Git on Stack Overflow. It first constructs a keyword-command mapping database from Git-related posts, then analyzes the similarity between the query given by the developer and the keywords in the database to retrieve the candidate commands, and proposes an algorithm to rank the candidate commands.
Results:
Our approach’s recommendation results significantly outperform the baseline approaches in several metrics (e.g., Top-K accuracy). Meanwhile, the experimental results have shown that the favorable efficiency of our approach can promise its use by developers in real-world scenarios.
Conclusion:
The Git command recommendation approach proposed in this paper is effective and can be helpful for developers to use Git commands for more efficient development.",Information and Software Technology,04 Mar 2025,8,"The automatic approach for recommending Git commands based on natural language queries can enhance developers' efficiency and productivity, which can also benefit early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000708,The lifecycle of Technical Debt that manifests in both source code and issue trackers,July 2023,"Technical Debt, Source code, Issue tracker",Jie=Tan: j.tanjie@outlook.com; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
Although Technical Debt (TD) has increasingly gained attention in recent years, most studies exploring TD are based on a single source (e.g., source code, code comments or issue trackers).
Objective:
Investigating information combined from different sources may yield insight that is more than the sum of its parts. In particular, we argue that exploring how TD items are managed in both issue trackers and software repositories (including source code and commit messages) can shed some light on what happens between the commits that incur TD and those that pay it back.
Method:
To this end, we randomly selected 3,000 issues from the trackers of five projects, manually analyzed 300 issues that contained TD information, and identified and investigated the lifecycle of 312 TD items.
Results:
The results indicate that most of the TD items marked as resolved in issue trackers are also paid back in source code, although many are not discussed after being identified in the issue tracker. Test Debt items are the least likely to be paid back in source code. We also learned that although TD items may be resolved a few days after being identified, it often takes a long time to be identified (around one year). In general, time is reduced if the same developer is involved in consecutive moments (i.e., introduction, identification, repayment decision-making and remediation), but whether the developer who paid back the item is involved in discussing the TD item does not seem to affect how quickly it is resolved.
Conclusions:
Investigating how developers manage TD across both 
source code repositories
 and issue trackers can lead to a more comprehensive oversight of this activity and support efforts to shorten the lifecycle of undesirable debt.",Information and Software Technology,04 Mar 2025,7,"The investigation of Technical Debt across source code repositories and issue trackers can lead to a more comprehensive understanding and support efforts to reduce software debt, which is beneficial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492300054X,Continuous deployment in software-intensive system-of-systems,July 2023,Not Found,Anas=Dakkak: anas.dakkak@ericsson.com; Jan=Bosch: jan.bosch@chalmers.se; Helena Holmström=Olsson: helena.holmstrom.olsson@mau.se; David=Issa Mattos: david.mattos@volvocars.com,"Abstract
Context:
While continuous deployment is popular among web-based software development organizations, adopting continuous deployment in software-intensive system-of-systems is more challenging. On top of the challenges arising from deploying software to a single software-intensive embedded system, software-intensive system-of-systems (SiSoS) add a layer of complexity as new software undergoes an extensive field validation applied to individual components of the SiSoS, as well as the overall SiSoS, to ensure that both legacy and new functionalities are working as desired.
Objectives:
This paper aims to study how SiSoS transitions to continuous deployment by exploring how continuous deployment impacts field testing and validation activities, how continuous deployment can be practiced in SiSoS, and to identify the 
success factors
 that companies need to consider when transitioning to continuous deployment.
Method:
We conducted a 
case study
 at Ericsson AB focusing on the 
embedded software
 of the Third Generation 
Radio Access Network
 (3G RAN). The 3G RAN consists of two large-scale software-intensive embedded systems, representing a simple SiSoS composed of two systems. 3G RAN software was the first to transition to continuous deployment and is used as a reference case for other products within Ericsson AB.
Results:
Software deployment, in addition to field testing and validation, have transitioned from being a discrete activity performed at the end of software development to a continuous process performed in parallel to software development. Further, our study reveals an orchestrating approach for software deployment, which allows pre/post validation of legacy behavior and new features in a shorter release and deployment cadence. Furthermore, we identified the essential 
success factors
 that organizations should consider when transitioning to continuous deployment.
Conclusion:
Transition to continuous deployment, in addition to field testing and validation, shall be considered and planned carefully. In this paper, we provide a set of success factors and orchestration technique that helps organization when transitioning to continuous deployment in the software-intensive embedded system-of-systems context.",Information and Software Technology,04 Mar 2025,9,"The study on transitioning to continuous deployment in software-intensive system-of-systems provides valuable insights and success factors that can have a significant impact on European early-stage ventures, especially startups."
https://www.sciencedirect.com/science/article/pii/S0950584923000289,Double-counting in software engineering tertiary studies — An overlooked threat to validity,June 2023,"Bias, Double-counting, Empirical, Guidelines, Meta-review, Overview of reviews, Recommendations, Research methods, Review of reviews, Tertiary review, Tertiary study, Umbrella review",Jürgen=Börstler: jurgen.borstler@bth.se; Nauman=bin Ali: nauman.ali@bth.se; Kai=Petersen: kai.petersen@bth.se,"Abstract
Context:
Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in 
software engineering
.
Objective:
We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue.
Method:
We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them.
Results:
In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies.
Conclusions:
Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies.",Information and Software Technology,04 Mar 2025,5,"Although the issue of double-counting in tertiary studies is addressed, the practical value for early-stage ventures may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923000228,SedSVD: Statement-level software vulnerability detection based on Relational Graph Convolutional Network with subgraph embedding,June 2023,Not Found,Yukun=Dong: dongyk@upc.edu.cn; Yeer=Tang: Not Found; Xiaotong=Cheng: Not Found; Yufei=Yang: Not Found; Shuqi=Wang: Not Found,"Abstract
Context:
Current deep-learning based 
vulnerability detection
 methods have been proven more automatic and correct to a certain extent, nonetheless, they are limited to detect at function-level or file-level, which can hinder software developers from acquiring more detailed information and conducting more targeted repairs. Graph-based detection methods have shown dominant performance over others. Unfortunately, the information they reveal has not been fully utilized.
Objective:
We design SedSVD (Subgraph embedding driven Statement-level Vulnerability Detection) with two objectives: (i) to better utilize the information the code-related graphs can reflect; (ii) to detect vulnerabilities at a finer-grained level.
Method:
In our work, we propose a novel graph-based detection framework that embeds graphs at subgraph-level to realize statement-level detection. It first leverages Code Property Graph (CPG) to learn both semantic and 
syntactic information
 from source code, and then selects several center nodes (code elements) in CPG to build their subgraphs. After embedding each subgraph with its nodes and edges, we apply Relational 
Graph Convolutional Network
 (RGCN) to process different edges differently. A Multi-Layer 
Perceptron
 (MLP) layer is further added to ensure its prediction performance.
Results:
We conduct our experiments on C/C++ projects from NVD and SARD. Experimental results show that SedSVD achieves 95.15% in F1-measure which proves our work to be more effective.
Conclusion:
Our work detects at a finer-grained level and achieves higher F1-measure than existing state-of-art 
vulnerability detection
 techniques. Besides, we provide a more detailed detection report pointing the specific error code elements within statements.",Information and Software Technology,04 Mar 2025,8,The development of SedSVD for statement-level vulnerability detection provides advanced methods that can benefit European early-stage ventures by enhancing security measures in software development.
https://www.sciencedirect.com/science/article/pii/S0950584923000265,Scripted and scriptless GUI testing for web applications: An industrial case,June 2023,"Case study, Complementarity, Scriptless testing, Scripted testing",Axel=Bons: Not Found; Beatriz=Marín: Not Found; Pekka=Aho: Not Found; Tanja E.J.=Vos: Not Found,"Abstract
Context:
Automation is required in the software development to reduce the high costs of producing software and to address the short release cycles of modern 
development processes
. Lot of effort has been performed to automate testing, which is one of the most resource-consuming development phases. Automation of testing through the Graphical User Interface (GUI) has been researched to improve the system testing.
Objective:
We aim to evaluate the complementarity of automated GUI testing tools in a real industrial context, which refers to the capability of the tools to work usefully together.
Methods:
To address the objective, we conduct an exploratory 
case study
 in an IT development company from The Netherlands. We select two representative tools for automated GUI testing, one for scripted and another for scriptless testing. We measure the complementarity by measuring the effectiveness, the efficiency, and 
subjective satisfaction
 of the tools.
Results:
It can be observed that the scripted tool performs better in detecting process failures, and the scriptless tool performs better in detecting visible failures and also reaching higher coverage. Both tools perform in a similar way in terms of efficiency. Additionally, both tools were perceived to be useful in the survey performed for the subjective satisfaction.
Conclusion:
We conclude that scriptless and scripted testing approaches are complementary, and they can improve the effectiveness compared to manual testing processes performed in an industrial context by detecting different failures and reducing the effort and time to find these failures and to reproduce them.",Information and Software Technology,04 Mar 2025,6,"The evaluation of automated GUI testing tools in an industrial context presents insights that can be helpful for early-stage ventures, but the impact may not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923000435,Application of Project-Based Learning to a Software Engineering course in a hybrid class environment,June 2023,Not Found,Edgar=Ceh-Varela: eduardo.ceh@enmu.edu; Carlos=Canto-Bonilla: carlos.canto@utmetropolitana.edu.mx; Dhimitraq=Duni: dhimitraq.duni@enmu.edu,"Abstract
Context:
This paper centers on Project-Based Learning (PBL). In PBL, the student is now the center of the whole 
teaching and learning
 process, while the instructor‘s role is now of a facilitator presenting to the students the resources and guidance to solve the given problem. Most existing studies, apply PBL to courses having in-person students.
Objective:
The paper presents the application of a 
PBL approach
 to a Software Engineering (SE) course having a hybrid class environment (i.e., online and in-person students). The main objective of this paper is to analyze the students’ attitudes after experiencing working on a real-life problem as part of our 
PBL approach
 in a hybrid class environment.
Methods:
We propose a 
relaxed plan-based
 software development model as basis for guiding the project execution. At the end of the course, we applied a survey to the students to evaluate their experience in the course.
Results:
We obtained the answers of 70.8% of students taking a 
SE
 course. With these answers, we could measure the students’ perception of using PBL in a 
SE
 course and how this strategy helped them to gain soft and hard skills in software development. We divided the answers for their analysis into different categories: soft skills, technical skills, 
learning experience
, and other results. Moreover, we compare the performance of the teams and students based on their type (i.e., online and in-person).
Conclusion:
We found qualitative differences in the experience of online and in-person students. Based on our experience with this study, we provide guidelines for applying PBL in a hybrid environment. Overall, our study has demonstrated a positive contribution in supporting teaching SE using a PBL in a hybrid class environment.",Information and Software Technology,04 Mar 2025,7,"The application of Project-Based Learning to a hybrid class environment in Software Engineering courses can have a positive impact on students' skills and learning experience, providing practical guidelines for early-stage ventures in education technology."
https://www.sciencedirect.com/science/article/pii/S0950584923000022,A light-weight data augmentation method for fault localization,May 2023,Not Found,Jian=Hu: jianhu@cqu.edu.cn; Huan=Xie: huanxie@cqu.edu.cn; Yan=Lei: yanlei@cqu.edu.cn; Ke=Yu: keyu@cqu.edu.cn,"Abstract
Context:
Fault localization (FL) is essentially a search over the space of program statements to find suspicious entities that might have caused a program failure. However, the input data is high-dimensional and extremely imbalanced since the real-world programs are large in size and the number of failing test cases is much less than that of passing test cases, which limits the effectiveness and efficiency of existing FL methods. The state-of-the-art FL method (Aeneas) solves the imbalanced and high-dimensional problem but in a complex and time-consuming process.
Objective:
Due to the limited effectiveness of original FL methods and the low efficiency of Aeneas, this paper proposes 
Lamont
, a 
L
ight-weight d
a
ta aug
m
entati
on
 me
t
hod to improve the effectiveness of original FL methods and the efficiency of Aeneas.
Methods:
Lamont
 uses revised linear 
discriminant analysis
 (LDA) to reduce the dimensionality of the original coverage matrix and leverage synthetic minority over-sampling (SMOTE) to generate the synthesized failing tests. The balanced coverage matrix with reduced dimensionality is fed into FL methods to obtain the ranked suspicious list of statements. To evaluate the efficiency and effectiveness, we compare 
Lamont
 with six representative FL methods and Aeneas on 458 versions of 10 real-life programs.
Results:
It can be observed that 
Lamont
 outperforms in most cases for Top-K metric and reduces the number of statements that need to be checked from 17.45% to 79.81% compared with the original six FL methods. Furthermore, Lamont saves the time over the state-of-the-art data augmentation method Aeneas from 55.33% to 68.39% with comparable effectiveness.
Conclusion:
This work conducts a large-scale experimental study to investigate the effectiveness and efficiency of 
Lamont
. Two conclusions can be obtained based on the experimental results. First, it shows that 
Lamont
 is more effective than the original FL methods. Second, it shows 
Lamont
 is more efficient than Aeneas with similar effectiveness in six FL methods.",Information and Software Technology,04 Mar 2025,8,"The proposed Lamont method for Fault Localization shows significant improvements in effectiveness and efficiency over existing methods, which can benefit startups in software development by aiding in faster bug detection and resolution."
https://www.sciencedirect.com/science/article/pii/S0950584923000204,A novel detection model for abnormal network traffic based on bidirectional temporal convolutional network,May 2023,Not Found,Jinfu=Chen: Not Found; Tianxiang=Lv: Not Found; Saihua=Cai: caisaih@ujs.edu.cn; Luo=Song: Not Found; Shang=Yin: Not Found,"Abstract
Context:
The increasingly complex and diverse network environment has increased traffic intrusion behaviors, but the traditional machine learning-based model has the problems of time-consuming and low detection accuracy due to the need of manually selecting features. Therefore, it is very important to construct an automatically abnormal network traffic detection model with a high detection accuracy.
Objective:
The goal of this paper is to train the network traffic through 
deep learning
 technology to generate an automatic abnormal network traffic detection model without manual design of features.
Methods:
We propose an abnormal network traffic detection model called BiTCN based on bidirectional time convolution network, it first uses 
temporal convolutional network
 (TCN) model to better grasp the sequence characteristics of network traffic, and then uses Exponential Linear Unit (ELU) 
activation function
 to replace ReLU in the model training stage to avoid the problem of neuron “death” leading to the reduction of detection accuracy, as well as improves the original one-way model to a two-way model to capture the two-way semantic fusion characteristics of network traffic.
Results:
We evaluate the efficiency and effectiveness of the proposed BiTCN model by comparing it with different models on the CTU and USTC-TFC2016 datasets. The experimental results show that the proposed BiTCN model outperforms other models in terms of the precision, accuracy, recall and F1-measure.
Conclusion:
In this paper, we propose a novel detection model for abnormal network traffic based on bidirectional 
temporal convolutional network
 , it solves some shortcomings and limitations of existing models, and obtains a high detection accuracy of abnormal network traffic with a high stability.",Information and Software Technology,04 Mar 2025,9,"The development of a BiTCN model for abnormal network traffic detection using deep learning technology can greatly enhance automated security systems, providing valuable support for startups dealing with cybersecurity solutions."
https://www.sciencedirect.com/science/article/pii/S0950584922002427,Progress on class integration test order generation approaches: A systematic literature review,April 2023,Not Found,Yanru=Ding: yrding@cumt.edu.cn; Yanmei=Zhang: ymzhang@cumt.edu.cn; Guan=Yuan: yuanguan@cumt.edu.cn; Shujuan=Jiang: shjjiang@cumt.edu.cn; Wei=Dai: weidai@cumt.edu.cn,"Abstract
Context:
Integration testing is an effective way to detect unit test results and ensure the correct and stable operation of software modules. One of the crucial problems in integration testing is the class integration test order (CITO) generation problem. Its purpose is to reasonably determine the test order of each class in a program to reduce test consumption. In recent years, the CITO generation problem has made a lot of progress but also faces more challenges.
Objective:
The goal of this paper is to provide an overview of the research progress on the CITO generation problem. By summarizing applied techniques, evaluation indicators, and datasets, this paper aims to identify research challenges and suggest future opportunities.
Method:
We conduct a systematic literature review of CITO generation approaches, including the problems investigated, the solutions proposed, the techniques applied, the evaluation indicators used, and the datasets covered.
Results:
Based on research techniques and evaluation indicators, we classified and analyzed 30 papers published between 2011 and 2022. Our analysis reveals that more (47%) of the studies on the CITO generation problem still prefer to use search-based techniques, and the vast majority (90%) of the studies choose to use the stubbing complexity as the indicator to evaluate the stubbing cost of generating CITOs. We have extracted five challenges that the CITO generation problem is facing, corresponding to which we have given suggestions for future research.
Conclusion:
In this paper, we have outlined the research status of CITO generation approaches, summarized the challenges, and proposed corresponding opportunities for future study. We expect this paper to better help software testing workers understand the CITO generation problem and improve efficiency in practical work.",Information and Software Technology,04 Mar 2025,5,"While the overview of the CITO generation problem provides insights into research progress and challenges, its direct practical impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922002415,An expressive and modular layer activation mechanism for Context-Oriented Programming,April 2023,Not Found,Paul=Leger: pleger@ucn.cl; Nicolás=Cardozo: Not Found; Hidehiko=Masuhara: Not Found,"Abstract
Context.
There is a trend in the software industry towards 
building systems
 that dynamically adapt their behavior in response to their surrounding environment, given the proliferation of various technological devices, such as notebooks, smartphones, and wearables, capable of capturing their execution context. Context-oriented Programming (COP) allows developers to use layer abstractions to adapt software behavior to the context. A layer is associated with a context and can be dynamically activated in direct response to gathered information from its surrounding execution environment. However, most existing layer activation mechanisms have been tailored specifically to address a particular concern; implying that developers need to tweak layer definitions in contortive ways or create new specialized activation mechanisms altogether if their specific needs are not supported.
Objective.
Complementing ideas to expressively declare activation mechanism models with interfaces that define conditionals of activation mechanisms modularly, this paper proposes an Expressive and Modular Activation mechanism, named EMA.
Method.
To propose EMA, we analyze existing activation mechanisms in COP regarding activation features and scope strategies. After, we propose the design of EMA and validate it with a 
case study
 discussion.
Results.
Using a concrete JavaScript implementation of EMA, named EMAjs, we can implement two Web applications: a 
smartphone application
 as an example to illustrate EMAjs in action, and an application of home automation to discuss and compare our proposal.
Conclusions.
Our proposed mechanism allows developers to instantiate different activation scope strategies and interfaces to decouple the declaration of activation mechanism conditionals from the base code.",Information and Software Technology,04 Mar 2025,6,"The proposal of EMA for activation mechanisms in Context-oriented Programming offers a modular and expressive approach, which can be beneficial for startups developing adaptive software systems but may require further validation and implementation in real-world scenarios."
https://www.sciencedirect.com/science/article/pii/S0950584923000010,How SonarQube-identified technical debt is prioritized: An exploratory case study,April 2023,Not Found,Reem=Alfayez: reealfayez@ksu.edu.sa; Robert=Winn: Not Found; Wesam=Alwehaibi: Not Found; Elaine=Venson: Not Found; Barry=Boehm: Not Found,"Abstract
Context:
Repaying all technical debt (TD) in a system may be unviable, as there is typically a shortage of resources allocated for 
TD repayment
 activities. Therefore, TD prioritization is essential to best allocate such limited resources. Fortunately, one can utilize a 
static code analysis tool
, such as SonarQube, to aid in expediting the TD prioritization process.
Objective:
Given that SonarQube is one of the most utilized tools in the context of TD, this exploratory 
case study
 seeks to explore how SonarQube-identified TD items are perceived and prioritized for repayment.
Methods:
The study was designed, replicated, and conducted in four companies and a master’s level course, with a total of 89 participants. The participants were requested to select TD items to include for repayment under a 
resources constraint
.
Results:
The results revealed that the overwhelming majority of participants prioritized TD by factoring in a TD item’s value and cost, a smaller number prioritized higher value TD items, and only one participant prioritized lower cost TD items. Furthermore, it was revealed that the value of a TD item is subjective and context-dependent, and the majority of participants perceive the 
cost estimations
 provided by SonarQube for repaying TD items to be reliable and trustworthy when prioritizing TD.
Conclusion:
Based on the results, one can conclude that there is no silver bullet TD prioritization approach that addresses all of a developer’s objectives and needs. New TD prioritization approaches should be designed without concentrating on a specific prioritization perspective and should be independent of value estimation methods.",Information and Software Technology,04 Mar 2025,7,"The study offers insights into prioritizing technical debt, a crucial aspect for startups managing limited resources."
https://www.sciencedirect.com/science/article/pii/S095058492200252X,Probabilistic program performance analysis with confidence intervals,April 2023,"Program quality analysis, Software performance, Discrete-time Markov chains, Probabilistic model checking, Formal verification with confidence intervals",Ioannis=Stefanakos: ioannis.stefanakos@york.ac.uk; Radu=Calinescu: Not Found; Simos=Gerasimou: Not Found,"Abstract
Context:
More often than not, the algorithms implemented by software systems continue to operate correctly when executed on different platforms or with different inputs, and can be easily replaced with functionally equivalent ones. However, such changes can have a significant and difficult to predict impact on the software performance, resource use, and other key quality properties.
Objective:
The paper introduces a method for the formal analysis of timing, resource use, cost and other 
quality aspects
 of computer programs, and a tool that automates the application of the method to Java code.
Method:
A tool-supported 
p
robabilistic p
ro
gram 
per
formance analysis (PROPER) method was developed, and was evaluated using Java code from the Apache Commons Math library, the 
Android
 messaging app Telegram, and open-source implementations of the 
knapsack
, binary search, and minimum path sum algorithms. PROPER synthesises a parametric Markov-chain model of the analysed code, uses information from program logs to calculate confidence intervals for the parameters of this model, and employs 
formal verification
 with confidence intervals to obtain confidence intervals for the performance properties of interest. A PROPER variant that operates with point estimates instead of confidence intervals can be used when large program logs are available.
Results:
The PROPER point estimates for the analysed performance properties were accurate within 7.9% and 1.75% of the ground truth when using program logs with 
 and 
 entries, respectively. All PROPER confidence intervals for these properties contained the true property value, and became narrower when larger logs were used in the analysis. The analyses were completed in under 15 ms for point estimates, and in between 6.7 s and 7.8 s for confidence intervals on a regular laptop computer.
Conclusion:
PROPER can synthesise and reuse a parametric Markov model to accurately predict how software performance would change if the code ran on a different hardware platform, used a new function library, or had a different usage profile—supporting practitioners who are interested in these analyses.",Information and Software Technology,04 Mar 2025,9,"The PROPER method addresses key quality aspects of software performance, providing valuable tools for startups developing software products."
https://www.sciencedirect.com/science/article/pii/S0950584922002208,Detecting code smells in React-based Web apps,March 2023,Not Found,Fabio=Ferreira: fabio.ferreira@ifsudestemg.edu.br; Marco Tulio=Valente: Not Found,"Abstract
Context:
Facebook’s 
React
 is a widely popular 
JavaScript library
 to build rich and 
interactive user interfaces
 (UI). However, due to the complexity of modern Web UIs, 
React
 applications can have hundreds of components and 
source code
 files. Therefore, front-end developers are facing increasing challenges when designing and modularizing 
React
-based applications. As a result, it is natural to expect 
maintainability
 problems in 
React
-based UIs due to suboptimal design decisions.
Objective:
To help developers with these problems, we propose a catalog with twelve 
React
-related code smells and a prototype tool to detect the proposed smells in 
React
-based Web apps.
Method:
The smells were identified by conducting a grey literature review and by interviewing six professional software developers. We also use the tool in the top-10 most popular GitHub projects that use 
React
 and conducted a historical analysis to check how often developers remove the proposed smells.
Results:
We detect 2,565 instances of the proposed code smells. The results show that the removal rates range from 0.9% to 50.5%. The smell with the most significant removal rate is 
Large File
 (50.5%). The smells with the lowest removal rates are 
Inheritance Instead of Composition (IIC)
 (0.9%), and 
Direct DOM Manipulation
 (14.7%).
Conclusion:
The list of 
React
 smells proposed in this paper as well as the tool to detect them can assist developers to improve the 
source code
 quality of 
React
 applications. While the catalog describes common problems with 
React
 applications, our tool helps to detect them. Our historical analysis also shows the importance of each smell from the developers’ perspective, showing how often each smell is removed.",Information and Software Technology,04 Mar 2025,8,Identifying and addressing code smells in React applications can significantly improve the maintainability of startups' UI projects.
https://www.sciencedirect.com/science/article/pii/S0950584922002178,Enterprise architecture artifacts as boundary objects: An empirical analysis,March 2023,Not Found,Svyatoslav=Kotusev: kotusev@kotusev.com; Sherah=Kurnia: Not Found; Rod=Dilnutt: Not Found,"Abstract
Context
Enterprise architecture (EA) is a collection of artifacts describing various aspects of an organization from an integrated business and IT perspective. EA artifacts intend to bridge the communication gap between business and IT stakeholders to improve business and IT alignment in organizations and, therefore, can be considered as 
boundary objects
 between diverse business and IT communities. However, an intentional analysis of EA artifacts as boundary objects in the current EA literature has been rather shallow and insufficient.
Objective
This study aims to explore how exactly EA artifacts as boundary objects facilitate communication between different professional communities. Specifically, it intends to identify what types of EA artifacts represent boundary objects, analyze their properties and 
usage scenarios
, as well as the differences between them.
Method
This study is based on an in-depth case study of an organization with an established EA practice. 
Data collection procedures
 include both interviews with various participants of its EA practice and comprehensive scrutiny of its EA documentation.
Results
We identify five specific types of EA artifacts used in the organization as boundary objects and analyze them in detail. In particular, we analyze their 
informational contents
 and usage scenarios, their target audiences and value for cross-community collaboration, as well as their 
syntactic
, semantic and pragmatic boundary-spanning capacity. Moreover, we also introduce the notion of duality as a characteristic of interpretive flexibility of EA artifacts and distinguish two different types of duality leveraging somewhat different boundary-spanning mechanisms: implicit duality and explicit duality.
Conclusions
This paper provides arguably the first inductive qualitative analysis of EA artifacts as boundary objects available in the existing EA literature. It contributes to our understanding of their boundary-spanning properties, distinctive features and general roles in an EA practice. Also, the concepts of implicit and explicit duality that we introduce further advance the theory of boundary objects.",Information and Software Technology,04 Mar 2025,6,"The study on EA artifacts as boundary objects contributes to improving communication between business and IT stakeholders, relevant for startups with growing IT needs."
https://www.sciencedirect.com/science/article/pii/S0950584922002154,Lessons learned to improve the UX practices in agile projects involving data science and process automation,March 2023,Not Found,Bruna=Ferreira: bruna@exacta.inf.puc-rio.br; Silvio=Marques: Not Found; Marcos=Kalinowski: Not Found; Hélio=Lopes: Not Found; Simone D.J.=Barbosa: Not Found,"Abstract
Context:
User-Centered Design (UCD) and 
Agile methodologies
 focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation.
Objective:
Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams’ perceptions regarding user participation in these projects.
Method:
We 
collected data
 from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire.
Results:
From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects.
Conclusion:
We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users’ routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions.",Information and Software Technology,04 Mar 2025,7,Investigating user participation in agile projects for data science and process automation can benefit startups looking to improve problem definition and understanding.
https://www.sciencedirect.com/science/article/pii/S0950584922002002,COSMOS: A comprehensive framework for automatically generating domain-oriented test suite,February 2023,Not Found,Akram=Kalaee: Not Found; Saeed=Parsa: prasa@iust.ac.ir; Negar=Fathi: Not Found,"Abstract
Context
Coverage criteria are satisfied by at least one examination of the test target, while many faults are not revealed by one execution. However, despite executing the faulty statement, the test result is correct in certain circumstances. Such coincidentally passing test cases execute the faulty statement but do not cause failures.
Objective
This paper introduces the new concept of domain solver. Domain solvers attempt to detect the domain of inputs rather than a single input satisfying a path constraint. Domain coverage is a new metric to evaluate the relative accuracy of the detected domains. The promising point is that the proposed approach similarly treats nonlinear and 
linear constraints
.
Method
Domain solver splits a path constraint into conjunctions of simple conditions comparing two expressions. Such a splitting simplifies the constraint-solving task to detect regions of the input space satisfying a comparison between two expressions. After finding a region, an improved version of an algorithm, update, is used to determine the domain of variables involved in the comparing expressions.
Results
Our proposed approach, COSMOS, is implemented using the Roslyn compiler. We compared COSMOS with well-known 
constraint solvers
 using various linear/nonlinear constraints. The results show that COSMOS improves the number of supported 
data types
 involved in a constraint and solves100% of the instances in which the other solvers fail. Besides, COSMOS achieves the best relative accuracy of 84% compared to the existing domain-oriented test suite generation approaches. Moreover, our experiment results illustrate that COSMOS improves the fault-finding capability of other existing test coverage criteria by detecting coincidentally correct test cases.
Conclusion
Combining domain coverage and compiler as a service makes a powerful 
constraint satisfaction
 method outperforming the existing 
constraint solvers
 regarding the number of solved linear/nonlinear constraints. Augmenting other structural test coverage criteria with domain coverage reveals the coincidentally correct test cases.",Information and Software Technology,04 Mar 2025,9,"The proposed domain solver approach shows significant improvement in solving linear/nonlinear constraints and detecting test cases, which could greatly benefit early-stage ventures in optimizing their testing processes and improving fault-finding capabilities."
https://www.sciencedirect.com/science/article/pii/S0950584922001938,Visualizations for the evolution of Variant-Rich Systems: A systematic mapping study,February 2023,"Variant-rich systems, Software product lines, Visualization, Evolution, Maintenance, Mapping study",Raul=Medeiros: raul.medeiros@ehu.eus; Jabier=Martinez: jabier.martinez@tecnalia.com; Oscar=Díaz: oscar.diaz@ehu.eus; Jean-Rémy=Falleri: falleri@labri.fr,"Abstract
Context:
Variant-Rich Systems (VRSs), such as Software Product Lines or variants created through clone & own, aim at reusing existing assets. The long lifespan of families of variants, and the scale of both the code base and the workforce make VRS maintenance and evolution a challenge. Visualization tools are a needed companion.
Objective:
We aim at mapping the current state of visualization interventions in the area of VRS evolution. We tackle evolution in both functionality and architecture. Three research questions are posed: What sort of analysis is being conducted to assess VRS evolution? (Analysis perspective); What sort of visualizations are displayed? (Visualization perspective); What is the research maturity of the reported interventions? (Maturity perspective).
Methods:
We performed a 
systematic mapping study
 including automated search in digital libraries, expert knowledge, and snowballing.
Results:
The study reports on 41 visualization approaches to cope with VRS evolution. Analysis wise, feature identification and location is the most popular scenario, followed by variant integration towards a Software Product Line. As for visualization, nodelink diagram visualization is predominant while researchers have come up with a wealth of ingenious visualization approaches. Finally, maturity wise, almost half of the studies are solution proposals. Most of the studies provide proof-of-concept, some of them also include publicly available tools, yet very few 
face
 proof-of-value.
Conclusions:
This study introduces a comparison framework where to frame future studies. It also points out distinct research gaps worth investigating as well as shortcomings in the evidence about relevance and contextual considerations (e.g., scalability).",Information and Software Technology,04 Mar 2025,7,"The study on visualization interventions for Variant-Rich Systems provides valuable insights for managing the evolution of software product lines, which could be beneficial for startups dealing with complex code bases and workforce scalability."
https://www.sciencedirect.com/science/article/pii/S0950584922001653,"Concerns identified in code review: A fine-grained, faceted classification",January 2023,Not Found,Sanuri=Gunawardena: sanuri.gunawardena@auckland.ac.nz; Ewan=Tempero: e.tempero@auckland.ac.nz; Kelly=Blincoe: k.blincoe@auckland.ac.nz,"Abstract
Context:
Code review is a valuable software process that helps software practitioners to identify a variety of defects in code. Even though many code review tools and 
static analysis
 tools used to improve the efficiency of the process exist, code review is still costly.
Objective:
Understanding the types of defects that code reviews help to identify could reveal other means of cost improvement. Thus, our goal was to identify defect types detected in real-world code reviews, and the extent to which code review can be benefited from defect detection tools.
Method:
To this end, we classified 417 comments from code reviews of 7 OSS Java projects using thematic analysis.
Results:
We identified 116 defect types that we grouped into 15 groups to create a defect classification. Additionally, 38% of these defects could be automatically detected accurately.
Conclusion:
We learnt that even though many capable defect detection tools are available today, a substantial amount of defects that can be detected automatically, reach code review. Also, we identified several code review cost reduction opportunities.",Information and Software Technology,04 Mar 2025,5,"The identification of defect types in real-world code reviews and the potential for automated defect detection could offer some cost reduction opportunities, but the impact on early-stage ventures might not be as significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922001483,Uses of business process modeling in agile software development projects,December 2022,Not Found,Cielo=González Moyano: c.gonzalez.moyano@hu-berlin.de; Luise=Pufahl: Not Found; Ingo=Weber: Not Found; Jan=Mendling: Not Found,"Abstract
Context:
Agile 
methodologies and frameworks
 are widely used in software development projects because of their support for 
continuous change
 and delivery. 
Agile software development
 advocates de-prioritizing aspects such as processes and documentation. In traditional 
software engineering
 methodologies, however, business process models have been extensively used to support these aspects. Up until now, it is unclear to what extent recommendations to focus on code imply that 
conceptual modeling
 should be discontinued.
Objective:
The objective of this study is to investigate this hypothesis. More specifically, we develop a theoretical argument of how business process models are and can be used to support agile software development projects.
Method:
To this end, we use a multi-method study design. First, we conduct a systematic literature review, in which we identify studies on the usage of business process models in agile software development. Second, we apply procedures from thematic synthesis to analyze the connection between these uses and the phases of the 
development cycle
. Third, we use a focus group design with practitioners to systematically reflect upon how these uses can help regarding four categories of challenges in agile software development: management, team, technology, and process.
Results:
From 37 relevant studies, we distill 15 different uses. The results highlight the benefits of process modeling as an instrument to support agile software development projects from different angles and in all project phases. Process modeling appears to be particularly relevant for the first phases of the 
development cycle
, and for management and process issues in agile projects.
Conclusion:
We conclude that business process models indeed provide benefits for agile software development projects. Our findings have practical implications and emphasize the need for future research on modeling and 
agile development
.",Information and Software Technology,04 Mar 2025,7,"The investigation of how business process models can support agile software development projects provides practical implications for startups, especially in understanding the benefits of process modeling in different project phases and addressing management, team, technology, and process challenges."
https://www.sciencedirect.com/science/article/pii/S0950584922001513,Help me with this: A categorization of open source software problems,December 2022,Not Found,Georgia M.=Kapitsaki: Not Found; Nikolaos D.=Tselikas: ntsel@uop.gr; Kyriakos-Ioannis D.=Kyriakou: Not Found; Maria=Papoutsoglou: Not Found,"Abstract
Context:
Free and 
Open Source Software
 is widely used in the research community and the software industry. In this context, developers come across various issues they need to handle in order to use and create software responsibly and without causing legal violations. For instance, using open source software that carries a specific license or how contributions to open source software should be handled are among the issues that need to be considered.
Objective:
As practitioners turn primarily to Q&A sites to seek help, it is important to understand which specific open source software issues they face. In this research, our main objective is to provide a categorization of open source software problems present in the user questions of the Open Source Stack Exchange site and perform a meta-analysis on the encountered questions.
Method:
We have performed a qualitative study analyzing manually 1,500 most popular posts in the Open Source Stack Exchange site and have mapped them to categories and more generic clusters. The coding task was performed in iterations with the participation of three of the authors. Agreement was calculated and cases of disagreement were resolved. Meta-analysis on questions and answers was also performed for discussion purposes.
Results:
We have created 26 categories of problems discussed in the Open Source Stack Exchange site, and grouped them into 6 clusters. Our results show that posts on license texts/conditions and license/copyright notices are more common, whereas posts on license differences are the most popular in terms of views by other users.
Conclusion:
The results can assist any participant of the open source software community to understand on which basic issues she should focus on to gain a good understanding of open source software. They are also useful for improving education on open source software and community support using the implications presented for each category.",Information and Software Technology,04 Mar 2025,6,"The categorization of open source software issues in user questions and the meta-analysis results provide useful insights for the open source software community, but the direct impact on European early-stage ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584922001562,Crex: Predicting patch correctness in automated repair of C programs through transfer learning of execution semantics,December 2022,Not Found,Dapeng=Yan: dapeng.yan@nuaa.edu.cn; Kui=Liu: brucekuiliu@gmail.com; Yuqing=Niu: 977012358@qq.com; Li=Li: li.li@monash.edu; Zhe=Liu: zhe.liu@nuaa.edu.cn; Zhiming=Liu: zliu@nwpu.edu.cn; Jacques=Klein: jacques.klein@uni.lu; Tegawendé F.=Bissyandé: tegawende.bissyande@uni.lu,"Abstract
A significant body of automated program repair literature relies on test suites to assess the validity of generated patches. Because such oracles are weak, state-of-the-art repair tools can validate some patches that overfit the test cases but are actually incorrect. This situation has become a prime concern in APR, hindering its adoption by the industry. This work investigates execution 
semantic features
 based on micro-traces, a form of under-constrained dynamic traces. We build on 
transfer learning
 to explore function code representations that are amenable to semantic similarity computation and can therefore be leveraged for classifying patch correctness. Our 
Crex
 prototype implementation is based on the 
Trex
 framework. Experimental results on patches generated by the CoCoNut APR tool on CodeFlaws programs indicate that our approach can yield high accuracy in predicting patch correctness. The learned embeddings were proven to capture semantic similarities between functions, which was instrumental in training a classifier that identifies patch correctness by learning to discriminate between correctly patched code and incorrectly patched code based on their semantic similarity with the buggy function.",Information and Software Technology,04 Mar 2025,7,The research addresses a practical concern in the field of automated program repair which could have a significant impact on software development practices.
https://www.sciencedirect.com/science/article/pii/S0950584922001550,Influences of UX factors in the Agile UX context of software startups,December 2022,Not Found,Joelma=Choma: jchoma@ufscar.br; Eduardo M.=Guerra: eduardo.guerra@unibz.it; Alexandre=Alvaro: alvaro@ufscar.br; Roberto=Pereira: rpereira@inf.ufpr.br; Luciana=Zaina: lzaina@ufscar.br,"Abstract
Context:
Software startups work under uncertain market conditions, constant time pressures, and extremely limited resources. Startup practitioners commonly adopt agile practices and lean development to build and release software quickly. Within this context, User eXperience (UX) work is critical for generating user value and creating a competitive advantage. However, integrating agile and 
UX
 remains an open question and little explored in software startups.
Objective:
In this study, we investigate how startup practitioners understand the 
UX
 concept and what are the influences of UX factors on the agile context of software startups.
Method:
To achieve this goal, we surveyed software practitioners from software startups in Brazil. We obtained 97 valid responses from professionals working in different areas, in positions of UX experts, software engineers, and managers.
Results:
Our findings show that most software startup practitioners understand UX from a perspective that gives value to the user/customer interaction with the product and company, focusing on achieving a good UX. Regarding the influences of UX factors, we found that most selected factors carried the meaning of delivering value to the business and the user for producing successful products. On the other hand, the lack of resources is a factor that significantly hinders UX work in early-stage startups and with small teams.
Conclusion:
By analyzing our results on four dimensions, covering business & market, product & process, customers & users, and UX work & teams, we provided four takeaways to help practitioners with the adoption of Agile UX in software startups context.",Information and Software Technology,04 Mar 2025,6,"The study provides insights into the integration of agile and UX in software startups, which could be valuable for entrepreneurs in the early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922001318,UX professionals’ learning and usage of UX methods in agile,November 2022,"User experience, UX, Agile development, UX professionals, UX methods, Lifelong learning",Åsa=Cajander: asa.cajander@it.uu.se; Marta=Larusdottir: marta@ru.is; Johannes L.=Geiser: jogeiser@icloud.com,"Abstract
Context
The usage of 
User Experience
 (UX) methods has been studied through the years. However, little is known about UX professionals’ 
lifelong learning
 processes related to UX methods in Agile, choosing what UX methods to use, and the enablers and hindrances for using the UX methods.
Objective
The study aims to broaden current knowledge about UX professionals’ lifelong learning practices to understand their work situations better. The paper describes how UX professionals learn about and choose UX methods, their frequency of use, and the enablers and barriers when using the UX methods in Agile.
Method
An interview study was conducted with 13 UX professionals from various industries and two countries working with Agile and UX. We used a qualitative approach, and a thematic analysis was carried out to answer the research questions.
Results
The results show that support from colleagues is an essential component for learning about the methods and how to use UX methods. Time pressure makes UX professionals choose methods they know will deliver their desired results. Prototyping, user testing, user journeys, and workshops are the most frequently used UX methods. Additionally, the results show that UX professionals think that the UX methods are often too complicated and take too long to learn. Additionally, they find it challenging to integrate UX methods into Agile.
Conclusion
These findings indicate that UX methods might work better if designed to be less complicated and deliver results more efficiently. Moreover, collegial and 
peer learning
 is central to UX professionals. The HCI community could be more active in supporting this culture by sharing information and learning. Finally, the usability and UX of the tools affect which UX methods are used.",Information and Software Technology,04 Mar 2025,4,The study on UX professionals' lifelong learning processes provides some insights but may not have immediate practical implications for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584922001410,Dealing with imbalanced data for interpretable defect prediction,November 2022,Not Found,Yuxiang=Gao: gaoyx@jsnu.edu.cn; Yi=Zhu: zhuy@jsnu.edu.cn; Yu=Zhao: zhaoyu@jsnu.edu.cn,"Abstract
Context
Interpretation has been considered as a key factor to apply 
defect prediction
 in practice. As interpretation from rule-based interpretable models can provide insights about past defects with high quality, many prior studies attempt to construct interpretable models for both accurate prediction and comprehensible interpretation. However, 
class imbalance
 is usually ignored, which may bring huge 
negative impact
 on interpretation.
Objective
In this paper, we are going to investigate resampling techniques, a popular solution to deal with 
imbalanced data
, on interpretation for interpretable models. We also investigate the feasibility to construct interpretable 
defect prediction
 models directly on original data. Further, we are going to propose a rule-based interpretable model which can deal with 
imbalanced data
 directly.
Method
We conduct an empirical study on 47 publicly available datasets to investigate the impact of resampling techniques on rule-based interpretable models and the feasibility to construct such models directly on original data. We also improve gain function and tolerate lower confidence based on 
rule induction
 algorithms to deal with imbalanced data.
Results
We find that (1) resampling techniques impact on interpretable models heavily from both feature importance and model complexity, (2) it is not feasible to construct meaningful interpretable models on original but imbalanced data due to low coverage of defects and poor performance, and (3) our proposed approach is effective to deal with imbalanced data compared with other rule-based models.
Conclusion
Imbalanced data heavily impacts on the interpretable defect prediction models. Resampling techniques tend to shift the learned concept, while constructing rule-based interpretable models on original data may also be infeasible. Thus, it is necessary to construct rule-based models which can deal with imbalanced data well in further studies.",Information and Software Technology,04 Mar 2025,7,The investigation on resampling techniques for interpretable defect prediction models could be valuable for startups dealing with software quality issues.
https://www.sciencedirect.com/science/article/pii/S095058492200146X,S-DABT: Schedule and Dependency-aware Bug Triage in open-source bug tracking systems,November 2022,Not Found,Hadi=Jahanshahi: hadi.jahanshahi@ryerson.ca; Mucahit=Cevik: Not Found,"Abstract
Context:
In 
software engineering
 practice, fixing bugs in a timely manner lowers various potential costs in software maintenance. However, manual bug fixing scheduling can be time-consuming, cumbersome, and error-prone.
Objective:
In this paper, we propose the Schedule and Dependency-aware Bug Triage (S-DABT), a bug triaging method that utilizes 
integer programming
 and 
machine learning techniques
 to assign bugs to suitable developers.
Methods:
Unlike prior works that largely focus on a single component of the 
bug reports
, our approach takes into account the textual data, bug fixing costs, and bug dependencies. We further incorporate the schedule of developers in our formulation to have a more comprehensive model for this multifaceted problem. As a result, this complete formulation considers developers’ schedules and the blocking effects of the bugs while covering the most significant aspects of the previously proposed methods.
Results:
Our numerical study on four open-source software systems, namely, ECLIPSEJDT, LIBREOFFICE, GCC, and MOZILLA, shows that taking into account the schedules of the developers decreases the average bug fixing times. We find that S-DABT leads to a high level of developer utilization by a fair distribution of the tasks among the developers and efficient use of the free spots in their schedules. Via the simulation of the issue tracking system, we also show how incorporating the schedule in the model formulation reduces the bug fixing time, improves the assignment accuracy, and utilizes the capability of each developer without much comprising in the model run times.
Conclusion:
We find that S-DABT decreases the complexity of the bug 
dependency graph
 by prioritizing blocking bugs and effectively reduces the infeasible assignment ratio due to bug dependencies. Consequently, we recommend considering developers’ schedules while automating bug triage.",Information and Software Technology,04 Mar 2025,8,The bug triaging method proposed in the research could have practical value for startups by improving bug fixing efficiency and developer utilization.
https://www.sciencedirect.com/science/article/pii/S0950584922001471,Automatically repairing tensor shape faults in deep learning programs,November 2022,Not Found,Dangwei=Wu: wudangwei@sjtu.edu.cn; Beijun=Shen: bjshen@sjtu.edu.cn; Yuting=Chen: chenyt@sjtu.edu.cn; He=Jiang: jianghe@dlut.edu.cn; Lei=Qiao: fly2moon@163.com,"Abstract
Context:
Software developers frequently invoke 
deep learning
 (DL) APIs to incorporate 
artificial intelligence
 solutions into software systems. However, misuses of these APIs can cause various DL faults, such as 
tensor shape faults
. Tensor shape faults occur when restriction conditions of operations are not met; they are prevalent in practice, leading to many system crashes. Meanwhile, researchers and engineers still face a strong challenge in detecting tensor shape faults — static techniques incur heavy overheads in defining detection rules, and the only dynamic technique requires human engineers to rewrite APIs for tracking shape changes.
Objective:
This paper introduces a novel technique that leverages 
machine learning
 to detect tensor shape faults, and as well uses patterns to repair faults detected.
Methods:
We first construct SFData, a set of 146 buggy programs with 
crashing tensor shape faults
 (i.e., those causing programs to crash). We also conduct an empirical study on crashing tensor shape faults, categorizing them into four types and revealing twelve repair patterns. Then we propose Tensfa2, an automated approach to detecting and repairing crashing tensor shape faults. Tensfa2 employs a 
machine learning method
 to learn from crash messages and 
decision trees
 to detect tensor shape faults. Next, Tensfa2 tracks shape properties by a customized Python 
debugger
, analyzes their 
data dependences
, and uses the twelve patterns to generate patches. Tensfa2 is an extended version of Tensfa—our previous approach presented at ISSRE’21. Its performance is enhanced by two techniques: a search-based method for repairing shape value faults, and a bundle of three ranking strategies for prioritizing the repair patterns.
Results:
Tensfa2 is evaluated on SFData and IslamData (another dataset of tensor shape faults). The results show the effectiveness of Tensfa2. In particular, Tensfa2 achieves an F1-score of 96.88% in detecting the faults and repairs 82 out of 146 buggy programs in SFData.
Conclusion:
We believe that repair patches generated by our approach will help engineers fix their 
deep learning
 programs much more efficiently, saving their time and efforts.",Information and Software Technology,04 Mar 2025,8,The development of Tensfa2 to detect and repair crashing tensor shape faults in deep learning programs can significantly impact European early-stage ventures working in AI and software development.
https://www.sciencedirect.com/science/article/pii/S0950584922001173,Towards building a pragmatic cross-project defect prediction model combining non-effort based and effort-based performance measures for a balanced evaluation,October 2022,Not Found,Yogita=Khatri: 19403019@mail.jiit.ac.in; Sandeep Kumar=Singh: sandeepk.singh@jiit.ac.in,"Abstract
Context
Recent years have witnessed the growing trend in cross-project defect prediction (CPDP), where the training and the testing data come from different projects having different data distributions. Several CPDP methods have been presented in the literature to overcome differences in their distributions, but the majority of the existing approaches have been evaluated considering the availability of unlimited inspection effort, which is practically impossible, thus leading to fallacious conclusions. Further, they focused more on improving Recall over Precision leading to a high probability of false alarm (PF), causing significant wastage of developer's efforts and time.
Objective
Addressing these issues, we propose a Two-Phase Transfer Boosting (TPTB) model, which aims at improving the performance not only in terms of non-effort based measures (NEBMs) (making a balance between Recall and PF) but also in terms of effort based measures (EBMs), considering the availability of limited inspection effort.
Method
To mitigate the distribution differences, the first phase assigns initial weights to the training modules based on the feature distribution and feature importance. The second phase applies the Dynamic Transfer AdaBoost algorithm to build an ensemble classifier to lessen the impact of contradictory training modules. In addition, a sorting strategy is designed to prioritize the modules for further inspection.
Results
Statistical results on 62 datasets revealed a better-balanced performance of our TPTB model holistically over NN-filter, ManualDown, EASC, and Cruz model with performance comparable to WPDP (Within-project defect prediction) considering NEBMs. Besides, when considering EBMs together, TPTB showed statistically and practically more balanced performance as compared to ManualUP and Cruz with overall performance comparable to EASC.
Conclusions
Our results demonstrate the efficacy of the TPTB model in a practical setting empowering the quality assurance team to predict and prioritize the defective modules allocating limited inspection effort by optimally focusing on highly defective modules.",Information and Software Technology,04 Mar 2025,7,The Two-Phase Transfer Boosting model proposed for cross-project defect prediction addresses practical issues faced by startups and could enhance their software quality assurance processes.
https://www.sciencedirect.com/science/article/pii/S0950584922001070,A deep learning-based automated framework for functional User Interface testing,October 2022,Not Found,Zubair=Khaliq: zikayem@gmail.com; Sheikh Umar=Farooq: suf.cs@uok.edu.in; Dawood Ashraf=Khan: dawood.khan@uok.edu.in,"Abstract
Context:
The use of automation tools in software testing helps keep pace with the timeline of the deliverables. Over time with the inclusion of continuous integration/continuous delivery (CI/CD) pipelines, automation tools are becoming less effective. The testing community is turning to 
AI
 to help keep the pace.
Objective:
We study the use of transformers to automate the process of test case generation directly from the User Interface (UI) element description instead of relying on the test specification document from which test cases are extracted manually. We also demonstrate the capability of the proposed approach in repairing flaky tests.
Method:
We employ object 
detection algorithms
 
EfficientDet
 and 
DEtectionTRansformer
 for detecting the elements from an application UI automatically without requiring a tester to locate complex-scripted UI elements. We also use 
Tesseract
 to automatically identify the text present on the UI elements. We transform the generated UI element description to actual test designer-written test cases using text-generation transformers like 
GPT-2
 and 
T5
. The 
generated test cases
 are then translated into executable test scripts using a simple parser. We carry out our 
cases study
 on 30 e-commerce applications.
Results:
The percentage of correct executable test cases generated by the framework employing EfficientDet is 
93.82%
 and employing DEtectionTRansformer is 
98.08%
. The framework eliminates an average of 
96.05%
 flakiness across the applications selected for the study.
Conclusion:
It is concluded that the proposed approach can be used with current automation tools in the industry to enhance their capability in generating test cases and repairing the flaky tests.",Information and Software Technology,04 Mar 2025,8,The proposed approach of using transformers for test case generation and flaky test repair can benefit startups by improving automation processes and overall software quality.
https://www.sciencedirect.com/science/article/pii/S0950584922001021,The journey to technical excellence in agile software development,October 2022,"Agile software development, Software development methods, Technical excellence, Agile principles",Adam=Alami: adaa@itu.dk; Oliver=Krancher: Not Found; Maria=Paasivaara: Not Found,"Abstract
Context:
Technical excellence is a nebulous term in 
agile software development
. This vagueness is risky because it may lead to misunderstandings and to agile implementations that may overlook a key principle of 
agile development
.
Objective:
This study investigates how agile practitioners interpret the concept of technical excellence brought up in Principle 9 of the 
Agile manifesto
. Moreover, we investigate how agile practitioners put the concept into practice and what conditions facilitate putting technical excellence into practice.
Methods:
We conducted semi-structured interviews with twenty agile practitioners, coded the data inductively, and performed two sessions to validate the emerging findings.
Results:
We find that technical excellence is first and foremost a mindset that is underpinned by continuous attention to sustainable code, continuous learning, and teamwork. Fostering technical excellence requires the adoption of design and development practices, such as continuous architecting, and is supported by continuous learning. We also identify three enabling conditions for technical excellence: Leadership support, customer buy-in, and psychological safety. These enablers provide teams with leeway to nurture their pursuit of technical excellence.
Conclusion:
Our findings highlight the key role of people-based strategies in promoting technical excellence in agile software development. They show that the attainment of technical excellence does not only involve technical practices. On the contrary, it relies on social and 
organizational support
 and, most importantly, a mindset.",Information and Software Technology,04 Mar 2025,6,"The study on technical excellence in agile software development provides insights that can be valuable for European startups in enhancing their development practices, but the direct impact may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584922001057,Consolidating a common perspective on Technical Debt and its Management through a Tertiary Study,September 2022,Not Found,Helvio Jeronimo=Junior: jeronimohjr@cos.ufrj.br; Guilherme Horta=Travassos: ght@cos.ufrj.br,"Abstract
Context
Technical Debt
 (TD) contextualizes the technical decisions on shortcuts and workarounds during software development, positively and negatively influencing software evolution. However, TD still seems to confound with any issue occurring during software development, impacting its proper understanding and management in software projects.
Goal
To synthesize evidence regarding the conceptualization, characteristics, and management of TD in software projects.
Method
To undertake a tertiary study to strengthen the knowledge of TD using the principles of Grounded Theory to support qualitative analysis.
Results
Nineteen secondary studies provide evidence on TD and its management. They provided information regarding the TD's understanding (definitions and characteristics) and management (actions and technologies). Some causes, such as project constraints, technical decisions, and team members, promote different types of TD in software projects. The secondary studies also supported identifying the impacts of TD regarding project management, team members, the 
organization's business
, and internal software quality. Besides helping identify TD challenges, such studies contributed to integrating a conjectured conceptual model of TD that can support future discussions and investigations regarding TD's understanding and management.
Conclusions
The set of evidence regarding TD's understanding, actions, and technologies to manage TD can aid software practitioners in their software projects. However, it is observable an interpretation overload regarding its definition, inducing to classify any issue occurring during the software development as TD. Therefore, further discussions and investigations still represent essential steps towards consolidating a common perspective on TD and its management.",Information and Software Technology,04 Mar 2025,7,The synthesis of evidence on technical debt and its management in software projects can offer useful guidance to European early-stage ventures in handling technical challenges during software development.
https://www.sciencedirect.com/science/article/pii/S095058492200091X,The state of the art in measurement-based experiments on the mobile web,September 2022,"Measurement-based experiment, Mobile web, Systematic mapping study",Omar=de Munk: o.de.munk@student.vu.nl; Gian Luca=Scoccia: gianluca.scoccia@univaq.it; Ivano=Malavolta: i.malavolta@vu.nl,"Abstract
Context:
Nowadays the majority of all worldwide Web traffic comes from mobile devices, as we tend to primarily rely on the browsers installed on our smartphones and tablets (e.g., Chrome for 
Android
, Safari for iOS) for accessing 
online services
. A market of such a large scale leads to an extremely fierce competition, where it is of 
paramount importance
 that the developed mobile Web apps are of high quality, e.g., in terms of performance, energy consumption, security, usability. In order to objectively assess the quality of mobile Web apps, practitioners and researchers are conducting experiments based on the measurement of run-time metrics such as 
battery discharge
, CPU and memory usage, number and type of network requests, etc.
Objective:
The objective of this work is to identify, classify, and evaluate the state of the art of conducting measurement-based experiments on the mobile Web. Specifically, we focus on (i) which metrics are employed during experimentation, how they are measured, and how they are analyzed; (ii) the platforms chosen to run the experiments; (iii) what subjects are used; (iv) the used tools and environments under which the experiments are run.
Method:
We apply the 
systematic mapping
 methodology. Starting from a search process that identified 786 potentially relevant studies, we selected a set of 33 primary studies following a rigorous 
selection procedure
. We defined and applied a classification framework to them to extract data and gather relevant insights.
Results:
This work contributes with (i) a classification framework for measurement-based experiments on the mobile Web; (ii) a systematic map of current research on the topic; (iii) a discussion of emergent findings and challenges, and resulting implications for future research.
Conclusion:
This study provides a rigorous and replicable map of the state of the art of conducting measurement-based experiments on the mobile Web. Its results can benefit researchers and practitioners by presenting common techniques, empirical practices, and tools to properly conduct measurement-based experiments on the mobile Web.",Information and Software Technology,04 Mar 2025,7,"This study provides a systematic map of current research on measurement-based experiments on the mobile Web, offering insights and implications for future research that can benefit researchers and practitioners."
https://www.sciencedirect.com/science/article/pii/S0950584922000854,Self-adaptive systems: A systematic literature review across categories and domains,August 2022,Not Found,Terence=Wong: terence.wong@adelaide.edu.au; Markus=Wagner: markus.wagner@adelaide.edu.au; Christoph=Treude: christoph.treude@unimelb.edu.au,"Abstract
Context:
Championed by IBM’s vision of 
autonomic computing
 paper in 2003, the 
autonomic computing
 research field has seen increased research activity over the last 20 years. Several conferences (SEAMS, SASO, ICAC) and workshops (SISSY) have been established and have contributed to the 
autonomic computing
 
knowledge base
 in search of a new kind of system — a self-adaptive system (SAS). These systems are characterized by being context-aware and can act on that awareness. The actions carried out could be on the system or on the context (or environment). The underlying goal of a SAS is the sustained achievement of its goals despite changes in its environment.
Objective:
Despite a number of literature reviews on specific aspects of SASs ranging from their requirements to 
quality attributes
, we lack a systematic understanding of the current state of the art.
Method:
This paper contributes a 
systematic literature review
 into self-adaptive systems using the dblp computer science 
bibliography
 as a database. We filtered the records systematically in successive steps to arrive at 293 relevant papers. Each paper was critically analyzed and categorized into an attribute matrix. This matrix consisted of five categories, with each category having multiple attributes. The attributes of each paper, along with the summary of its contents formed the basis of the literature review that spanned 30 years (1990–2020).
Results:
We characterize the maturation process of the research area from theoretical papers over practical implementations to more holistic and generic approaches, frameworks, and exemplars, applied to areas such as networking, web services, and robotics, with much of the recent work focusing on 
IoT
 and 
IaaS
.
Conclusion:
While there is an ebb and flow of application domains, domains like bio-inspired approaches, security, and cyber–physical systems showed promise to grow heading into the 2020s.",Information and Software Technology,04 Mar 2025,8,"The systematic literature review on self-adaptive systems provides an in-depth analysis of the field's maturation process and application domains, which can be valuable for startups looking to implement such systems in their products."
https://www.sciencedirect.com/science/article/pii/S0950584922000374,Investigating replication challenges through multiple replications of an experiment,July 2022,Not Found,Daniel Amador=dos Santos: daniel.amador@email.com; Eduardo Santana=de Almeida: esa@dcc.ufba.br; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
As Empirical 
Software Engineering
 grows in maturity and number of publications, more replications are needed to provide a solid grounding to the evidence found through prior research. However, replication studies are scarce in general and some topics suffer more than others with such scarcity. On top, the challenges associated with replicating empirical studies are not well understood.
Objective:
In this study, we aim to fill this gap by investigating difficulties emerging when replicating an experiment.
Method:
We used participants with distinct backgrounds to play the role of a research group attempting to replicate an experimental study addressing Highly-Configurable Systems. Seven external close replications in total were performed. After obtaining the quantitative replication results, a 
focus group session
 was applied to each group inquiring about the replication experience. We used the grounded theory’s constant comparison method for the qualitative analysis.
Results:
We have seen in our study that, in the replications performed, most results hold when comparing them with the baseline. However, the participants reported many difficulties in replicating the original study, mostly related to the lack of clarity of the instructions and the presence of defects on replication artifacts. Based on our findings, we provide recommendations that can help mitigate the problems reported.
Conclusions:
The quality of replication artifacts and the lack of clear instructions might impact an experiment replication. We advocate having good quality replication instructions and well-prepared laboratory packages to foster and enable researchers to perform better replications.",Information and Software Technology,04 Mar 2025,6,"The investigation of difficulties in replicating experimental studies on Highly-Configurable Systems provides valuable recommendations for researchers, although the impact on startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000350,Defining adaptivity and logical architecture for engineering (smart) self-adaptive cyber–physical systems,July 2022,Not Found,Ana=Petrovska: ana.petrovska@tum.de; Stefan=Kugele: stefan.kugele@thi.de; Thomas=Hutzelmann: t.hutzelmann@tum.de; Theo=Beffart: theo.beffart@tum.de; Sebastian=Bergemann: sebastian.bergemann@tum.de; Alexander=Pretschner: alexander.pretschner@tum.de,"Abstract
Context:
Modern cyber–physical systems (CPSs) are embedded in the physical world and intrinsically operate in a continuously changing and uncertain environment or 
operational context
. To meet their business goals and preserve or even improve specific adaptation goals, besides the variety of run-time uncertainties and changes to which the CPSs are exposed—the systems need to self-adapt.
Objective:
The current literature in this domain still lacks a precise definition of what self-adaptive systems are and how they differ from those considered non-adaptive. Therefore, in order to answer 
how
 to engineer self-adaptive CPSs or self-adaptive systems in general, we first need to answer 
what
 is adaptivity, correspondingly self-adaptive systems.
Method:
In this paper, we first formally define the notion of adaptivity. Second, within the frame of the formal definitions, we propose a logical architecture for engineering decentralised self-adaptive CPSs that operate in dynamic, uncertain, and partially observable operational contexts. This logical architecture provides a structure and serves as a foundation for the implementation of a class of self-adaptive CPSs.
Results:
First, our results show that in order to answer if a system is adaptive, the right framing is necessary: the system’s adaptation goals, 
its context
, and the time period in which the system is adaptive. Second, we discuss the benefits of our architecture by comparing it with the MAPE-K conceptual model.
Conclusion:
Commonly accepted definitions of adaptivity and self-adaptive systems are necessary for work in this domain to be compared and discussed since the same terms are often used with different semantics. Furthermore, in modern self-adaptive CPSs, which operate in dynamic and uncertain contexts, it is insufficient if the adaptation logic is specified during the system’s design, but instead, the adaptation logic itself needs to adapt and “learn” during run-time.",Information and Software Technology,04 Mar 2025,9,"The formal definition of adaptivity and proposal of a logical architecture for self-adaptive cyber-physical systems offer practical insights for startups operating in dynamic and uncertain environments, showing potential for significant impact."
https://www.sciencedirect.com/science/article/pii/S0950584922000246,Detecting privacy requirements from User Stories with NLP transfer learning models,June 2022,Not Found,Francesco=Casillo: fcasillo@unisa.it; Vincenzo=Deufemia: deufemia@unisa.it; Carmine=Gravino: gravino@unisa.it,"Abstract
Context:
To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems.
Objective:
We present an approach to decrease privacy risks during 
agile software development
 by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile 
Requirement Engineering
 (RE).
Methods:
The proposed approach combines 
Natural Language Processing
 (NLP) and linguistic resources with 
deep learning algorithms
 to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and 
syntactic
 structure of the text. This information is then processed by a pre-trained 
convolutional neural network
, which paved the way for the implementation of a 
Transfer Learning
 technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories.
Results:
The experimental results show that 
deep learning algorithms
 allow to obtain better predictions than those achieved with conventional (shallow) 
machine learning methods
. Moreover, the application of 
Transfer Learning
 allows to considerably improve the accuracy of the predictions, ca. 10%.
Conclusions:
Our study contributes to encourage 
software engineering
 researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models.",Information and Software Technology,04 Mar 2025,8,"The approach to automatically detect privacy-related information in agile software development through deep learning algorithms presents an innovative solution to address privacy risks, which can be highly relevant and beneficial for startups developing privacy-aware systems."
https://www.sciencedirect.com/science/article/pii/S0950584922000337,Speeding up constraint-based program repair using a search-based technique,June 2022,Not Found,Jooyong=Yi: jooyong@unist.ac.kr; Elkhan=Ismayilzada: elkhan@unist.ac.kr,"Abstract
Context:
Constraint-based program repair has been developed as one of the main techniques for automated program repair. Given a buggy program and a test suite, constraint-based program repair first extracts a repair constraint 
φ
, and then synthesizes a patch satisfying 
φ
. Since a patch is synthesized in a correct-by-construction manner (rather than compiling and testing each repair candidate source code), the constraint-based approach, in theory, requires less runtime overhead than the G&V approach. Nevertheless, the performance of existing constraint-based approaches is still suboptimal.
Objective:
In this work, we propose a novel technique to expedite constraint-based program repair. We aim to boost runtime performance without sacrificing repairability and patch quality.
Method:
The existing constraint-based program repair searches for a patch specification in an unguided manner. We introduce a novel guided search algorithm based on 
MCMC
 sampling.
Results:
Our experimental results for the 50 buggy versions of 5 real-world subjects (i.e., 
Libtiff
, 
PHP
, 
GMP
, 
Gzip
, and 
Wireshark
) show that our method named 
FAngelix
 is on average an order of magnitude faster than 
Angelix
 (a state-of-the-art constraint-based program repair tool), showing up to 23 times speed-up. This speed-up is achieved without sacrificing repairability and patch quality.
Conclusion:
This paper proposes a novel technique that expedites constraint-based program repair, using a search-based technique based on 
MCMC
 sampling. Our experimental results show the promise of our approach.",Information and Software Technology,04 Mar 2025,8,"The proposed novel technique aims to boost runtime performance in constraint-based program repair without sacrificing repairability and patch quality, showing significant speed-up in experiments."
https://www.sciencedirect.com/science/article/pii/S095058492200026X,Context2Vector: Accelerating security event triage via context representation learning,June 2022,Not Found,Jia=Liu: Not Found; Runzi=Zhang: runzi_zhang@163.com; Wenmao=Liu: Not Found; Yinghua=Zhang: Not Found; Dujuan=Gu: Not Found; Mingkai=Tong: Not Found; Xingkai=Wang: Not Found; Jianxin=Xue: Not Found; Huanran=Wang: Not Found,"Abstract
Context:
Security teams are overwhelmed by thousands of alerts and events everyday, which are comprehensively collected for threat analysis in 
security operations center
. Although methods based on rules, intelligence and data mining are utilized, the alert fatigue situation is still a challenging problem, slowing down the overall threat investigation process.
Objective:
‘Event polysemy’ phenomenon broadly exists in large-scale event dataset, which means that events of the same category can reveal different purposes in different contexts. This paper aims at exploring, revealing and evaluating the latent patterns embedding in the event contexts, to gain insight on context semantics and reduce manual intervention in event triage tasks.
Method:
A context 
representation learning
 based method, named Context2Vector, is proposed. Contexts are extracted from multiple behavioral views. Then, both dense event representations and sparse topic representations are learnt at the same time and in the same space. A human-in-the-loop topic annotation process is involved and finally, a context deviation detection based method is integrated to generate explainable and informative labels for automated context semantic decoding.
Results:
Various experiments are conducted on a enterprise-scale event dataset. The topic annotation, context related feature importance and top-N event ranking evaluation results show that Context2Vector outperforms traditional methods on the high-risk event identification problems, improving the attacker recall rate by up to 2.25 times within limited events to be investigated.
Conclusion:
It is concluded that event contexts imply practicable and abundant information in regard to behaviors and intents of real threat actors. More precise profiling of network entities can be extracted from contexts, compared to rules, intelligence, and 
anomaly detectors
 used in practice.",Information and Software Technology,04 Mar 2025,7,"The method proposed in this abstract successfully outperforms traditional methods in high-risk event identification, improving attacker recall rate, and reducing manual intervention in event triage tasks."
https://www.sciencedirect.com/science/article/pii/S0950584921002378,A systematic literature review on counterexample explanation,May 2022,Not Found,Arut Prakash=Kaleeswaran: arutprakash.kaleeswaran@de.bosch.com; Arne=Nordmann: arne.nordmann@de.bosch.com; Thomas=Vogel: thomas.vogel@informatik.hu-berlin.de; Lars=Grunske: grunske@informatik.hu-berlin.de,"Abstract
Context:
Safety is of 
paramount importance
 for cyber–physical systems in domains such as automotive, robotics, and avionics. Formal methods such as model checking are one way to ensure the safety of cyber–physical systems. However, adoption of formal methods in industry is hindered by 
usability issues
, particularly the difficulty of understanding model checking results.
Objective:
We want to provide an overview of the state of the art for counterexample explanation by investigating the contexts, techniques, and evaluation of research approaches in this field. This overview shall provide an understanding of current and guide future research.
Method:
To provide this overview, we conducted a systematic literature review. The survey comprises 116 publications that address counterexample explanations for model checking.
Results:
Most primary studies provide counterexample explanations graphically or as traces, minimize counterexamples to reduce complexity, localize errors in the models expressed in the input formats of 
model checkers
, support 
linear temporal logic
 or computation tree logic specifications, and use 
model checkers
 of the Symbolic Model Verifier family. Several studies evaluate their approaches in safety-critical domains with industrial applications.
Conclusion:
We notably see a lack of research on counterexample explanation that targets probabilistic and real-time systems, leverages the explanations to domain-specific models, and evaluates approaches in user studies. We conclude by discussing the adequacy of different types of explanations for users with varying domain and formal methods expertise, showing the need to support laypersons in understanding model checking results to increase adoption of formal methods in industry.",Information and Software Technology,04 Mar 2025,5,"The overview of counterexample explanation approaches provides valuable insights but lacks focus on probabilistic and real-time systems, user studies evaluation, and domain-specific models."
https://www.sciencedirect.com/science/article/pii/S0950584922000064,Empirically developed framework for building trust in distributed agile teams,May 2022,Not Found,Sulabh=Tyagi: sulabhtyagi2k@yahoo.co.in; Ritu=Sibal: Not Found; Bharti=Suri: Not Found,"Abstract
Background:
Organizations are adopting agile practices in distributed software development in order to develop quality software in less time. Using 
agile software development
 in distributed set up has its own set of challenges pertaining to 
face to face interactions
, collaboration, time zone and cultural differences. A strong presence of trust helps to overcome these challenges. A relatively lesser number of empirical studies on multidimensional perspective of trust in distributed 
agile software development
 has motivated this study.
Objective:
This study aims to develop a comprehensive framework to build trust in distributed agile teams.
Method:
This study is based on Grounded Theory research methodology which involves 40 agile practitioners from diverse domains belonging to 19 different software organizations located across seven different countries. Besides, observations in two different software organizations were also performed to gather data. Data has been gathered in the form of semi-structured interviews and field notes.
Result:
Qualitative data analysis
 resulted into five different contributing categories for building trust amongst distributed agile teams. These categories represent multidimensional perspectives that influence trust building amongst agile team members working across different parts of the world.
Conclusion:
This study culminates into a framework for building trust in distributed agile teams. The proposed framework has been developed empirically and has five components that influence trust building. These components are related to working environment, leadership, organizational, personal and socio cultural perspectives. The multidimensional perspective of trust was investigated from an agile practitioners view through their real-life project experiences. Organizations and software practitioners may utilize the results of this study to create a hospitable environment for building trust while practicing agile in a distributed environment.",Information and Software Technology,04 Mar 2025,6,"The study on trust building in distributed agile teams offers a comprehensive framework based on empirical data, contributing to understanding multidimensional perspectives, but may benefit from broader industry application insights."
https://www.sciencedirect.com/science/article/pii/S0950584922000222,Collaboration in software ecosystems: A study of work groups in open environment,May 2022,Not Found,Zhifei=Chen: chenzhifei@njust.edu.cn; Wanwangying=Ma: wwyma@smail.nju.edu.cn; Lin=Chen: lchen@nju.edu.cn; Wei=Song: wsong@njust.edu.cn,"Abstract
Context:
As a particular type of software ecosystem, an 
open source software
 ecosystem (OSSECO) is a collection of interdependent 
open source software
 (OSS) projects which are developed and evolve together. Events happening within an OSSECO inherently involve the collaboration of participants from multiple OSS projects, forming a temporary work group. However, it is still unclear how different members of a work group collaborate to fix cross-project bugs, a typical event in the maintenance of OSSECOs.
Objective:
This study aims to investigate the characteristics of collaboration within a work group when fixing cross-project bugs in an OSSECO. It involves the participants from the upstream (which caused the bugs) and the downstream (which were affected by the bugs) OSS projects.
Method:
We conducted our study on 236 cross-project bugs from the scientific Python ecosystem, involving 571 participants and 91 OSS projects, to understand open collaboration within a work group. We established a quantitative analysis to investigate the members of a work group, along with a qualitative analysis to understand the roles of the members from different OSS communities.
Results:
The results show that: (1) A typical work group is constituted of four to eight members from the core development teams of the two OSS communities. More members concern with the upstream OSS projects and few can make active contributions to both sides; (2) Distinct responsibilities are taken by the two OSS communities, with the downstream members as the problem-finders and the upstream members as the decision-makers or gatekeepers.
Conclusions:
Our findings reveal the collaborative mechanism and the responsibility allocation between the upstream and downstream OSS communities in the ecosystems.",Information and Software Technology,04 Mar 2025,7,The investigation of collaboration in fixing cross-project bugs in an open source software ecosystem provides valuable insights into the collaborative mechanism and responsibility allocation between OSS communities.
https://www.sciencedirect.com/science/article/pii/S0950584921002147,"Software security patch management - A systematic literature review of challenges, approaches, tools and practices",April 2022,Not Found,Nesara=Dissanayake: nesara.madugodasdissanayakege@adelaide.edu.au; Asangi=Jayatilaka: asangi.jayatilaka@adelaide.edu.au; Mansooreh=Zahedi: mansooreh.zahedi@unimelb.edu.au; M. Ali=Babar: ali.babar@adelaide.edu.au,"Abstract
Context:
Software security patch management purports to support the process of patching known software 
security vulnerabilities
. Patching 
security vulnerabilities
 in large and complex systems is a hugely challenging process that involves multiple stakeholders making several interdependent technological and socio-technical decisions. Given the increasing recognition of the importance of software security patch management, it is important and timely to systematically review and synthesise the relevant literature on this topic.
Objective:
This paper aims at systematically reviewing the state of the art of software security patch management to identify the socio-technical challenges in this regard, reported solutions (i.e., approaches, tools, and practices), the rigour of the evaluation and the industrial relevance of the reported solutions, and to identify the gaps for future research.
Method:
We conducted a systematic literature review of 72 studies published from 2002 to March 2020, with extended coverage until September 2020 through forward snowballing.
Results:
We identify 14 socio-technical challenges in software security patch management, 18 solution approaches, tools and practices mapped onto the software security patch management process. We provide a mapping between the solutions and challenges to enable a reader to obtain a holistic overview of the gap areas. The findings also reveal that only 20.8% of the reported solutions have been rigorously evaluated in industrial settings.
Conclusion:
Our results reveal that 50% of the common challenges have not been directly addressed in the solutions and that most of them (38.9%) address the challenges in one phase of the process, namely vulnerability scanning, assessment and prioritisation. Based on the results that highlight the important concerns in software security patch management and the lack of solutions, we recommend a list of future research directions. This study also provides useful insights about different opportunities for practitioners to adopt new solutions and understand the variations of their practical utility.",Information and Software Technology,04 Mar 2025,7,"The systematic review on software security patch management provides valuable insights into challenges, solutions, and evaluation practices, offering practical guidance for improving software security in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921002330,An exploratory study of bug prediction at the method level,April 2022,Not Found,Ran=Mo: moran@mail.ccnu.edu; Shaozhi=Wei: wsz@mails.ccnu.edu.cn; Qiong=Feng: qiongfeng@njust.edu.cn; Zengyang=Li: zengyangli@mail.ccnu.edu.cn,"Abstract
Context:
During the past decades, researchers have proposed numerous studies to predict bugs at different 
granularity
 levels, such as the file level, package level, module level, etc. However, the prediction models at the method level are rarely investigated.
Objective:
In this paper, we investigate to predict bug-prone methods based on method-level 
code metrics
 or history measures, and analyze the prediction importance of each metric.
Method:
To proceed our study, we first propose a series of 
code metrics
 and history measures for conducting method-level bug predictions. Next, we compare the performance of different types of prediction models. Finally, we conduct analyses about the prediction power of each metric, based on which, we further analyze whether we can simplify the prediction models.
Results:
Through our evaluation on eighteen large-scale projects, we have presented: (1) conducting method-level bug prediction has potentials of saving a large portion of effort on code reviews and inspections; (2) models using the proposed code metrics or history measures could achieve a good prediction performance; (3) the prediction importance of each metric distributes differently; (4) a highly simplified prediction model could be derived by just using a few important metrics.
Conclusion:
This study presents how to systematically build models for predicting bug-prone methods, and provides empirical evidence for developers to best select metrics to build method-level bug prediction models.",Information and Software Technology,04 Mar 2025,8,The study on predicting bug-prone methods at the method level presents potential efficiency gains for startups by reducing effort on code reviews and providing empirical evidence for selecting metrics for bug prediction models.
https://www.sciencedirect.com/science/article/pii/S095058492100238X,Introduction to the Special Issue on value and waste in software engineering,April 2022,Not Found,Matthias=Galster: Not Found; Clemente=Izurieta: Not Found; Carolyn=Seaman: Not Found,"Abstract
In the context of software engineering, “value” and “waste” can mean different things to different stakeholders. While traditionally value and waste have been considered from a business or economic point of view, there has been a trend in recent years towards a broader perspective that also includes wider human and societal values. This Special Issue explores value and waste aspects in all areas of software engineering, including identifying, quantifying, reasoning about, and representing value and waste, driving value and avoiding waste, and managing value and waste. In this editorial we provide an introduction to the topic and provide an overview of the contributions included in this Special Issue.",Information and Software Technology,04 Mar 2025,5,"While the exploration of value and waste aspects in software engineering is important, the broader perspective may have limited immediate practical impact on early-stage ventures compared to more specific studies on security or bug prediction."
https://www.sciencedirect.com/science/article/pii/S0950584921001865,A comparison of machine learning algorithms on design smell detection using balanced and imbalanced dataset: A study of God class,March 2022,"Software quality, Design smell detection, Machine learning, God class, Balanced data",Khalid=Alkharabsheh: khalidkh@bau.edu.jo; Sadi=Alawadi: sadi.alawadi@it.uu.se; Victor R.=Kebande: victor.kebande@bth.se; Yania=Crespo: yania@infor.uva.es; Manuel=Fernández-Delgado: manuel.fernandez.delgado@usc.es; José A.=Taboada: joseangel.taboada@usc.es,"Abstract
Context:
Design smell detection has proven to be a significant activity that has an aim of not only enhancing the software quality but also increasing its life cycle.
Objective:
This work investigates whether 
machine learning approaches
 can effectively be leveraged for 
software design
 smell detection. Additionally, this paper provides a comparatively study, focused on using balanced datasets, where it checks if avoiding dataset balancing can be of any influence on the accuracy and behavior during design smell detection.
Method:
A set of experiments have been conducted-using 28 
Machine Learning
 classifiers aimed at detecting God classes. This experiment was conducted using a dataset formed from 12,587 classes of 24 software systems, in which 1,958 classes were manually validated.
Results:
Ultimately, most classifiers obtained high performances,-with Cat Boost showing a higher performance. Also, it is evident from the experiments conducted that data balancing does not have any significant influence on the accuracy of detection. This reinforces the application of machine learning in real scenarios where the data is usually imbalanced by the inherent nature of design smells.
Conclusions:
Machine learning approaches can effectively be used as a leverage for God class detection. While in this paper we have employed SMOTE technique for data balancing, it is worth noting that there exist other methods of data balancing and with other design smells. Furthermore, it is also important to note that application of those other methods may improve the results, in our experiments SMOTE did not improve God class detection.
The results are not fully generalizable because only one design smell is studied with projects developed in a single programming language, and only one balancing technique is used to compare with the imbalanced case. But these results are promising for the application in real design smells detection scenarios as mentioned above and the focus on other measures, such as Kappa, ROC, and MCC, have been used in the assessment of the 
classifier behavior
.",Information and Software Technology,04 Mar 2025,6,"The investigation into machine learning approaches for software design smell detection offers insights on dataset balancing and classifier performance, which could benefit startups in improving software quality through automated detection of design issues."
https://www.sciencedirect.com/science/article/pii/S0950584921002123,An Adaptive Penalty based Parallel Tabu Search for Constrained Covering Array Generation,March 2022,Not Found,Yan=Wang: Not Found; Huayao=Wu: hywu@nju.edu.cn; Xintao=Niu: Not Found; Changhai=Nie: Not Found; Jiaxi=Xu: Not Found,"Abstract
Context:
The generation of the optimal constrained covering arrays is a key challenge in the research field of combinatorial testing, where a variety of Constrained Covering Array Generation (CCAG) algorithms have been developed. However, existing algorithms typically reuse 
constraint solver
 or forbidden tuple-based techniques to handle constraints, which might restrict their potentials on finding smaller arrays.
Objective:
This work dedicates to exploring more effective constraint handling techniques for CCAG, so that the sizes of constrained covering arrays can be further minimized.
Methods:
We propose a novel Adaptive Penalty based Parallel Tabu Search (APPTS) algorithm to address the CCAG problem. 
APPTS
 incorporates a penalty term into the fitness function to handle the constrained 
search space
, and employs an adaptive penalty mechanism to dynamically adjust the penalty weight in different search phases. Moreover, 
APPTS
 adopts Java Parallel Stream to compute the fitness values of candidate solutions to speed up the generation process.
Results:
The performance of APPTS is evaluated against three alternative tabu search-based algorithms (with different penalty and 
parallelization
 mechanisms), and seven state-of-the-art algorithms for CCAG. The results demonstrate the superiority of APPTS over these existing algorithms. In particular, APPTS finds 22 new upper bounds on the sizes of 2-way and 3-way constrained covering arrays.
Conclusion:
The adaptive penalty mechanism provides an effective choice for handling constraints in CCAG, and the 
parallelization
 can help APPTS reduce the generation cost.",Information and Software Technology,04 Mar 2025,8,"The proposal of a novel algorithm for Constrained Covering Array Generation demonstrates practical value by improving the efficiency of testing processes, which is crucial for early-stage ventures seeking to optimize their software testing strategies."
https://www.sciencedirect.com/science/article/pii/S0950584921002305,Programming language implementations for context-oriented self-adaptive systems,March 2022,Not Found,Nicolás=Cardozo: n.cardozo@uniandes.edu.co; Kim=Mens: kim.mens@uclouvain.be,"Abstract
Context
The context-oriented programming paradigm is designed to enable self-adaptation, or dynamic behavior modification of software systems, in response to changes in their surrounding environment. Contextoriented programming offers an adaptation model, from a programming language perspective, that maintains a clean modularisation between the application and adaptation logic, as well as between the components providing adaptations.
Objective
We use three implementation techniques for context-oriented programming languages to assess their appropriateness to foster self-adaptive systems. These approaches take advantage of the capabilities offered by the host programming language to realize self-adaptation as proposed by context-oriented languages.
Method
We evaluate each of these approaches by assessing their modularity and complexity when defining adaptations, and by comparing their run-time performance on a simple benchmark.
Results
Our results show a higher modularity than that for common architecture based self-adaptive systems, while maintaining comparable performance.
Conclusion
We conclude that context-oriented programming is an appropriate paradigm to realize self-adaptation.",Information and Software Technology,04 Mar 2025,8,"The abstract presents a detailed evaluation of context-oriented programming languages, highlighting their modularity and performance benefits for self-adaptive systems. This has practical value for startups looking to build adaptable software systems."
https://www.sciencedirect.com/science/article/pii/S0950584921001889,Does it matter who pays back Technical Debt? An empirical study of self-fixed TD,March 2022,Not Found,Jie=Tan: j.tan@rug.nl; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
Technical Debt (TD) can be paid back either by those that incurred it or by others. We call the former self-fixed TD, and it can be particularly effective, as developers are experts in their own code and are well-suited to fix the corresponding TD issues.
Objective:
The goal of our study is to investigate self-fixed technical debt, especially the extent in which TD is self-fixed, which types of TD are more likely to be self-fixed, whether the remediation time of self-fixed TD is shorter than non-self-fixed TD and how development behaviors are related to self-fixed TD.
Method:
We report on an empirical study that analyzes the self-fixed issues of five types of TD (i.e., Code, Defect, Design, Documentation and Test), captured via 
static analysis
, in more than 44,000 commits obtained from 20 Python and 16 Java projects of the Apache Software Foundation.
Results:
The results show that about half of the fixed issues are self-fixed and that the likelihood of contained TD issues being self-fixed is negatively correlated with project size, the number of developers and total issues. Moreover, there is no significant difference of the survival time between self-fixed and non-self-fixed issues. Furthermore, developers are more keen to pay back their own TD when it is related to lower code level issues, e.g., Defect Debt and Code Debt. Finally, developers who are more dedicated to or knowledgeable about the project contribute to a higher chance of self-fixing TD.
Conclusions:
These results can benefit both researchers and practitioners by aiding the prioritization of TD remediation activities and refining strategies within development teams, and by informing the development of TD 
management tools
.",Information and Software Technology,04 Mar 2025,9,"The study on self-fixed technical debt provides valuable insights for developers, researchers, and practitioners to prioritize TD remediation activities and improve development strategies. This can have a significant impact on the efficiency of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001890,Classifying issue reports according to feature descriptions in a user manual based on a deep learning model,February 2022,"Deep learning, Classification, Issue reports, User manual, Software features, Data-based software engineering, Convolution neural network, Recurrent neural network, Machine learning",Heetae=Cho: Not Found; Seonah=Lee: saleese@gnu.ac.kr; Sungwon=Kang: Not Found,"Abstract
Context
Issue reports are documents with which users report problems and state their opinions on a software system. Issue reports are useful for software maintenance, but managing them requires developers’ considerable manual effort. To reduce such effort, previous studies have mostly suggested methods for automatically classifying issue reports. However, most of those studies classify issue reports according to issue types, based only on whether the report is relevant to a bug, whether the report is duplicated, or whether the issue is functional or nonfunctional.
Objective
In this paper, we intend to link issue reports and a user manual and so propose a deep learning model-based method that classifies issue reports according to software features that are described in the user manual in order to help developers relate issue reports to features to make changes to a software system.
Method
In order to classify issue reports according to the feature descriptions in a user manual, our method uses a 
deep learning technique
 with a 
word embedding
 technique. The key insight in our method is that the sections of a user manual that describe software features contain the words and sentences similar to those in issue reports. Based on the insight, we construct a 
classification model
 that learns the feature descriptions (i.e. sections) in a user manual and classifies issue reports according to the feature descriptions.
Results
We evaluate the proposed method by comparing its classification performance with that of the state-of-the-art method, TicketTagger. The experimental results show that the proposed method yields 10% ∼ 24% higher classification f1-score than that of TicketTagger. We also experiment with two deep learning models and four word embedding techniques and find out that the 
Convolution Neural Network model
 with FastText (or GloVe) yields the best performance.
Conclusion
Our study shows the feasibility of classifying issue reports according to software features, which can be the basis for successive studies to classify issue reports into software features.",Information and Software Technology,04 Mar 2025,7,"The proposed method for classifying issue reports according to software features using deep learning techniques can streamline software maintenance efforts. While this has practical value, it may not directly impact startups as much as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001919,Understanding in-app advertising issues based on large scale app review analysis,February 2022,Not Found,Cuiyun=Gao: gaocuiyun@hit.edu.cn; Jichuan=Zeng: jczeng@cse.cuhk.edu.hk; David=Lo: davidlo@smu.edu.sg; Xin=Xia: xin.xia@monash.edu; Irwin=King: king@cse.cuhk.edu.hk; Michael R.=Lyu: lyu@cse.cuhk.edu.hk,"Abstract
Context:
In-app advertising closely relates to app revenue. Reckless ad integration could adversely impact app quality and 
user experience
, leading to loss of income. It is very challenging to balance the ad revenue and 
user experience
 for app developers.
Objective:
Towards tackling the challenge, we conduct a study on analyzing user concerns about in-app advertisement.
Method:
Specifically, we present a large-scale analysis on ad-related user feedback. The large user feedback data from App Store and Google Play allow us to summarize ad-related app issues comprehensively and thus provide practical ad integration strategies for developers. We first define common ad issues by manually labeling a statistically 
representative sample
 of ad-related feedback, and then build an automatic classifier to categorize ad-related feedback. We study the relations between different ad issues and user ratings to identify the ad issues poorly scored by users. We also explore the fix durations of ad issues across platforms for extracting insights into prioritizing ad issues for ad maintenance.
Results:
(1) We summarize 15 types of ad issues by manually annotating 903 out of 36,309 ad-related user reviews. From a statistical analysis of 36,309 ad-related reviews, we find that users care most about the number of unique ads and ad display frequency during usage. (2) Users tend to give relatively lower ratings when they report the security and notification related issues. (3) Regarding different platforms, we observe that the distributions of ad issues are significantly different between App Store and Google Play. (4) Some ad issue types are addressed more quickly by developers than other ad issues.
Conclusion:
We believe the findings we discovered can benefit app developers towards balancing ad revenue and user experience while ensuring app quality.",Information and Software Technology,04 Mar 2025,8,"Analyzing user concerns about in-app advertising can help developers balance ad revenue and user experience, which is crucial for app quality and revenue generation. This study provides practical insights for startups in the app development space."
https://www.sciencedirect.com/science/article/pii/S0950584921001580,Relationships between software architecture and source code in practice: An exploratory survey and interview,January 2022,Not Found,Fangchao=Tian: tianfangchao@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Muhammad Ali=Babar: ali.babar@adelaide.edu.au,"Abstract
Context
Software Architecture (SA) and Source Code (SC) are two intertwined artefacts that represent the interdependent design decisions made at different levels of abstractions - High-Level (HL) and Low-Level (LL). An understanding of the relationships between SA and SC is expected to bridge the gap between SA and SC for supporting maintenance and evolution of software systems.
Objective
We aimed at exploring practitioners’ understanding about the relationships between SA and SC.
Method
We used a mixed-method that combines an online survey with 87 respondents and an interview with 8 participants to collect the views of practitioners from 37 countries about the relationships between SA and SC.
Results
Our results reveal that: practitioners mainly discuss five features of relationships between SA and SC; a few practitioners have adopted dedicated approaches and tools in the literature for identifying and analyzing the relationships between SA and SC despite recognizing the importance of such information for improving a system's quality attributes, especially 
maintainability
 and reliability. It is felt that cost and effort are the major impediments that prevent practitioners from identifying, analyzing, and using the relationships between SA and SC.
Conclusions
The results have empirically identified five features of relationships between SA and SC reported in the literature from the perspective of practitioners and a systematic framework to manage the five features of relationships should be developed with dedicated approaches and tools considering the cost and benefit of maintaining the relationships.",Information and Software Technology,04 Mar 2025,6,"The study on understanding the relationships between Software Architecture and Source Code provides insights that can benefit software maintenance and system quality. While valuable, the impact on early-stage ventures may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492100152X,Introduction to the Special Issue on: Grey Literature and Multivocal Literature Reviews (MLRs) in software engineering,January 2022,Not Found,Vahid=Garousi: v.garousi@qub.ac.uk; Michael=Felderer: michael.felderer@uibk.ac.at; Mika V.=Mäntylä: mika.mantyla@oulu.fi,"Abstract
In parallel to academic (peer-reviewed) literature (e.g., journal and conference papers), an enormous extent of grey literature (GL) has accumulated since the inception of software engineering (SE). GL is often defined as “literature that is not formally published in sources such as books or journal articles”, e.g., in the form of trade magazines, online blog-posts, technical reports, and online videos such as tutorial and presentation videos. GL is typically produced by SE practitioners. We have observed that researchers are increasingly using and benefitting from the knowledge available within GL. Related to the notion of GL is the notion of Multivocal Literature Reviews (MLRs) in SE, i.e., a MLR is a form of a Systematic Literature Review (SLR) which includes knowledge and/or evidence from the GL in addition to the peer-reviewed literature. MLRs are useful for both researchers and practitioners because they provide summaries of both the state-of-the-art and -practice in a given area. MLRs are popular in other fields and have started to appear in SE community. It is timely then for a Special Issue (SI) focusing on GL and MLRs in SE. From the pool of 13 submitted papers, and after following a rigorous peer review process, seven papers were accepted for this SI. In this introduction we provide a brief overview of GL and MLRs in SE, and then a brief summary of the seven papers published in this SI.",Information and Software Technology,04 Mar 2025,4,"While the idea of Multivocal Literature Reviews in Software Engineering is interesting, the practical impact on early-stage ventures may be limited as it focuses more on academic research."
https://www.sciencedirect.com/science/article/pii/S0950584921001701,Improving Agile Software Development using User-Centered Design and Lean Startup,January 2022,Not Found,Maximilian=Zorzetti: maximilian.zorzetti@acad.pucrs.br; Ingrid=Signoretti: ingrid.manfrim@acad.pucrs.br; Larissa=Salerno: larissa.salerno@acad.pucrs.br; Sabrina=Marczak: sabrina.marczak@pucrs.br; Ricardo=Bastos: bastos@pucrs.br,"Abstract
Context:
Agile methods have limitations concerning problem understanding and solution finding, which can cause organizations to push misguided products and accrue waste. Some authors suggest combining agile methods with discovery-oriented approaches to overcome this, with notable candidates being User-Centered Design (UCD) and Lean Startup, a combination of which there is yet not a demonstrated, comprehensive study on how it works.
Objective:
To characterize a development approach combination of 
Agile Software Development
, UCD, and Lean Startup; exposing how the three approaches can be intertwined in a single 
development process
 and how they affect development.
Method:
We conducted a 
case study
 with two industry software development teams that use this combined approach, investigating them through interviews, observation, focus groups, and a workshop during a nine-month period in which they were stationed in a custom-built development lab.
Results:
The teams are made up of user advocates, business advocates, and solution builders; while their development approach emphasizes experimentation by making heavy use of build-measure-learn cycles. The combined approach promotes a problem-oriented mindset, encouraging team members to work together and engage with the entire 
development process
, actively discovering stakeholders needs and how to fulfill them. Each of its approaches provide a unique contribution to the development process: UCD fosters empathy with stakeholders and enables teams to better understand the problem they are tasked with solving; Lean Startup introduces experimentation as the guiding force of development; and 
Extreme Programming
 (the teams’ agile method) provides support to experimentation and achieving technical excellence.
Conclusion:
The combined approach pushes teams to think critically throughout the development effort. Our practical example provides insight on its essence and might inspire industry practitioners to seek a similar development approach based on the same precepts.",Information and Software Technology,04 Mar 2025,8,"The combination of Agile, UCD, and Lean Startup methodologies in software development has a high practical value for early-stage ventures, offering insights into a comprehensive approach that can benefit startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001488,Developing Mobile Applications Via Model Driven Development: A Systematic Literature Review,December 2021,Not Found,Md.=Shamsujjoha: md.shamsujjoha@monash.edu; John=Grundy: john.grundy@monash.edu; Li=Li: li.li@monash.edu; Hourieh=Khalajzadeh: hourieh.khalajzadeh@monash.edu; Qinghua=Lu: qinghua.lu@data61.csiro.au,"Abstract
Context:
Mobile applications (known as “apps”) usage continues to rapidly increase, with many new apps being developed and deployed. However, developing a mobile app is challenging due to its dependencies on devices, technologies, platforms, and deadlines to reach the market. One potential approach is to use 
M
odel 
D
riven 
D
evelopment (MDD) techniques that simplify the app 
development process
, reduce complexity, increase abstraction level, help achieve scalable solutions and maximize cost-effectiveness and productivity.
Objective:
This paper systematically investigates what 
MDD
 techniques and methodologies have been used to date to support mobile app development and how these techniques have been employed, to identify key benefits, limitations, gaps and future research potential.
Method:
A Systematic Literature Review approach was used for this study based on a formal protocol. The rigorous search protocol identified a total of 1,042 peer-reviewed academic research papers from four major 
software engineering
 databases. These papers were subsequently filtered, and 55 high quality relevant studies were selected for analysis, synthesis, and reporting.
Results:
We identified the popularity of different applied 
MDD
 approaches, supporting tools, artifacts, and evaluation techniques. Our analysis found that architecture, domain model, and code generation are the most crucial purposes in MDD-based app development. Three qualities – productivity, scalability and reliability – can benefit from these modeling strategies. We then summarize the key collective strengths, limitations, gaps from the studies and made several future recommendations.
Conclusion:
There has been a steady interest in MDD approaches applied to mobile app development over the years. This paper guides future researchers, developers, and stakeholders to improve app development techniques, ultimately that will help end-users in having more effective apps, especially when some recommendations are addressed, e.g., taking into account more human-centric aspects in app development.",Information and Software Technology,04 Mar 2025,7,"The study on Model Driven Development techniques for mobile apps provides valuable insights that can help improve development processes for mobile startups, offering practical benefits and future research potential."
https://www.sciencedirect.com/science/article/pii/S0950584921001373,Feature-based insight for forks in social coding platforms,December 2021,Not Found,Hamzeh=Eyal Salman: hamzehmu@mutah.edu.jo,"Abstract
Context:
Recently, fork-based development has shown to be an easy and straightforward technique to reuse the 
source code
 of existing projects (upstream projects and their forks) in 
open source communities
 (for example, GitHub) and industry. This technique allows developers to tailor the existing forks to build their applications and thus reduce the development’s burden.
Objective:
However, when the number of forks of a given repository increases, it is difficult to manually maintain and keep track of the development activities across all existing forks. Consequently, this leads to redundant development activities and to lose the efforts of the developers and maintainers. In this article, an automatic approach is proposed to overcome the above-mentioned problems.
Method:
The proposed approach incorporates a mathematical research technique called formal 
concept analysis
 with other proposed algorithms.
Results:
To evaluate the effectiveness of the proposed approach, it is applied on four software projects from different domains and sizes. The results show that the proposed approach gives promising results according to well-known metrics in the subject.
Conclusion:
Also, it significantly outperforms the existing state-of-the-art and gives developers in 
open source communities
 and industry a development overview about forks of a given repository.",Information and Software Technology,04 Mar 2025,5,"The automatic approach for managing forks of software projects is useful for open source communities and industry, but its direct impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921001464,Sequential coding patterns: How to use them effectively in code recommendation,December 2021,Not Found,Luiz Laerte Nunes=da Silva: luiznunes@id.uff.br; Troy Costa=Kohwalter: troy@ic.uff.br; Alexandre=Plastino: plastino@ic.uff.br; Leonardo Gresta Paulino=Murta: leomurta@ic.uff.br,"Abstract
Context:
Some programming constructs frequently appear together in different parts of the code, representing sequential coding patterns throughout the project. These sequential coding patterns can be mined from the project repository and, whenever the code a developer is writing coincides with the beginning of a sequential pattern, the remainder of this pattern can be suggested to the developer. This is equivalent to the usual Code Completion, which suggests 
syntactic
 structures based on the line being programmed. However, instead of providing 
syntactic
 suggestions for completing the current line, such feature suggests code snippets containing multiple lines.
Objective:
This paper contributes with an in-depth study on how code pattern recommendation can be used effectively.
Method:
We answer three research questions through a quantitative study using a robust experimental infrastructure with a corpus of five open-source projects: (1) “In a code recommendation, how many frequent coding patterns should be presented?”, (2) “What is the impact of filtering sequential patterns by their confidence?”, and (3) “Does the effectiveness of the sequential coding patterns degrade over time?”.
Results:
Our study shows that it is possible to achieve correctness above 80% when using suggestions with the highest confidence values and that a threshold confidence of 30% generally provides better outcomes. Furthermore, it shows that frequent code pattern completion effectiveness tends to degrade 50 commits after the patterns have been mined.
Conclusion:
We could observe that: (1) the top five ranked suggestions are the ones that deliver the best results; (2) the code recommendations that deliver the best results are the ones with the highest confidence values; and (3) the code recommendation performance degrades as the source code evolves because patterns become outdated.",Information and Software Technology,04 Mar 2025,6,"The study on code pattern recommendation provides insights that can enhance coding practices, benefiting startups in the software development process, but may not have immediate practical application for all early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001270,The impact of using biased performance metrics on software defect prediction research,November 2021,Not Found,Jingxiu=Yao: JingxiuYao@buaa.edu.cn; Martin=Shepperd: martin.shepperd@brunel.ac.uk,"Abstract
Context:
Software engineering
 researchers have undertaken many experiments investigating the potential of software 
defect prediction
 algorithms. Unfortunately some widely used performance metrics are known to be problematic, most notably F1, but nevertheless F1 is widely used.
Objective:
To investigate the potential impact of using F1 on the validity of this large body of research.
Method:
We undertook a 
systematic review
 to locate relevant experiments and then extract all 
pairwise comparisons
 of 
defect prediction
 performance using F1 and the unbiased Matthews 
correlation coefficient
 (MCC).
Results:
We found a total of 38 primary studies. These contain 12,471 pairs of results. Of these comparisons, 21.95% changed direction when the MCC metric is used instead of the biased F1 metric. Unfortunately, we also found evidence suggesting that F1 remains widely used in 
software defect
 prediction research.
Conclusion:
We reiterate the concerns of statisticians that the F1 is a problematic metric outside of an information retrieval context, since we are concerned about both classes (defect-prone and not defect-prone units). This inappropriate usage has led to a substantial number (more than one fifth) of erroneous (in terms of direction) results. Therefore we urge researchers to (i) use an unbiased metric and (ii) publish detailed results including 
confusion matrices
 such that alternative analyses become possible.",Information and Software Technology,04 Mar 2025,6,"The research raises concerns about the use of a widely used metric (F1) in software defect prediction, highlighting potential impact on research validity. This could be valuable for startups looking to improve their defect prediction algorithms."
https://www.sciencedirect.com/science/article/pii/S0950584921000999,RBAC protection-impacting changes identification: A case study of the security evolution of two PHP applications,November 2021,Not Found,Marc-André=Laverdière: marc-andre.laverdiere-papineau@polymtl.ca; Karl=Julien: karl.julien@polymtl.ca; Ettore=Merlo: ettore.merlo@polymtl.ca,"Abstract
Abstract:
Web applications often use Role-Based Access Control (RBAC) to restrict operations and protect security 
sensitive information
 and resources.
Context:
Web applications’ RBAC security may be affected by 
source code
 changes between releases. Developers should re-validate their application prior to release, but this may be labor and resource-intensive.
Objective:
Among all changes between two versions during software evolution, we define Protection-Impacting Changes (PICs) as changed statements that potentially alter privilege protection of other statement(s). PICs may focus the attention of developers towards root cause candidates for security protection changes, especially when these protection changes are unexpected.
Method:
The proposed automated 
static analysis
 identifies PICs between two versions of an application. It is based on the interprocedural 
flow graph
 
reachability
 analysis of security checks and statements.
Results:
We examined the software evolution of two PHP web applications. We examined 210 versions of WordPress, and 192 versions of MediaWiki. Additional experiments have been performed on 19 fix commits corresponding to Common Vulnerabilities and Exposures CVEs from WordPress. They are presented and discussed in this paper and show that PICs contain 98.2% of the CVE oracle root causes.
Conclusion:
PICs represent overall only 8% and 2% of total code changes, respectively for WordPress and MediaWiki. PICs may help developers to focus onto a smaller number of candidate security-related problems, during software evolution. Consequently, developers may re-validate application security and perform repairs more efficiently.",Information and Software Technology,04 Mar 2025,7,Developing an automated tool to identify Protection-Impacting Changes in web applications could be highly beneficial for startups looking to enhance their security measures efficiently during software evolution.
https://www.sciencedirect.com/science/article/pii/S0950584921001300,Crowdsourced test report prioritization considering bug severity,November 2021,Not Found,Yao=Tong: Not Found; Xiaofang=Zhang: xfzhang@suda.edu.cn,"Abstract
In crowdsourced testing, a large number of test reports will be generated in a short time. How to efficiently inspect these reports becomes one of the critical steps in the testing process. In recent years, many automated techniques like clustering, classification, and prioritization have emerged to provide an automated inspection order over test reports. Even though these methods have achieved 
good performance
, they did not consider the priority to image and text information. Simultaneously, existing prioritization approaches only focus on the rate of detecting faults but ignore the severity of the faults. In fact, bug severity is a vital indicator that the users provide to flag the 
criticality
 of a bug, so developers can then use it to set their priority for the resolution process. For these reasons, this paper presents a novel prioritization approach for crowdsourcing test reports. It extracts features from text and screenshot information of the test reports, uses the hash technique to index test reports, and finally designs a prioritization algorithm. To validate our approach, we conducted experiments on six industrial projects. The results and the hypotheses analysis show that our approach can detect all faults faster in a limited time and can prioritize reports that have higher severity faults compared with the existing methods.",Information and Software Technology,04 Mar 2025,8,"The proposed novel prioritization approach for crowdsourcing test reports addresses important factors like bug severity, text, and image information. Startups could benefit from a more efficient and effective way to prioritize test reports and detect faults faster."
https://www.sciencedirect.com/science/article/pii/S0950584921001002,Visual Resume: Exploring developers’ online contributions for hiring,October 2021,Not Found,Sandeep Kaur=Kuttal: sandeep-kuttal@utulsa.edu; Xiaofan=Chen: Not Found; Zhendong=Wang: Not Found; Sogol=Balali: Not Found; Anita=Sarma: Not Found,"Abstract
Context:
Recruiters and practitioners are increasingly relying on online activities of developers to find a suitable candidate. Past empirical studies have identified technical and soft skills that managers use in online peer 
production sites
 when making hiring decisions. However, finding candidates with relevant skills is a labor-intensive task for managers, due to the sheer amount of information online peer 
production sites
 contain.
Objective:
We designed a profile aggregation tool—Visual Resume—that aggregates contribution information across two types of peer production sites: a code hosting site (GitHub) and a technical Q&A forum (Stack Overflow). Visual Resume displays summaries of developers’ contributions and allows easy access to their contribution details. It also facilitates 
pairwise comparisons
 of candidates through a card-based design. We present the motivation for such a design and design guidelines for creating such recruitment tool.
Methods:
We performed a scenario-based evaluation to identify how participants use developers’ online contributions in peer production sites as well as how they used Visual Resume when making hiring decisions.
Results:
Our analysis helped in identifying the technical and soft skill cues that were most useful to our participants when making hiring decisions in online production sites. We also identified the information features that participants used and the ways the participants accessed that information to select a candidate.
Conclusions:
Our results suggest that Visual Resume helps in participants evaluate cues for technical and soft skills more efficiently as it presents an aggregated view of candidate’s contributions, allows drill down to details about contributions, and allows easy comparison of candidates via movable cards that could be arranged to match participants’ needs.",Information and Software Technology,04 Mar 2025,7,The Visual Resume tool could be valuable for recruiters and practitioners in startups by simplifying the evaluation of candidates based on their online contributions. It could streamline the hiring process by providing a consolidated view of candidates' skills.
https://www.sciencedirect.com/science/article/pii/S0950584921000872,TIDY: A PBE-based framework supporting smart transformations for entity consistency in PowerPoint,October 2021,Not Found,Shuguan=Liu: liu_shuguan@126.com; Huiyan=Wang: cocowhy1013@gmail.com; Chang=Xu: changxu@nju.edu.cn,"Abstract
Context:
Programming by Example (PBE) is increasingly assisting human users by recognizing and executing repetitive tasks, such as text editing and spreadsheet manipulation. Yet, existing work falls short on dealing with rich-formatted documents like PowerPoint (PPT) files, when examples are few and collecting them is intrusive.
Objective:
This article presents 
TIDY
, a PBE-based framework, to assist automated entity transformations for their layout and style consistency in rich-formatted documents like PowerPoint, in a way adaptive to entity contexts and flexible with user selections.
Methods:
TIDY
 achieves this by examining entities’ operation histories, and proposes a two-stage framework to first identify user intentions behind histories and then make wise next-operation recommendations for users, in order to maintain the entity consistency for rich-formatted documents.
Results:
We implemented 
TIDY
 as a prototype tool and integrated it into PowerPoint as a plug-in module. We experimentally evaluated 
TIDY
 with real-world user operation data. The evaluation reports that 
TIDY
 achieved promising effectiveness with a hit rate of 77.3% on average, which was stably holding for a variety of editing tasks. Besides, 
TIDY
 took only marginal time overhead, costing several to several tens of milliseconds, to complete each recommendation.
Conclusion:
TIDY
 assists users to complete repetitive tasks in rich-formatted documents by non-intrusive user intention recognition and smart next-operation recommendations, which is effective and practically useful.",Information and Software Technology,04 Mar 2025,8,"The TIDY framework offers a practical solution for automating entity transformations in rich-formatted documents like PowerPoint, which could be beneficial for startups looking to streamline repetitive tasks and maintain consistency in document editing."
https://www.sciencedirect.com/science/article/pii/S0950584921000884,Overcoming cultural barriers to being agile in distributed teams,October 2021,Not Found,Darja=Šmite: Darja.Smite@bth.se; Nils Brede=Moe: Nils.B.Moe@bth.se; Javier=Gonzalez-Huerta: Javier.Gonzalez.Huerta@bth.se,"Abstract
Context:
 Agile methods in offshored projects have become increasingly popular. Yet, many companies have found that the use of agile methods in coordination with companies located outside the regions of early agile adopters remains challenging. 
India
 has received particular attention as the leading destination of offshoring contracts due to significant cultural differences between sides of such contracts. Alarming differences are primarily rooted in the hierarchical business culture of Indian organizations and related command-and-control management behavior styles.
Objective:
 In this study, we attempt to understand whether cultural barriers persist in distributed projects in which Indian engineers work with a more empowering Swedish management, and if so, how to overcome them. The present work is an invited extension of a 
conference paper
.
Method:
 We performed a multiple-case study in a mature agile company located in Sweden and a more hierarchical Indian vendor. We 
collected data
 from five group interviews with a total of 34 participants and five workshops with 96 participants in five distributed 
DevOps
 teams, including 36 Indian members, whose preferred behavior in different situations we surveyed.
Results:
 We identified twelve cultural barriers, six of which were classified as impediments to 
agile software development
 practices, and report on the manifestation of these barriers in five 
DevOps
 teams. Finally, we put forward recommendations to overcome the identified barriers and emphasize the importance of 
cultural training
, especially when onboarding new team members.
Conclusions:
 Our findings confirm previously reported behaviors rooted in cultural differences that impede the adoption of agile approaches in offshore collaborations, and identify new barriers not previously reported. In contrast to the existing opinion that cultural characteristics are rigid and unchanging, we found that some barriers present at the beginning of the studied collaboration disappeared over time. Many offshore members reported behaving similarly to their onshore colleagues.",Information and Software Technology,04 Mar 2025,6,"The study addresses cultural barriers in distributed projects involving Indian engineers and Swedish management, providing recommendations and emphasizing cultural training. It has practical value for early-stage ventures looking to improve offshore collaborations."
https://www.sciencedirect.com/science/article/pii/S0950584921000896,Evaluating and comparing memory error vulnerability detectors,September 2021,Not Found,Yu=Nong: yu.nong@wsu.edu; Haipeng=Cai: haipeng.cai@wsu.edu; Pengfei=Ye: pengfei.ye@wsu.edu; Li=Li: Li.Li@monash.edu; Feng=Chen: Feng.Chen@utdallas.edu,"Abstract
Context:
Memory error vulnerabilities have been consequential and several well-known, open-source memory error vulnerability detectors exist, built on static and/or dynamic code analysis. Yet there is a lack of assessment of such detectors based on rigorous, quantitative accuracy and efficiency measures while not being limited to 
specific application domains
.
Objective:
Our study aims to assess and explain the 
strengths
 and weaknesses of state-of-the-art memory error vulnerability detectors based on static and/or dynamic code analysis, so as to inform tool selection by practitioners and future design of better detectors by researchers and tool developers.
Method:
We empirically evaluated and compared five state-of-the-art memory error vulnerability detectors against two benchmark datasets of 520 and 474 C/C++ programs, respectively. We conducted 
case studies
 to gain in-depth explanations of successes and failures of individual tools.
Results:
While generally fast, these detectors had largely varied accuracy across different vulnerability categories and moderate overall accuracy. Complex code (e.g., deep loops and recursions) and data (e.g., deeply embedded linked lists) structures appeared to be common, major barriers. Hybrid analysis did not always outperform purely static or dynamic analysis for memory error 
vulnerability detection
. Yet the evaluation results were noticeably different between the two datasets used. Our 
case studies
 further explained the performance variations among these detectors and enabled additional actionable insights and recommendations for improvements.
Conclusion:
There was no single most effective tool among the five studied. For future research, integrating different techniques is a promising direction, yet simply combining different classes of code analysis (e.g., static and dynamic) may not. For practitioners to choose right tools, making various tradeoffs (e.g., between precision and recall) might be inevitable.",Information and Software Technology,04 Mar 2025,8,"The study evaluates memory error vulnerability detectors based on static and/or dynamic code analysis, providing insights for practitioners and future tool developers. The findings can impact startups by informing tool selection for better software security."
https://www.sciencedirect.com/science/article/pii/S0950584921000707,Automated formalization of structured natural language requirements,September 2021,Not Found,Dimitra=Giannakopoulou: dimitra.giannakopoulou@nasa.gov; Thomas=Pressburger: tom.pressburger@nasa.gov; Anastasia=Mavridou: anastasia.mavridou@nasa.gov; Johann=Schumann: johann.m.schumann@nasa.gov,"Abstract
The use of structured natural languages to capture requirements provides a reasonable trade-off between ambiguous natural language and unintuitive 
formal notations
. There are two major challenges in making structured natural language amenable to formal analysis: (1) formalizing requirements as formulas that can be processed by analysis tools and (2) ensuring that the formulas conform to the semantics of the structured natural language. 
fretish
 is a structured natural language that incorporates features from existing research and from NASA applications. Even though 
fretish
 is quite expressive, its underlying semantics is determined by the types of four fields: 
scope
, 
condition
, 
timing
, and 
response
. Each combination of field types defines a template with Real-Time Graphical Interval Logic (RTGIL) semantics. We have developed a framework that constructs temporal logic formulas for each template compositionally, from its fields. The compositional nature of our algorithms facilitates maintenance and extensibility. Our goal is to be inclusive not only in terms of language expressivity, but also in terms of requirements analysis tools that we can interface with. For this reason we generate metric-temporal logic formulas with (1) exclusively future-time operators, over both finite and infinite traces, and (2) exclusively past-time operators. To establish trust in the produced formalizations for each template, our framework: (1) extensively tests the generated formulas against the template semantics and (2) proves equivalence between its past-time and future-time formulas. Our approach is available through the open-source tool 
fret
 and has been used to capture and analyze requirements for a Lockheed Martin Cyber–Physical System challenge.",Information and Software Technology,04 Mar 2025,7,The framework for capturing requirements using structured natural language and temporal logic formulas has practical implications for startups in enhancing requirements analysis processes. The inclusive approach and open-source tool availability can benefit early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000665,Exploring the communication functions of comments during bug fixing in Open Source Software projects,August 2021,Not Found,Sandra L.=Ramírez-Mora: sandra.ramirez@ciencias.unam.mx; Hanna=Oktaba: Not Found; Helena=Gómez-Adorno: Not Found; Gerardo=Sierra: Not Found,"Abstract
Context:
Bug fixing is a frequent and important task in 
Open Source Software
 (OSS) development and involves the communication of messages, which can serve for multiple purposes and affect the efficiency and effectiveness of corrective software activities.
Objective:
This work is aimed at studying the communication functions of bug comments and their associations with fast and complete bug fixing in 
OSS
 development.
Method:
Over 500K comments and 89K bugs of 100 
OSS projects
 were extracted from three Issue Tracking Systems. Six thousand comments were manually tagged to create a corpus of communication functions. The extracted comments were automatically tagged using 
machine learning algorithms
 and the corpus of communication functions. Statistical and correlation analyses were performed and the most frequent comments communicated during fast and successful bug fixing were identified.
Results:
Significant differences in the distribution of comments of fixed and not fixed bugs were found. Variations in the distribution of comments of bugs with different fixing time were also found. Referential comments that provided objective information were found to be the most frequent messages. Results showed that the percentages of conative and emotive comments are greater when bugs are resolved without the requested fixes and when fixes are implemented in a long time.
Conclusion:
Associations between communication functions and bug fixing exist. The results of this work could be used to improve corrective tasks in 
OSS
 development and some other specific linguistic aspects should be studied in detail in OSS communities.",Information and Software Technology,04 Mar 2025,5,"The study on bug fixing communication functions in OSS development provides insights for improving corrective tasks, but the direct impact on startups may be limited. The findings can be useful for OSS communities, but may not have high practical value for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000501,Test case generation for agent-based models: A systematic literature review,July 2021,Not Found,Andrew G.=Clark: agclark2@sheffield.ac.uk; Neil=Walkinshaw: n.walkinshaw@sheffield.ac.uk; Robert M.=Hierons: r.hierons@sheffield.ac.uk,"Abstract
Context:
Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a 
software fault
 may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential.
Objective:
Our objective is to summarise the state-of-the-art techniques for test case generation in agent-based models and identify future research directions.
Method:
We have conducted a systematic literature review in which we pose five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test?
Results:
Out of the 464 
initial search results
, we identified 24 primary publications. Based on these primary publications, we formed a taxonomy to summarise the state-of-the-art techniques for test case generation in agent-based models. Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Furthermore, the majority of techniques cannot test non-functional requirements or “soft goals”.
Conclusions:
This paper reports insights into the key developments and open challenges concerning test case generation in agent-based models that may be of interest to both researchers and practitioners. In particular, we identify the need for test case generation techniques that focus on societal and non-functional behaviour, and a more thorough evaluation using realistic 
case studies
 that feature challenging properties associated with a typical agent-based model.",Information and Software Technology,04 Mar 2025,7,"The review on test case generation in agent-based models identifies key techniques and challenges, offering insights for researchers and practitioners. The findings can benefit startups by improving testing processes for complex emergent phenomena simulations."
https://www.sciencedirect.com/science/article/pii/S0950584921000410,Generating feasible protocol test sequences from EFSM models using Monte Carlo tree search,July 2021,Not Found,Ting=Shu: shuting@zstu.edu.cn; Yechao=Huang: 1766254653@qq.com; Zuohua=Ding: zouhuading@hotmail.com; Jinsong=Xia: js_xia@126.com; Mingyue=Jiang: jiang_my@126.com,"Abstract
Context:
Feasible test sequences generation is a key step in protocol conformance testing based on the Extended 
Finite State Machine
 (EFSM) model. To guarantee the feasibility of generated test sequences, transition executability analysis (TEA) technique is widely applied in automatic test derivation. However, the TEA method often suffers from the famous state explosion problem, which has become a major obstacle to its efficient application.
Objective:
In order to mitigate this issue, this paper proposed a novel heuristic TEA method (MTEA) that uses Monte Carlo tree search (MCTS) to guide the TEA tree expansion for efficiently deriving feasible test sequences.
Method:
The approach first provides a framework to apply the MCTS algorithm based on multiple decision subtrees, in the context of test sequence generation for EFSM-specified systems, to more efficiently expanding the TEA tree with huge state space, and thus alleviating the problem of state explosion. To achieve this, we then design a reward function to calculate the fitness of nodes currently being expanded in the TEA tree and heuristically direct the search towards a near-optimal solution. Next, an adaptive reduction mechanism of search budget is also introduced to accelerate the convergence of the analysis. Finally, a MTEA-based algorithm for automatically generating feasible test sequences is presented under a specific transition coverage criterion.
Results:
A detailed 
case study
 on 6 popular EFSMs was carried out to evaluate the effectiveness and efficiency of our method. Experimental results show that the MTEA significantly outperforms Breadth-First-Search based TEA method (BTEA) and the standard MCTS-based method (SMCTS), regarding time and space performance. Compared with the BTEA, SMCTS and random TEA method (RTEA), the success rate of test generation of MTEA (98.14% on average) is approximately 2, 1.85 and 3 times higher, respectively. For successful test derivation, MTEA only needs to explore on average 9.95% of the nodes and consume on average 61.68% of the runtime of the BTEA method.
Conclusion:
The experiments illustrate the promise of our approach for alleviating the state explosion problem in test generation for EFSM-specified systems.",Information and Software Technology,04 Mar 2025,8,The proposed MTEA method addresses a significant challenge in test sequence generation for EFSM-specified systems and shows promising results in improving efficiency and performance.
https://www.sciencedirect.com/science/article/pii/S0950584921000471,A practical algorithm for learning disjunctive abstraction heuristics in static program analysis,July 2021,Not Found,Donghoon=Jeon: donghoon_jeon@korea.ac.kr; Minseok=Jeon: minseok_jeon@korea.ac.kr; Hakjoo=Oh: hakjoo_oh@korea.ac.kr,"Abstract
Context:
The precision and cost of 
static analysis
 are determined by abstraction heuristics (e.g., strategies for abstracting calling contexts, heap locations, etc.), but manually designing effective abstraction heuristics requires a huge amount of engineering effort and domain knowledge. Recently, data-driven 
static analysis
 has emerged to address this challenge by learning such heuristics automatically from a set of training programs.
Objective:
We present a practical algorithm for learning disjunctive abstraction heuristics in data-driven static analysis. We build on a recently proposed approach that can learn nontrivial program properties by disjunctive 
boolean functions
. However, the existing approach is practically limited as it assumes that the most precise abstraction is cheap for the training programs; the algorithm is inapplicable if the most precise abstraction is not scalable. The objective of this paper is to mitigate this limitation.
Method:
Our algorithm overcomes the limitation with two new ideas. It systematically decomposes the learning problem into feasible 
subproblems
, and it can search through the abstraction space from the coarse- to fine-grained abstractions. With this approach, our algorithm is able to learn heuristics when static analysis with the most precise abstraction is not scalable over the training programs.
Results:
We show our approach is effective and generally applicable. We applied our approach to a context-sensitive points-to analysis for Java and a flow-sensitive interval analysis for C. Experimental results show that our algorithm is efficient. For example, our algorithm can learn heuristics for 3-object-sensitive analysis for which the existing learning algorithm is too expensive to learn any useful heuristics.
Conclusion:
Our algorithm makes a state-of-the-art technique for data-driven static analysis more practical.",Information and Software Technology,04 Mar 2025,7,"The algorithm for learning disjunctive abstraction heuristics in data-driven static analysis is practical and addresses a common limitation, enhancing the effectiveness of the approach."
https://www.sciencedirect.com/science/article/pii/S0950584921000288,Self-Attention Networks for Code Search,June 2021,Not Found,Sen=Fang: Not Found; You-Shuai=Tan: Not Found; Tao=Zhang: tazhang@must.edu.mo; Yepang=Liu: liuyp1@sustech.edu.cn,"Abstract
Context:
Developers tend to search and reuse code snippets from a large-scale codebase when they want to implement some functions that exist in the previous projects, which can enhance the efficiency of software development.
Objective:
As the first deep learning-based code search model, DeepCS outperforms prior models such as Sourcere and CodeHow. However, it utilizes two separate 
LSTM
 to represent code snippets and natural language descriptions respectively, which ignores 
semantic relations
 between code snippets and their descriptions. Consequently, the performance of DeepCS falls into the bottleneck, and thus our objective is to break this bottleneck.
Method:
We propose a self-attention 
joint
 
representation learning
 model, named SAN-CS (
S
elf-
A
ttention 
N
etwork for 
C
ode 
S
earch). Comparing with DeepCS, we directly utilize the self-attention network to construct our code search model. By a weighted average operation, self-attention networks can fully capture the contextual information of code snippets and their descriptions. We first utilize two individual self-attention networks to represent code snippets and their descriptions, respectively, and then we utilize the self-attention network to conduct an extra 
joint
 representation network for code snippets and their descriptions, which can build 
semantic relationships
 between code snippets and their descriptions. Therefore, SAN-CS can break the 
performance bottleneck
 of DeepCS.
Results:
We evaluate SAN-CS on the dataset shared by 
Gu et al.
 and choose two 
baseline models
, DeepCS and CARLCS-CNN. Experimental results demonstrate that SAN-CS achieves significantly better performance than DeepCS and CARLCS-CNN. In addition, SAN-CS has better execution efficiency than DeepCS at the training and testing phase.
Conclusion:
This paper proposes a code search model, SAN-CS. It utilizes the self-attention network to perform the joint attention representations for code snippets and their descriptions, respectively. Experimental results verify the effectiveness and efficiency of SAN-CS.",Information and Software Technology,04 Mar 2025,9,"The SAN-CS model introduces a novel approach to code search by utilizing self-attention networks, improving performance and efficiency compared to existing models."
https://www.sciencedirect.com/science/article/pii/S0950584921000021,Spectrum-based multi-fault localization using Chaotic Genetic Algorithm,May 2021,Not Found,Debolina=Ghosh: debolina442@gmail.com; Jagannath=Singh: jagannath.singhfcs@kiit.ac.in,"Abstract
Context:
In the field of 
software engineering
, the most complex and time consuming activity is fault-finding. Due to increasing size and complexity of software, there is a necessity of automated fault detection tool which can detect fault with minimal human intervention. A programmer spends a lot of time and effort on 
software fault
 localization. Various Spectrum Based Fault Localization (SBFL) techniques have already been developed to automate the 
fault localization
 in single-fault software. But, there is a scarcity of 
fault localization
 technique for multi-fault software. In our study, we have found that pure SBFL is not always sufficient for effective fault localization in multi-fault programs.
Objective:
To address the above challenge, we propose an automated framework using Chaos-based 
Genetic Algorithm
 for Multi-fault Localization (CGAML) based on SBFL technique.
Methods:
Traditional 
Genetic Algorithm
 (GA) sometimes stuck in local optima, and it takes more time to converge. Different chaos 
mapping functions
 have been applied to GA for better performance. We have used logistic mapping function to achieve 
chaotic sequence
. The proposed technique CGAML first calculates the 
suspiciousness
 score for each program statement and then assigns ranks according to that score. The statements having smaller rank means there is a high probability of the statements to be faulty.
Results:
Five open-source 
benchmark programs
 are tested to evaluate the efficiency of CGAML technique. The experimental results show CGAML gives better results for both single-fault and multi-fault programs in comparison with existing spectrum-based fault localization techniques.
Conclusion:
E
X
A
M
 metric is used to compare the performance of our proposed technique with other existing techniques. Smaller 
E
X
A
M
 score denotes the higher accuracy of the technique. The proposed framework generates smaller 
E
X
A
M
 score in comparison with other existing techniques. We found that, overall CGAML works on an average 8.5% better than GA for both single-fault and multi-fault software.",Information and Software Technology,04 Mar 2025,6,"The CGAML framework addresses the challenge of fault localization in multi-fault software using Chaos-based Genetic Algorithm, showing improved results over existing techniques."
https://www.sciencedirect.com/science/article/pii/S0950584921000021,Spectrum-based multi-fault localization using Chaotic Genetic Algorithm,May 2021,Not Found,Debolina=Ghosh: debolina442@gmail.com; Jagannath=Singh: jagannath.singhfcs@kiit.ac.in,"Abstract
Context:
In the field of 
software engineering
, the most complex and time consuming activity is fault-finding. Due to increasing size and complexity of software, there is a necessity of automated fault detection tool which can detect fault with minimal human intervention. A programmer spends a lot of time and effort on 
software fault
 localization. Various Spectrum Based Fault Localization (SBFL) techniques have already been developed to automate the 
fault localization
 in single-fault software. But, there is a scarcity of 
fault localization
 technique for multi-fault software. In our study, we have found that pure SBFL is not always sufficient for effective fault localization in multi-fault programs.
Objective:
To address the above challenge, we propose an automated framework using Chaos-based 
Genetic Algorithm
 for Multi-fault Localization (CGAML) based on SBFL technique.
Methods:
Traditional 
Genetic Algorithm
 (GA) sometimes stuck in local optima, and it takes more time to converge. Different chaos 
mapping functions
 have been applied to GA for better performance. We have used logistic mapping function to achieve 
chaotic sequence
. The proposed technique CGAML first calculates the 
suspiciousness
 score for each program statement and then assigns ranks according to that score. The statements having smaller rank means there is a high probability of the statements to be faulty.
Results:
Five open-source 
benchmark programs
 are tested to evaluate the efficiency of CGAML technique. The experimental results show CGAML gives better results for both single-fault and multi-fault programs in comparison with existing spectrum-based fault localization techniques.
Conclusion:
E
X
A
M
 metric is used to compare the performance of our proposed technique with other existing techniques. Smaller 
E
X
A
M
 score denotes the higher accuracy of the technique. The proposed framework generates smaller 
E
X
A
M
 score in comparison with other existing techniques. We found that, overall CGAML works on an average 8.5% better than GA for both single-fault and multi-fault software.",Information and Software Technology,04 Mar 2025,6,"The CGAML framework addresses the challenge of fault localization in multi-fault software using Chaos-based Genetic Algorithm, showing improved results over existing techniques."
https://www.sciencedirect.com/science/article/pii/S0950584920302111,Understanding Hypotheses Engineering in Software Startups through a Gray Literature Review,May 2021,Not Found,Jorge=Melegati: jmelegatigoncalves@unibz.it; Eduardo=Guerra: eduardo.guerra@unibz.it; Xiaofeng=Wang: xiaofeng.wang@unibz.it,"Abstract
Context
The 
higher availability
 of software usage data and the influence of the Lean Startup led to the rise of experimentation in 
software engineering
, a new approach for development based on experiments to understand the user needs. In the models proposed to guide this approach, the first step is generally to identify, prioritize, and specify the hypotheses that will be tested through experimentation. However, although practitioners have proposed several techniques to handle hypotheses, the scientific literature is still scarce.
Objective
The goal of this study is to understand what activities, as proposed in industry, are entailed to handle hypotheses, facilitating the comparison, creation, and evaluation of relevant techniques.
Methods
We performed a gray literature review (GLR) on the practices proposed by practitioners to handle hypotheses in the context of software startups. We analyzed the identified documents using thematic synthesis.
Results
The analysis revealed that techniques proposed for software startups in practice compress five different activities: elicitation, prioritization, specification, analysis, and management. It also showed that practitioners often classify hypotheses in types and which qualities they aim for these statements.
Conclusion
Our results represent the first description for hypotheses engineering grounded in practice data. This mapping of the state-of-practice indicates how research could go forward in investigating hypotheses for experimentation in the context of software startups. For practitioners, they represent a catalog of available practices to be used in this context.",Information and Software Technology,04 Mar 2025,3,"While the study provides insights into handling hypotheses in software startups, the practical impact on early-stage ventures may be limited due to the focus on theoretical activities. The findings could be valuable for guiding future research."
https://www.sciencedirect.com/science/article/pii/S0950584920302214,A systematic review of scheduling approaches on multi-tenancy cloud platforms,April 2021,Not Found,Ru=Jia: jiaruweiwei@gmail.com; Yun=Yang: yyang@swin.edu.au; John=Grundy: john.grundy@monash.edu; Jacky=Keung: Jacky.Keung@cityu.edu.hk; Li=Hao: liucoolhao@gmail.com,"Abstract
Context:
Scheduling in cloud is complicated as a result of multi-tenancy. Diverse tenants have different requirements, including service functions, response time, QoS and throughput. Diverse tenants require different scheduling capabilities, resource consumption and competition. Multi-tenancy scheduling approaches have been developed for different service models, such as 
Software as a Service
 (SaaS), Platform as a service (PaaS), Infrastructure as a Service (IaaS), and Database as a Service (DBaaS).
Objective:
In this paper, we survey the current landscape of multi-tenancy scheduling, laying out the challenges and complexity of 
software engineering
 where multi-tenancy issues are involved. This study emphasises scheduling policies, cloud provisioning and deployment with regards to multi-tenancy issues. We conduct a systematic literature review of research studies related to multi-tenancy scheduling approaches on cloud platforms determine the primary scheduling approaches currently used and the challenges for addressing key multi-tenancy scheduling issues.
Method:
We adopted a systematic literature review method to search and review many major journal and 
conference papers
 on four major online electronic databases, which address our four predefined research questions. Defining inclusion and exclusion criteria was the initial step before extracting data from the selected papers and deriving answers addressing our enquiries.
Results:
Finally, 53 papers were selected, of which 62 approaches were identified. Most of these methods are developed without cloud layers’ limitation (43.40%) and on SaaS, most of scheduling approaches are oriented to framework design (43.75%).
Conclusion:
The results have demonstrated most of multi-tenancy scheduling solutions can work at any delivery layer. With the difference of tenants’ requirements and functionalities, the choice of cloud service delivery models is changed. Based on our study, designing a multi-tenancy scheduling framework should consider the following 3 factors: computing, QoS and storage resource. One of the potential research foci of multi-tenancy scheduling approaches is on GPU scheduling.",Information and Software Technology,04 Mar 2025,5,The study on multi-tenancy scheduling in cloud platforms offers practical insights for startups utilizing cloud services. The emphasis on challenges and approaches could benefit European early-stage ventures leveraging cloud technologies.
https://www.sciencedirect.com/science/article/pii/S095058492030238X,Identifying method-level mutation subsumption relations using Z3,April 2021,Not Found,Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Márcio=Ribeiro: marcio@ic.ufal.br; Beatriz=Souza: beatriz.souza@ccc.ufcg.edu.br; Marcio=Guimarães: masg@ic.ufal.br; Leo=Fernandes: leonardo.oliveira@ifal.edu.br; Marcelo=d’Amorim: damorim@cin.ufpe.br; Vander=Alves: valves@unb.br; Leopoldo=Teixeira: lmt@cin.ufpe.br; Baldoino=Fonseca: baldoino@ic.ufal.br,"Abstract
Context:
Mutation analysis is a popular but costly approach to assess the quality of test suites. One recent promising direction in reducing costs of mutation analysis is to identify redundant mutations, i.e., mutations that are subsumed by some other mutations. A previous approach found redundant mutants manually through truth tables but it cannot be applied to all mutations. Another work derives them using automatic test suite generators but it is a time consuming task to generate mutants and tests, and to execute tests.
Objective:
This article proposes an approach to discover redundant mutants by proving 
subsumption relations
 among method-level 
mutation operators
 using weak mutation testing.
Method:
We conceive and encode a theory of 
subsumption relations
 in the Z3 
theorem prover
 for 37 mutation targets (mutations of an expression or statement).
Results:
We automatically identify and prove a number of subsumption relations using Z3, and reduce the number of mutations in a number of mutation targets. To evaluate our approach, we modified 
MuJava
 to include the results of 24 mutation targets and evaluate our approach in 125 classes of 5 large open source popular projects used in prior work. Our approach correctly discards mutations in 75.93% of the cases, and reduces the number of mutations by 71.38%.
Conclusions:
Our approach offers a good balance between the effort required to derive subsumption relations and the effectiveness for the targets considered in our evaluation in the context of strong mutation testing.",Information and Software Technology,04 Mar 2025,8,The approach to reducing costs in mutation analysis through discovering redundant mutants using weak mutation testing could have significant practical value for startups focusing on software quality assurance. The results of the study show promising outcomes for improving test suites efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584920302184,Industry-Academia research collaboration in software engineering: The Certus model,April 2021,"Software engineering, Industry-academia collaboration, Research collaboration, Research knowledge co-creation, Collaboration model, Technology transfer, Knowledge transfer, Research exploitation, Research-based innovation",Dusica=Marijan: dusica@simula.no; Arnaud=Gotlieb: Not Found,"Abstract
Context
Research collaborations between 
software engineering
 industry and academia can provide significant benefits to both sides, including improved innovation capacity for industry, and real-world environment for motivating and validating research ideas. However, building scalable and effective research collaborations in software engineering is known to be challenging. While such challenges can be varied and many, in this paper we focus on the challenges of achieving participative knowledge creation supported by active dialog between industry and academia and continuous commitment to 
joint
 problem solving.
Objective
This paper aims to understand what are the elements of a successful industry-academia collaboration that enable the culture of participative knowledge creation.
Method
We conducted participant observation collecting qualitative data spanning 8 years of collaborative research between a software engineering research group on software V&V and the Norwegian IT sector. The 
collected data
 was analyzed and synthesized into a practical collaboration model, named the Certus Model.
Results
The model is structured in seven phases, describing activities from setting up research projects to the exploitation of 
research results
. As such, the Certus model advances other collaborations models from literature by delineating different phases covering the complete life cycle of participative research knowledge creation.
Conclusion
The Certus model describes the elements of a research collaboration process between researchers and practitioners in software engineering, grounded on the principles of research knowledge co-creation and continuous commitment to joint problem solving. The model can be applied and tested in other contexts where it may be adapted to the local context through experimentation.",Information and Software Technology,04 Mar 2025,6,The exploration of successful industry-academia collaborations in software engineering can provide valuable insights for early-stage ventures seeking to establish research partnerships. The Certus model offers a structured approach that could be beneficial for startups engaging in collaborative research.
https://www.sciencedirect.com/science/article/pii/S0950584920302469,Multifaceted infrastructure for self-adaptive IoT systems,April 2021,Not Found,Rossana M.C.=Andrade: rossana@great.ufc.br; Belmondo R.=Aragão: belmondorodrigues@great.ufc.br; Pedro Almir M.=Oliveira: pedromartins@great.ufc.br; Marcio E.F.=Maia: marcio@great.ufc.br; Windson=Viana: windson@great.ufc.br; Tales P.=Nogueira: tales@great.ufc.br,"Abstract
Background:
Internet of Things
 (IoT) enables the interaction among objects to provide services to their users. Areas such as eHealth, smart energy, and smart buildings have been benefiting from the IoT potential. However, the development of IoT systems is still complex because it deals with a highly dynamic, volatile, and heterogeneous environment. These characteristics require discovering devices, managing these devices’ context, and self-adapt their behavior.
Goal
: In this work, we propose a self-adaptive IoT infrastructure to support multiple facets, 
i.e.
, the contextual discovery of smart objects, the context management, and the self-adaptation process of the development of these systems.
Methods
: We evaluated the proposed infrastructure by developing a smart building application with and without it. The evaluation focused on four issues: the feasibility of integrating the context management through middleware platforms with adaptation based on workflows in a request/response communication model, the impact of our infrastructure on the development of self-adaptive IoT systems considering 
cyclomatic complexity
 and coupling 
code metrics
; the impact of using contextual filters on the orchestrator of self-adaptation; and the impact on the quality of the self-adaptation.
Results
: The results suggest that: (i) it is feasible to use the proposed infrastructure in the development of self-adaptive IoT systems; (ii) there is a reduction in the 
cyclomatic complexity
 and the coupling with our approach, (iii) there is a considerable decrease in the number of rules evaluated at runtime, (iv) our infrastructure reduces the execution time of the adaptations when using contextual filters, and (v) the self-adaptation process was effective when using the orchestrator of self-adaptations.
Conclusion
: With these results, we observed that the proposed multifaceted infrastructure could reduce the complexity related to the development of IoT systems, in addition to optimizing their self-adaptation process.",Information and Software Technology,04 Mar 2025,7,"The proposal of a self-adaptive IoT infrastructure addresses the complex nature of IoT systems, which can be relevant for startups operating in IoT-related sectors. The evaluation results demonstrate practical benefits in reducing complexity and optimizing self-adaptation, potentially impacting European early-stage ventures in IoT."
https://www.sciencedirect.com/science/article/pii/S0950584920302019,Feature selection and embedding based cross project framework for identifying crashing fault residence,March 2021,Not Found,Zhou=Xu: Not Found; Tao=Zhang: Not Found; Jacky=Keung: Not Found; Meng=Yan: mengy@cqu.edu.cn; Xiapu=Luo: csxluo@comp.polyu.edu.hk; Xiaohong=Zhang: Not Found; Ling=Xu: Not Found; Yutian=Tang: Not Found,"Abstract
Context: The automatically produced crash reports are able to analyze the root of fault causing the crash (crashing fault for short) which is a critical activity for 
software quality assurance
.
Objective: Correctly predicting the existence of crashing fault residence in stack traces of crash report can speed up 
program debugging
 process and optimize debugging efforts. Existing work focused on the collected label information from bug-fixing logs, and the extracted features of crash instances from stack traces and 
source code
 for 
I
dentification of 
C
rashing 
F
ault 
R
esidence (
ICFR
) of newly-submitted crashes. This work develops a novel cross project ICFR framework to address the data scarcity problem by using labeled crash data of other project for the ICFR task of the project at hand. This framework removes irrelevant features, reduces distribution differences, and eases the 
class imbalance
 issue of cross project data since these factors may negatively impact the ICFR performance.
Method: The proposed framework, called 
FSE
, combines 
F
eature 
S
election and feature 
E
mbedding techniques. The FSE framework first uses an 
information gain
 ratio based feature ranking method to select a relevant feature subset for cross project data, and then employs a state-of-the-art 
W
eighted 
B
alanced 
D
istribution 
A
daptation (
WBDA
) method to map features of cross project data into a common space. WBDA considers both marginal and conditional distributions as well as their weights to reduce 
data distribution
 discrepancies. Besides, WBDA balances the class proportion of each project data to alleviate the 
class imbalance
 issue.
Results: We conduct experiments on 7 projects to evaluate the performance of our FSE framework. The results show that FSE outperforms 25 methods under comparison.
Conclusion: This work proposes a cross project learning framework for ICFR, which uses feature selection and embedding to remove irrelevant features and reduce distribution differences, respectively. The results illustrate the performance superiority of our FSE framework.",Information and Software Technology,04 Mar 2025,8,"The abstract presents a novel framework that addresses data scarcity problems in software quality assurance, showing superior performance compared to existing methods. This can have a positive impact on early-stage ventures by optimizing debugging efforts and improving software quality."
https://www.sciencedirect.com/science/article/pii/S0950584920302317,Exploring the software repositories of embedded systems: An industrial experience,March 2021,Not Found,Jakub=Polaczek: Not Found; Janusz=Sosnowski: j.sosnowski@ii.pw.edu.pl,"Abstract
Context
Tracing reports for software repositories have attracted many researchers. Most of them have focused on defect analysis and development processes in relation to open source programs. There exists a gap between open source and industrial software projects, which, in particular, relates to different schemes for creating software repositories and development schemes. This is especially true for embedded systems that gain large markets and become more complex.
Objective
The aim is to explore the software repositories of industrial embedded systems and derive characteristic features in order to evaluate quality and identify problems to do with development processes.
Method
In this paper we have proposed a novel approach to software repository analysis based on the fine grained exploration of issue tracking and code control repositories. In particular, we distinguish the various activities of project actors (e.g. creating new functions, correcting defects, improving performance, modifying tests) and analyse them in a context, not only of a single project, but also a set of correlated projects that have been developed in the company. These issues have been neglected in the literature. These analyses needed new holistic schemes for repository exploration, including various statistical metrics, text mining, and machine learning techniques.
Results
In exploring selected industrial projects we have identified that only 40–75% of issues relate to defects; the issue reports and commit descriptions included here comprise a lot of data that has been disregarded in the literature. These data allow us to trace diverse types of code changes and identify imperfections in software repositories.
Conclusion
We show that fine grained repository analysis gives a broader and more complete view of project development, which may lead to its improvement.",Information and Software Technology,04 Mar 2025,6,"The abstract explores software repositories of industrial embedded systems, contributing to the understanding of development processes. While this may not directly impact early-stage ventures, the insights gained could be valuable for startups in the industrial embedded systems sector."
https://www.sciencedirect.com/science/article/pii/S0950584920302287,A study of effectiveness of deep learning in locating real faults,March 2021,Not Found,Zhuo=Zhang: zz8477@126.com; Yan=Lei: yanlei@cqu.edu.cn; Xiaoguang=Mao: xgmao@nudt.edu.cn; Meng=Yan: mengy@cqu.edu.cn; Ling=Xu: xuling@cqu.edu.cn; Xiaohong=Zhang: xhongz@cqu.edu.cn,"Abstract
Context:
 The recent progress of 
deep learning
 has shown its promising learning ability in making sense of data, and many fields have utilized this learning ability to learn an effective model, successfully solving their problems. 
Fault localization
 has explored and used 
deep learning
 to server an aid in debugging, showing the promising results on fault localization. However, as far as we know, there is no detailed studies on evaluating the benefits of using 
deep learning
 for locating real faults present in programs. 
Objective:
 To understand the benefits of 
deep learning
 in locating real faults, this paper explores more about 
deep learning
 by studying the effectiveness of fault localization using 
deep learning
 for a set of real bugs reported in the widely used programs. 
Method:
 We use three representative deep learning architectures (
i.e.
 
convolutional neural network
, 
recurrent neural network
 and multi-layer perceptron) for fault localization, and conduct large-scale experiments on 8 real-world programs equipped with all real faults to evaluate their effectiveness on fault localization. 
Results:
 We observe that the localization effectiveness varies considerably among three 
neural networks
 in the context of real faults. Specifically, convolutional 
neural network
 performs the best in locating real faults, showing an average of 38.97% and 26.22% saving over multi-layer 
perceptron
 and 
recurrent neural network
 respectively; 
recurrent neural network
 and multi-layer 
perceptron
 yield comparable effectiveness even if the effectiveness of 
recurrent neural network
 is marginally higher than multi-layer 
perceptron
. 
Conclusion:
 In context of real faults, 
convolutional neural network
 is the most effective for fault localization among the investigated architectures, and we suggest potential factors of deep learning for improving fault localization.",Information and Software Technology,04 Mar 2025,9,"The abstract evaluates the effectiveness of deep learning in fault localization, providing valuable insights for improving debugging processes. This can be highly beneficial for early-stage ventures in the tech industry, helping them enhance their software development and debugging capabilities."
https://www.sciencedirect.com/science/article/pii/S0950584920301968,An empirical study of performance using Clone & Own and Software Product Lines in an industrial context,February 2021,Not Found,Jorge=Echeverría: jecheverria@usj.es; Francisca=Pérez: mfperez@usj.es; José Ignacio=Panach: joigpana@uv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Clone and Own (CaO) is a widespread approach to generate new software products from existing software products by adding small changes. The Software Product Line (SPL) approach addresses the development of families of products with similar features, moving away from the production of isolated products. Despite the popularity of both approaches, no experiment has yet compared them directly.
Objective:
The goal of this paper is to know the different performances of software engineers in the software 
products development process
 using two different approaches (SPL and CaO).
Method:
We conducted an experiment in the induction hobs software environment with software engineers. This experiment is a single factor experiment where the factor is the approach that is used to develop software products, with two treatments: (SPL or CaO). We compared the results obtained by the software engineers when they develop software products related to effectiveness, efficiency, and satisfaction.
Results:
The findings show that: (1) the SPL approach is more efficient even though the number of checking actions required by this approach is greater than the number required by the CaO approach; (2) the SPL approach offers more possibilities than software engineers need to perform their daily tasks; and (3) software engineers require better search capabilities in the CaO approach. The possible explanations for these results are presented in the paper.
Conclusions:
The results show that there are significant differences in effectiveness, efficiency, and satisfaction, with the SPL approach yielding the best results.",Information and Software Technology,04 Mar 2025,7,"The abstract compares two different approaches in software product development, highlighting significant differences in effectiveness, efficiency, and satisfaction. While the findings are relevant, the direct impact on early-stage ventures may vary depending on their specific product development strategies."
https://www.sciencedirect.com/science/article/pii/S095058492030197X,Test data generation using genetic programming,February 2021,Not Found,M.=Nosrati: mohammad.nosrati@gmail.com; H.=Haghighi: h_haghighi@sbu.ac.ir; M.=Vahidi Asl: m.vahidi.asl@gmail.com,"Abstract
Context:
Typically, search-based test data generation methods search on a population of program input values. Program input values can be regarded as solutions to underlying path constraints over program input parameters. One way to discover these path constraints is to use the symbolic execution method. Search-based methods attempt to find input values which are solutions to these path constraints, without knowing the actual constraints.
Objective:
In this paper, we show that we can search for the underlying path constraints using search-based methods, without resorting to symbolic execution. Trying to discover the exact or a good enough 
approximation
 of the underlying constraints may lead to a more targeted search, compared to directly searching for program input values. Besides, the construction of approximate constraints by searching may help to avoid some problems of symbolic execution.
Method:
The proposed method uses 
genetic programming
 for 
learning constraints
 on program input parameters.
Results:
To evaluate the performance of the proposed approach, a series of experiments have been conducted on a number of different 
benchmark programs
. For 91.8% of 
benchmark programs
, the proposed method achieved the best efficiency among the competitive algorithms.
Conclusion:
The results show that, if constraint solving can be provided for some or all parameter types of the methods of programs under test, our approach can improve the efficiency and effectiveness of search-based test data generation.",Information and Software Technology,04 Mar 2025,8,"The abstract introduces a method for learning constraints on program input parameters, showing improved efficiency in test data generation. This could benefit early-stage ventures by enhancing their testing processes and ensuring the quality of their software products."
https://www.sciencedirect.com/science/article/pii/S0950584920301944,Revisiting heterogeneous defect prediction methods: How far are we?,February 2021,Not Found,Xiang=Chen: xchencs@ntu.edu.cn; Yanzhou=Mu: myz_2019218009@tju.edu.cn; Ke=Liu: l51415370@163.com; Zhanqi=Cui: czq@bistu.edu.cn; Chao=Ni: jacknichao920209@gmail.com,"Abstract
Context:
 Cross-project 
defect prediction
 applies to the scenarios that the target projects are new projects. Most of the previous studies tried to utilize the 
training data
 from other projects (i.e., the source projects). However, metrics used by practitioners to measure the extracted program modules from different projects may not be the same, and performing heterogeneous 
defect prediction
 (HDP) is challenging.
Objective:
 Researchers have proposed many novel HDP methods with promising performance until now. Recently, unsupervised defect prediction (UDP) methods have received more attention and show competitive performance. However, to our best knowledge, whether HDP methods can perform significantly better than UDP methods has not yet been thoroughly investigated.
Method:
 In this article, we perform a comparative study to have a holistic look at this issue. Specifically, we compare five HDP methods with four UDP methods on 34 projects in five groups under the same experimental setup from three different perspectives: non-effort-aware performance indicators (NPIs), effort-aware performance indicators (EPIs) and diversity analysis on identifying defective modules.
Result:
 We have the following findings: (1) HDP methods do not perform significantly better than some of UDP methods in terms of two NPIs and four EPIs. (2) According to two satisfactory criteria recommended by previous studies, the satisfactory ratio of existing HDP methods is pessimistic. (3) The diversity of prediction for defective modules across HDP 
vs
. UDP methods is more than that within HDP methods or UDP methods.
Conclusion:
 The above findings implicate there is still a long way for the HDP issue to go. Given this, we present some observations about the road ahead for HDP.",Information and Software Technology,04 Mar 2025,7,"The study on comparing HDP and UDP methods can provide valuable insights for developers working on defect prediction in new projects, impacting their decision-making processes."
https://www.sciencedirect.com/science/article/pii/S0950584920301725,Architectural decision-making as a financial investment: An industrial case study,January 2021,Not Found,Areti=Ampatzoglou: areti.ampatzoglou@rug.nl; Elvira-Maria=Arvanitou: e.arvanitou@uom.edu.gr; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Paris=Avgeriou: paris@cs.rug.nl; Angeliki-Agathi=Tsintzira: Not Found; Alexander=Chatzigeorgiou: achat@uom.edu.gr,"Abstract
Context
Making 
architectural decisions
 is a crucial task but also very difficult, considering the scope of the decisions and their impact on 
quality attributes
. To make matters worse, 
architectural decisions
 need to combine both technical and business factors, which are very dissimilar by nature.
Objectives
We provide a cost-benefit approach and supporting tooling that treats architectural decisions as financial investments by: (a) combining both technical and business factors; and (b) transforming the involved factors into currency, allowing their uniform aggregation. Apart from illustrating the method, we validate both the proposed approach and the tool, in terms of fitness for purpose, usability, and potential limitations.
Method
To validate the approach, we have performed a 
case study
 in a software development company, in the domain of low-energy embedded systems. We employed triangulation in the 
data collection phase
 of the 
case study
, by performing interviews, focus groups, an observational session, and questionnaires.
Results
The results of the study suggested that the proposed approach: (a) provides a structured process for systematizing decision-making; (b) enables the involvement of multiple stakeholders, distributing the decision-making responsibility to more knowledgeable people; (c) uses monetized representations that are important for assessing decisions in a unified manner; and (d) enables decision reuse and documentation.
Conclusions
The results of the study suggest that architectural decision-making can benefit from treating this activity as a financial investment. The various benefits that have been identified from mixing financial and technological aspects are well-accepted from 
industrial stakeholders
.",Information and Software Technology,04 Mar 2025,8,"The cost-benefit approach for architectural decision-making can significantly benefit startups by providing a structured process that enables multiple stakeholders to make informed decisions, potentially improving the quality of their products."
https://www.sciencedirect.com/science/article/pii/S0950584920301464,The effectiveness of data augmentation in code readability classification,January 2021,Not Found,Qing=Mi: miqing@bjut.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Zhi=Cai: caiz@bjut.edu.cn; Xibin=Jia: jiaxibin@bjut.edu.cn,"Abstract
Context:
 Training 
deep learning
 models for code readability classification requires large datasets of quality pre-labeled data. However, it is almost always time-consuming and expensive to acquire readability data with manual labels.
Objective:
 We thus propose to introduce 
data augmentation
 approaches to artificially increase the size of training set, this is to reduce the risk of overfitting caused by the lack of readability data and further improve the 
classification accuracy
 as the ultimate goal.
Method:
 We create transformed versions of code snippets by manipulating original data from aspects such as comments, indentations, and names of classes/methods/variables based on domain-specific knowledge. In addition to basic transformations, we also explore the use of Auxiliary Classifier 
GANs
 to produce 
synthetic data
.
Results:
 To evaluate the proposed approach, we conduct a set of experiments. The results show that the classification performance of 
deep neural networks
 can be significantly improved when they are trained on the augmented corpus, achieving a state-of-the-art accuracy of 87.38%.
Conclusion:
We consider the findings of this study as primary evidence of the effectiveness of 
data augmentation
 in the field of code readability classification.",Information and Software Technology,04 Mar 2025,9,"The proposed data augmentation approach for code readability classification can greatly benefit early-stage ventures by improving the classification accuracy and reducing the risk of overfitting, leading to better quality software products."
https://www.sciencedirect.com/science/article/pii/S0950584920301907,Archetypes of delay: An analysis of online developer conversations on delayed work items in IBM Jazz,January 2021,Not Found,Abdoul-Djawadou=Salaou: adsalaou@unistra.fr; Daniela=Damian: Daniela.damian@uvic.ca; Casper=Lassenius: casper.lassenius@aalto.fi; Dragoş=Voda: dragos.voda@aalto.fi; Pierre=Gançarski: gancarski@unistra.fr,"Abstract
Context.
A widely adopted methodology, 
agile software development
 provides enhanced flexibility to actively adjust a project scope. In agile teams, particularly in distributed environment, developers interact, manage requirements knowledge, and coordinate primarily in online collaboration tools. Developer conversations become invaluable sources to track and understand developers’ interactions around implementation of requirements, as well as the progress of implementation relative to the project scope and the planned iterations in agile projects. Although extensive research around iteration planning exists, there is a lack of research that leverages developer conversation data to understand delays in implementing planned requirements in agile projects.
Objective.
By using developer conversations in a large agile project at IBM, this work aims to analyze conversation in work items (WIs) that are delayed and derive patterns that suggest reasons for delay in the project.
Method.
We conducted a 
case study
 of the IBM Jazz project, and used thematic analysis to code the developer conversations as time-series, and cluster analysis to identify patterns that differentiated the evolution of discussions in WIs that were late vs. not late in the project.
Results.
We identified six main patterns of WI delay. Through semantic analysis of developer conversations within particular clusters we were able to explain the reasons for delays in each pattern. In comparison to non-late WIs, we find that the major reason for delay is a lack of frequent communication associated with a poor project management of WIs. Similarly, non-late tasks more often delegate to children tasks to accelerate the implementation of requirements, in addition to processing requests quickly to resolve bottlenecks in implementation.
Conclusion.
Our study complements existing research in bringing evidence that developer conversations are a useful resource that can highlight delays in requirement implementation, as well as recommend patterns in the dynamics of developers interactions relevant to such delays.",Information and Software Technology,04 Mar 2025,6,"Analyzing developer conversations to understand delays in implementing requirements in agile projects can provide valuable insights for startups working in distributed environments, potentially improving project management practices and reducing delays."
https://www.sciencedirect.com/science/article/pii/S0950584920301695,The impact of personality traits and knowledge collection behavior on programmer creativity,December 2020,Not Found,Aamir=Amin: aamir@utar.edu.my; Shuib=Basri: Not Found; Mobashar=Rehman: Not Found; Luiz Fernando=Capretz: Not Found; Rehan=Akbar: Not Found; Abdul Rehman=Gilal: Not Found; Muhammad Farooq=Shabbir: Not Found,"Abstract
Context: Creativity is one of the essential ingredients in successful software engineering. However, majority of the work related to creativity in software engineering has focused on creativity in requirement engineering. Furthermore, there are very few studies that examine programmer creativity and the impact of individual and contextual factors on it.
Objective: The objective of the study is to analyze the impact of the 
big five personality traits
 including extraversion, agreeableness, conscientiousness, 
neuroticism
 and openness to experience, as well as knowledge collection behavior on a programmer's creativity intention.
Method: A quantitative survey was conducted and data from 294 programmers, working in offshore software development projects, was collected. The data was later analyzed using Smart-PLS (3.0).
Results and Conclusions: The results indicated that openness to experience, extraversion, conscientiousness and knowledge collection behavior positively predicted a programmer's creativity intention. On the other hand, 
neuroticism
 negatively predicts creativity intention of the programmer. The study also concluded that all of the independent variables, except the agreeableness trait, significantly predict creativity intention which in turn significantly predicts creativity. As a result, our conclusions indicate that programmer's 
personality traits
 and knowledge collection behavior play a key role in shaping their intention to be creative. Hence, 
personality traits
 and knowledge collection behavior should be given due attention during the hiring process of creativity-oriented software companies.",Information and Software Technology,04 Mar 2025,5,"Studying the impact of personality traits and knowledge collection behavior on programmer creativity intention can provide some insights for startups in shaping their hiring processes, but the practical applicability may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492030118X,An empirical evaluation of the use of models to improve the understanding of safety compliance needs,October 2020,Not Found,Jose Luis=de la Vara: joseluis.delavara@uclm.es; Beatriz=Marín: beatriz.marin@mail.udp.cl; Clara=Ayora: claraayora@gmail.com; Giovanni=Giachetti: ggiachetti@inacap.cl,"Abstract
Context
Critical systems in application domains such as automotive, railway, aerospace, and healthcare are required to comply with safety standards. The understanding of the safety compliance needs specified in these standards can be difficult from their text. A possible solution is to use models.
Objective
We aim to evaluate the use of models to understand safety compliance needs.
Method
We have studied the effectiveness, efficiency, and perceived benefits in understanding these needs, with models and with the text of safety standards, by means of an experiment. The standards considered are DO-178C and EN 50128. We use SPEM-like diagrams to graphically represent the models.
Results
The mean effectiveness of 20 undergraduate students in understanding the needs and the mean efficiency were higher with models (22% and 38%, respectively), and the difference is statistically significant (p-value ≤ 0.02). Most of the students agreed upon the ease of understanding the structure of safety compliance needs with models when compared to the text, but on average, the students were undecided about whether the models are easy to understand or easier to understand than the text.
Conclusions
The results allow us to claim that the use of models can improve the understanding of safety compliance needs. Nonetheless, there seems to be room for improvement in relation to the perceived benefits. It must be noted that our conclusions may differ if the subjects were experienced practitioners.",Information and Software Technology,04 Mar 2025,7,"The use of models to understand safety compliance needs can significantly improve effectiveness and efficiency, which is essential for startups in critical application domains."
https://www.sciencedirect.com/science/article/pii/S0950584920300732,Semantically find similar binary codes with mixed key instruction sequence,September 2020,Not Found,Yuancheng=Li: yuancheng@ncepu.cn; Boyan=Wang: Not Found; Baiji=Hu: Not Found,"Abstract
Context
Software similarity comparison has always been a common technique for 
software reuse
 detection, 
plagiarism detection
, and defect detection.
Objective
Considering the role of API calls and 
arithmetic operations
 in software execution, a semantic-based dynamic software analysis method–mixed key instruction sequence (MKIS) is proposed.
Method
MKIS embeds key value sets into a vector and constructs a novel software execution sequence that contains API calls and 
arithmetic operations
 during software execution. To determine the location of key values, a key-value equivalent 
matching algorithm
 is proposed, combined with the longest common subsequence algorithm to optimize the software execution sequence.
Results
Experiments show that MKIS can accurately compare the similarity of binary programs without obtaining the software source code, and has better resiliency and credibility.
Conclusion
Moreover, in the case when the software source code is changed with some main function-independent modification and code obfuscator, 
software reuse
 can be successfully detected.",Information and Software Technology,04 Mar 2025,8,"The proposed semantic-based dynamic software analysis method shows promising results in accurately comparing binary programs, which can benefit startups in software development and reuse."
https://www.sciencedirect.com/science/article/pii/S0950584920301038,Developer portraying: A quick approach to understanding developers on OSS platforms,September 2020,Not Found,Wenhua=Yang: ywh@nuaa.edu.cn; Minxue=Pan: mxp@nju.edu.cn; Yu=Zhou: zhouyu@nju.edu.cn; Zhiqiu=Huang: zqhuang@nuaa.edu.cn,"Abstract
Context
Millions of software developers are using open-source software (OSS) platforms to host their code and collaborate with each other. They possess different programming skills, styles, and preferences, etc., and it is important to understand them for making collaborative decisions such as programming task assignment. Existing OSS platforms do not provide sufficient information about developers, and we need to spend significant effort in searching the OSS platforms for such information.
Objective
Different than the basic developer information displayed on OSS platforms, we propose portraying developers as a quick approach for characterizing and understanding them. We discuss how to build developer portraits to make them concise yet informative.
Method
We propose a multi-dimensional developer portrait model to specify the attributes of various aspects concerning software development about developers. Then, a method that leverages text analysis, web data analysis, and code analysis techniques is presented to analyze a developer’s various sources of data on OSS platforms for constructing the portrait.
Results
The constructed portraits can be vividly displayed on the web to help people quickly understand developers and make better decisions during 
collaborative software development
. 
Case studies
 on two representative problems in the 
software engineering
 area—code recommendation and programming task assignment—are conducted, and the results show the improvement in recommendation and the potential for proper assignments when using our portraits.
Conclusion
The developer portrait is an effective form to characterize developers. It can help people quickly understand the developers and can be applied to various applications in the software development process.",Information and Software Technology,04 Mar 2025,9,"The development of developer portraits to understand and characterize developers can greatly enhance collaborative decision-making in software development, providing practical value to startups."
https://www.sciencedirect.com/science/article/pii/S0950584920300434,Software product line applied to the internet of things: A systematic literature review,August 2020,Not Found,Ricardo Theis=Geraldi: ricardo.geraldi@ppgia.pucpr.br; Sheila=Reinehr: sheila.reinehr@pucpr.br; Andreia=Malucelli: malu@ppgia.pucpr.br,"Abstract
Context
Internet of Things
 (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. 
SPL engineering
 consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities.
Objective
This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out.
Method
A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria.
Results
The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.",Information and Software Technology,04 Mar 2025,6,"Investigating the application of SPL engineering with the IoT paradigm can offer insights into managing IoT systems efficiently, but the lack of detailed specifications and guidelines may limit immediate practical impact on startups."
https://www.sciencedirect.com/science/article/pii/S0950584920300458,Search-based fault localisation: A systematic mapping study,July 2020,Not Found,Plinio S.=Leitao-Junior: plinio@inf.ufg.br; Diogo M.=Freitas: diogom42@gmail.com; Silvia R.=Vergilio: silvia@inf.ufpr.br; Celso G.=Camilo-Junior: celso@inf.ufg.br; Rachel=Harrison: rachel.harrison@brookes.ac.uk,"Abstract
Context
Software 
Fault Localisation
 (FL) refers to finding faulty software elements related to failures produced as a result of test case execution. This is a laborious and time consuming task. To allow FL automation search-based algorithms have been successfully applied in the field of Search-Based Fault Localisation (SBFL). However, there is no study mapping the SBFL field to the best of our knowledge and we believe that such a map is important to promote new advances in this field.
Objective
To present the results of a mapping study on SBFL, by characterising the proposed methods, identifying sources of used information, adopted evaluation functions, applied algorithms and elements regarding reported experiments.
Method
Our mapping followed a defined process and a search protocol. The conducted analysis considers different dimensions and categories related to the main characteristics of 
SBFL methods
.
Results
All methods are grounded on the coverage spectra category. Overall the methods search for solutions related to suspiciousness formulae to identify possible faulty code elements. Most studies use 
evolutionary algorithms
, mainly 
Genetic Programming
, by using a single-objective function. There is little investigation of real-and-multiple-fault scenarios, and the subjects are mostly written in C and Java. No consensus was observed on how to apply the 
evaluation metrics
.
Conclusions
Search-based fault localisation has seen a rise in interest in the past few years and the number of studies has been growing. We identified some research opportunities such as exploring new sources of fault data, exploring multi-objective algorithms, analysing benchmarks according to some classes of faults, as well as, the use of a unique definition for evaluation measures.",Information and Software Technology,04 Mar 2025,7,"The mapping study on Search-Based Fault Localisation provides valuable insights for improving automation in FL, which can benefit startups by enhancing software testing and quality assurance processes."
https://www.sciencedirect.com/science/article/pii/S0950584920300422,Orientation-based Ant colony algorithm for synthesizing the test scenarios in UML activity diagram,July 2020,Not Found,Vinay=Arora: vinay.arora@thapar.edu; Maninder=Singh: Not Found; Rajesh=Bhatia: Not Found,"Abstract
Context
The model-based analysis is preferred over the code-based analysis as it speeds up the development process and directs the guiding effort. In the software industry, the Unified Modeling Language (UML) is a set standard followed by the developers as well as system analysts to extract all attainable paths of controls, usually known as scenarios under an activity diagram.
Objective
In this manuscript, a bio-inspired methodology has been applied on concurrent sub-part of a UML activity diagram to fetch various feasible test scenarios.
Method
The food search pattern of an ant has been taken as a base heuristic. An orientation factor has been introduced in the existing ant colony optimization algorithm. Experiments have been performed using three student projects, five synthetic models and an openly available model repository named LINDHOLMEN data-set at Github.
Results
The statistical analysis has validated the results obtained through various existing approaches and the proposed approach. Experimentation shows that the orientation-based ant colony algorithm has produced better results as compared to the existing Genetic Algorithm (GA) and Ant Colony Optimization (ACO) on the basis of feasible test scenarios generated.",Information and Software Technology,04 Mar 2025,7,The application of a bio-inspired methodology on UML activity diagrams for test scenarios generation shows promise in optimizing software development processes.
https://www.sciencedirect.com/science/article/pii/S0950584920300446,On the performance of hybrid search strategies for systematic literature reviews in software engineering,July 2020,Not Found,Erica=Mourão: ericamourao@id.uff.br; João Felipe=Pimentel: Not Found; Leonardo=Murta: Not Found; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Emilia=Mendes: Not Found; Claes=Wohlin: Not Found,"Abstract
Context
When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort.
Objective
The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing.
Method
We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and 
hybrid strategies
 using precision, recall, and F-measure to investigate the performance of each strategy.
Results
Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall.
Conclusion
We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.",Information and Software Technology,04 Mar 2025,9,The proposal and evaluation of hybrid search strategies combining database searches and snowballing for Systematic Literature Reviews can significantly impact research quality and review effort management.
https://www.sciencedirect.com/science/article/pii/S0950584920300215,Software architectures of the convergence of cloud computing and the Internet of Things: A systematic literature review,June 2020,Not Found,Ahmad=Banijamali: ahmad.banijamali@oulu.fi; Olli-Pekka=Pakanen: Not Found; Pasi=Kuvaja: Not Found; Markku=Oivo: Not Found,"Abstract
Context
Over the last few years, there has been an increasing interest in the convergence of 
cloud computing
 and the 
Internet of Things
 (IoT). Although software systems in this domain have attracted researchers to develop a large body of knowledge on 
software architecture designs
, there is no 
systematic analysis
 of this knowledge.
Objective
This study aims to identify and synthesise state-of-the-art 
architectural elements
 including the 
design patterns
, styles, views, 
quality attributes
, and evaluation methodologies in the convergence of 
cloud computing
 and IoT.
Method
We used systematic literature review (SLR) methodology for a detailed analysis of 82 primary studies of a total of 1618 studies.
Results
We extracted six 
architectural design
 patterns in this domain; among them, edge connectivity patterns stand out as the most popular choice. The service-oriented architecture is the most frequently applied style in this context. Among all applicable 
quality attributes
, scalability, timeliness, and security were the most investigated quality attributes. In addition, we included nine cross analyses to address the relationship between 
architectural patterns
, styles, views, and evaluation methodologies with respect to different quality attributes and 
application areas
.
Conclusions
Our findings indicate that research on software architectures in this domain is increasing. Although few studies were found in which industrial evaluations were presented, industry requires more scientific and empirically validated design frameworks to guide 
software engineering
 in this domain. This work provides an overview of the field while identifying areas for future research.",Information and Software Technology,04 Mar 2025,8,The systematic analysis of architectural elements in the convergence of cloud computing and IoT provides valuable insight for software architecture designs in a rapidly growing domain.
https://www.sciencedirect.com/science/article/pii/S0950584919302290,A focus area maturity model for software ecosystem governance,February 2020,Not Found,Slinger=Jansen: slinger.jansen@uu.nl,"Abstract
Context
Increasingly, software companies are realizing that they can no longer compete through product excellence alone. The ecosystems that surround platforms, such as operating systems, enterprise applications, and even social networks are undeniably responsible for a large part of a platform’s success. With this realization, software producing organizations need to devise tools and strategies to improve their ecosystems and reinvent tools that others have invented many times before.
Objective
In this article, the software ecosystem governance maturity model (SEG-
M
2
) is presented, which has been designed along the principles of a focus area maturity model. The SEG-
M
2
 has been designed for software producing organizations to assess their ecosystem governance practices, set a goal for improvement, and execute an improvement plan.
Method
The model has been created following an established focus area maturity model design method. The model has been evaluated in six evaluating 
case studies
 with practitioners, first by applying the model to their organizations and secondly by evaluating with the practitioners whether the evaluation and improvement advice from the model is valid, useful, and effective.
Result
The model is extensively described and illustrated using six desk studies and six 
case studies
.
Conclusions
The model is evaluated by both researchers and practitioners as a useful collection of practices that enable decision making about software ecosystem governance. We find that maturity models are an effective tool in disseminating a large collection of knowledge, but that research and creation tooling for maturity models is limited.",Information and Software Technology,04 Mar 2025,6,The software ecosystem governance maturity model (SEG-M2) offers a practical tool for software producing organizations to assess and improve ecosystem governance practices.
https://www.sciencedirect.com/science/article/pii/S0950584919301685,Package-Level stability evaluation of object-oriented systems,December 2019,Not Found,Jawad Javed Akbar=Baig: Not Found; Sajjad=Mahmood: smahmood@kfupm.edu.sa; Mohammad=Alshayeb: Not Found; Mahmood=Niazi: Not Found,"Abstract
Context
Software stability is an important object-oriented design characteristic that contributes to the 
maintainability
 
quality attribute
. Software stability quantifies a given systems sensitivity to change between different versions. Stable software tends to reduce the maintenance effort. Assessing software stability during the object-oriented design phase is one of the measures to obtain maintainable software. To determine software stability, there are several metrics at the architecture, system and class levels, but few studies have investigated stability at the package level.
Objective
In this paper, we propose a new package stability metrics (PSM) based on the notion of change between package contents, intra-package connections and inter-package connections.
Method
We validate the PSM theoretically and empirically. The theoretical validation is based on a study of the 
mathematical properties
 of the metrics. The empirical validation is carried out using five open source software programs and we also present a comparison with comparable existing stability metrics packages. For the empirical validation, we perform correlation analysis, 
principal component
 analysis and prediction analysis.
Results
Correlation analysis shows that our proposed metrics provides a better indication of package stability than the existing stability metrics and they are negatively correlated with the maintenance effort. Principal component analysis shows that the proposed metrics captures new dimensions of package stability and helps to increase the maintenance prediction accuracy.
Conclusion
We found there was a negative correlation between our metric and maintenance effort. We also found a 
positive correlation
 between the existing package stability metrics which are based on changes in lines of code and class names.",Information and Software Technology,04 Mar 2025,8,The proposal of a new package stability metrics (PSM) and its validation through theoretical and empirical analysis can enhance software stability assessment during the design phase.
https://www.sciencedirect.com/science/article/pii/S0950584919301703,Evaluating probabilistic software development effort estimates: Maximizing informativeness subject to calibration,November 2019,Not Found,Magne=Jørgensen: magnej@simula.no,"Abstract
Context
Probabilistic effort estimates inform about the uncertainty and may give useful input to plans, budgets and investment analyses.
Objective & method
This paper introduces, motivates and illustrates two principles on how to evaluate the accuracy and other performance criteria of probabilistic effort estimates in software development contexts.
Results
The first principle emphasizes a consistency between the 
estimation error
 measure and the loss function of the chosen type of probabilistic single point effort estimates. The second 
principle points
 at the importance of not just measuring calibration, but also informativeness of estimated prediction intervals and distributions. The relevance of the evaluation principles is illustrated by a performance evaluation of estimates from twenty-eight software professionals using two different uncertainty assessment methods to estimate the effort of the same thirty software maintenance tasks.",Information and Software Technology,04 Mar 2025,4,"While the paper introduces principles for evaluating effort estimates in software development, the practical impact on startups is limited as it focuses on a specific aspect of estimation."
https://www.sciencedirect.com/science/article/pii/S0950584919301478,Multi-reviewing pull-requests: An exploratory study on GitHub OSS projects,November 2019,Not Found,Dongyang=Hu: hudongyang17@163.com; Yang=Zhang: yangzhang15@nudt.edu.cn; Junsheng=Chang: Not Found; Gang=Yin: Not Found; Yue=Yu: Not Found; Tao=Wang: Not Found,"Abstract
Context:
GitHub has enabled developers to easily contribute their review comments on multiple pull-requests and switch their review focus 
between
 different pull-requests, 
i.e.
, multi-reviewing. Reviewing multiple pull-requests simultaneously may enhance work efficiency. However, multi-reviewing also relies on developers’ rationally allocating their focus, which may bring a different influence to the resolution of pull-requests.
Objective:
 In this paper, we present an ongoing study of the impact of multi-reviewing on pull-request resolution in GitHub 
open source projects
.
Method:
 We collected and analyzed 1,836,280 pull-requests from 760 GitHub projects to explore how multi-reviewing affects the resolution of a pull-request.
Results:
 We find that multi-reviewing is a common behavior in GitHub. However, more multi-reviewing behaviors tend to bring longer pull-request resolution latency.
Conclusion:
 Multi-reviewing is a complex behavior of developers, and has an important impact on the efficiency of pull-request resolution. Our study motivates the need for more research on multi-reviewing.",Information and Software Technology,04 Mar 2025,6,"The study on the impact of multi-reviewing on pull-request resolution in GitHub projects has practical value for startups using GitHub, highlighting the importance of efficient resolution processes."
https://www.sciencedirect.com/science/article/pii/S0950584919301375,Model driven transformation development (MDTD): An approach for developing model to model transformation,October 2019,Not Found,Ana Patrícia Fontes=Magalhaes: apmagalhaes@uneb.br; Aline Maria Santos=Andrade: Not Found; Rita Suzana Pitangueira=Maciel: Not Found,"Abstract
Context
In the Model Driven Development (MDD) approach, model transformations are responsible for the semi-automation of software development process converting models between different abstraction levels. The development of model transformations involves a complexity inherent to the transformation domain, in addition to the complexity of software development in general. Therefore, the construction of model transformations requires software engineering feature such as processes and languages to facilitate its development and maintenance.
Objective
This paper presents a framework to develop unidirectional relational model transformation using the MDD approach itself, which integrates: (i) a software development process suitable for the model transformation domain (ii) a Domain specific language for transformation modeling (iii) a transformation chain, to (semi) automate the proposed process, and (iv) a development environment to support it.
Methods
The proposal systematizes the development of model transformation, following the MDD principles. An iterative and incremental process guides transformation development from requirement specification to transformation codification. The proposal has been evaluated through a 
case study
 and a controlled experiment.
Results
The framework enables model transformation specification at a high abstraction level and (semi) automatically transforms it into models at a low abstraction level until the transformation code. The results of the case study showed that people with different levels of knowledge of MDD, or without experience in transformation languages, were able to develop transformations through the framework and generated executable code.
Conclusions
The framework integrates the essential elements involved in the development of model transformation and enables the abstraction of technological details. The results of the case study and controlled experiment showed the feasibility of the proposal and its use in dealing with the complexity involved in model transformation development.",Information and Software Technology,04 Mar 2025,8,"The framework for developing model transformations using MDD principles has a high practical value for startups involved in software development, offering a systematic approach that can benefit early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301429,Log mining to re-construct system behavior: An exploratory study on a large telescope system,October 2019,Not Found,Michele=Pettinato: Not Found; Juan Pablo=Gil: juan.gil@alma.cl; Patricio=Galeas: patricio.galeas@ufrontera.cl; Barbara=Russo: barbara.russo@unibz.it,"Abstract
Context
A large amount of information about 
system behavior
 is stored in logs that record system changes. Such information can be exploited to discover anomalies of a system and the operations that cause them. Given their large size, manual inspection of logs is hard and infeasible in a desired timeframe (e.g., real-time), especially for 
critical systems
.
Objective
This study proposes a semi-automated method for reconstructing sequences of tasks of a system, revealing system anomalies, and associating tasks and anomalies to code components.
Method
The proposed approach uses unsupervised 
machine learning
 (Latent Dirichlet Allocation) to discover latent topics in messages of log events and introduces a novel technique based on pattern recognition to derive the semantic of such topics (topic labelling). The approach has been applied to the 
big data
 generated by the ALMA telescope system consisting of more than 2000 log events collected in about five hours of telescope operation.
Results
With the application of our approach to such data, we were able to model the behavior of the telescope over 16 different observations. We found five different behavior models and three different types of errors. We use the models to interpret each error and discuss its cause.
Conclusions
With this work, we have also been able to discuss some of the known challenges in log mining. The experience we gather has been then summarized in lessons learned.",Information and Software Technology,04 Mar 2025,7,"The semi-automated method for reconstructing sequences of tasks from system logs and identifying anomalies has a significant impact on startups dealing with critical systems, offering a practical way to manage system behavior and errors."
https://www.sciencedirect.com/science/article/pii/S0950584919301004,Interactive semi-automated specification mining for debugging: An experience report,September 2019,Not Found,Mohammad Jafar=Mashhadi: mohammadjafar.mashha@ucalgary.ca; Taha R.=Siddiqui: trsiddiqui1989@gmail.com; Hadi=Hemmati: hadi.hemmati@ucalgary.ca; Howard=Loewen: hloewen@micropilot.com,"Abstract
Context
Specification mining techniques are typically used to extract the specification of a software in the absence of (up-to-date) specification documents. This is useful for 
program comprehension
, testing, and 
anomaly detection
. However, specification mining can also potentially be used for debugging, where a faulty behavior is abstracted to give developers a context about the bug and help them locating it.
Objective
In this project, we investigate this idea in an industrial setting. We propose a very basic semi-automated specification mining approach for debugging and apply that on real reported issues from an AutoPilot software system from our industry partner, MicroPilot Inc. The objective is to assess the feasibility and usefulness of the approach in a real-world setting.
Method
The approach is developed as a prototype tool, working on C code, which accept a set of relevant state fields and functions, per issue, and generates an extended 
finite state machine
 that represents the faulty behavior, abstracted with respect to the relevant context (the selected fields and functions).
Results
We qualitatively evaluate the approach by a set of interviews (including observational studies) with the company’s developers on their real-world reported bugs. The results show that (a) our approach is feasible, (b) it can be automated to some extent, and (c) brings advantages over only using their code-level debugging tools. We also compared this approach with traditional fully automated state-merging algorithms and reported several issues when applying those techniques on a real-world debugging context.
Conclusion
The main conclusion of this study is that the idea of an “interactive” specification mining rather than a fully automated mining tool is NOT impractical and indeed is useful for the debugging use case.",Information and Software Technology,04 Mar 2025,5,The investigation into semi-automated specification mining for debugging in an industrial setting provides insight into real-world application but has limited direct impact on early-stage ventures in Europe.
https://www.sciencedirect.com/science/article/pii/S0950584919301247,M-Lean: An end-to-end development framework for predictive models in B2B scenarios,September 2019,Not Found,Mona=Nashaat: nashaata@ualberta.ca; Aindrila=Ghosh: Not Found; James=Miller: jimm@ualberta.ca; Shaikh=Quader: Not Found; Chad=Marston: Not Found,"Abstract
Context
The need for 
business intelligence
 has led to advances in 
machine learning
 in the business domain, especially with the rise of 
big data analytics
. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine 
learning systems
 in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying 
machine learning
 in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents 
MLean
, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the 
Lean Startup
 methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a 
case study
 to build a predictive product. The 
case study
 resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.",Information and Software Technology,04 Mar 2025,7,"The framework presented (MLean) offers practical guidance for businesses in designing, developing, evaluating, and deploying predictive systems, showcasing innovation and potential impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919300710,Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions,August 2019,Not Found,Markus=Borg: markus.borg@ri.se; Panagiota=Chatzipetrou: Not Found; Krzysztof=Wnuk: Not Found; Emil=Alégroth: Not Found; Tony=Gorschek: Not Found; Efi=Papatheocharous: Not Found; Syed Muhammad Ali=Shah: Not Found; Jakob=Axelsson: Not Found,"Abstract
Context
Component-based software engineering (CBSE) is a common approach to develop and evolve contemporary software systems. When evolving a system based on components, make-or-buy decisions are frequent, i.e., whether to develop components internally or to acquire them from 
external sources
. In CBSE, several different sourcing options are available: (1) developing software in-house, (2) outsourcing development, (3) buying commercial-off-the-shelf software, and (4) integrating 
open source software
 components.
Objective
Unfortunately, there is little available research on how organizations select component sourcing options (CSO) in industry practice. In this work, we seek to contribute empirical evidence to CSO selection.
Method
We conduct a cross-domain survey on CSO selection in industry, implemented as an online questionnaire.
Results
Based on 188 responses, we find that most organizations consider multiple CSOs during software evolution, and that the CSO decisions in industry are dominated by 
expert judgment
. When choosing between candidate components, functional suitability acts as an initial filter, then reliability is the most important quality.
Conclusion
We stress that future solution-oriented work on decision support has to account for the dominance of expert judgment in industry. Moreover, we identify considerable variation in CSO decision processes in industry. Finally, we encourage software development organizations to reflect on their decision processes when choosing whether to make or buy components, and we recommend using our survey for a first benchmarking.",Information and Software Technology,04 Mar 2025,6,"The empirical evidence on component sourcing options (CSO) in industry provides insights for decision-making, but the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919300977,A bug finder refined by a large set of open-source projects,August 2019,Not Found,Jaechang=Nam: jcnam@handong.edu; Song=Wang: song.wang@uwaterloo.ca; Yuan=Xi: y25xi@uwaterloo.ca; Lin=Tan: lintan@uwaterloo.ca,"Abstract
Context
Static bug detection techniques are commonly used to automatically detect software bugs. The biggest obstacle to the wider adoption of static bug detection tools is 
false positives
, i.e., reported bugs that developers do not have to act on.
Objective
The objective of this study is to reduce 
false positives
 resulting from static bug detection tools and to detect new bugs by exploring the effectiveness of a feedback-based bug detection rule design.
Method
We explored a large number of software projects and applied an iterative feedback-based process to design bug detection rules. The outcome of the process is a set of ten bug detection rules, which we used to build a feedback-based bug finder, 
FeeFin
. Specifically, we manually examined 1622 patches to identify bugs and fix patterns, and implement bug detection rules. Then, we refined the rules by repeatedly using feedback from a large number of software projects.
Results
We applied 
FeeFin
 to the latest versions of the 1880 projects on GitHub to detect previously unknown bugs. 
FeeFin
 detected 98 new bugs, 63 of which have been reviewed by developers: 57 were confirmed as true bugs, and 9 were confirmed as false positives. In addition, we investigated the benefits of our 
FeeFin
 process in terms of new and improved bug patterns. We verified our bug patterns with four existing tools, namely PMD, FindBugs, Facebook Infer, and Google Error Prone, and found that our FeeFin process has the potential to identify new bug patterns and also to improve existing bug patterns.
Conclusion
Based on the results, we suggest that static bug detection tool designers identify new bug patterns by mining real-world patches from a large number of software projects. In addition, the 
FeeFin
 process is helpful in mitigating false positives generated from existing tools by refining their bug detection rules.",Information and Software Technology,04 Mar 2025,8,"The feedback-based bug detection rule design (FeeFin) shows promising results in reducing false positives and detecting new bugs, which can have significant practical value for startups in improving software quality."
https://www.sciencedirect.com/science/article/pii/S0950584919300540,Search-based test case implantation for testing untested configurations,July 2019,Not Found,Dipesh=Pradhan: dipesh@simula.no; Shuai=Wang: shuai.wang@testify.no; Tao=Yue: tao@simula.no; Shaukat=Ali: shaukat@simula.no; Marius=Liaaen: marliaae@cisco.com,"Abstract
Context
Modern large-scale software systems are highly configurable, and thus require a large number of test cases to be implemented and revised for testing a variety of system configurations. This makes testing highly configurable systems very expensive and time-consuming.
Objective
Driven by our industrial collaboration with a video conferencing company, we aim to automatically analyze and implant existing test cases (i.e., an original test suite) to test the untested configurations.
Method
We propose a search-based test case implantation approach (named as SBI) consisting of two key components: 1) 
Test case analyzer
 that statically analyzes each test case in the original test suite to obtain the program 
dependence graph
 for test case statements and 2) 
Test case implanter
 that uses multi-objective search to select suitable test cases for implantation using three operators, i.e., selection, crossover, and mutation (at the test suite level) and implants the selected test cases using a 
mutation operator
 at the test case level including three operations (i.e., addition, modification, and deletion).
Results
We empirically evaluated SBI with an industrial 
case study
 and an open source 
case study
 by comparing the implanted test suites produced by three variants of SBI with the original test suite using 
evaluation metrics
 such as 
statement coverage
 (
SC
), branch coverage (
BC
), and mutation score (
MS
). Results show that for both the case studies, the test suites implanted by the three variants of SBI performed significantly better than the original test suites. The best variant of SBI achieved on average 19.3% higher coverage of configuration 
variable values
 for both the case studies. Moreover, for the open source case study, the best variant of SBI managed to improve 
SC, BC
, and 
MS
 with 5.0%, 7.9%, and 3.2%, respectively.
Conclusion
SBI can be applied to automatically implant a test suite with the aim of testing untested configurations and thus achieving higher configuration coverage.",Information and Software Technology,04 Mar 2025,9,"The SBI approach for automatically implanting test cases to test untested configurations demonstrates significant performance improvements, offering practical value and potential impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919300606,A domain analysis of resource and requirements monitoring: Towards a comprehensive model of the software monitoring domain,July 2019,Not Found,Rick=Rabiser: rick.rabiser@jku.at; Klaus=Schmid: schmid@sse.uni-hildesheim.de; Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Michael=Vierhauser: mvierhau@nd.edu; Sam=Guinea: sam.guinea@polimi.it; Paul=Grünbacher: paul.gruenbacher@jku.at,"Abstract
[Context]
 Complex and heterogeneous software systems need to be monitored as their full behavior often only emerges at runtime, e.g., when interacting with other systems or the environment. Software monitoring approaches observe and check properties or quality attributes of software systems during operation. Such approaches have been developed in diverse communities for various kinds of systems and purposes. For instance, requirements monitoring aims to check at runtime whether a software system adheres to its requirements, while resource or performance monitoring collects information about the consumption of computing resources by the monitored system. Many venues publish research on software monitoring, often using diverse terminology, and focusing on different monitoring aspects and phases. The lack of a comprehensive overview of existing research often leads to re-inventing the wheel. 
[Objective]
 We provide a domain model to structure and systematize the field of software monitoring, starting with requirements and resource monitoring. 
[Method]
 We developed an initial domain model based on (i) our extensive experiences with requirements and resource monitoring, (ii) earlier efforts to develop a comparison framework for monitoring approaches, and (iii) an earlier systematic literature review on requirements monitoring frameworks. We then systematically analyzed 47 existing requirements and resource monitoring approaches to iteratively refine the domain model and to develop a reference architecture for software monitoring approaches. 
[Results]
 Our domain model covers the key elements of monitoring approaches and allows analyzing their commonalities and differences. Together with the reference architecture, our domain model supports the development of integrated monitoring solutions. We provide details on 47 approaches we analyzed with the model to assess its coverage. We also evaluate the reference architecture by instantiating it for five different monitoring solutions. 
[Conclusions]
 We conclude that requirements and resource monitoring have more commonalities than differences, which is promising for the future integration of existing monitoring solutions.",Information and Software Technology,04 Mar 2025,5,"While the domain model for software monitoring has potential for structuring the field, the direct impact on European early-stage ventures or startups may be less immediate compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918301344,GoalD: A Goal-Driven deployment framework for dynamic and heterogeneous computing environments,July 2019,Not Found,Gabriel S.=Rodrigues: gabrielsr@aluno.unb.br; Felipe P.=Guimarães: felipe.guimaraes@aeb.gov.br; Genaína N.=Rodrigues: genaina@unb.br; Alessia=Knauss: alessia.knauss@chalmers.se; João Paulo C.=de Araújo: Not Found; Hugo=Andrade: sica@chalmers.se; Raian=Ali: rali@bournemouth.ac.uk,"Abstract
Context
Emerging paradigms like 
Internet of Things
 and Smart Cities utilize advanced sensing and communication infrastructures, where heterogeneity is an inherited feature. Applications targeting such environments require adaptability and context-sensitivity to uncertain availability and failures in resources and their ad-hoc networks. Such heterogeneity is often hard to predict, making the 
deployment process
 a challenging task.
Objective
This paper proposes GoalD as a goal-driven framework to support autonomous deployment of heterogeneous 
computational resources
 to fulfill requirements, seen as goals, and their correlated components on one hand, and the variability space of the hosting computing and sensing environment on the other hand.
Method
GoalD comprises an offline and an online stage to fulfill autonomous deployment by leveraging the use of goals. Deployment configuration strategies arise from the variability structure of the Contextual Goal Model as an underlying structure to guide autonomous planning by selecting available as well as suitable resources at runtime.
Results
We evaluate GoalD on an existing exemplar from the self-adaptive systems community – the Tele Assistance Service provided by Weyns and Calinescu [1]. Furthermore, we evaluate the scalability of GoalD on a repository consisting of 430,500 artifacts. The evaluation results demonstrate the usefulness and scalability of GoalD in planning the deployment of a system with thousands of components in a few milliseconds.
Conclusion
GoalD is a framework to systematically tackle autonomous deployment in highly 
heterogeneous computing
 environments, partially unknown at design-time following a goal-oriented approach to achieve the user goals in a target environment. GoalD has demonstrated itself able to scale for deployment planning dealing with thousands of components in a few milliseconds.",Information and Software Technology,04 Mar 2025,9,"The proposed GoalD framework addresses the autonomous deployment challenges in heterogeneous computing environments, demonstrating scalability and usefulness which can have a significant impact on early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S095058491930031X,Investigation on test effort estimation of mobile applications: Systematic literature review and survey,June 2019,Not Found,Anureet=Kaur: anumahal@gmail.com; Kulwant=Kaur: kulwantkaur@apjimtc.org,"Abstract
Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of 
agile methodology
, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.",Information and Software Technology,04 Mar 2025,6,"The study on test estimation for mobile applications provides valuable insights and suggestions for improving testing processes, but may have a more limited impact on early-stage ventures compared to more technical innovations."
https://www.sciencedirect.com/science/article/pii/S0950584919300436,FineLocator: A novel approach to method-level fine-grained bug localization by query expansion,June 2019,Not Found,Wen=Zhang: zhangwen@bjut.buct.edu.cn; Ziqiang=Li: 2016200859@mail.buct.edu.cn; Qing=Wang: wq@itechs.iscas.ac.cn; Juan=Li: lijuan@bjut.edu.cn,"Abstract
Context
Bug localization, namely, to locate suspicious snippets from 
source code
 files for developers to fix the bug, is crucial for 
software quality assurance
 and software maintenance. Effective bug localization technique is desirable for software developers to reduce the effort involved in bug resolution. State-of-the-art bug localization techniques concentrate on file-level coarse-grained localization by lexical matching 
bug reports
 and 
source code files
. However, this would bring about a heavy burden for developers to locate feasible code snippets to make change with the goal of fixing the bug.
Objective
This paper proposes a novel approach called FineLocator to method-level fine-grained bug localization by using semantic similarity, temporal proximity and call dependency for method expansion.
Method
Firstly, the 
bug reports
 and the methods of 
source code
 are represented by numeric vectors using 
word embedding
 (word2vec) and the TF-IDF method. Secondly, we propose three 
query expansion
 scores as semantic similarity score, temporal proximity score and call dependency score to address the representation sparseness problem caused by the short lengths of methods in the source code. Then, the representation of a method with short length is augmented by elements of its neighboring methods with 
query expansion
. Thirdly, when a new bug report is incoming, FineLocator will retrieve the methods in source code by similarity ranking on the bug report and the augmented methods for bug localization.
Results
We collect bug repositories of ArgoUML, Maven, Kylin, Ant and AspectJ projects to investigate the performance of the proposed FineLocator approach. Experimental results demonstrate that the proposed FineLocator approach can improve the performances of method-level bug localization at average by 20%, 21% and 17% measured by Top-N indicator, 
MAP
 and MRR respectively, in comparison with state-of-the-art techniques.
Conclusion
This is the first paper to demonstrate how to make use of method expansion to address the representation sparseness problem for method-level fine-grained bug localization.",Information and Software Technology,04 Mar 2025,8,"The FineLocator approach for method-level bug localization offers a novel solution to improve bug resolution efficiency, which can benefit startups dealing with software quality assurance and maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584919300527,Reference Coupling: An exploration of inter-project technical dependencies and their characteristics within large software ecosystems,June 2019,Not Found,Kelly=Blincoe: kblincoe@acm.org; Francis=Harrison: francish@uvic.ca; Navpreet=Kaur: nkaur@uvic.ca; Daniela=Damian: danielad@uvic.ca,"Abstract
Context
Software projects often depend on other projects or are developed in tandem with other projects. Within such software ecosystems, knowledge of cross-project technical dependencies is important for (1) practitioners understanding of the impact of their code change and coordination needs within the ecosystem and (2) researchers in exploring properties of software ecosystems based on these technical dependencies. However, identifying technical dependencies at the ecosystem level can be challenging.
Objective
In this paper, we describe Reference Coupling, a new method that uses solely the information in developers online interactions to detect technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We then use the output of this method to explore the properties of large software ecosystems.
Method
We validate our method on two datasets — one from open-source projects hosted on GitHub and one commercial dataset of IBM projects. We manually analyze the identified dependencies, categorize them, and compare them to dependencies specified by the development team. We examine the types of projects involved in the identified ecosystems, the structure of the identified ecosystems, and how the ecosystems structure compares with the social behavior of project contributors and owners.
Results
We find that our Reference Coupling method often identifies technical dependencies between projects that are untracked by developers. We describe empirical insights about the characteristics of large software ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. By exploring the socio-technical alignment within the GitHub ecosystems, we also found that the project owners social behavior aligns well with the technical dependencies within the ecosystem, but the project contributors social behavior does not align with these dependencies.
Conclusions
We conclude with a discussion on future research that is enabled by our Reference Coupling method.",Information and Software Technology,04 Mar 2025,7,"The Reference Coupling method to detect technical dependencies between projects can be valuable for developers and researchers in software ecosystems, providing insights that could be beneficial for early-stage ventures collaborating with other projects."
https://www.sciencedirect.com/science/article/pii/S0950584919300035,Deriving architectural models from requirements specifications: A systematic mapping study,May 2019,Not Found,Eric=Souza: er.souza@campus.fct.unl.pt; Ana=Moreira: amm@fct.unl.pt; Miguel=Goulão: mgoul@fct.unl.pt,"Abstract
Context
Software architecture design
 creates and documents the high-level structure of a software system. Such structure, expressed in 
architectural models
, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive 
architectural models
 from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects.
Object
The purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation.
Method
To achieve this goal, we performed a 
systematic mapping study
 following the good practices from the Evidence-Based 
Software Engineering
 field.
Results
This study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved.
Conclusion
The major findings indicate that current architectural derivation methods rely heavily on the architects’ 
tacit knowledge
 (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners.",Information and Software Technology,04 Mar 2025,5,"While the overview of architectural derivation methods is comprehensive and provides a research roadmap, the practical impact on early-stage ventures may be more indirect compared to more directly applicable solutions."
https://www.sciencedirect.com/science/article/pii/S0950584919300084,Mining software repositories for adaptive change commits using machine learning techniques,May 2019,Not Found,Omar=Meqdadi: ommeqdadi@just.edu.jo; Nouh=Alhindawi: hindawi@jadara.edu.jo; Jamal=Alsakran: j.alsakran@ju.edu.jo; Ahmad=Saifan: ahmads@yu.edu.jo; Hatim=Migdadi: hatims@hu.edu.jo,"Abstract
Context
Version Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance.
Objective
This work examines the version histories of an 
open source system
 to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits.
Method
We collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a 
machine learning approach
 to classify whether a commit was adaptive or not.
Results
It is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the 
machine learning
 classifier developed has approximately 75% prediction accuracy within labeled change histories.
Conclusion
The proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of 
F
-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have 
predictive power
 of adaptive commits without the need of manual efforts.",Information and Software Technology,04 Mar 2025,7,"The proposed method automates the process of examining version history of a software system, identifying commits related to maintenance tasks. The evaluation supports its applicability and efficiency, paving the way for more advanced classifiers."
https://www.sciencedirect.com/science/article/pii/S0950584918302489,Software feature refinement prioritization based on online user review mining,April 2019,Not Found,Jianzhang=Zhang: jianzhang.zhang2017@gmail.com; Yinglin=Wang: wang.yinglin@shufe.edu.cn; Tian=Xie: xietiansh@gmail.com,"Abstract
Context
Online software reviews have provided a wealth of user feedback on 
software applications
. User reviews along with ratings have been influential in a series of 
software engineering
 tasks e.g. software maintenance and release planning.
Objective
Our research aims to assist managers in prioritizing features to be refined in next release from the perspective of enhancing user ratings via mining online reviews.
Method
We first extract software features from user reviews and determine their probability distribution in each review with 
LDA
. Then the ground truth rating of each feature is estimated by linear regression under the assumption that the software functionality rating is a 
convex combination
 of all feature ratings weighted by their distribution probabilities over the review. Finally, we formalize feature refinement prioritization as an 
optimization problem
 which maximizes user group’s rating on the software functionality under the constraint of development budget.
Results
The proposed approach can use topic model to jointly extract features from user reviews semi-supervisedly and determine each feature’s weight in each user’s rating on the software functionality. The estimated ground truth ratings of all features reveal how reviewer group evaluate those features. Finally, we provide an illustrative example to demonstrate the key idea of our framework.
Conclusion
Our proposed framework is general to various software products with mass user reviews and semi-automatic without much human efforts and intervention. The framework’s 
interpretability
 helps managers better understand user feedback on the software functionality and make feature refinement plan for the upcoming releases.",Information and Software Technology,04 Mar 2025,8,"The research aims to assist managers in prioritizing feature refinement based on user reviews, enhancing user ratings. The framework is generalizable with interpretability, helping managers understand and act on user feedback."
https://www.sciencedirect.com/science/article/pii/S0950584918302222,API recommendation for event-driven Android application development,March 2019,Not Found,Weizhao=Yuan: weizhaoy@163.com; Hoang H.=Nguyen: mr.erichoang@gmail.com; Lingxiao=Jiang: lxjiang@smu.edu.sg; Yuting=Chen: chenyt@cs.sjtu.edu.cn; Jianjun=Zhao: zhao@ait.kyushu-u.ac.jp; Haibo=Yu: haibo_yu@sjtu.edu.cn,"Abstract
Context
Software development is increasingly dependent on existing libraries. Developers need help to find suitable library APIs. Although many studies have been proposed to recommend relevant functional APIs that can be invoked for implementing a functionality, few studies have paid attention to an orthogonal need associated with event-driven programming frameworks, such as the 
Android
 framework. In addition to invoking functional APIs, Android developers need to know where to place functional code according to various events that may be triggered within the framework.
Objective
This paper aims to develop an API recommendation engine for 
Android application
 development that can recommend both (1) functional APIs for implementing a functionality and (2) the event callback APIs that are to be overridden to contain the functional code.
Method
We carry out an empirical study on actual Android programming questions from StackOverflow to confirm the need of recommending callbacks. Then we build Android-specific API databases to contain the correlations among various functionalities and APIs, based on customized 
parsing
 of code snippets and 
natural language processing
 of texts in Android tutorials and SDK documents, and then textual and code similarity metrics are adapted for recommending relevant APIs.
Results
We have evaluated our prototype recommendation engine, named LibraryGuru, with about 1500 questions on Android programming from StackOverflow, and demonstrated that our top-5 results on recommending callbacks and functional APIs can on estimate achieve up to 43.5% and 50.9% respectively in precision, 24.6% and 32.5% respectively in 
mean average precision
 (MAP) scores, and 51.1% and 44.0% respectively in recall.
Conclusion
We conclude that it is important and possible to recommend both functional APIs and callbacks for 
Android application
 development, and future work is needed to take more data sources into consideration to make more relevant recommendations for developers’ needs.",Information and Software Technology,04 Mar 2025,9,"The paper aims to develop an API recommendation engine for Android app development, recommending functional and event callback APIs. The prototype achieves promising results on StackOverflow questions, showing potential impact on developers."
https://www.sciencedirect.com/science/article/pii/S0950584918302404,Athena: A framework to automatically generate security test oracle via extracting policies from source code and intended software behaviour,March 2019,Not Found,Hossein=Homaei: homayi@aut.ac.ir; Hamid Reza=Shahriari: shahriari@aut.ac.ir,"Abstract
Context:
 Software security testing aims to check the security behaviour of a program. To determine whether the program behaves securely on a particular execution, we need an oracle who knows the expected security behaviour. Security test oracle decides whether test cases violate the intended security policies of the program. Thus, it is necessary for the oracle to model the detailed security policies. Unfortunately, these policies are usually poorly documented. Even worse, in some cases, the source code is the only available document of the program.
Objective:
 We propose a method to automatically extract the intended security policies of the program under test from the source code and expected execution traces. We introduce a security test oracle, Athena, which utilises these policies to differentiate between the secure and potentially insecure behaviour of the program.
Method:
 We use a hybrid analysis approach to obtain the intended security policies. We investigate the program statements (gates) in which the software communicates with the environment. We analyse the transmitted messages in the gates and the control and data flow of the program to extract some security properties. Moreover, we specify the intended navigation paths of the program. These properties and paths form the expected security policies. Athena utilises these policies to detect potential 
security breaches
.
Results:
 Investigating common types of software vulnerabilities illustrates the flexibility of Athena in modelling various kinds of security policies. Moreover, we show the usefulness of the method by applying it to the real web applications and evaluating its capability to detect actual attacks.
Conclusions:
 Our proposed approach takes a step towards solving the test oracle automation problem in the domain of security testing.",Information and Software Technology,04 Mar 2025,8,"The method proposes extracting security policies from source code to enhance security testing automation. The results showcase the method's flexibility in detecting vulnerabilities, potentially improving security testing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584918302453,VFL: Variable-based fault localization,March 2019,Not Found,Jeongho=Kim: jeonghodot@skku.edu; Jindae=Kim: jdkim@cse.ust.hk; Eunseok=Lee: leees@skku.edu,"ABSTRACT
Context
Fault localization
 is one of the most important debugging tasks. Hence, many automatic fault localization techniques have been proposed to reduce the burden on developers for such tasks. Among them, Spectrum-based Fault Localization (SFL) techniques leverage coverage information and localize faults based on the coverage difference between the failed and passed test cases.
Objective
However, such SFL techniques cannot localize faults effectively when coverage differences are not clear. To address this issue and improve the fault localization performance of the SFL techniques, we propose a Variable-based Fault Localization (VFL) technique.
Method
The VFL technique identifies suspicious variables and uses them to generate a ranked list of suspicious source code lines. Since it only requires additional information about variables that are also available in the SFL techniques, the proposed technique is lightweight and can be used to improve the performance of existing the SFL techniques.
Results
In an evaluation with 224 real Java faults and 120 C faults, the VFL technique outperforms the SFL techniques using the same similarity coefficient. The average Exam scores of the VFL techniques are reduced by more than 55% compared to the SFL techniques, and the VFL techniques localize faults at a lower rank than the SFL techniques for about 73% of the 344 faults.
Conclusion
We proposed a novel variable-based fault localization technique for more effective debugging. The VFL technique has better performance than the existing techniques and the results were more useful for actual fault localization tasks. In addition, this technique is very lightweight and scalable, so it is very easy to collaborate with other fault localization techniques.",Information and Software Technology,04 Mar 2025,7,Introducing a Variable-based Fault Localization technique to improve fault localization performance is significant. The evaluation results demonstrate the technique's effectiveness and lightweight nature for debugging tasks.
https://www.sciencedirect.com/science/article/pii/S0950584918301940,A pilot empirical study of applying a usability technique in an open source software project,February 2019,Not Found,Lucrecia=Llerena: lucrecia.llerena@estudiante.uam.es; John W.=Castro: john.castro@alumni.uam.es; Silvia T.=Acuña: silvia.acunna@uam.es,"Abstract
Context
The growth in the number of non-technical 
open source software
 (OSS) application users and the escalating use of these applications have redoubled the need for, and interest in, developing usable OSS. OSS communities are unclear about which techniques to use in each 
development process
 activity.
Objective
The aim of our research is to adapt a usability technique (visual brainstorming) to an 
OSS project
 and evaluate the feasibility of its application.
Method
We used the 
case study
 research method to investigate technique application and participation in a project. To do this, we participated as volunteers in the HistoryCal project.
Results
We identified adverse conditions that were an obstacle to technique application (like it was not easy to recruit OSS users to participate) and modified the technique to make it applicable.
Conclusion
We conclude that these changes were helpful for applying the technique using web artifacts like blogs.",Information and Software Technology,04 Mar 2025,5,"The research on adapting usability techniques for OSS projects can have practical value for early-stage ventures looking to make their software more user-friendly, but the impact may be limited due to the specific focus on visual brainstorming."
https://www.sciencedirect.com/science/article/pii/S0950584918302088,Software defect prediction based on kernel PCA and weighted extreme learning machine,February 2019,Not Found,Zhou=Xu: Not Found; Jin=Liu: jinliu@whu.edu.cn; Xiapu=Luo: Not Found; Zijiang=Yang: Not Found; Yifeng=Zhang: Not Found; Peipei=Yuan: Not Found; Yutian=Tang: Not Found; Tao=Zhang: Not Found,"Abstract
Context
Software 
defect prediction
 strives to detect defect-prone software modules by mining the 
historical data
. Effective prediction enables reasonable testing 
resource allocation
, which eventually leads to a more reliable software.
Objective
The complex structures and the imbalanced class distribution in software defect data make it challenging to obtain suitable data features and learn an effective 
defect prediction
 model. In this paper, we propose a method to address these two challenges.
Method
We propose a defect prediction framework called 
KPWE
 that combines two techniques, i.e., 
Kernel 
Principal Component
 A
nalysis (
KPCA
) and 
Weighted 
Extreme Learning Machine
 (
WELM
). Our framework consists of two major stages. In the first stage, KPWE aims to extract representative data features. It leverages the KPCA technique to project the original data into a latent 
feature space
 by 
nonlinear mapping
. In the second stage, KPWE aims to alleviate the 
class imbalance
. It exploits the WELM technique to learn an effective defect prediction model with a weighting-based scheme.
Results
We have conducted extensive experiments on 34 projects from the PROMISE dataset and 10 projects from the NASA dataset. The experimental results show that KPWE achieves promising performance compared with 41 
baseline methods
, including seven basic classifiers with KPCA, five variants of KPWE, eight representative feature selection methods with WELM, 21 imbalanced learning methods.
Conclusion
In this paper, we propose KPWE, a new software defect prediction framework that considers the feature extraction and 
class imbalance
 issues. The empirical study on 44 software projects indicate that KPWE is superior to the 
baseline methods
 in most cases.",Information and Software Technology,04 Mar 2025,8,"The proposed defect prediction framework KPWE addresses important challenges in software development and shows promising results compared to baseline methods, making it potentially valuable for early-stage ventures in improving software reliability."
https://www.sciencedirect.com/science/article/pii/S0950584918301654,Improving bug localization with word embedding and enhanced convolutional neural networks,January 2019,Not Found,Yan=Xiao: yanxiao6-c@my.cityu.edu.hk; Jacky=Keung: Jacky.Keung@cityu.edu.hk; Kwabena E.=Bennin: kebennin2-c@my.cityu.edu.hk; Qing=Mi: Qing.Mi@my.cityu.edu.hk,"Abstract
Context:
 Automatic localization of buggy files can speed up the process of bug fixing to improve the efficiency and productivity of 
software quality assurance
 teams. Useful 
semantic information
 is available in 
bug reports
 and 
source code
, but it is usually underutilized by existing bug localization approaches.
Objective:
 To improve the performance of bug localization, we propose DeepLoc, a novel deep learning-based model that makes full use of 
semantic information
.
Method:
 DeepLoc is composed of an enhanced convolutional 
neural network
 (CNN) that considers bug-fixing recency and frequency, together with word-embedding and feature-detecting techniques. DeepLoc uses word embeddings to represent the words in 
bug reports
 and source files that retain their semantic information, and different CNNs to detect features from them. DeepLoc is evaluated on over 18,500 bug reports extracted from AspectJ, Eclipse, JDT, SWT, and Tomcat projects.
Results:
 The experimental results show that DeepLoc achieves 10.87%–13.4% higher MAP (mean average precision) than conventional CNN. DeepLoc outperforms four current state-of-the-art approaches (DeepLocator, HyLoc, LR+WE, and BugLocator) in terms of Accuracy@k (the percentage of bug reports for which at least one real buggy file is located within the top 
k
 rank), MAP, and MRR (mean reciprocal rank) using less 
computation time
.
Conclusion:
 DeepLoc is capable of automatically connecting bug reports to the corresponding buggy files and achieves better performance than four state-of-the-art approaches based on a 
deep understanding
 of semantics in bug reports and 
source code
.",Information and Software Technology,04 Mar 2025,9,"DeepLoc presents a novel deep learning-based model for bug localization that outperforms current state-of-the-art approaches, offering significant practical value for early-stage ventures in software quality assurance by improving bug fixing efficiency and productivity."
