link,title,published_year,keywords,author_email,abstract,publication_title,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S235267342400057X,Not all leavers are equal: How rank and destination influence enforcement of restrictive covenants,June 2025,"Spinout ventures, Employee mobility, Employee spinouts, Employee entrepreneurship, Restrictive covenants, Non-competes",Sepideh=Yeganegi: syeganegi@wlu.ca; André O.=Laplume: alaplume@torontomu.ca; Bradley=Bernard: bradley.bernard@torontomu.ca,"Abstract
Restrictive covenants like non-competes, non-solicitations, and non-disclosures may pose barriers to spinout ventures and mobility to competitors. However, we know little about the enforceability of these agreements despite their widespread use and associated chilling effects. Examining 332 Canadian court decisions, we find a higher rate of enforcement in cases involving high rank leavers (i.e., managers and owners) versus low rank leavers (regular employees and contractors) especially those who form spinout ventures. Our key insight is that enforcement rates differ significantly across different types of leavers. Low rank leavers and their previous employers may overestimate the potential for enforcement, creating chilling effects (i.e., where employees think they are more restricted by their employment agreements than they really are) that can deter employee mobility and entrepreneurship.",Business Venturing Insights,04 Mar 2025,8,"This study provides valuable insights into the enforceability of restrictive covenants, which can have a significant impact on employee mobility and entrepreneurship in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673423000537,CEO's industry experience and emerging market SME performance: The effects of corruption and political uncertainty,November 2023,Not Found,Juan Carlos=Morales-Solis: jcmorales@wtamu.edu; Vincent L.=Barker III: vbarker3@ku.edu; Arkangel M.=Cordero: arkangel.cordero@utsa.edu,"Abstract
We examine how Chief Executive Officer (CEO) industry-specific experience influences firm performance in small and medium 
enterprises
 (SMEs) in emerging markets. Drawing on the upper echelons perspective and learning theory, we propose an inverted U-shaped relationship between an SME CEO's industry-specific experience and firm performance. We also argue that country 
corruption
 and political instability moderate this relationship, resulting in lower performance for SME CEOs with little 
industry
 experience or many years of 
industry
 experience in countries with high 
corruption
 or political instability. We test our hypotheses using data from the World Bank's Enterprise Survey of firms in emerging economies from 2006 to 2019. The results support our hypotheses that corruption and political instability primarily hurt the performance of SMEs with CEOs having very long industry experience. We discuss implications of this research for scholars studying SMEs in lesser-developed institutional environments and how leaders may influence SME performance.",Business Venturing Insights,04 Mar 2025,6,"The study examines the impact of CEO industry-specific experience on SME performance, which can be relevant for startups. However, the focus on emerging markets may limit its direct applicability to European ventures."
https://www.sciencedirect.com/science/article/pii/S235267342200035X,The values work of restorative ventures: The role of founders’ embodied embeddedness with at-risk social groups,November 2022,"Restorative entrepreneuring, Values, Work, Stakeholders, Qualitative, Homelessness",Mohamed Hassan=Awad: mawad5@calstatela.edu; Mabel=Sanchez: msanch173@calstatela.edu; Matthew A.=Abikenari: matthewabi@g.ucla.edu,"Abstract
In line with the recent turn to pro-social ventures, restorative entrepreneuring is a promising approach for delivering social impact to marginalized and at-risk social groups with its focus on establishing ventures to rehabilitate and integrate individuals back into the community. However, the restorative project requires entrepreneurs to engage broadly with diverse sets of stakeholders with divergent worldviews, ideologies, and interests, many of which were the causes of 
marginalization
 and stigma for the at-risk social groups the entrepreneurs seek to serve. In this 
case study
, we focus on the role of values as a critical arena where entrepreneurs navigate 
stakeholders engagement
. We analyze Occupy Medical, a restorative venture in Oregon, United States, which provides healthcare services to the unhoused. We analyze how the founders captured and enacted values to establish and sustain the venture in a resource-constrained and often hostile environment. We identify a unique form of values work, embodied 
embeddedness
, as central to these efforts. Our study unpacks the role of values in restorative entrepreneuring as a tool for mitigating social exclusion of at-risk groups, enabling community reclamation of the social problem, and maneuvering local pushback and stigma.",Business Venturing Insights,04 Mar 2025,7,The research on restorative entrepreneuring and stakeholder engagement provides a valuable perspective on social impact ventures. Understanding how values influence venture establishment and sustainability can be beneficial for early-stage startups.
https://www.sciencedirect.com/science/article/pii/S2352673420300433,Staying alive during an unfolding crisis: How SMEs ward off impending disaster,November 2020,"Shock, Crisis, Entrepreneurship, Coronavirus, COVID-19, Outcome, Actions",Sara=Thorgren: sara.thorgren@ltu.se; Trenton Alma=Williams: trenwill@iu.edu,"Abstract
What measures are SMEs most likely to take in order to make ends meet in the face of a “black swan” external shock? That is the question we explore in this study, drawing upon unique data from 456 SMEs 
in the midst
 of an unfolding crisis. Our findings demonstrate how SMEs acted immediately by deferring investments, reducing 
labor costs
, reducing expenses, and negotiating contracts and terms. Moreover, the data highlight how SMEs in an unfolding crisis are reluctant to commit to any action that will increase their debt-to-equity ratio. The findings suggest new questions to be explored in relation to actions during an unfolding crisis, post-crisis businesses, entrepreneurial failure, and entrepreneur/entrepreneurial team characteristics. Implications for policy and practice are provided.",Business Venturing Insights,04 Mar 2025,5,"The study on SMEs' actions during an unfolding crisis offers practical insights for startups facing unpredictable external shocks. However, the focus on crisis management may limit its relevance to general early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673420300640,Disinhibition predicts both psychopathy and entrepreneurial intentions,November 2020,Not Found,Benjamin R.=Walker: ben.walker1@monash.edu; Chris J.=Jackson: c.jackson@unsw.edu.au; Genevieve=Sovereign: genevieve.sovereign@alumni.utoronto.ca,"Abstract
Most research has suggested that 
disinhibition
, defined as persistence despite negative feedback, generally leads to dysfunctional outcomes. However, some traits related to 
disinhibition
 such as sensation seeking, impulsivity, and risk-taking are also associated with functional outcomes. This study examined 157 full-time workers to determine whether disinhibition positively predicted 
psychopathy
 and entrepreneurial intentions, using an adapted Balloon Analogue Risk Task (BART) as a measure of disinhibition. This approach was then replicated in a sample of 143 university staff and students. Across both samples, disinhibition was found to predict both subclinical 
psychopathy
 and entrepreneurial intentions. These results suggest disinhibition can be a driver that potentially leads to entrepreneurial action or antisocial outcomes.",Business Venturing Insights,04 Mar 2025,4,"Exploring the relationship between disinhibition, psychopathy, and entrepreneurial intentions offers an interesting perspective. However, the focus on subclinical psychopathy and dysfunctional outcomes may limit the direct applicability to European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001484,Automated description generation for software patches,January 2025,"Software patch, Patch description, Neural machine translation, Code-to-text",Thanh Trong=Vu: thanhvu@vnu.edu.vn; Tuan-Dung=Bui: 21020006@vnu.edu.vn; Thanh-Dat=Do: 20020045@vnu.edu.vn; Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn; Son=Nguyen: sonnguyen@vnu.edu.vn,"Abstract
Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers. However, manual description creation poses challenges in terms of time consumption and variations in quality and detail. In this paper, we propose 
PatchExplainer
, an approach that addresses these challenges by framing patch description generation as a machine translation task. In 
PatchExplainer
, we leverage explicit representations of critical elements, historical context, and syntactic conventions. Moreover, the translation model in 
PatchExplainer
 is designed with an awareness of description similarity. Particularly, the model is 
explicitly
 trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches. The dual objectives maximize similarity and accurately predict affiliating groups. Our experimental results on a large dataset of real-world software patches show that 
PatchExplainer
 consistently outperforms existing methods, with improvements up to 189% in 
BLEU
, 5.7X in 
Exact Match
 rate, and 154% in 
Semantic Similarity
, affirming its effectiveness in generating software patch descriptions.",Information and Software Technology,04 Mar 2025,8,"The proposed approach, PatchExplainer, offers a significant advancement in generating software patch descriptions, leading to improvements in accuracy and consistency, which can benefit early-stage ventures by enhancing their software development process."
https://www.sciencedirect.com/science/article/pii/S0950584924001307,The quantum frontier of software engineering: A systematic mapping study,November 2024,"Quantum computing, Quantum software engineering, Software engineering for quantum programming, Empirical software engineering, Systematic mapping study",Manuel=De Stefano: madestefano@unisa.it; Fabiano=Pecorelli: fpecorelli@unisa.it; Dario=Di Nucci: ddinucci@unisa.it; Fabio=Palomba: fpalomba@unisa.it; Andrea=De Lucia: adelucia@unisa.it,"Abstract
Context:
Quantum computing is becoming a reality, and quantum software engineering (QSE) is emerging as a new discipline to enable developers to design and develop quantum programs.
Objective:
This paper presents a systematic mapping study of the current state of QSE research, aiming to identify the most investigated topics, the types and number of studies, the main reported results, and the most studied quantum computing tools/frameworks. Additionally, the study aims to explore the research community’s interest in QSE, how it has evolved, and any prior contributions to the discipline before its formal introduction through the Talavera Manifesto.
Method:
We searched for relevant articles in several databases and applied inclusion and exclusion criteria to select the most relevant studies. After evaluating the quality of the selected resources, we extracted relevant data from the primary studies and analyzed them.
Results:
We found that QSE research has primarily focused on software testing, with little attention given to other topics, such as software engineering management. The most commonly studied technology for techniques and tools is Qiskit, although, in most studies, either multiple or none specific technologies were employed. The researchers most interested in QSE are interconnected through direct collaborations, and several strong collaboration clusters have been identified. Most articles in QSE have been published in non-thematic venues, with a preference for conferences.
Conclusions:
The study’s implications are providing a centralized source of information for researchers and practitioners in the field, facilitating knowledge transfer, and contributing to the advancement and growth of QSE.",Information and Software Technology,04 Mar 2025,6,"The systematic mapping study on quantum software engineering provides valuable insights into the current state of research in this emerging field, potentially guiding startups in leveraging quantum computing tools and frameworks for innovative solutions."
https://www.sciencedirect.com/science/article/pii/S0950584924000946,The impact of human aspects on the interactions between software developers and end-users in software engineering: A systematic literature review,September 2024,"Systematic literature review, Human aspects, Software developers, Software users, Software engineering",Hashini=Gunatilake: hashini.gunatilake@monash.edu; John=Grundy: john.grundy@monash.edu; Rashina=Hoda: rashina.hoda@monash.edu; Ingo=Mueller: ingo.mueller@monash.edu,"Abstract
Context:
Research on human aspects within the field of 
software engineering
 (SE) has been steadily gaining prominence in recent years. These human aspects have a significant impact on SE due to the inherently interactive and 
collaborative nature
 of the discipline.
Objective:
In this paper, we present a systematic literature review (SLR) on human aspects affecting developer-user interactions. The objective of this SLR is to plot the current landscape of primary studies by examining the human aspects that influence developer-user interactions, their implications, interrelationships, and how existing studies address these implications.
Method:
We conducted this SLR following the guidelines proposed by Kitchenham et al. We performed a comprehensive search in six digital databases, and an exhaustive backward and forward snowballing process. We selected 46 primary studies for data extraction.
Results:
We identified various human aspects affecting developer-user interactions in SE, assessed their interrelationships, identified their positive impacts and 
mitigation strategies
 for negative effects. We present specific recommendations derived from the identified research gaps.
Conclusion:
Our findings suggest the importance of leveraging positive effects and addressing negative effects in developer-user interactions through the implementation of effective mitigation strategies. These insights may benefit software practitioners for effective user interactions, and the recommendations proposed by this SLR may aid the research community in further human aspects related studies.",Information and Software Technology,04 Mar 2025,7,"The systematic literature review on human aspects affecting developer-user interactions presents recommendations to software practitioners for effective user interactions, which could be beneficial for startups aiming to improve user experience in their products."
https://www.sciencedirect.com/science/article/pii/S095058492400051X,Search-based approaches to optimizing software product line architectures: A systematic literature review,June 2024,Not Found,Sedigheh=Khoshnevis: Sedigheh.Khoshnevis@iau.ac.ir; Omid=Ardestani: Not Found,"Abstract
Context
Software product line architecture (PLA) plays an important role in developing software product lines (SPLs) and other configurable systems. Search-based (SB) approaches can optimize the design of PLAs according to a given set of metrics as fitness functions. Although this area has been explored by researchers, there is a lack of synthesis of search-based PLA (SBPLA) research. A comprehensive review would offer valuable insights into previous contributions and identify areas for further research.
Objective
The objective of this work is to identify and summarize quality-assessed peer-reviewed studies on search-based PLA design from the aspects of the research scope, problems, contributions, evaluation, and open issues.
Methods
We conducted a systematic literature review based on Kitchenham's methodology. Based on a predefined search protocol we identified related studies limited to the ones published between 2000 and 2022 in journals and conference proceedings.
Results
Out of 686 initial search results, 34 papers were finally selected after a set of deep search, and criteria application activities. We provided a taxonomy of optimization problems in SBPLA and found that PLA remodularization and refactoring were the two categories most emphasized by the researchers. We also provided several other categorizations regarding contributions, research design, open issues, and other subjects of interest.
Conclusions
The interest in SBPLA design has been growing since 2014. PLA cloning and re-engineering problems have never been addressed in the literature. Performing subjective evaluation with the participation of experts from the industry will be profitable, as a complementary method to objective experimental evaluation, and therefore carrying out quanti-qualitative research.",Information and Software Technology,04 Mar 2025,5,"The work on search-based PLA design offers insights into optimizing software product line architecture, which may not have immediate practical implications for early-stage ventures but could be valuable for startups focusing on scalable software solutions."
https://www.sciencedirect.com/science/article/pii/S0950584924000296,"Stakeholders collaborations, challenges and emerging concepts in digital twin ecosystems",May 2024,"Digital twin, Digital twin ecosystem, Stakeholders, Systematic literature review, Empirical study, Definition, Software development",Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Heidi=Hietala: Not Found; Yueqiang=Xu: Not Found; Reshani=Liyanage: Not Found,"Abstract
Context:
Digital twin
 (DT) ecosystems are rapidly evolving, connecting many stakeholders, such as manufacturers, customers, and application platform providers. These ecosystems require collaboration and interaction between diverse actors to create value. This study delves into the collaboration of such stakeholders within DT-focused ecosystems.
Objective:
This research aims to understand stakeholder collaboration within DT ecosystems, identify potential challenges, and provide insights for managing these stakeholders. It also seeks to define the DT ecosystem and its implications for both research and practice.
Method:
A systematic literature review was conducted, supplemented by empirical evidence gathered from interviews with DT experts who were knowledgeable about the DT ecosystem. The study also analyzed DT systems, stakeholder roles, and the challenges with ecosystem-focused DT development.
Results:
The study identified various stakeholders and their roles in adding value to a DT ecosystem. It highlighted the benefits of stakeholder collaboration, such as knowledge gain during DT system development. The research also revealed the technical and non-technical challenges encountered in ecosystem-focused DTs, emphasizing the importance of standardization as a solution. A new definition of the DT ecosystem was proposed, emphasizing its data-driven nature, interconnected DTs, stakeholder value creation, and technology enablement.
Conclusion:
Stakeholder collaboration is pivotal in DT ecosystems, with each actor playing a distinct role. Addressing challenges, especially through standardization (OPC UA and ISO 23247), can lead to more efficient and coherent DT ecosystems. The insights provided by this study can guide industries in designing, developing, and maintaining their DT ecosystems, ensuring value creation and stakeholder satisfaction. Future research avenues that emphasize the importance of understanding the challenges involved and deploy appropriate solutions were suggested.",Information and Software Technology,04 Mar 2025,6,"The research on stakeholder collaboration within digital twin ecosystems provides valuable insights for industries in managing diverse actors, potentially guiding startups in improving collaboration within their ecosystems for value creation and stakeholder satisfaction."
https://www.sciencedirect.com/science/article/pii/S0950584923001933,A systematic mapping study of bug reproduction and localization,January 2024,"Bug reproduction, Bug localization, Bug fixing, Mapping study",Di=Wang: di.wang@pg.canterbury.ac.nz; Matthias=Galster: matthias.galster@canterbury.ac.nz; Miguel=Morales-Trujillo: miguel.morales@canterbury.ac.nz,"Abstract
Context:
Identifying the root cause of a software bug and fixing it is challenging. One reason for this is that many bugs are not reproducible during bug fixing.
Objective:
We aim to provide an overview of existing works on bug reproduction and localization. We ask four research questions: RQ1: What types of problems have been studied in the area of bug reproduction and localization? RQ2: How are problems studied in previous research? RQ3: What are the main findings and outcomes of previous studies? RQ4: What are the gaps and challenges identified in previous studies?
Method:
We conducted a 
systematic mapping study
 analyzing research literature published between 2011 and 2021. The search for primary studies involved four major computer science digital libraries and resulted in 134 studies for analysis.
Results:
Regarding RQ1 we found that many studies focus on information retrieval-based approaches to support bug reproduction and localization. Regarding RQ2 we found that 
bug reports
 and 
source code
 are the typical data sources of bug reproduction and localization. Also, most studies include experiments with historical data but do not investigate ongoing projects. Regarding RQ3 we found that many studies adapt or combine existing approaches for bug reproduction and localization to improve their accuracy or applicability (e.g., combine requirements-related information and bug reports to increase information-retrieval-based techniques). Regarding RQ4 we found that existing solutions for bug reproduction and localization have rarely been integrated into the workflow of developers.
Conclusion:
Although bug reproduction and localization have been studied in quite some detail, new challenges and gaps emerge due to the evolution of software technologies and practices and the practical needs of software developers. For example, bug reproduction approaches for 
traditional web applications
 do not work well with modern “Single Page Web Applications” (SPA) and related technologies, e.g., Angular or React.",Information and Software Technology,04 Mar 2025,6,The study on bug reproduction and localization provides insights into existing challenges but does not offer immediate practical solutions for startups in the European market.
https://www.sciencedirect.com/science/article/pii/S0950584923001635,Auto-COP: Adaptation generation in Context-oriented Programming using Reinforcement Learning options,December 2023,"Context-oriented programming, Reinforcement learning, Macro actions, Option learning, Self-adaptive systems",Nicolás=Cardozo: n.cardozo@uniandes.edu.co; Ivana=Dusparic: ivana.dusparic@scss.tcd.ie,"Abstract
Context:
Self-adaptive software systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The Context-oriented Programming (COP) paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized programming language constructs. In COP, adaptations are specified as independent modules that are composed in and out of the base system as contexts are activated and deactivated in response to sensed circumstances from the surrounding environment. However, the definition of adaptations, their contexts and associated specialized behavior, need to be specified at design time. In complex cyber–physical systems this is intractable, if not impossible, due to new unpredicted operating conditions arising.
Objective:
In this paper, we propose Auto-COP, a new technique to enable generation of adaptations at run time. Auto-COP uses 
Reinforcement Learning
 (RL) options to build action sequences, based on the previous instances of the system execution (for example, atomic system actions enacted by human operators). Options are further explored in interaction with the environment, and the most suitable options for each context are used to generate the adaptations, exploiting COP abstractions.
Method:
To validate Auto-COP, we present two 
case studies
 exhibiting different system characteristics and application domains: a driving assistant and a robot delivery system. We present examples of Auto-COP to illustrate the types of circumstances (contexts) requiring adaptation at run time, and the corresponding generated adaptations for each context.
Results:
We confirm that the generated adaptations exhibit correct 
system behavior
 measured by domain-specific performance metrics (
e.g.,
 conformance to specified speed limit), while reducing the number of required execution/actuation steps by a factor of two showing that the adaptations are regularly selected by the running system as adaptive behavior is more appropriate than the execution of 
atomic actions
.
Conclusion:
Therefore, we demonstrate that Auto-COP is able to increase system adaptivity by enabling run-time generation of new adaptations for conditions detected at run time, while retaining the modularity offered by COP languages, and reducing the upfront specification required by system developers.",Information and Software Technology,04 Mar 2025,8,"The Auto-COP technique demonstrates an innovative approach to system adaptivity with real-world case studies, showing potential value for early-stage ventures in improving system performance and reducing development time."
https://www.sciencedirect.com/science/article/pii/S0950584922002142,An empirical study on bugs in JavaScript engines,March 2023,Not Found,Ziyuan=Wang: wangziyuan@njupt.edu.cn; Dexin=Bu: Not Found; Nannan=Wang: Not Found; Sijie=Yu: Not Found; Shanyi=Gou: Not Found; Aiyue=Sun: Not Found,"Abstract
Context:
JavaScript is a prototype-based dynamic type 
scripting language
. The correct running of a JavaScript program depends on the correctness of both the program and the JavaScript engine.
Objective:
An in-depth understanding of the characteristics of bugs in JavaScript engines can help detect and fix them.
Methods:
We conduct an empirical study on the bugs in three mainstream JavaScript engines: V8, SpiderMonkey, and Chakra. Such an empirical study involves 19,019 
bug reports
, 16,437 revisions, 805 test cases, and root causes of randomly selected 540 bugs.
Results:
(1) The Compiler and the 
DOM
 are the most buggy component in V8 and SpiderMonkey, respectively. Most of the source files contain only one bug. (2) The scales of the testing programs that reveal bugs are usually small. Most bug fixes involve only limited modifications since the number of modified source files and lines of code modified are small. (3) Most bugs can be fixed within half a year (80.33% for V8 and 91.9% for SpiderMonkey). Only 4.33% of SpiderMonkey bugs need more than a year to fix. Bugs in SpiderMonkey are usually fixed faster than bugs in V8. (4) High priority tends to be assigned to Infrastructure bugs in V8 and Release Automation bugs in SpiderMonkey. The duration of bugs is not strictly correlated with their priorities. (5) Semantic bugs are the most common root causes of bugs. And among semantic bugs, the processing bugs, missing features bugs and function call bugs are more than others.
Conclusion:
This study deepens our understanding of bugs in JavaScript engines, and empirical results could indicate some potential problems during the detecting and fixing of bugs in JavaScript engines, assist developers of JavaScript engines in improving their development quality, assist maintainers in detecting and fixing bugs more effectively, and suggest users of JavaScript evade potential risks.",Information and Software Technology,04 Mar 2025,7,"The empirical study on bugs in JavaScript engines offers valuable insights for developers, but the practical impact on European early-stage ventures may be limited as it focuses on engine development rather than startup needs."
https://www.sciencedirect.com/science/article/pii/S0950584922001896,Detecting false-passing products and mitigating their impact on variability fault localization in software product lines,January 2023,Not Found,Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Kien-Tuan=Ngo: tuanngokien@vnu.edu.vn; Son=Nguyen: sonnguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn,"Abstract
In a Software Product Line (SPL) system, variability bugs can cause failures in certain products (buggy products), not in the others. In practice, variability bugs are not always exposed, and buggy products can still pass all the tests due to their ineffective test suites (so-called 
false-passing
 products). The misleading indications caused by those 
false-passing
 products’ test results can negatively impact variability fault 
localization performance
. In this paper, we introduce 
Clap
, a novel approach to detect 
false-passing
 products in SPL systems failed by variability bugs. Our key idea is that given a set of tested products of an SPL system, we collect failure indications in failing products based on their implementation and test quality. For a passing product, we evaluate these indications, and the stronger indications, the more likely the product is 
false-passing
. Specifically, the possibility of the product to be false-passing is evaluated based on if it has a large number of the statements which are highly suspicious in the failing products, and if its test suite is in lower quality compared to the failing products’ test suites. We conducted several experiments to evaluate our 
false-passing
 product detection approach on a large benchmark of 14,191 
false-passing
 products and 22,555 
true-passing
 products in 823 buggy versions of the existing SPL systems. The experimental results show that 
Clap
 can effectively detect 
false-passing
 and 
true-passing
 products with the average accuracy of more than 90%. Especially, the precision of 
false-passing
 product detection by 
Clap
 is up to 96%. This means, among 10 products predicted as 
false-passing
 products, more than 9 products are precisely detected. Furthermore, we propose two simple and effective methods to mitigate the 
negative impact
 of 
false-passing
 products on variability 
fault localization
. These methods can improve the performance of the state-of-the-art variability 
fault localization
 techniques by up to 34%.",Information and Software Technology,04 Mar 2025,9,"The Clap approach addresses a critical issue in Software Product Line systems with high accuracy levels, providing tangible benefits for startups by improving fault detection and localization performance."
https://www.sciencedirect.com/science/article/pii/S0950584922001732,CodeCity: A comparison of on-screen and virtual reality,January 2023,"CodeCity, City metaphor, Software visualization, Software evolution, Reverse engineering, Virtual reality, Web, 3D",David=Moreno-Lumbreras: d.morenolu@alumnos.urjc.es; Roberto=Minelli: Not Found; Andrea=Villaverde: Not Found; Jesus M.=Gonzalez-Barahona: Not Found; Michele=Lanza: Not Found,"Abstract
Context:
Over the past decades, researchers proposed numerous approaches to visualize source code. A popular one is 
CodeCity
, an interactive 3D software visualization representing software system as cities: buildings represent classes (or files) and districts represent packages (or folders). Building dimensions represent values of software metrics, such as number of methods or lines of code. There are many implementations of 
CodeCity
, the vast majority of them running on-screen. Recently, some implementations using virtual reality (VR) have appeared, but the usefulness of 
CodeCity
 in VR is still to be proven.
Aim:
Our comparative study aims to answer the question 
“Is VR well suited for 
CodeCity
, compared to the traditional on-screen implementation?”
Methods:
We performed two experiments with our web-based implementation of 
CodeCity
, which can be used on-screen or in 
immersive VR
. First, we conducted a controlled experiment involving 24 participants from academia and industry. Taking advantage of the obtained feedback, we improved our approach and conducted a second controlled experiment with 26 new participants.
Results:
Our results show that people using the VR version performed the assigned tasks in much less time, while maintaining a comparable level of correctness.
Conclusion:
VR is at least equally well-suited as on-screen for visualizing 
CodeCity
, and likely better.",Information and Software Technology,04 Mar 2025,8,"The comparative study on CodeCity visualization in VR versus traditional on-screen implementations highlights potential advantages of VR, offering valuable insights for startups looking to enhance their software visualization capabilities."
https://www.sciencedirect.com/science/article/pii/S0950584922001045,"Like, dislike, or just do it? How developers approach software development tasks",October 2022,Not Found,Zainab=Masood: zmas690@aucklanduni.ac.nz; Rashina=Hoda: Not Found; Kelly=Blincoe: Not Found; Daniela=Damian: Not Found,"Abstract
Context:
Software developers work on various tasks and activities that contribute towards creating and maintaining 
software applications
, frameworks, or other software components. These include technical (e.g., writing code and fixing bugs) and non-technical activities (e.g., communicating within or outside teams to understand, clarify, and resolve issues) as part of their day-to-day responsibilities. Interestingly, there is an aspect of desirability associated with these tasks and activities.
Objective:
However, not all of these tasks are desirable to developers, and yet they still need to be done. This study explores desirability and undesirability of developers for software development tasks.
Method:
Based on semi-structured interviews from 32 software developers and applying a grounded theory research approach, the study investigates what tasks are desirable and undesirable for developers, what makes tasks desirable and undesirable for them, what are the perceived consequences of working on these tasks, and how do they deal with such tasks.
Results:
We identified a set of underlying factors that make tasks (un)desirable for developers, categorised as personal, social, organisational, technical, and operational factors. We also found that working on desirable tasks has positive consequences while working on undesirable tasks has negative consequences. We reported different standard, assisted, and 
mitigation strategies
 that aid software practitioners manage developers’ likes and dislikes.
Conclusion:
Understanding these likes and dislikes, contributing factors, and strategies can help the managers and teams ensure balanced work distribution, developers’ happiness, and productivity, ultimately increasing the value developers add to software products.",Information and Software Technology,04 Mar 2025,6,"The study explores desirability and undesirability of software development tasks which can help in ensuring balanced work distribution and developers' happiness, contributing to the productivity of European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000866,A search-based framework for automatic generation of testing environments for cyber–physical systems,September 2022,Not Found,Dmytro=Humeniuk: dmytro.humeniuk@polymtl.ca; Foutse=Khomh: Not Found; Giuliano=Antoniol: Not Found,"Abstract
Background:
Many modern cyber–physical systems incorporate 
computer vision technologies
, complex sensors and advanced control software, allowing them to interact with the environment autonomously. Examples include drone swarms, self-driving vehicles, autonomous robots, etc. Testing such systems poses numerous challenges: not only should the system inputs be varied, but also the surrounding environment should be accounted for. A number of tools have been developed to test the system model for the possible inputs falsifying its requirements. However, they are not directly applicable to autonomous cyber–physical systems, as the inputs to their models are generated while operating in a virtual environment.
Aims:
In this paper, we aim to design a search-based framework, named AmbieGen, for generating diverse fault-revealing test scenarios for autonomous cyber–physical systems. The scenarios represent an environment in which an 
autonomous agent
 operates. The framework should be applicable to generating different types of environments.
Methods:
To generate the test scenarios, we leverage the NSGA-II algorithm with two objectives. The first objective evaluates the deviation of the observed system’s behaviour from its expected behaviour. The second objective is the test case diversity, calculated as a Jaccard distance with a reference test case. To guide the first objective we are using a simplified system model rather than the full model. The full model is used to run the system in the simulation environment and can take substantial time to execute (several minutes for one scenario). The simplified system model is derived from the full model and can be used to get an 
approximation
 of the results obtained from the full model without running the simulation.
Results:
We evaluate AmbieGen on three scenario generation 
case studies
, namely a smart-thermostat, a robot 
obstacle avoidance
 system, and a vehicle lane-keeping assist system. For all the 
case studies
, our approach outperforms the available baselines in fault revealing and several other metrics such as the diversity of the revealed faults and the proportion of valid test scenarios.
Conclusion:
AmbieGen could find scenarios, revealing failures for all the three autonomous agents considered in our 
case studies
. We compared three configurations of AmbieGen: based on a single objective genetic algorithm, multi-objective, and random search. Both single and multi objective configurations outperform the random search. Multi objective configuration can find the individuals of the same quality as the single objective, producing more unique test scenarios in the same time budget. Our framework can be used to generate virtual environments of different types and complexity and reveal the system’s faults early in the design stage.",Information and Software Technology,04 Mar 2025,9,"The framework AmbieGen for generating fault-revealing test scenarios for autonomous cyber-physical systems outperforms baselines in fault revealing and diversity metrics, which can have a significant impact on the efficiency and effectiveness of European early-stage ventures utilizing such systems."
https://www.sciencedirect.com/science/article/pii/S0950584922000623,A Systematic Literature Review on prioritizing software test cases using Markov chains,July 2022,Not Found,Gerson=Barbosa: gerson.barbosa@unesp.br; Érica Ferreira=de Souza: ericasouza@utfpr.edu.br; Luciana Brasil Rebelo=dos Santos: lurebelo@ifsp.edu.br; Marlon=da Silva: marlon.silva@ifsp.edu.br; Juliana Marino=Balera: juliana.balera@inpe.br; Nandamudi Lankalapalli=Vijaykumar: vijay.nl@inpe.br,"Abstract
Context:
Software Testing is a costly activity since the size of the test case set tends to increase as the construction of the software evolves. Test Case Prioritization (TCP) can reduce the effort and cost of software testing. TCP is an activity where a subset of the existing test cases is selected in order to maximize the possibility of finding defects. On the other hand, 
Markov Chains
 representing a reactive system, when solved, can present the occupation time of each of their states. The idea is to use such information and associate priority to those test cases that consist of states with the highest probabilities.
Objective:
The objective of this paper is to conduct a survey to identify and understand key initiatives for using 
Markov Chains
 in TCP. Aspects such as approaches, developed techniques, programming languages, analytical and simulation results, and validation tests are investigated.
Methods:
A Systematic Literature Review (SLR) was conducted considering studies published up to July 2021 from five different databases to answer the three research questions.
Results:
From SLR, we identified 480 studies addressing 
Markov Chains
 in TCP that have been reviewed in order to extract relevant information on a set of research questions.
Conclusion:
The final 12 studies analyzed use 
Markov Chains
 at some stage of test case prioritization in a distinct way, that is, we found that there is no strong relationship between any of the studies, not only on how the technique was used but also in the context of the application. Concerning the fields of application of this subject, 6 forms of approach were found: Controlled Markov Chain, Usage Model, Model-Based Test, 
Regression Test
, Statistical Test, and 
Random Test
. This demonstrates the versatility and robustness of the tool. A large part of the studies developed some prioritization tool, being its validation done in some cases analytically and in others numerically, such as: Measure of the software specification, Optimal Test Transition Probabilities, Adaptive Software Testing, Automatic Prioritization, 
Ant Colony Optimization
, Model Driven approach, and Monte Carlo Random Testing.",Information and Software Technology,04 Mar 2025,7,"The survey on using Markov Chains in Test Case Prioritization can aid in reducing the effort and cost of software testing, potentially benefiting European early-stage ventures by maximizing the possibility of finding defects in a cost-effective manner."
https://www.sciencedirect.com/science/article/pii/S0950584922000581,Quantum computing challenges in the software industry. A fuzzy AHP-based approach,July 2022,"Fuzzy analytic hierarchy process (F-AHP), Software process automation, Multiple-criteria decision-making (MCDM), Quantum software requirement, Quantum computing",Usama=Awan: usama.awan@lut.fi; Lea=Hannola: Not Found; Anushree=Tandon: Not Found; Raman Kumar=Goyal: Not Found; Amandeep=Dhir: Not Found,"Abstract
Context
The current technology revolution has posed unexpected challenges for the software 
industry
. In recent years, the field of 
quantum computing
 (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise 
software engineering
. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented.
Objective
The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy 
analytic hierarchy process
 (F-AHP).
Method
First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers.
Results
The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC.
Conclusion
By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges.",Information and Software Technology,04 Mar 2025,9,"The study on prioritizing challenges in adopting quantum computing technologies in the software industry can provide valuable insights for European early-stage ventures looking to leverage QC, potentially revolutionizing their software engineering processes."
https://www.sciencedirect.com/science/article/pii/S0950584922000052,Proactive hybrid learning and optimisation in self-adaptive systems: The swarm-fleet infrastructure scenario,May 2022,Not Found,Christian=Krupitzer: christian.krupitzer@uni-hohenheim.de; Christian=Gruhl: Not Found; Bernhard=Sick: Not Found; Sven=Tomforde: Not Found,"Abstract
Context:
Smart and adaptive Systems, such as self-adaptive and self-organising (SASO) systems, typically consist of a large set of highly autonomous and heterogeneous subsystems that are able to adapt their behaviour to the requirements of ever-changing, dynamic environments. Their successful operation is based on appropriate modelling of the internal and external conditions.
Objective:
The control problem for establishing a near-to-optimal coordinated behaviour of systems with multiple, potentially conflicting objectives is either approached in a distributed (i.e., fully autonomous by the autonomous subsystems) or in a centralised way (i.e. one instance controlling the optimisation and planning process). In the distributed approach, selfish behaviour and being limited to local knowledge may lead to sub-optimal 
system behaviour
, while the 
centralised approach
 ignores the 
autonomy
 and the coordination efforts of parts of the system.
Method:
In this article, we present a concept for a hybrid (i.e., integrating a central optimisation with a distributed decision-making process) 
system management
 that combines local 
reinforcement learning
 and self-awareness mechanisms of fully autonomous subsystems with external system-wide planning and optimisation of adaptation freedom that steers the behaviour dynamically by issuing plans and guidelines augmented with incentivisation schemes.
Results:
This work addresses the inherent uncertainty of the dynamic 
system behaviour
, the local autonomous and context-aware learning of subsystems, and proactive control based on adaptiveness. We provide the ‘swarm-fleet infrastructure’ – a self-organised taxi service established by autonomous, privately-owned cars – as a 
testbed
 for structured comparison of systems.
Conclusion:
The ‘swarm-fleet infrastructure’ supports the advantages of a proactive hybrid self-adaptive and self-organising system operation. Further, we provide a system model to combine the system-wide optimisation while ensuring local decision-making through 
reinforcement learning
 for individualised configurations.",Information and Software Technology,04 Mar 2025,8,"The concept of a hybrid system management for self-adaptive and self-organising systems can offer a structured approach to dynamic system behaviour, potentially enhancing the operational efficiency of European early-stage ventures utilizing such systems."
https://www.sciencedirect.com/science/article/pii/S0950584921002111,Review4Repair: Code review aided automatic program repairing,March 2022,Not Found,Faria=Huq: 1505052.fh@ugrad.cse.buet.ac.bd; Masum=Hasan: masum@ra.cse.buet.ac.bd; Md Mahim Anjum=Haque: mahim@vt.edu; Sazan=Mahbub: 1505020.sm@ugrad.cse.buet.ac.bd; Anindya=Iqbal: anindya@cse.buet.ac.bd; Toufique=Ahmed: tfahmed@ucdavis.edu,"Abstract
Context:
Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the 
source code
 of the software. These tools mostly exploit 
historical data
 of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers’ expertise and experience are reflected in these suggestions.
Objective:
The natural language instructions scripted on the review comments are enormous sources of information about the bug’s nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the 
performance improvement
 of repair techniques using code review comments.
Method:
We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques.
Results:
We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques.
Conclusion:
We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort.",Information and Software Technology,04 Mar 2025,8,"The study improves existing techniques significantly and addresses a practical challenge faced by developers, potentially saving time and effort in bug fixing."
https://www.sciencedirect.com/science/article/pii/S0950584921001816,An approach to explore sequential interactions in cognitive activities of software engineering,January 2022,Not Found,Joelma=Choma: jh.choma@hotmail.com; Eduardo M.=Guerra: guerraem@gmail.com; Tiago S.=da Silva: silvadasilva@unifesp.br; Luciana M.=Zaina: lzaina@ufscar.br,"Abstract
Context
: The study of cognitive aspects around software activities can provide valuable insights to improve 
software engineering
 practices. Objective: This paper presents an approach based on distributed cognition and sequential analysis to explore cognitive activities in the software development context by analyzing the interactions between software practitioners and the resources used to support them. Method: We conducted nine laboratory-based observation sessions to record qualitative audio/video data of interactions between the study participants and at-hand resources during the planning and managing of 
software analytics
 tasks. Results: The interaction strategies of the resources model included 21 emergent actions, and the sequential analysis revealed two different patterns of interaction over time. Conclusion: Our approach has been useful for evaluating how well an artifact works to support a team in 
software analytics
 activities. Furthermore, it can be applied to explore and discover interaction patterns in different software activities.",Information and Software Technology,04 Mar 2025,5,"While exploring cognitive aspects in software development is valuable, the impact on practical software ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001555,DeepBackground: Metamorphic testing for Deep-Learning-driven image recognition systems accompanied by Background-Relevance,December 2021,Not Found,Zhiyi=Zhang: Not Found; Pu=Wang: Not Found; Hongjing=Guo: Not Found; Ziyuan=Wang: wangziyuan@njupt.edu.cn; Yuqian=Zhou: Not Found; Zhiqiu=Huang: Not Found,"Abstract
Context:
Recently, advances in 
Deep Learning
 (DL) have promoted the development of DL-driven image recognition systems in various fields, such as medical treatment, face detection, etc., almost achieving the same level of performance as the human brain. Nevertheless, using DL-driven image recognition systems in these safety-critical domains requires ensuring the accuracy and the stability of these systems. Recent research in this direction mainly focuses on using the image transformations for the overall image to detect the inconsistency of image recognition systems. However, the influence of the image background region (
i
.
e
.
, the region of the image other than the target object) on the recognition result of the systems and the robustness evaluation of the systems are not considered.
Objective:
To evaluate the robustness of DL-driven image recognition systems about image background region changes, this paper introduces DeepBackground, a novel metamorphic 
testing method
 for DL-driven image recognition systems.
Method:
First, we define a new metric, termed Background-Relevance (BRC) to assess the influence degree of the image background region on the recognition result of the image recognition systems. DeepBackground defines a series of domain-specific metamorphic relations (MRs) combined with BRC and automatically generates many follow-up test images based on these MRs. Finally, DeepBackground detects the inconsistency of these systems and evaluates their robustness about image background changes according to BRC.
Results:
Our empirical validation on 3 commercial image recognition services and 6 popular 
convolutional neural networks
 (CNNs) models shows that DeepBackground can not only evaluate the robustness of these image recognition systems about image background changes according to BRC, but also can detect their inconsistent behaviors.
Conclusion:
DeepBackground is capable of automatically generating high-quality test input images to detect the inconsistency of the image recognition systems, and evaluating the robustness of these systems about image background changes according to BRC.",Information and Software Technology,04 Mar 2025,9,"The method introduced, DeepBackground, addresses a critical issue in DL-driven image recognition systems and provides a practical solution for improving robustness."
https://www.sciencedirect.com/science/article/pii/S0950584921000689,Joint feature representation learning and progressive distribution matching for cross-project defect prediction,September 2021,Not Found,Quanyi=Zou: zouquanyi2010@163.com; Lu=Lu: lul@scut.edu.cn; Zhanyu=Yang: yangzhanyu@hotmail.com; Xiaowei=Gu: amyxwgu@163.com; Shaojian=Qiu: qiushaojian@outlook.com,"Abstract
Context:
Cross-Project 
Defect Prediction
 (CPDP) aims to leverage the knowledge from label-rich source software projects to promote tasks in a label-poor target software project. Existing CPDP methods have two major flaws. One is that previous CPDP methods only consider global feature representation and ignores local relationship between instances in the same category from different projects, resulting in ambiguous predictions near the decision boundary. The other one is that CPDP methods based on pseudo-labels assume that the conditional distribution can be well matched at one stroke, when instances of target project are correctly annotated pseudo labels. However, due to the great gap between projects, the pseudo-labels seriously deviate from the real labels.
Objective:
To address above issues, this paper proposed a novel CPDP method named 
Joint
 Feature Representation with Double Marginalized 
Denoising
 
Autoencoders
 (DMDA_JFR).
Method:
Our method mainly includes two parts: joint feature 
representation learning
 and progressive distribution matching. We utilize two novel 
autoencoders
 to jointly learn the global and 
local feature
 representations simultaneously. To achieve progressive distribution matching, we introduce a repetitious pseudo-labels strategy, which makes it possible that distributions are matched after each stack layer learning rather than in one stroke.
Results:
The effectiveness of the proposed method was evaluated through experiments conducted on 10 open-source projects, including 29 software releases from PROMISE repository. Overall, experimental results show that our proposed method outperformed several state-of-the-art baseline CPDP methods.
Conclusions:
It can be concluded that (1) joint deep representations are promising for CPDP compared with only considering global feature representation methods, (2) progressive distribution matching is more effective for adapting probability distributions in CPDP compared with existing CPDP methods based on pseudo-labels.",Information and Software Technology,04 Mar 2025,7,"The proposed CPDP method shows improvement over existing techniques, but the impact on early-stage ventures may not be as direct."
https://www.sciencedirect.com/science/article/pii/S0950584921000240,Stakeholder engagement in enterprise architecture practice: What inhibitors are there?,June 2021,Not Found,Sherah=Kurnia: sherahk@unimelb.edu.au; Svyatoslav=Kotusev: kotusev@kotusev.com; Graeme=Shanks: gshanks@unimelb.edu.au; Rod=Dilnutt: rpd@unimelb.edu.au; Simon=Milton: simon.milton@unimelb.edu.au,"Abstract
Context
Enterprise
 architecture (EA) is a collection of artifacts describing various aspects of an organization from an integrated business and IT perspective. EA practice is an organizational activity that implies using EA artifacts for facilitating decision-making and improving business and IT alignment. EA practice involves numerous participants ranging from C-level executives to project teams and effective engagement between these stakeholders and architects is critically important for success. Moreover, many practical problems with EA practice can be also attributed to insufficient engagement between architects and other EA stakeholders. However, the notion of engagement received only limited attention in the EA literature and the problem of establishing engagement has not been intentionally studied.
Objective
This paper intends to explore in detail the problem of achieving effective engagement between architects and other EA stakeholders in an organization, identify the main inhibitors of engagement and present a theoretical model explaining the problem of establishing engagement in practice.
Method
This paper is based on a single in-depth revelatory 
case study
 including nine interviews with different participants of EA practice (e.g. architects and other EA stakeholders) and documentation analysis. It leverages the 
grounded theory method
 to construct a conceptual model explaining the problem of engagement in the studied organization.
Results
This paper identifies 28 direct and indirect inhibitors of engagement and unifies them into a holistic conceptual model addressing the problem of achieving engagement that covers the factors undermining both strategic and initiative-based engagement between architects and other EA stakeholders.
Conclusions
This paper focuses on the notion of engagement and offers arguably the first available theoretical model that explains how typical engagement problems between architects and other stakeholders inhibit the realization of value from EA practice. However, the developed model has a number of limitations and we call for further empirical research on engagement problems in EA practice and coping strategies for addressing these problems.",Information and Software Technology,04 Mar 2025,6,"The exploration of engagement issues in EA practice is important, but the practical impact on startups may be more indirect compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000409,Challenges and recommendations to publishing and using credible evidence in software engineering,June 2021,"Evidence-based software engineering, EBSE, Credible evidence, Validity, Relevance",Claes=Wohlin: claes.wohlin@bth.se; Austen=Rainer: Not Found,"Abstract
Context:
An evidence-based 
scientific discipline
 should produce, consume and disseminate credible evidence. Unfortunately, mistakes are sometimes made, resulting in the production, consumption and dissemination of invalid or otherwise questionable evidence. In the worst cases, such questionable evidence achieves the status of accepted knowledge. There is, therefore, the need to ensure that producers and consumers seek to identify and rectify such situations.
Objectives:
To raise awareness of the 
negative impact
 of misinterpreting evidence and of propagating that misinterpreted evidence, and to provide guidance on how to improve on the type of issues identified.
Methods:
We use a case-based approach to present and analyse the production, consumption and dissemination of evidence. The cases are based on the literature and our professional experience. These cases illustrate a range of challenges confronting evidence-based researchers as well as the consequences to research when invalid evidence is not corrected in a timely way.
Results:
We use the cases and the challenges to formulate a framework and a set of recommendations to help the community in producing and consuming credible evidence.
Conclusions:
We encourage the community to collectively remain alert to the emergence and dissemination of invalid, or otherwise questionable, evidence, and to proactively seek to identify and rectify it.",Information and Software Technology,04 Mar 2025,7,"This abstract addresses the importance of producing and consuming credible evidence, which can significantly impact early-stage ventures by guiding decision-making processes and avoiding potential pitfalls. The recommendations provided can help startups in conducting more reliable research."
https://www.sciencedirect.com/science/article/pii/S0950584921000239,Alignment and granularity of requirements and architecture in agile development: A functional perspective,May 2021,"Requirements engineering, Software architecture, Twin Peaks, Alignment, Granularity, Case study, Agile development",Tjerk=Spijkman: tjerk@fizor.io; Sabine=Molenaar: Not Found; Fabiano=Dalpiaz: Not Found; Sjaak=Brinkkemper: Not Found,"Abstract
Context:
Requirements engineering
 and software architecture are tightly linked disciplines. The Twin Peaks model suggests that requirements and architectural components should stay aligned while the system is designed and as the level of detail increases. Unfortunately, this is hardly the case in practical settings.
Objective:
We surmise that a reason for the absence of conjoint evolution is that existing models, such as the Twin Peaks, do not provide concrete guidance for practitioners. We propose the Requirements Engineering for Software Architecture (RE4SA) model to assist in analyzing the alignment and the 
granularity
 of functional requirements and architectural components.
Methods:
After detailing the RE4SA model in notation-independent terms, we propose a concrete instance, called RE4SA-Agile, that connects common artifacts in 
agile development
, such as user stories and features. We introduce metrics that measure the alignment between the requirements and architecture, and we define 
granularity
 smells to pinpoint situation in which the granularity of one high-level requirement or high-level component is not uniform with the norm. We show two applications of RE4SA-Agile, including the use of the metrics, to real-world 
case studies
.
Results:
Our applications of RE4SA-Agile, which were discussed with representatives from the development teams, prove to be able to pinpoint problematic situations regarding the relationship between functional requirements and architecture.
Conclusion:
RE4SA and its metrics can be seen as a first attempt to provide a concrete approach for supporting the application of the Twin Peaks model. We expect future research to apply our metrics to additional cases and to provide variants for RE4SA that support different concrete notations, and extend the perspective beyond the functional perspective of this research, similar to what we did with RE4SA-Agile in this paper.",Information and Software Technology,04 Mar 2025,8,"The RE4SA model proposed in this abstract offers concrete guidance for practitioners in aligning requirements and architectural components, which is crucial for the success of software development projects. Startups can benefit from this model to improve their development process and avoid costly mistakes."
https://www.sciencedirect.com/science/article/pii/S0950584920302251,Improving requirements specification use by transferring attention with eye tracking data,March 2021,Not Found,Maike=Ahrens: maike.ahrens@inf.uni-hannover.de; Kurt=Schneider: kurt.schneider@inf.uni-hannover.de,"Abstract
Context
Software requirements specifications are the main point of reference in traditional software projects. Especially in large projects, these documents get read by multiple people, multiple times. Several guidelines and templates already exist to support writing a 
good specification
. However, not much research has been done in investigating how to support the use of specifications and help readers to find relevant information and navigate in the document more efficiently.
Objective
We aim to ease the reading process of requirements specifications by making use of previously recorded attention data. Therefore, we created three different attention transfer features based on eye tracking data obtained from observing readers when using specifications.
Method
In a student experiment, we evaluated if these attention visualizations positively affect the roles software architect, UI-designer and tester when reading a specification for the first time.
Results
The results show that the attention visualizations did not decrease navigation effort, but helped to draw the readers’ attention towards highlighted parts and decreased the average time spent on pages. They were mostly perceived as valuable by the readers.
Conclusions
We explored and evaluated the approach of visualizing other readers’ 
attention focus
 to help support new readers. Our results include interesting findings on what works well, what does not and what could be enhanced. We present several suggestions on how attention data could be used to fasten document navigation, direct reading and facilitate user-specific reading.",Information and Software Technology,04 Mar 2025,6,"The approach of using attention data to ease the reading process of requirements specifications can help startup teams improve their understanding of project requirements and enhance communication. While not groundbreaking, this method can provide some practical value to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920301312,Crowdsourced software testing: A systematic literature review,November 2020,Not Found,Sultan=Alyahya: sualyahya@ksu.edu.sa,"Abstract
Context
Crowdsourced software testing (CST) refers to the use of 
crowdsourcing techniques
 in the domain of software testing. CST is an emerging area with its applications rapidly increasing in the last decade.
Objective
A comprehensive review on CST has been conducted to determine the current studies aiming to improve and assess the value of using CST as well as the challenges identified by these evaluation studies.
Method
We conducted a systematic literature review on CST by searching six popular databases. We identified 50 primary studies that passed our quality assessment criteria and defined two research questions covering the aim of the study.
Results
There are three main process activities that the current literature aims to improve, namely selection of suitable testers, reporting of defects, and validation of submitted defects. In addition, there are 23 CST evaluation studies and most of them involve a large group and real crowd. These studies have identified 27 different challenges encountered during the application of crowdsourcing in software testing.
Conclusions
The improvements achieved for the specific process activities in CST help explore other unexplored process activities. Similarly, knowing the characteristics of the evaluation studies can direct us on what other studies are worth investigating. Additionally, many of the challenges identified by the evaluation studies represent research problems that need better understanding and alternative solutions. This research also offers opportunities for practitioners to understand and apply new solutions proposed in the literature and the variations between them. Moreover, it provides awareness to the related parties regarding the challenges reported in the literature, which they may encounter during CST tasks.",Information and Software Technology,04 Mar 2025,9,The comprehensive review on crowdsourced software testing (CST) and the identified challenges can be highly beneficial for startups looking to leverage CST for software testing. Understanding the challenges and improvements in CST can help startups make informed decisions and optimize their testing processes.
https://www.sciencedirect.com/science/article/pii/S0950584920301300,From software architecture to analysis models and back: Model-driven refactoring aimed at availability improvement,November 2020,"Software architecture, Availability, Bidirectional model transformation, Refactoring",Vittorio=Cortellessa: vittorio.cortellessa@univaq.it; Romina=Eramo: romina.eramo@univaq.it; Michele=Tucci: michele.tucci@univaq.it,"Abstract
Context
With the ever-increasing evolution of software systems, their architecture is subject to frequent changes due to multiple reasons, such as new requirements. Appropriate architectural changes driven by non-functional requirements are particularly challenging to identify because they concern quantitative analyses that are usually carried out with specific languages and tools. A considerable number of approaches have been proposed in the last decades to derive non-functional analysis models from architectural ones. However, there is an evident lack of automation in the backward path that brings the analysis results back to the software architecture.
Objective
In this paper, we propose a model-driven approach to support designers in improving the availability of their software systems through refactoring actions.
Method
The proposed framework makes use of bidirectional model transformations to map UML models onto Generalized 
Stochastic Petri Nets
 (GSPN) analysis models and vice versa. In particular, after availability analysis, our approach enables the application of model refactoring, possibly based on well-known fault tolerance patterns, aimed at improving the availability of the 
architectural model
.
Results
We validated the effectiveness of our approach on an 
Environmental Control System
. Our results show that the approach can generate: (i) an analyzable availability model from a software architecture description, and (ii) valid software architecture models back from availability models. Finally, our results highlight that the application of fault tolerance patterns significantly improves the availability in each considered scenario.
Conclusion
The approach integrates bidirectional model transformation and fault 
tolerance techniques
 to support the availability-driven refactoring of architectural models. The results of our experiment showed the effectiveness of the approach in improving the software availability of the system.",Information and Software Technology,04 Mar 2025,8,The model-driven approach proposed in this abstract for supporting designers in improving software availability through refactoring actions can have a significant impact on startups by helping them enhance the reliability and performance of their software systems. This approach offers practical value for early-stage ventures in improving their software architecture.
https://www.sciencedirect.com/science/article/pii/S0950584920301361,PostFinder: Mining Stack Overflow posts to support software developers,November 2020,Not Found,Riccardo=Rubei: riccardo.rubei@univaq.it; Claudio=Di Sipio: claudio.disipio@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Juri=Di Rocco: juri.dirocco@univaq.it; Davide=Di Ruscio: davide.diruscio@univaq.it,"Abstract
Context –
 During the development of complex software systems, programmers look for external resources to understand better how to use specific APIs and to get advice related to their current tasks. Stack Overflow provides developers with a broader insight into API usage as well as useful code examples. Given the circumstances, tools and techniques for mining Stack Overflow are highly desirable. 
Objective –
 In this paper, we introduce PostFinder, an approach that analyzes the project under development to extract suitable context, and allows developers to retrieve messages from Stack Overflow being relevant to the API function calls that have already been invoked. 
Method –
 PostFinder augments posts with additional data to make them more exposed to queries. On the client side, it boosts the context code with various factors to construct a query containing information needed for matching against the stored indexes. Multiple facets of the data available are used to optimize the search process, with the ultimate aim of recommending highly relevant SO posts. 
Results –
 The approach has been validated utilizing a user study involving a group of 12 developers to evaluate 500 posts for 50 contexts. Experimental results indicate the suitability of PostFinder to recommend relevant Stack Overflow posts and concurrently show that the tool outperforms a well-established baseline. 
Conclusions –
 We conclude that PostFinder can be deployed to assist developers in selecting relevant Stack Overflow posts while they are programming as well as to replace the module for searching posts in a code-to-code search engine.",Information and Software Technology,04 Mar 2025,7,"The research introduces a practical tool, PostFinder, to assist developers in finding relevant Stack Overflow posts during programming, which can have a positive impact on early-stage ventures by providing efficient solutions and reducing development time."
https://www.sciencedirect.com/science/article/pii/S0950584920301373,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,November 2020,Not Found,Lucy Ellen=Lwakatare: llucy@chalmers.se; Aiswarya=Raj: Not Found; Ivica=Crnkovic: Not Found; Jan=Bosch: Not Found; Helena Holmström=Olsson: Not Found,"Abstract
Background
: Developing and maintaining large scale 
machine learning
 (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.
Objective
: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.
Method
: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four 
quality attributes
: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.
Results
: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.
Conclusion
: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in 
ML system
 development practice and the lack of solutions point to directions for future research.",Information and Software Technology,04 Mar 2025,5,"The study highlights challenges faced in developing ML-based systems in industrial settings, which can provide insights for startups working with ML technology, but may not offer immediate practical solutions for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920301166,Engineering human-in-the-loop interactions in cyber-physical systems,October 2020,Not Found,Miriam=Gil: mgil@pros.upv.es; Manoli=Albert: malbert@pros.upv.es; Joan=Fons: jjfons@pros.upv.es; Vicente=Pelechano: pele@pros.upv.es,"Abstract
Context:
 Cyber-Physical Systems (CPSs) are gradually and widely introducing autonomous capabilities into everything. However, human participation is required to accomplish tasks that are better performed with humans (often called human-in-the-loop). In this way, human-in-the-loop solutions have the potential to handle complex tasks in unstructured environments, by combining the cognitive skills of humans with 
autonomous systems
 behaviors.
Objective:
 The objective of this paper is to provide appropriate techniques and methods to help designers analyze and design human-in-the-loop solutions. These solutions require interactions that engage the human, provide natural and understandable collaboration, and avoid disturbing the human in order to improve human experience.
Method:
 We have analyzed several works that identified different requirements and critical factors that are relevant to the design of human-in-the-loop solutions. Based on these works, we have defined a set of design principles that are used to build our proposal. Fast-prototyping techniques have been applied to simulate the designed human-in-the-loop solutions and validate them.
Results:
 We have identified the technological challenges of designing human-in-the-loop CPSs and have provided a method that helps designers to identify and specify how the human and the system should work together, focusing on the 
control strategies
 and interactions required.
Conclusions:
 The use of our approach facilitates the design of human-in-the-loop solutions. Our method is practical at earlier stages of the 
software life cycle
 since it allows domain experts to focus on the problem and not on the solution.",Information and Software Technology,04 Mar 2025,8,"The paper offers techniques and methods to design human-in-the-loop solutions for CPSs, which can be valuable for startups dealing with autonomous systems and human interactions, potentially leading to innovative and efficient solutions."
https://www.sciencedirect.com/science/article/pii/S0950584920301117,Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems,October 2020,Not Found,Behzad=Soleimani Neysiani: Not Found; Seyed Morteza=Babamir: babamir@kashanu.ac.ir; Masayoshi=Aritsugi: Not Found,"Abstract
Context
There are many duplicate 
bug reports
 in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems.
Objective
The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between 
bug reports
 accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model.
Method
This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and 
inverse document frequency
 of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones.
Results
The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the 
Android
, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for 
accuracy
 and 
precision
 and more than 4.5% and 5.9% improvement for 
recall
 and 
F1-measure
, respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier.
Conclusion
Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based 
machine learning algorithms
 are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers.",Information and Software Technology,04 Mar 2025,6,"The study presents a feature extraction model to improve duplicate bug report detection, which can have practical implications for startups dealing with software triage systems, but may not bring groundbreaking innovations to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300719,Automated isolation for white-box test generation,September 2020,"Testing, Test generation, White-box, Isolation, Mocking, Code transformation, Empirical evaluation",Dávid=Honfi: honfi@mit.bme.hu; Zoltán=Micskei: micskeiz@mit.bme.hu,"Abstract
Context:
 White-box test generation is a technique used for automatically selecting test inputs using only the code under test. However, such techniques encounter challenges when applying them to complex programs. One of the challenges is handling invocations to external modules or dependencies in the code under test.
Objective:
 Without using proper isolation, like mocks, generated tests cannot cover all parts of the source code. Moreover, invoking 
external dependencies
 may cause unexpected side effects (e.g., accessing the file system or network). Our goal was to tackle this issue while maintaining the advantages of white-box test generation.
Method:
 In this paper, we present an automated approach addressing the external dependency challenge for white-box test generation. This technique isolates the test generation and execution by transforming the code under test and creating a parameterized sandbox with generated mocks. We implemented the approach in a ready-to-use tool using Microsoft Pex as a test generator, and evaluated it on 10 open-source projects from GitHub having more than 38.000 lines of code in total.
Results:
 The results from the evaluation indicate that if the lack of isolation hinders white-box test generation, then our approach is able to help: it increases the code coverage reached by the automatically generated tests, while it prevents invoking any external module or dependency. Also, our results act as a unique baseline for the test generation performance of Microsoft Pex on open-source projects.
Conclusion:
 Based on the results, our technique might serve well for handling external dependencies in white-box test generation as it increases the coverage reached in such situations, while maintaining the practical applicability of the tests generated on the isolated code.",Information and Software Technology,04 Mar 2025,7,"The approach addresses challenges in white-box test generation by handling external dependencies, which can benefit startups by improving test coverage and reliability of generated tests, ultimately leading to higher quality software products."
https://www.sciencedirect.com/science/article/pii/S095058492030029X,Detecting Java software similarities by using different clustering techniques,June 2020,Not Found,Andrea=Capiluppi: andrea.capiluppi@brunel.ac.uk; Davide=Di Ruscio: davide.diruscio@univaq.it; Juri=Di Rocco: juri.dirocco@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Nemitari=Ajienka: nemitari.ajienka@edgehill.ac.uk,"Abstract
Background
Research on empirical 
software engineering
 has increasingly been conducted by analysing and measuring vast amounts of software systems. Hundreds, thousands and even millions of systems have been (and are) considered by researchers, and often within the same study, in order to test theories, demonstrate approaches or run prediction models. A much less investigated aspect is whether the collected metrics might be context-specific, or whether systems should be better analysed in clusters.
Objective
The objectives of this study are (i) to define a set of 
clustering techniques
 that might be used to group similar software systems, and (ii) to evaluate whether a suite of well-known object-oriented metrics is context-specific, and its values differ along the defined clusters.
Method
We group software systems based on three different 
clustering techniques
, and we collect the values of the metrics suite in each cluster. We then test whether clusters are statistically different between each other, using the Kolgomorov-Smirnov (KS) hypothesis testing.
Results
Our results show that, for two of the used techniques, the KS null hypothesis (e.g., the clusters come from the same population) is rejected for most of the metrics chosen: the clusters that we extracted, based on application domains, show statistically different 
structural properties
.
Conclusions
The implications for researchers can be profound: metrics and their interpretation might be more sensitive to context than acknowledged so far, and application domains represent a promising filter to cluster similar systems.",Information and Software Technology,04 Mar 2025,7,This study offers insights into software metrics clustering techniques which could have practical applications for startups in improving their software systems based on different contexts.
https://www.sciencedirect.com/science/article/pii/S0950584920300045,Time pressure in software engineering: A systematic review,May 2020,Not Found,Miikka=Kuutila: miikka.kuutila@oulu.fi; Mika=Mäntylä: Not Found; Umar=Farooq: Not Found; Maëlick=Claes: Not Found,"Abstract
Context
Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. Experiments and 
case studies
 have investigated the relationship between time pressure and software quality and productivity.
Objective
The present work aims to provide an overview of studies related to time pressure in 
software engineering
; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence.
Method
Our search strategy examined 5414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in 
software engineering
.
Results
The majority of high quality studies report increased productivity and decreased quality under time pressure. The most frequent categories of studies focus on quality assurance, 
cost estimation
, and process simulation. It appears that time pressure is usually caused by errors in 
cost estimation
. The effect of time pressure is most often identified during 
software quality assurance
.
Conclusions
The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.",Information and Software Technology,04 Mar 2025,8,"The study on time pressure in software engineering provides valuable insights for startups to manage productivity and quality assurance under tight deadlines, offering practical takeaways based on empirical evidence."
https://www.sciencedirect.com/science/article/pii/S095058491930271X,"Views on quality requirements in academia and practice: commonalities, differences, and context-dependent grey areas",May 2020,Not Found,Andreas=Vogelsang: andreas.vogelsang@tu-berlin.de; Jonas=Eckhardt: jonas@eckhardt.tv; Daniel=Mendez: daniel.mendez@bth.se; Moritz=Berger: moritz.berger@imbie.uni-bonn.de,"Abstract
Context:
 Quality requirements (QRs) are a topic of constant discussions both in industry and academia. Debates entwine around the definition of quality requirements, the way how to handle them, or their importance for project success. While many academic endeavors contribute to the body of knowledge about QRs, practitioners may have different views. In fact, we still lack a consistent body of knowledge on QRs since much of the discussion around this topic is still dominated by observations that are strongly context-dependent. This holds for both academic and practitioners’ views. Our assumption is that, in consequence, those views may differ.
Objective:
 We report on a study to better understand the extent to which available research statements on quality requirements, as found in exemplary peer-reviewed and frequently cited publications, are reflected in the perception of practitioners. Our goal is to analyze differences, commonalities, and context-dependent grey areas in the views of academics and practitioners to allow a discussion on potential misconceptions (on either sides) and opportunities for future research.
Method:
 We conducted a survey with 109 practitioners to assess whether they agree with research statements about QRs reflected in the literature. Based on a statistical model, we evaluate the impact of a set of context factors to the perception of research statements.
Results:
 Our results show that a majority of the statements is well respected by practitioners; however, not all of them. When examining the different groups and backgrounds of respondents, we noticed interesting deviations of perceptions within different groups that may lead to new research questions.
Conclusions:
Our results help identifying prevalent context-dependent differences about how academics and practitioners view QRs and pinpointing statements where further research might be useful.",Information and Software Technology,04 Mar 2025,8,The research on quality requirements perception differences between academia and practitioners can provide startups with valuable information on aligning their views and addressing potential misconceptions for project success.
https://www.sciencedirect.com/science/article/pii/S0950584919302630,Simulation environment for the choice of the decision making algorithm in multi-version real-time system,April 2020,Not Found,Igor V.=Kovalev: Not Found; Mikhail V.=Saramud: msaramud@sfu-kras.ru; Vasiliy V.=Losev: Not Found,"Abstract
Context
Nowadays the most effective way to improve the reliability of software is an approach with the introduction of software redundancy - multi-version programming. The reliability of a multi-version system is determined not only by the reliability of the versions that make it up, but to a greater degree by the decision making algorithm.
Objective
Our objective is evaluation and selection of the most reliable voting algorithms in multi-version environments. In order to get this objective there is a need to check all the algorithms in the execution environment, simulating characteristic of the developed system. Thus, we obtain the characteristics of the quality of the algorithm operation in precisely those conditions in which it will work in the system that is developed.
Method
The article suggests weighted voting algorithms with a forgetting element, as well as modifications of existing voting algorithms. To be able to check the quality of their work, the simulation environment has been implemented that simulates the operation of the software multi-version execution environment.
Results
The article substantiates the use of the most reliable decision making algorithms in the decision block of the real-time operating system. A comparative analysis of decision making algorithms for the operation of the decision making block of the multi-version real-time execution environment has been carried out.
Conclusions
The software implementation of the simulation environment that implements the simulations of versions with given characteristics is considered, not only classical decision making algorithms, but also the author's modifications are investigated. The environment allows to obtain the 
quality characteristics
 of all implemented decision making algorithms with given system characteristics. The modeling results are considered, the dependence of the system 
reliability indicators
 on its input parameters is shown, a comparative analysis of various decision making algorithms based on the modeling results is made.",Information and Software Technology,04 Mar 2025,6,The evaluation and selection of reliable voting algorithms in multi-version programming environments can be useful for startups looking to enhance the reliability of their software systems.
https://www.sciencedirect.com/science/article/pii/S0950584919302484,Software trustworthiness evaluation model based on a behaviour trajectory matrix,March 2020,Not Found,Junfeng=Tian: Not Found; Yuhui=Guo: tjf@hbu.edu.cn,"Abstract
Context
Software trustworthiness is a highly important 
research topic
. 
Trustworthiness evaluation
 based on factors that affect software behaviour is conducted mainly according to the influence degrees of these factors on the software behaviour to evaluate trustworthiness. As a result, minimization of the interference of human factors is considered.
Objective
In this study, to ensure the objectivity of evaluating the trustworthiness of software behaviour, a software trustworthiness evaluation model based on a behaviour trajectory matrix, namely, BTBM-TM was proposed.
Method
Checkpoints were set up in the trajectory of the software behaviour, and binary code was introduced to express the software behaviour trajectory tree. The scenario information of the checkpoints was acquired, and used to construct behaviour trajectory matrices, which were used to represent the behaviour trajectory. The behaviour trajectory matrices were transformed into grayscale images, which were used to train the 
deep residual network
 (ResNet) to classify the software behaviour. The trained 
deep residual network
 was used to categorize the current software behaviour, and the 
cosine similarity
 algorithm was used to calculate the deviation degree of the software behaviour trajectory; to perform a dual evaluation of the trustworthiness of software behaviour.
Results
The behaviour trajectory information of the Model class of 300 cycles was used to evaluate the trustworthiness of the mine-sweeping game. The trustworthiness evaluation results of the software behaviour of the scheme proposed in this paper (BTBM-TM) were compared with those of the schemes from references [6] and [10]. The accuracies of the schemes from [6] and [10] are lower than that of the BTBM-TM scheme.
Conclusions
The trajectory of software behaviour is represented by a matrix and converted into a grayscale image, whose 
processing method
 is used to evaluate the trustworthiness of software behaviour more objectively and accurately.",Information and Software Technology,04 Mar 2025,7,"The study on software trustworthiness evaluation model based on behaviour trajectory matrix offers startups a method for objectively evaluating their software behavior, potentially improving their trustworthiness."
https://www.sciencedirect.com/science/article/pii/S0950584919302502,"A fine-grained requirement traceability evolutionary algorithm: Kromaia, a commercial video game case study",March 2020,Not Found,Daniel=Blasco: dblasco@usj.es; Carlos=Cetina: ccetina@usj.es; Óscar=Pastor: opastor@dsic.upv.es,"Abstract
Context:
Commercial 
video games
 usually feature an extensive 
source code
 and requirements that are related to code lines from multiple methods. Traceability is vital in terms of maintenance and content update, so it is necessary to explore such 
search spaces
 properly.
Objective:
This work presents and evaluates CODFREL (Code Fragment-based Requirement Location), our approach to fine-grained 
requirement traceability
, which lies in an 
evolutionary algorithm
 and includes encoding and genetic operators to manipulate code fragments that are built from 
source code
 lines. We compare it with a baseline approach (Regular-LSI) by configuring both approaches with different 
granularities
 (code lines / complete methods).
Method:
We evaluated our approach and Regular-LSI in the Kromaia video game 
case study
, which is a commercial video game released on PC and PlayStation 4. The approaches are configured with method and code line granularity and work on 20 requirements that are provided by the development company. Our approach and Regular-LSI calculate similarities between requirements and code fragments or methods to propose possible solutions and, in the case of CODFREL, to guide the 
evolutionary algorithm
.
Results:
The results, which compare code line and method granularity configurations of CODFREL with different granularity configurations of Regular-LSI, show that our approach outperforms Regular-LSI in precision and recall, with values that are 26 and 8 times better, respectively, even though it does not achieve the optimal solutions. We make an open-source implementation of CODFREL available.
Conclusions:
Since our approach takes into consideration key issues like the source code size in commercial 
video games
 and the requirement dispersion, it provides better starting points than Regular-LSI in the search for solution candidates for the requirements. However, the results and the influence of domain-specific language on them show that more explicit knowledge is required to improve such results.",Information and Software Technology,04 Mar 2025,8,"The CODFREL approach presents a significant improvement over Regular-LSI in precision and recall, providing better starting points for finding solutions in commercial video games. The open-source implementation adds value to the practical applications in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919302575,An empirically evaluated checklist for surveys in software engineering,March 2020,Not Found,Jefferson Seide=Molléri: jefferson@simula.no; Kai=Petersen: Not Found; Emilia=Mendes: Not Found,"Abstract
Context:
 Over the past decade 
Software Engineering
 research has seen a steady increase in survey-based studies, and there are several guidelines providing support for those willing to carry out surveys. The need for auditing survey research has been raised in the literature. Checklists have been used both to conduct and to assess different types of empirical studies, such as experiments and 
case studies
.
Objective:
 To operationalize the assessment of survey studies by means of a checklist. To fulfill such goal, we aim to derive a checklist from standards for survey research and further evaluate the appropriateness of the checklist in the context of software engineering research.
Method:
 We systematically aggregated knowledge from 12 methodological studies supporting survey-based research in software engineering. We identified the key stages of the survey process and its recommended practices through thematic analysis and vote counting. We evaluated the checklist by applying it to existing surveys and analyzed the results. Thereafter, we gathered the feedback of experts (the surveys’ authors) on our analysis and used the feedback to improve the survey checklist.
Results:
 The evaluation provided insights regarding limitations of the checklist in relation to its understanding and objectivity. In particular, 19 of the 38 checklist items were improved according to the feedback received from experts.
Conclusion:
 The proposed checklist is appropriate for auditing survey reports as well as a support tool to guide ongoing research with regard to the survey design process. A discussion on how to use the checklist and what its implications are for research practice is also provided.",Information and Software Technology,04 Mar 2025,4,"The checklist proposed for survey studies provides support for auditing survey reports but shows limitations in understanding and objectivity. While it can guide ongoing research, the impact on early-stage ventures may not be as significant."
https://www.sciencedirect.com/science/article/pii/S0950584919302010,Requirements specification for developers in agile projects: Evaluation by two industrial case studies,January 2020,Not Found,Juliana=Medeiros: juliana.medeiros@ifpb.edu.br; Alexandre=Vasconcelos: amlv@cin.ufpe.br; Carla=Silva: ctlls@cin.ufpe.br; Miguel=Goulão: mgoul@fct.unl.pt,"Abstract
Context
An inadequate requirements specification activity acts as a catalyst to other problems, such as low team productivity and difficulty in maintaining software. Although 
Agile Software Development
 (ASD) has grown in recent years, research pointed out several limitations concerning its 
requirements engineering
 activities, such as Software Requirements Specification (SRS) provided in high level and targeted to the customer, lack of information required to perform design activities and low availability of the customer. To overcome these issues, the RSD (Requirements Specification for Developers) approach was proposed to create an SRS that provides information closer to development needs. In addition, existing literature reviews identify a demand for more empirical studies on the requirements specification activity in ASD.
Objective
Face to this, this work presents the evaluation of the RSD approach with respect to how it affects the teamwork and to identify its strengths and limitations.
Methods
This evaluation was performed by means of two industrial 
case studies
 conducted using a multiple-case design, focusing on software engineers as the analysis unit. Data were collected during 15 months from documents, observations, and interviews. They were triangulated, analyzed, and synthesized using techniques of grounded theory.
Results
The findings pointed out that the readability of SRS was compromised when several requirements are specified in the same RSD artifact. Evaluation also indicated the need of prioritization and categorization of the 
acceptance criteria
, a tool for creating, searching and tracing the artifacts, and obtaining 
acceptance tests
 from 
acceptance criteria
. On the other hand, the findings showed that the practices used to specify requirements using the RSD approach have the potential to produce a more objective SRS, tailored for the development team.
Conclusion
As a consequence, the structure of the RSD artifact was considered as a factor that improved the team performance in the two 
case studies
.",Information and Software Technology,04 Mar 2025,7,The RSD approach shows potential in improving team performance in two industrial case studies by creating a more objective SRS tailored for the development team. The findings offer practical insights for early-stage ventures in software development.
https://www.sciencedirect.com/science/article/pii/S0950584919301715,Software process line as an approach to support software process reuse: A systematic literature review,December 2019,Not Found,Eldânae=Nogueira Teixeira: danny@cos.ufrj.br; Fellipe Araújo=Aleixo: Not Found; Francisco Dione de Sousa=Amâncio: Not Found; Edson=OliveiraJr: Not Found; Uirá=Kulesza: Not Found; Cláudia=Werner: Not Found,"Abstract
Context
Software 
Process Line
 (SPrL) aims at providing a systematic reuse technique to support reuse experiences and knowledge in the definition of software processes for new projects thus contributing to reduce effort and costs and to achieve improvements in quality. Although the research body in SPrL is expanding, it is still an immature area with results offering an overall view scattered with no consensus.
Objective
The goal of this work is to identify existing approaches for developing, using, managing and visualizing the evolution of SPrLs and to characterize their support, especially during the development of reusable process family artefacts, including an overview of existing SPrL supporting tools in their multiple stages; to 
analyse variability
 management and component-based aspects in SPrL; and, finally, to list practical examples and conducted evaluations. This research aims at reaching a broader and more consistent view of the research area and to provide perspectives and gaps for future research.
Method
We performed a systematic literature review according to well-established guidelines set. We used tools to partially support the process, which relies on a six-member research team.
Results
We report on 49 primary studies that deal mostly with conceptual or theoretical proposals and the domain engineering stage. Years 2014, 2015, and 2018 yielded the largest number of articles. This can indicate SPrL as a recent research theme and one that attracts ever-increasing interest.
Conclusion
Although this research area is growing, there is still a lack of practical experiences and approaches for actual applications or project-specific process derivations and decision-making support. The concept of an integrated reuse infrastructure is less discussed and explored; and the development of integrated tools to support all reuse stages is not fully addressed. Other topics for future research are discussed throughout the paper with gaps pointed as opportunities for improvements in the area.",Information and Software Technology,04 Mar 2025,5,"The research on Software Process Line (SPrL) provides a broader view of the area, but lacks practical experiences and integrated tools for reuse infrastructure. While it offers perspectives for future research, the immediate impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919301259,Scaling-up domain-specific modelling languages through modularity services,November 2019,Not Found,Antonio=Garmendia: antonio.garmendia@uam.es; Esther=Guerra: Not Found; Juan=de Lara: Not Found; Antonio=García-Domínguez: Not Found; Dimitris=Kolovos: Not Found,"Abstract
Context
Model-driven engineering (MDE) promotes the active use of models in all phases of software development. Even though models are at a high level of abstraction, large or complex systems still require building monolithic models that prove to be too big for their processing by existing tools, and too difficult to comprehend by users. While modularization techniques are well-known in programming languages, they are not the norm in MDE.
Objective
Our goal is to ease the modularization of models to allow their efficient processing by tools and facilitate their management by users.
Method
We propose five patterns that can be used to extend a 
modelling language
 with services related to modularization and scalability. Specifically, the patterns allow defining model fragmentation strategies, scoping and visibility rules, model indexing services, and scoped constraints. Once the patterns have been applied to the meta-model of a 
modelling language
, we synthesize a customized modelling environment enriched with the defined services, which become applicable to both existing monolithic legacy models and new models.
Results
Our proposal is supported by a tool called EMF-Splitter, combined with the Hawk model indexer. Our experiments show that this tool improves the validation performance of large models. Moreover, the analysis of 224 meta-models from 
OMG
 standards, and a public repository with more than 300 meta-models, demonstrates the applicability of our patterns in practice.
Conclusions
Modularity mechanisms typically employed in programming IDEs can be successfully transferred to MDE, leading to more scalable and structured domain-specific modelling languages and environments.",Information and Software Technology,04 Mar 2025,9,"The proposal of patterns for modularization in Model-driven engineering (MDE) addresses the scalability and management issues of large models efficiently. The EMF-Splitter tool demonstrates practical applicability, offering valuable solutions for early-stage ventures dealing with complex systems."
https://www.sciencedirect.com/science/article/pii/S0950584919301442,A conceptual perspective on interoperability in context-aware software systems,October 2019,Not Found,Rebeca C.=Motta: rmotta@cos.ufrj.br; Káthia M.=de Oliveira: Not Found; Guilherme H.=Travassos: Not Found,"Abstract
Context
Context-aware software systems can interact with different devices to complete their tasks and act according to the context, regardless of their development and organizational differences. Interoperability is a big challenge in the engineering of such systems.
Objective
To discuss how interoperability has been addressed in context-aware software systems, strengthening the scientific basis for its understanding and conceptualization.
Method
A 
quasi
-systematic literature review was undertaken to observe interoperability in such context-aware software systems to support the discussions. Its dataset includes 17 from 408 papers identified in the technical literature. The extracted information was qualitatively analyzed by following the principles of Grounded Theory.
Results
The analysis allowed to identify ten interoperability concepts, organized into a Theoretical Framework according to structural and behavioral perspectives, which deals with interoperability as the ability of things (an object, a place, an application or anything that can engage interaction with a system) to interact for a particular purpose, once their differences (development platforms, 
data formats
, culture, legal issues) have been overcome. Once the interoperability is established from structural concepts (context, perspective, purpose, the level of provided support and system attributes), it can be measured, improved and observed from the behavioral concepts (evaluation method, challenges, issues, and benefits).
Conclusions
The Interoperability Theoretical Framework provides relevant information to organize the knowledge related to interoperability, considering context, and can be used to guide the evolution of software systems regarding changes focused on interoperability.",Information and Software Technology,04 Mar 2025,4,"While the topic of interoperability in context-aware software systems is relevant, the practical impact on early-stage ventures is limited as it focuses more on a theoretical framework."
https://www.sciencedirect.com/science/article/pii/S0950584919300916,Using a many-objective approach to investigate automated refactoring,August 2019,Not Found,M.=Mohan: Not Found; D.=Greer: des.greer@qub.ac.uk,"Abstract
Context
Software maintenance is expensive and so anything that can be done to reduce its cost is potentially of huge benefit. However, it is recognised that some maintenance, especially refactoring, can be automated. Given the number of possible refactorings and combinations of refactorings, a search-based approach may provide the means to optimise refactorings.
Objective
This paper describes the investigation of a many-objective 
genetic algorithm
 used to automate software refactoring, implemented as a Java tool, MultiRefactor.
Method
The approach and tool is evaluated using a set of open source Java programs. The tool contains four separate measures of software looking at the software quality as well as measures of code priority, refactoring coverage and element recentness. The many-objective algorithm combines the four objectives to improve the software in a holistic manner. An experiment has been constructed to compare the many-objective approach against a mono-objective approach that only uses a single objective to measure software quality. Different permutations of the objectives are also tested and compared to see how well the different objectives can work together in a multi-objective refactoring approach. The eight approaches are tested on six different open source Java programs.
Results
The many-objective approach is found to give better objective scores on average than the mono-objective approach and in less time. However, the priority and element recentness objectives are both found to be less successful in multi/many-objective setups when they are used together.
Conclusion
A many-objective approach is suitable and effective for optimising 
automated refactoring
 to improve quality. Including other objectives does not unduly degrade the quality improvements, but is less effective for those objectives than if they were used in a mono-objective approach.",Information and Software Technology,04 Mar 2025,7,Automating software refactoring through many-objective genetic algorithms has a significant practical value for startups in improving software quality efficiently.
https://www.sciencedirect.com/science/article/pii/S095058491830243X,Little’s law based validation framework for load testing,April 2019,Not Found,Raghu=Ramakrishnan: raghuramakrishnan71@gmail.com; Arvinder=Kaur: arvinder70@gmail.com,"Abstract
Context:
 Performance is a key quality consideration for large-scale software systems which supports thousands of concurrent users. Load testing is an integral part of the 
development lifecycle
 and is used to address performance issues before deploying the system in production. But, how do we validate the output reported by load testing tools? Little’s Law is useful for validating the accuracy of load testing output. Though IT industry is flooded with various enterprise and 
open source tools
 for load testing, but they do not offer support for validation of its result using Little’s Law. 
Manual validation
 is time intensive and infeasible with increase in the complexity of testing scripts.
Objective:
 In this paper we provide a Little’s law-based validation framework which will enable the researchers and industry practitioners to validate load testing results. The implementation of the framework is also demonstrated.
Method:
 To understand the constructs commonly used in 
load test
 scripts, we analysed scripts of two open source 
benchmark applications
 and eight large-scale software systems used in industry. We found that transactions are arranged using control flow patterns like sequential, loop and conditional. Based on the analysis, we devised the framework.
Results:
 The efficacy of the proposed framework is successfully evaluated on two systems - an open source Dell DVD Store benchmarking application and a real-world 
large scale system
 used in industry. The framework is independent of load testing tool used and can be used with complex testing scripts.
Conclusions
 There are no known frameworks or inbuilt support in existing load testing tools for guiding practitioners on applying Little’s Law using output generated by tools. We address this significant gap by providing a framework which combines information from test scripts/reports and Little’s Law to determine whether the results are valid. The provided implementation can be easily integrated with existing load testing tools.",Information and Software Technology,04 Mar 2025,8,Providing a framework for validating load testing results using Little’s Law is highly valuable for early-stage ventures dealing with large-scale software systems.
https://www.sciencedirect.com/science/article/pii/S0950584918301915,Automated metamodel/model co-evolution: A search-based approach,February 2019,Not Found,Wael=Kessentini: kessentw@iro.umontreal.ca; Houari=Sahraoui: Not Found; Manuel=Wimmer: Not Found,"Abstract
Context:
 Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules.
Objective:
 We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (
i
) the non-conformities with new metamodel version, (
ii
) the changes to existing models, and (
iii
) the textual and structural dissimilarities between the initial and revised models.
Method:
 We formulated the metamodel/model co-evolution as a multi-objective 
optimization problem
 to handle the different conflicting objectives using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Multi-Objective 
Particle Swarm Optimization
 (MOPSO).
Results:
 We evaluated our approach on several evolution scenarios extracted from different widely used metamodels. The results confirm the effectiveness of our approach with average manual correctness, precision and recall respectively higher than 91%, 88% and 89% on the different co-evolution scenarios.
Conclusion:
 A comparison with our previous work confirms the out-performance of our multi-objective formulation.",Information and Software Technology,04 Mar 2025,6,The automated approach for metamodel/model co-evolution using multi-objective optimization techniques has practical relevance for startups dealing with evolving metamodels.
https://www.sciencedirect.com/science/article/pii/S095058491830171X,An exploratory study of waste in software development organizations using agile or lean approaches: A multiple case study at 14 organizations,January 2019,Not Found,Hiva=Alahyari: hiva@chalmers.se; Tony=Gorschek: tony.gorschek@bth.se; Richard=Berntsson Svensson: richard.berntsson.svensson@bth.se,"Abstract
Context
The principal focus of lean is the identification and elimination of waste from the process with respect to maximizing customer value. Similarly, the purpose of agile is to maximize customer value and minimize unnecessary work and 
time delays
. In both cases the concept of waste is important. Through an empirical study, we explore how waste is approached in 
agile software development
 organizations.
Objective
This paper explores the concept of waste in agile/lean software development organizations and how it is defined, used, prioritized, reduced, or eliminated in practice
Method
The data were collected using semi-structured open-interviews. 23 practitioners from 14 
embedded software
 development organizations were interviewed representing two core roles in each organization.
Results
Various wastes, categorized in 10 different categories, were identified by the respondents. From the mentioned wastes, not all were necessarily waste per se but could be symptoms caused by wastes. From the seven wastes of lean, Task-switching was ranked as the most important, and Extra-features, as the least important wastes according to the respondents’ opinion. However, most companies do not have their own or use an established definition of waste, more importantly, very few actively identify or try to eliminate waste in their organizations beyond local initiatives on project level.
Conclusion
In order to identify, recognize and eliminate waste, a common understanding, and a 
joint
 and holistic view of the concept is needed. It is also important to optimize the whole organization and the whole product, as waste on one level can be important on another, thus sub-optimization should be avoided. Furthermore, to achieve a sustainable and effective waste handling, both the short-term and the long-term perspectives need to be considered.",Information and Software Technology,04 Mar 2025,5,"Exploring waste in agile/lean software development organizations provides valuable insights, but the direct impact on early-stage ventures may be limited without specific actionable solutions."
https://www.sciencedirect.com/science/article/pii/S0950584918301861,Adaptive monitoring: A systematic mapping,January 2019,Not Found,Edith=Zavala: zavala@essi.upc.edu; Xavier=Franch: franch@essi.upc.edu; Jordi=Marco: jmarco@cs.upc.edu,"Abstract
Context
Adaptive monitoring is a method used in a variety of domains for responding to changing conditions. It has been applied in different ways, from monitoring systems’ 
customization
 to re-composition, in different application domains. However, to the best of our knowledge, there are no studies analyzing how adaptive monitoring differs or resembles among the existing approaches.
Objective
To characterize the current state of the art on adaptive monitoring, specifically to: (a) identify the main concepts in the adaptive monitoring topic; (b) determine the demographic characteristics of the studies published in this topic; (c) identify how adaptive monitoring is conducted and evaluated by the different approaches; (d) identify patterns in the approaches supporting adaptive monitoring.
Method
We have conducted a 
systematic mapping study
 of adaptive monitoring approaches following recommended practices. We have applied automatic search and snowballing sampling on different sources and used rigorous selection criteria to retrieve the final set of papers. Moreover, we have used an existing qualitative analysis method for extracting relevant data from studies. Finally, we have applied 
data mining
 techniques for identifying patterns in the solutions.
Results
We have evaluated 110 studies organized in 81 approaches that support adaptive monitoring. By analyzing them, we have: (1) surveyed related terms and definitions of adaptive monitoring and proposed a generic one; (2) visualized studies’ demographic data and arranged the studies into approaches; (3) characterized the main approaches’ contributions; (4) determined how approaches conduct the adaptation process and evaluate their solutions.
Conclusions
This cross-domain overview of the current state of the art on adaptive monitoring may be a solid and comprehensive baseline for researchers and practitioners in the field. Especially, it may help in identifying opportunities of research; for instance, the need of proposing generic and flexible 
software engineering
 solutions for supporting adaptive monitoring in a variety of systems.",Information and Software Technology,04 Mar 2025,7,The study on adaptive monitoring provides valuable insights into a relevant topic for early-stage ventures by offering a comprehensive baseline for researchers and practitioners. The identification of opportunities for research and the proposal of flexible software engineering solutions can benefit startups in various domains.
