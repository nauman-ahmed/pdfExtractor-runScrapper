link,title,published_year,keywords,author_email,abstract,publication_title,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S0950584924002337,Markov model based coverage testing of deep learning software systems,March 2025,Not Found,Ying=Shi: shiying2017@buaa.edu.cn; Beibei=Yin: yinbeibei@buaa.edu.cn; Jing-Ao=Shi: Shi_jingao@buaa.edu.cn,"Abstract
Context:
Deep Learning (DL) software systems have been widely deployed in safety and security-critical domains, which calls for systematic testing to guarantee their accuracy and reliability. Objective measurement of test quality is one of the key issues in software testing. Recently, many coverage criteria have been proposed to measure the testing adequacy of Deep Neural Networks (DNNs).
Objective:
Recent research demonstrates that existing criteria have some limitations on interpreting the increasingly diverse behaviors of DNNs or clarifying the relationship between the coverage and the decision logic of DNNs. Moreover, some evaluations argue against the correlation between coverage and defect detection. In this paper, a novel coverage approach is proposed to interpret the internal information of programs.
Methods:
The process of coverage testing is formalized and quantified by constructing Markov models based on critical neurons extracted using Layer-wise Relevance Propagation in the structure of DNNs. The difference in the transition matrix of Markov chains between training and testing data is measured by KL divergence, and it is developed as a coverage criterion.
Results:
The values of the proposed coverage increase as the number of classes increases. The values are different for various test suites, and they become higher with the addition of new samples. Higher coverage values are observed to correlate with an increased fault detection capability.
Conclusion:
The experimental results illustrate that the proposed approach can effectively measure actual diversity and exhibit more adaptability to additional test cases. Furthermore, there is a positive correlation between the proposed coverage and fault detection, which provides support for test case selection guided by coverage.",Information and Software Technology,18 Mar 2025,8,"The proposed novel coverage approach for Deep Neural Networks shows positive correlation with fault detection, which can be beneficial for startups in developing reliable AI-based systems."
https://www.sciencedirect.com/science/article/pii/S0950584924002337,Markov model based coverage testing of deep learning software systems,March 2025,Not Found,Ying=Shi: shiying2017@buaa.edu.cn; Beibei=Yin: yinbeibei@buaa.edu.cn; Jing-Ao=Shi: Shi_jingao@buaa.edu.cn,"Abstract
Context:
Deep Learning (DL) software systems have been widely deployed in safety and security-critical domains, which calls for systematic testing to guarantee their accuracy and reliability. Objective measurement of test quality is one of the key issues in software testing. Recently, many coverage criteria have been proposed to measure the testing adequacy of Deep Neural Networks (DNNs).
Objective:
Recent research demonstrates that existing criteria have some limitations on interpreting the increasingly diverse behaviors of DNNs or clarifying the relationship between the coverage and the decision logic of DNNs. Moreover, some evaluations argue against the correlation between coverage and defect detection. In this paper, a novel coverage approach is proposed to interpret the internal information of programs.
Methods:
The process of coverage testing is formalized and quantified by constructing Markov models based on critical neurons extracted using Layer-wise Relevance Propagation in the structure of DNNs. The difference in the transition matrix of Markov chains between training and testing data is measured by KL divergence, and it is developed as a coverage criterion.
Results:
The values of the proposed coverage increase as the number of classes increases. The values are different for various test suites, and they become higher with the addition of new samples. Higher coverage values are observed to correlate with an increased fault detection capability.
Conclusion:
The experimental results illustrate that the proposed approach can effectively measure actual diversity and exhibit more adaptability to additional test cases. Furthermore, there is a positive correlation between the proposed coverage and fault detection, which provides support for test case selection guided by coverage.",Information and Software Technology,18 Mar 2025,8,"The proposed novel coverage approach for Deep Neural Networks shows positive correlation with fault detection, which can be beneficial for startups in developing reliable AI-based systems."
https://www.sciencedirect.com/science/article/pii/S0950584924002283,An alternative to code comment generation? Generating comment from bytecode,March 2025,Not Found,Xiangping=Chen: Not Found; Junqi=Chen: Not Found; Zhilu=Lian: Not Found; Yuan=Huang: huangyuan5@mail.sysu.edu.cn; Xiaocong=Zhou: Not Found; Yunzhi=Wu: Not Found; Zibin=Zheng: Not Found,"Abstract
Context:
Due to the importance and necessity of code comments, recent works propose many comment generation models with source code as input. But sometimes there has no access to obtain the source code, only the bytecode, such as many Apps.
Objective:
If there is a way to generate comments for bytecode directly, tasks such as malware detection and understanding closed-source software can benefit from the generated comment because it improves the understandability of the system. Therefore, we propose a novel approach called ByteGen to generate comments from bytecode.
Methods:
Specifically, to extract the structure characteristic of the bytecode, we utilize the control flow graph (CFG) of the bytecode and use a special traversal named enhanced SBT to serialize CFG. The enhanced SBT can completely preserve the structure of the CFG in a sequence. We set up experiments on a dataset with a scale of about 50,000 bytecode-comment pairs collected from Maven.
Results:
Experimental results show that the average BLEU-4 score of ByteGen is 28.67, which outperforms several baselines, and a human study also indicates the effectiveness of ByteGen in generating comments from bytecodes.
Conclusion:
In general, ByteGen performs better than other baselines. Therefore, this also proves the effectiveness of our approach in the code comment generation scenario without source code.",Information and Software Technology,18 Mar 2025,6,"The ByteGen approach for generating comments from bytecode could be useful for tasks like malware detection, but the practical application in startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002349,Redefining crowdsourced test report prioritization: An innovative approach with large language model,March 2025,Not Found,Yuchen=Ling: yuchenling@smail.nju.edu.cn; Shengcheng=Yu: yusc@smail.nju.edu.cn; Chunrong=Fang: fangchunrong@nju.edu.cn; Guobin=Pan: panguobin@cmss.chinamobile.com; Jun=Wang: wangjun@cmss.chinamobile.com; Jia=Liu: liujia@nju.edu.cn,"Abstract
Context:
Crowdsourced testing has gained popularity in software testing, especially for mobile app testing, due to its ability to bring diversity and tackle fragmentation issues. However, the openness of crowdsourced testing presents challenges, particularly in the manual review of numerous test reports, which is time-consuming and labor-intensive.
Objective:
The primary goal of this research is to improve the efficiency of review processes in crowdsourced testing. Traditional approaches to test report prioritization lack a deep understanding of semantic information in textual descriptions of these reports. This paper introduces LLMPrior, a novel approach for prioritizing crowdsourced test reports using large language models (LLMs).
Method:
LLMPrior leverages LLMs for the analysis and clustering of crowdsourced test reports based on the types of bugs revealed in their textual descriptions. This involves using prompt engineering techniques to enhance the performance of LLMs. Following the clustering, a recurrent selection algorithm is applied to prioritize the reports.
Results:
Empirical experiments are conducted to evaluate the effectiveness of LLMPrior. The findings indicate that LLMPrior not only surpasses current state-of-the-art approaches in terms of performance but also proves to be more feasible, efficient, and reliable. This success is attributed to the use of prompt engineering techniques and the cluster-based prioritization strategy.
Conclusion:
LLMPrior represents a significant advancement in crowdsourced test report prioritization. By effectively utilizing large language models and a cluster-based strategy, it addresses the challenges in traditional prioritization approaches, offering a more efficient and reliable solution for app developers dealing with crowdsourced test reports.",Information and Software Technology,18 Mar 2025,7,"The LLMPrior approach for prioritizing crowdsourced test reports using large language models presents a significant advancement in software testing efficiency, which can benefit startups dealing with testing challenges."
https://www.sciencedirect.com/science/article/pii/S0950584924002313,Native cross-platform app development using the SequalsK transpiler,March 2025,"Mobile app, Transpilers, Swift, Kotlin, Cross-platform, Mining software repositories",Dominik=Schultes: dominik.schultes@iem.thm.de; Larissa=Schneider: lar.m.sch@gmail.com; Tobias=Heymann: tobiasheymann@outlook.com; Franziska=Wild: wild.franziska@googlemail.com,"Abstract
Context:
Developing two separate versions of an app for iOS and Android requires significant effort. Existing cross-platform development frameworks may reduce this effort, but they also come with tradeoffs such as high tool dependency.
Objective:
To avoid the drawbacks of current methods, we introduce a new approach to cross-platform app development, provide the necessary tools, and conduct a thorough evaluation to demonstrate the feasibility of our proposed approach.
Method:
The central idea of the new 
native cross-platform development
 approach is to actively develop apps in both native programming languages, Kotlin for Android and Swift for iOS, while exchanging considerable parts of the source code in a bidirectional fashion using a deterministic transpiler. As the centerpiece of our proposed development approach, we present such a 
bidirectional
 Swift-Kotlin transpiler, called 
SequalsK
. It supports the majority of the important constructs of both languages and is able to generate syntactically and semantically correct Kotlin code out of Swift code 
and vice versa
.
Results:
In our evaluation, we determined that SequalsK is the sole existing bidirectional transpiler, distinguishing it from other transpilers that support only one direction. For the Kotlin-to-Swift direction, SequalsK emerges as the premier transpiler, while for the reverse direction, it stands among the top transpilers. Through six distinct case studies, we applied our native cross-platform development approach, showcasing its ability to fulfill all goals. Across each study, we successfully generated fully-functional native Android and iOS apps, achieving significant time savings as up to 86 percent of the source code has to be programmed only once and can be transpiled to the other involved programming language automatically.
Conclusion:
At the moment, in particular data structures and business logic can be transpiled successfully. In the future, we expect further improvements by extending the SequalsK transpiler in order to process user-interface parts as well.",Information and Software Technology,18 Mar 2025,6,"The bidirectional transpiler SequalsK for cross-platform app development shows promise in reducing development effort, but the practical implications and impact on startups might not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002325,Improving operational decision-making through decision mining - utilizing method engineering for the creation of a decision mining method,March 2025,"Decision making, Decision mining, Method engineering, Decision mining method, Systematic literature review",Sam=Leewis: sam.leewis@hu.nl; Koen=Smit: Not Found; Bas=van den Boom: Not Found; Johan=Versendaal: Not Found,"Abstract
Context
This study addresses the challenge of enhancing the efficiency and agility of decision support software supporting both operational decision-making and software production teams developing decision support software. It centers on creating a method that assists in mining decisions, checking decisions on conformance, and improving decisions, which supports software production teams in developing decision support software.
Objective
The primary objective is to develop an explicit, clear, and structured approach for discovering, checking, and improving decisions using decision support software. The study aims to create a blueprint for software production teams to develop Decision Mining (DM) software, in line with recent advancements in the field. Additionally, it seeks to provide a consolidated, methodical overview of activities and deliverables in the DM research field.
Method
The research employs method engineering principles to construct a method for DM that leverages the existing body of knowledge by utilizing a Systematic Literature Review (SLR). The study focuses on developing individual building blocks and method fragments incorporated into seven DM scenarios.
Results
The study led to the creation of a Decision Mining Method (DMM), which includes 138 method fragments grouped into eleven categories. These fragments were systematically merged to form a comprehensive DMM. The method encapsulates the complexity of DM and provides practical applicability in real-world scenarios, highlighted by the identification of seven distinct scenarios in DM phases. The study also conducted the first SLR in the DM field, providing a comprehensive overview of current practices and outcomes.
Conclusion
The study helps in advancing the DM field by creating a structured approach and a comprehensive method for DM, aligning with recent developments in the field. It successfully aggregated the fragmented DM domain into a cohesive methodological overview, crucial for future research. The study also lays out a detailed agenda for future research, focusing on expanding and validating the DMM, incorporating cross-disciplinary insights, and addressing the challenges in machine learning within DM. The future research directions aim to refine and broaden the applicability of the DMM, ensuring its effectiveness in diverse practical contexts and contributing to a more holistic and comprehensive approach to decision mining.",Information and Software Technology,18 Mar 2025,8,"The study addresses the challenge of decision support software development, providing a structured approach and method for Decision Mining. It has practical applicability in real-world scenarios and contributes to advancing the field. The comprehensive overview and future research agenda add value to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924002167,On the road to interactive LLM-based systematic mapping studies,February 2025,"Systematic mapping studies, Large language models, GPT",Kai=Petersen: kai.petersen@hs-flensburg.de; Jan M.=Gerken: jan.gerken@hs-flensburg.de,"Abstract
Context:
The research volume is continuously increasing. Manual analysis of large topic scopes and continuously updating literature studies with the newest research results is effort intensive and, therefore, difficult to achieve.
Objective:
To discuss possibilities and next steps for using LLMs (e.g., GPT-4) in the mapping study process.
Method:
The research can be classified as a solution proposal. The solution was iteratively designed and discussed among the authors based on their experience with LLMs and literature reviews.
Results:
We propose strategies for the mapping process, outlining the use of agents and prompting strategies for each step.
Conclusion:
Given the potential of LLMs in literature studies, we should work on a holistic solutions for LLM-supported mapping studies.",Information and Software Technology,18 Mar 2025,5,"While discussing the use of LLMs in mapping studies is relevant, the abstract lacks a clear indication of practical implementation and impact on early-stage ventures. More concrete strategies and outcomes would increase the score."
https://www.sciencedirect.com/science/article/pii/S0950584924002167,On the road to interactive LLM-based systematic mapping studies,February 2025,"Systematic mapping studies, Large language models, GPT",Kai=Petersen: kai.petersen@hs-flensburg.de; Jan M.=Gerken: jan.gerken@hs-flensburg.de,"Abstract
Context:
The research volume is continuously increasing. Manual analysis of large topic scopes and continuously updating literature studies with the newest research results is effort intensive and, therefore, difficult to achieve.
Objective:
To discuss possibilities and next steps for using LLMs (e.g., GPT-4) in the mapping study process.
Method:
The research can be classified as a solution proposal. The solution was iteratively designed and discussed among the authors based on their experience with LLMs and literature reviews.
Results:
We propose strategies for the mapping process, outlining the use of agents and prompting strategies for each step.
Conclusion:
Given the potential of LLMs in literature studies, we should work on a holistic solutions for LLM-supported mapping studies.",Information and Software Technology,18 Mar 2025,5,"While discussing the use of LLMs in mapping studies is relevant, the abstract lacks a clear indication of practical implementation and impact on early-stage ventures. More concrete strategies and outcomes would increase the score."
https://www.sciencedirect.com/science/article/pii/S0950584924002039,Systematic review on the current state of computer-supported argumentation learning systems,February 2025,Not Found,Laura=Sinikallio: laura.sinikallio@helsinki.fi; Lili=Aunimo: Lili.Aunimo@haaga-helia.fi; Tomi=Männistö: tomi.mannisto@helsinki.fi,"Abstract
Context:
Argumentation is a fundamental part of learning, communication and problem-solving not only in software engineering but all education. Teaching argumentation is a long-standing practice, and with the advance of digital learning, it, too, has been transitioning to an online format.
Objective:
As computer-supported argumentation learning progresses, other learning domains have much to learn from it on how to enable argumentation and reasoning in automated and scalable online learning solutions.
Methods:
To review the current state of the field, we conducted a systematic literature review on the last decade of academic research and design on computer-supported argumentation learning systems. We reviewed and summarised the central aspects and approaches of reported systems.
Results:
We reviewed 34 different argumentation learning tools. The review showed that approaches to computer-supported argumentation vary significantly in many aspects, e.g., argumentation theory, learning task types and collaboration status. However, the use of argumentation graphs is quite common. Most modern tools seem to embrace the role of feedback in learning.
Conclusions:
The role of individual learning has risen in computer-supported argumentation learning. This is in opposition to previous predictions and statements on the role of collaborative learning of argumentation. Automated feedback has, on the other hand, become commonplace in collaborative and individual-use argumentation learning tools. The modern generation of argumentation teaching tools is Web-based but recently we have also seen the emergence of mobile-based solutions.",Information and Software Technology,18 Mar 2025,7,"The abstract discusses the role of argumentation in education and learning, highlighting the transition to online formats. The systematic review of argumentation learning systems provides valuable insights. The emphasis on individual learning and feedback aligns with the current trends in education technology."
https://www.sciencedirect.com/science/article/pii/S0950584924002039,Systematic review on the current state of computer-supported argumentation learning systems,February 2025,Not Found,Laura=Sinikallio: laura.sinikallio@helsinki.fi; Lili=Aunimo: Lili.Aunimo@haaga-helia.fi; Tomi=Männistö: tomi.mannisto@helsinki.fi,"Abstract
Context:
Argumentation is a fundamental part of learning, communication and problem-solving not only in software engineering but all education. Teaching argumentation is a long-standing practice, and with the advance of digital learning, it, too, has been transitioning to an online format.
Objective:
As computer-supported argumentation learning progresses, other learning domains have much to learn from it on how to enable argumentation and reasoning in automated and scalable online learning solutions.
Methods:
To review the current state of the field, we conducted a systematic literature review on the last decade of academic research and design on computer-supported argumentation learning systems. We reviewed and summarised the central aspects and approaches of reported systems.
Results:
We reviewed 34 different argumentation learning tools. The review showed that approaches to computer-supported argumentation vary significantly in many aspects, e.g., argumentation theory, learning task types and collaboration status. However, the use of argumentation graphs is quite common. Most modern tools seem to embrace the role of feedback in learning.
Conclusions:
The role of individual learning has risen in computer-supported argumentation learning. This is in opposition to previous predictions and statements on the role of collaborative learning of argumentation. Automated feedback has, on the other hand, become commonplace in collaborative and individual-use argumentation learning tools. The modern generation of argumentation teaching tools is Web-based but recently we have also seen the emergence of mobile-based solutions.",Information and Software Technology,18 Mar 2025,7,"The abstract discusses the role of argumentation in education and learning, highlighting the transition to online formats. The systematic review of argumentation learning systems provides valuable insights. The emphasis on individual learning and feedback aligns with the current trends in education technology."
https://www.sciencedirect.com/science/article/pii/S0950584924002271,Systematic mapping study on requirements engineering for regulatory compliance of software systems,February 2025,"Requirements engineering, Software engineering, Secondary research, Regulatory requirements engineering, Regulatory compliance, Compliance requirements, Software compliance",Oleksandr=Kosenkov: oleksandr.kosenkov@bth.se; Parisa=Elahidoost: Not Found; Tony=Gorschek: Not Found; Jannik=Fischbach: Not Found; Daniel=Mendez: Not Found; Michael=Unterkalmsteiner: Not Found; Davide=Fucci: Not Found; Rahul=Mohanani: Not Found,"Abstract
Context:
As the diversity and complexity of regulations affecting Software-Intensive Products and Services (SIPS) is increasing, software engineers need to address the growing regulatory scrutiny. We argue that, as with any other non-negotiable requirements, SIPS compliance should be addressed early in SIPS engineering—i.e., during requirements engineering (RE).
Objectives:
In the conditions of the expanding regulatory landscape, existing research offers scattered insights into regulatory compliance of SIPS. This study addresses the pressing need for a structured overview of the state of the art in software RE and its contribution to regulatory compliance of SIPS.
Method:
We conducted a systematic mapping study to provide an overview of the current state of research regarding challenges, principles, and practices for regulatory compliance of SIPS related to RE. We focused on the role of RE and its contribution to other SIPS lifecycle process areas. We retrieved 6914 studies published from 2017 (January 1) until 2023 (December 31) from four academic databases, which we filtered down to 280 relevant primary studies.
Results:
We identified and categorized the RE-related challenges in regulatory compliance of SIPS and their potential connection to six types of principles and practices addressing challenges. We found that about 13.6% of the primary studies considered the involvement of both software engineers and legal experts in developing principles and practices. About 20.7% of primary studies considered RE in connection to other process areas. Most primary studies focused on a few popular regulation fields (privacy, quality) and application domains (healthcare, software development, avionics). Our results suggest that there can be differences in terms of challenges and involvement of stakeholders across different fields of regulation.
Conclusion:
Our findings highlight the need for an in-depth investigation of stakeholders’ roles, relationships between process areas, and specific challenges for distinct regulatory fields to guide research and practice.",Information and Software Technology,18 Mar 2025,6,"The study addresses a pressing need for a structured overview of the state of the art in software RE and its contribution to regulatory compliance of SIPS, which can be valuable for startups in tackling regulatory challenges."
https://www.sciencedirect.com/science/article/pii/S0950584924002003,A family of experiments to quantify the benefits of adopting WebDriverManager and Selenium-Jupiter,February 2025,"E2E testing, Selenium WebDriver, WebDriverManager, Selenium-Jupiter, Family of experiments",Maurizio=Leotta: maurizio.leotta@unige.it; Boni=García: Not Found; Filippo=Ricca: Not Found,"Abstract
Context:
While test automation offers numerous benefits, it also introduces significant challenges. Two challenges that developers and testers face on a daily basis, particularly when using Selenium WebDriver to test web applications, are driver management (involving tasks such as version identification, download, installation, and maintenance) and management of test lifecycle phases (using specific test libraries, as for example JUnit, and inserting annotations into the code). These manual tasks make test suite development particularly tedious, error-prone, and expensive. Recently, to ease the burden on developers and testers, some Java libraries have been proposed, called 
WebDriverManager
 and 
Selenium-Jupiter
, capable of automatically carrying out the driver management process for Selenium WebDriver and simplifying the development of test suites. These libraries appear to be very promising but until now no one has experimentally evaluated their effectiveness.
Objective:
To investigate the effectiveness of 
WebDriverManager
 and 
Selenium-Jupiter
 in reducing driver management times and boilerplate code.
Method:
We designed and conducted a family of experiments (three for 
WebDriverManager
 and two for 
Selenium-Jupiter
) with 104 master student participants from the University of Genoa, Italy (across academic years 2021/2022 and 2022/2023) and nine professional participants.
Results:
Results indicate that the adoption of Selenium WebDriver with 
WebDriverManager
 significantly reduces setup time for multi-browser test suites from 33% to 50% (depending on the tester experience). Additionally, 
Selenium-Jupiter
 reduces test suite development time significantly (20% on average). Although it also decreases total code length, the reduction is relatively small compared to overall code length.
Conclusion:
WebDriverManager
 and 
Selenium-Jupiter
 can be seen as valuable solutions for enhancing testers’ productivity by shortening the time needed to develop test suites and minimizing the amount of code to write.",Information and Software Technology,18 Mar 2025,8,"The study evaluates the effectiveness of WebDriverManager and Selenium-Jupiter in reducing setup time for test suites, which can significantly benefit early-stage ventures by enhancing testers' productivity."
https://www.sciencedirect.com/science/article/pii/S0950584924002003,A family of experiments to quantify the benefits of adopting WebDriverManager and Selenium-Jupiter,February 2025,"E2E testing, Selenium WebDriver, WebDriverManager, Selenium-Jupiter, Family of experiments",Maurizio=Leotta: maurizio.leotta@unige.it; Boni=García: Not Found; Filippo=Ricca: Not Found,"Abstract
Context:
While test automation offers numerous benefits, it also introduces significant challenges. Two challenges that developers and testers face on a daily basis, particularly when using Selenium WebDriver to test web applications, are driver management (involving tasks such as version identification, download, installation, and maintenance) and management of test lifecycle phases (using specific test libraries, as for example JUnit, and inserting annotations into the code). These manual tasks make test suite development particularly tedious, error-prone, and expensive. Recently, to ease the burden on developers and testers, some Java libraries have been proposed, called 
WebDriverManager
 and 
Selenium-Jupiter
, capable of automatically carrying out the driver management process for Selenium WebDriver and simplifying the development of test suites. These libraries appear to be very promising but until now no one has experimentally evaluated their effectiveness.
Objective:
To investigate the effectiveness of 
WebDriverManager
 and 
Selenium-Jupiter
 in reducing driver management times and boilerplate code.
Method:
We designed and conducted a family of experiments (three for 
WebDriverManager
 and two for 
Selenium-Jupiter
) with 104 master student participants from the University of Genoa, Italy (across academic years 2021/2022 and 2022/2023) and nine professional participants.
Results:
Results indicate that the adoption of Selenium WebDriver with 
WebDriverManager
 significantly reduces setup time for multi-browser test suites from 33% to 50% (depending on the tester experience). Additionally, 
Selenium-Jupiter
 reduces test suite development time significantly (20% on average). Although it also decreases total code length, the reduction is relatively small compared to overall code length.
Conclusion:
WebDriverManager
 and 
Selenium-Jupiter
 can be seen as valuable solutions for enhancing testers’ productivity by shortening the time needed to develop test suites and minimizing the amount of code to write.",Information and Software Technology,18 Mar 2025,8,"Same as Abstract 12, the study evaluates the effectiveness of WebDriverManager and Selenium-Jupiter in reducing setup time for test suites, which can significantly benefit early-stage ventures by enhancing testers' productivity."
https://www.sciencedirect.com/science/article/pii/S0950584924001976,Detecting and Explaining Python Name Errors,February 2025,Not Found,Jiawei=Wang: jiawei.wang1@monash.edu; Li=Li: Not Found; Kui=Liu: Not Found; Xiaoning=Du: Not Found,"Abstract
Python has become one of the most popular programming languages nowadays but has not received enough attention from the software engineering community. Many errors, either fixed or not yet, have been scattered in the lifetime of Python projects, including popular Python libraries that have already been reused. NameError is among one of those errors that are widespread in the Python community, as confirmed in our empirical study. Yet, our community has not put effort into helping developers mitigate its introductions. To fill this gap, we propose in this work a static analysis-based approach called 
DENE
 (short for 
D
etecting and 
E
xplaining 
N
ame 
E
rrors) to automatically detect and explain name errors in Python projects. To this end, 
DENE
 builds control-flow graphs for Python projects and leverages a scope-aware reaching definition analysis to locate identifiers that may cause name errors at runtime and report their locations. Experimental results on carefully crafted ground truth demonstrate that 
DENE
 is effective in detecting name errors in real-world Python projects. The results also confirm that unknown name errors are still widely presented in popular Python projects and libraries, and the outputs of 
DENE
 can indeed help developers understand why the name errors are flagged as such.",Information and Software Technology,18 Mar 2025,4,The proposed approach to detect and explain name errors in Python projects may not provide immediate practical value for early-stage ventures or startups.
https://www.sciencedirect.com/science/article/pii/S0950584924002052,Enabling efficient and low-effort decentralized federated learning with the EdgeFL framework,February 2025,"Federated learning, Machine learning, Software engineering, Decentralized architecture, Information privacy",Hongyi=Zhang: hongyiz@chalmers.se; Jan=Bosch: Not Found; Helena Holmström=Olsson: Not Found,"Abstract
Context:
Federated Learning (FL) has gained prominence as a solution for preserving data privacy in machine learning applications. However, existing FL frameworks pose challenges for software engineers due to implementation complexity, limited customization options, and scalability issues. These limitations prevent the practical deployment of FL, especially in dynamic and resource-constrained edge environments, preventing its widespread adoption.
Objective:
To address these challenges, we propose EdgeFL, an efficient and low-effort FL framework designed to overcome centralized aggregation, implementation complexity and scalability limitations. EdgeFL applies a decentralized architecture that eliminates reliance on a central server by enabling direct model training and aggregation among edge nodes, which enhances fault tolerance and adaptability to diverse edge environments.
Methods:
We conducted experiments and a case study to demonstrate the effectiveness of EdgeFL. Our approach focuses on reducing weight update latency and facilitating faster model evolution on edge devices.
Results:
Our findings indicate that EdgeFL outperforms existing FL frameworks in terms of learning efficiency and performance. By enabling quicker model evolution on edge devices, EdgeFL enhances overall efficiency and responsiveness to changing data patterns.
Conclusion:
EdgeFL offers a solution for software engineers and companies seeking the benefits of FL, while effectively overcoming the challenges and privacy concerns associated with traditional FL frameworks. Its decentralized approach, simplified implementation, combined with enhanced customization and fault tolerance, make it suitable for diverse applications and industries.",Information and Software Technology,18 Mar 2025,9,"The EdgeFL framework offers a solution for software engineers and companies seeking the benefits of FL, while effectively overcoming challenges, which can have a significant impact on European early-stage ventures looking to deploy FL in dynamic and resource-constrained edge environments."
https://www.sciencedirect.com/science/article/pii/S095058492400209X,A survey on Cryptoagility and Agile Practices in the light of quantum resistance,February 2025,"Agile methods, Cryptographic agility, Cryptographic algorithms, Quantum resistance",Lodovica=Marchesi: lodovica.marchesi@unica.it; Michele=Marchesi: Not Found; Roberto=Tonelli: Not Found,"Abstract
Context:
Crypto-agility, a name that stems from agile methodologies for software development, means the ability to modify quickly and securely cryptographic algorithms in the event of a compromise. The advent of quantum computing poses existential threats to current cryptography, having the power to breach current cryptography systems.
Objective:
We investigated whether and to what extent agile practices for software development are suited to support crypto-agility, or not. In particular, we discuss their usefulness in the context of substituting current algorithms with quantum-resistant ones.
Method:
First, we analyzed the literature to define a subset of 15 agile practices potentially relevant to cryptographic software development. Then, we developed a questionnaire to assess the suitability of agile practices for obtaining crypto-agility. We performed a Web search of relevant documents about crypto-agility and quantum resistance and sent their authors the questionnaire. We also sent the questionnaire to cybersecurity officers of four Italian firms. We analyzed and discussed the responses to 32 valid questionnaires.
Results:
The respondents’ affiliations are evenly distributed between researchers and developers. Most of them are active, or somehow active, in quantum-resistant cryptography and use agile methods. Most of the agile practices are deemed to be quite useful, or very useful to get crypto-agility, the most effective being Continuous Integration and Coding Standards; the least appreciated is Self-organizing Team.
Conclusion:
According to researchers and developers working in the field, the safe transition of cryptographic algorithms to quantum-resistant ones can benefit from the adoption of many agile practices. Further software engineering research is needed to integrate agile practices in more formal cryptographic software development processes.",Information and Software Technology,18 Mar 2025,8,The study addresses a critical issue of transitioning cryptographic algorithms amidst quantum computing threats and provides practical insights for agile practices adoption in the field.
https://www.sciencedirect.com/science/article/pii/S0950584924002076,E-code: Mastering efficient code generation through pretrained models and expert encoder group,February 2025,Not Found,Yue=Pan: pany@mail.sdu.edu.cn; Chen=Lyu: lvchen@sdnu.edu.cn; Zhenyu=Yang: yangzycs@mail.sdu.edu.cn; Lantian=Li: lilantian@mail.sdu.edu.cn; Qi=Liu: 1642339035@qq.com; Xiuting=Shao: shaoxiuting@126.com,"Abstract
Context:
With the waning of Moore’s Law, the software industry is placing increasing importance on finding alternative solutions for continuous performance enhancement. The significance and research results of software performance optimization have been on the rise in recent years, especially with the advancement propelled by 
L
arge 
L
anguage 
M
odel
s
 (LLMs). However, traditional strategies for rectifying performance flaws have shown significant limitations at the competitive code efficiency optimization level, and research on this topic is surprisingly scarce.
Objective:
This study aims to address the research gap in this domain, offering practical solutions to the various challenges encountered. Specifically, we have overcome the constraints of traditional performance error rectification strategies and developed a 
L
anguage 
M
odel (LM) tailored for the competitive code efficiency optimization realm.
Methods:
We introduced E-code, an advanced program synthesis LM. Inspired by the recent success of expert LMs, we designed an innovative structure called the Expert Encoder Group. This structure employs multiple expert encoders to extract features tailored for different input types. We assessed the performance of E-code against other leading models on a competitive dataset and conducted in-depth ablation experiments.
Results:
Upon systematic evaluation, E-code achieved a 54.98% improvement in code efficiency, significantly outperforming other advanced models. In the ablation experiments, we further validated the significance of the expert encoder group and other components within E-code.
Conclusion:
The research findings indicate that the expert encoder group can effectively handle various inputs in efficiency optimization tasks, significantly enhancing the model’s performance. In summary, this study paves new avenues for developing systems and methods to assist programmers in writing efficient code.",Information and Software Technology,18 Mar 2025,9,"The research introduces a novel language model for code efficiency optimization, offering practical solutions and paving new avenues for programmers to enhance code efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584924002088,Dynamic robustness evaluation for automated model selection in operation,February 2025,"Artificial neural network classifier, Automated model selection, Robustness, Dynamic evaluation, Distance-based metrics",Jin=Zhang: zhang.jin@swjtu.edu.cn; Jingyue=Li: jingyue.li@ntnu.no; Zhirong=Yang: zhirong.yang@ntnu.no,"Abstract
Context:
The increasing use of artificial neural network (ANN) classifiers in systems, especially safety-critical systems (SCSs), requires ensuring their robustness against out-of-distribution (OOD) shifts in operation, which are changes in the underlying data distribution from the data training the classifier. However, measuring the robustness of classifiers in operation with only unlabeled data is challenging. Additionally, machine learning engineers may need to compare different models or versions of the same model and switch to an optimal version based on their robustness.
Objective:
This paper explores the problem of dynamic robustness evaluation for automated model selection. We aim to find efficient and effective metrics for evaluating and comparing the robustness of multiple ANN classifiers using unlabeled operational data.
Methods:
To quantitatively measure the differences between the model outputs and assess robustness under OOD shifts using unlabeled data, we choose distance-based metrics. An empirical comparison of five such metrics, suitable for higher-dimensional data like images, is performed. The selected metrics include Wasserstein distance (WD), maximum mean discrepancy (MMD), Hellinger distance (HL), Kolmogorov–Smirnov statistic (KS), and Kullback–Leibler divergence (KL), known for their efficacy in quantifying distribution differences. We evaluate these metrics on 20 state-of-the-art models (ten CIFAR10-based models, five CIFAR100-based models, and five ImageNet-based models) from a widely used robustness benchmark (
RobustBench
) using data perturbed with various types and magnitudes of corruptions to mimic real-world OOD shifts.
Results:
Our findings reveal that the WD metric outperforms others when ranking multiple ANN models for CIFAR10- and CIFAR100-based models, while the KS metric demonstrates superior performance for ImageNet-based models. MMD can be used as a reliable second option for both datasets.
Conclusion:
This study highlights the effectiveness of distance-based metrics in ranking models’ robustness for automated model selection. It also emphasizes the significance of advancing research in dynamic robustness evaluation.",Information and Software Technology,18 Mar 2025,7,"The study explores dynamic robustness evaluation for automated model selection, offering efficient metrics for comparing ANN classifiers' robustness, contributing to advancements in the field."
https://www.sciencedirect.com/science/article/pii/S0950584924002118,Software aging oriented trustworthiness measurement based on weighted Boltzmann entropy,February 2025,Not Found,Hongwei=Tao: hongweitao@zzuli.edu.cn; Han=Liu: hanliu@stu.ecnu.edu.cn; Xiaoxu=Niu: xiaoxuniu2022@163.com; Licheng=Ding: 51184501105@stu.ecnu.edu.cn; Yixiang=Chen: yxchen@sei.ecnu.edu.cn; Qiaoling=Cao: 17839640477@163.com,"Abstract
Context:
With the rapid development of software, various software accidents emerge one after another. The catastrophic consequences caused by these accidents make people realize the importance of software trustworthiness. As an indispensable means to ensure software quality, traditional trustworthiness measurement evaluates the software trustworthiness by studying the trustworthy attributes in a static way. However, most of the factors considered in trustworthy attributes tend to be dynamic with time. The current research often ignores the changes in software after running for some time, and cannot reflect the changes in software trustworthiness at different running times.
Objective:
Our objective in this paper is to study the relationship between running time and software trustworthiness, and design a running time-related software trustworthiness measurement model from the untrustworthy evidence related to software aging.
Method:
We first extract the untrustworthy evidence from the bugs related to software aging in 5 subsystems of 4 public defect databases and 18 well-known software accidents, establish a risk level model, and design metric elements of untrustworthy evidence based on software aging. Then we construct a software aging cause category trustworthiness measurement model based on Boltzmann entropy. Finally, we build a software trustworthiness measurement model based on weighted Boltzmann entropy. For the weight values used in the model, the Brassard Priority Synthesis Analysis method was used to determine them.
Result:
Different from the common resource consumption parameter and performance parameter, a model based on weighted Boltzmann entropy can describe the influence of various parameters on the software’s trustworthiness through risk state. It can reflect the change of system state and describe the system state completely.
Conclusion:
The empirical study shows the effectiveness and practicality of our method for evaluating software dynamic trustworthiness. Meanwhile, it also indicates a promising avenue for future research and application in the field of software trustworthiness measurement.",Information and Software Technology,18 Mar 2025,6,"The paper investigates software trustworthiness over running time, proposing a novel measurement model but lacks practical implementation insights for early-stage European ventures."
https://www.sciencedirect.com/science/article/pii/S095058492400212X,"Energize sustainability: EnSAF for sustainability aware, software intensive energy management systems",February 2025,"Decision map, Energy management system, Quality attribute, Software impact, Software-intensive system, Sustainability framework",Anjana=M.S.: anjanams@am.amrita.edu; Patricia=Lago: p.lago@vu.nl; Aryadevi Remanidevi=Devidas: aryadevird@am.amrita.edu; Maneesha Vinodini=Ramesh: maneesha@amrita.edu,"Abstract
Context:
India’s coal use for electricity jumped 13% in 2021–22. Energy management systems (EnMS) are seen as a solution, but only sustainable EnMS can have a discernable impact on the carbon footprint and the Return On Investment (ROI).
Objective:
Designing a software-intensive sustainable energy management system requires considering technical, environmental, social, and economic factors. This helps evaluate an EnMS’s overall impact and improve its design. We proposed EnSAF for efficient utilization of the energy incurred for the design of sustainability-aware EnMSs.
Method:
In this work, EnMSs in diverse use cases were selected and analyzed in terms of technical, social, environmental, and economic dimensions of sustainability in collaboration with various stakeholders. The set of application-specific design concerns and Quality Attributes (QAs) were addressed by the Sustainability Assessment Framework (SAF) toolkit. The resultant SAF instances of each EnMS, derived through the analysis and discussion with the stakeholders, were then analyzed to advocate the DMs and SQ model for generic EnMSs.
Results:
This study demonstrated the following outcomes (i) technical concerns dominate the existing EnMSs (ii) integration of renewable energy resources reduces dependency to the main power grid and nurtures a sustainable environment by diminishing carbon footprint, and minimizing payback time, in the economic dimension; (iii) extant definitions of quality attributes need significant scrutiny and updates apropos of objectives of EnMSs
Conclusion:
The SAF toolkit was found to be deficient in the representation of relevant design concerns and quality attributes concomitant with sustainable EnMS. Prevailing DMs are inept to factor in stakeholder’s concerns, as the model is ill-equipped to account for spatio-temporal representation of QAs. Pursuant to the insights from the 4 SAF instances, a generic framework, EnSAF, is proposed to tackle the relevant concerns apropos of EnMS sustainability. This work proposed a representation of DMs in the SAF toolkit specifically for sustainability-aware EnMS.",Information and Software Technology,18 Mar 2025,7,"The research focuses on designing sustainable energy management systems, considering technical, environmental, social, and economic factors. It offers a framework for efficient energy utilization but lacks specific impact insights for startups."
https://www.sciencedirect.com/science/article/pii/S0950584924002064,Top-down: A better strategy for incremental covering array generation,February 2025,Not Found,Yan=Wang: Not Found; Xintao=Niu: niuxintao@nju.edu.cn; Huayao=Wu: Not Found; Changhai=Nie: Not Found; Lei=Yu: Not Found; Xiaoyin=Wang: Not Found; Jiaxi=Xu: Not Found,"Abstract
Context:
The Incremental Covering Array (ICA) offers a flexible and efficient test schedule for Combinatorial Testing (CT) by enabling dynamic adjustment of test strength. Despite its importance, ICA generation has been under-explored in the CT community, resulting in limited and suboptimal existing approaches.
Objective:
To address this gap, we introduce a novel strategy, namely 
Top-down
, for optimizing ICA generation.
Method:
In contrast to the traditional strategy, named 
Bottom-up
, 
Top-down
 starts with a higher-strength test set and then extracts lower-strength sets from it, thus leveraging test case generation algorithms more effectively.
Results:
We conducted a comparative evaluation of the two strategies across 17 real-world software with 84 total versions. The results demonstrate that compared with 
Bottom-up
, the 
Top-down
 strategy requires less time and generates smaller ICAs while covering more higher-strength interactions. Furthermore, 
Top-down
 outperforms 
Bottom-up
 in early fault detection and code line coverage, while also surpassing the random and direct CA generation strategies.
Conclusion:
The 
Top-down
 strategy not only improved the efficiency of test case generation but also enhanced the effectiveness of fault detection in the incremental testing scenarios.",Information and Software Technology,18 Mar 2025,8,"The research introduces a novel strategy for optimizing ICA generation in Combinatorial Testing, which can significantly improve efficiency and fault detection in testing scenarios."
https://www.sciencedirect.com/science/article/pii/S0950584924002155,"Using AI-based coding assistants in practice: State of affairs, perceptions, and ways forward",February 2025,Not Found,Agnia=Sergeyuk: agnia.sergeyuk@jetbrains.com; Yaroslav=Golubev: yaroslav.golubev@jetbrains.com; Timofey=Bryksin: timofey.bryksin@jetbrains.com; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
The last several years saw the emergence of 
AI assistants
 for code — multi-purpose AI-based helpers in software engineering. As they become omnipresent in all aspects of software development, it becomes critical to understand their usage patterns.
Objective:
We aim to better understand 
how specifically
 developers are using AI assistants, why they are 
not
 using them in certain parts of their development workflow, and what needs to be improved in the future.
Methods:
In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.
Results:
Our results provide a novel comparison of different stages where AI assistants are used that is both comprehensive and detailed. It highlights specific activities that developers find less enjoyable and want to delegate to an AI assistant, 
e.g.
, writing tests and natural-language artifacts. We also determine more granular stages where AI assistants are used, such as generating tests and generating docstrings, as well as less studied parts of the workflow, such as generating test data. Among the reasons for not using assistants, there are general aspects like trust and company policies, as well as more concrete issues like the lack of project-size context, which can be the focus of the future research.
Conclusion:
The provided analysis highlights stages of software development that developers want to delegate and that are already popular for using AI assistants, which can be a good focus for features aimed to help developers right now. The main reasons for not using AI assistants can serve as a guideline for future work.",Information and Software Technology,18 Mar 2025,6,"The study provides insights into the usage patterns of AI assistants in software development, highlighting areas where developers want to delegate tasks, but the practical impact on startups may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584924002301,A software product line approach for developing hybrid software systems,February 2025,Not Found,Samuel=Sepúlveda: samuel.sepulveda@ufrontera.cl; Ricardo=Pérez-Castillo: Not Found; Mario=Piattini: Not Found,"Abstract
Context:
Quantum computing is rapidly emerging as a transformative force in technology. We will soon increasingly encounter hybrid systems that combine quantum technology with classical software. Software engineering techniques will be required to manage the complexity of designing such systems and their reuse.
Objective:
This paper introduces preliminary ideas concerning developing quantum–classical software using a Software Product Line approach.
Method:
This approach addresses the mentioned challenges and provides a feature model and a whole process to manage variability during the design and development of hybrid quantum–classical software. The usage of this approach is illustrated and discussed using an example in the logistics domain.
Results:
The preliminary insights show the feasibility and suitability of applying the proposed approach to develop complex quantum–classical software.
Conclusions:
The main implication of this research is that it can help to manage complexity, maximize the reuse of classical and quantum software components, and deal with the highly changing technological stack in the current quantum computing field.",Information and Software Technology,18 Mar 2025,7,"The paper discusses the use of Software Product Line approach in developing quantum-classical software, offering insights into managing complexity and maximizing reuse, which can be valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001903,Improving DevOps team performance through context-capability coalignment: Towards a profile for public sector organizations,February 2025,"Software delivery team performance, DevOps, Team capabilities, Coalignment, Contextual factors, Action design research",Olivia H.=Plant: o.h.plant@utwente.nl; Adina=Aldea: a.i.aldea@utwente.nl; Jos=van Hillegersberg: j.vanhillegersberg@utwente.nl,"Abstract
Context
Many IT organizations turn to agile software delivery approaches such as DevOps in order to reduce the number of IT projects that are running behind schedule and above budget. However, the DevOps paradigm calls for an increased set of capabilities that need to be built and aligned with their context in order to ensure superior team performance.
Objective
This research aims to develop a context-capability coalignment profile for DevOps teams in public organizations. This profile and the corresponding design approach may serve as a model for other software production teams seeking to enhance their performance through improved coalignment. The resulting set of design principles places the traditional information systems theories of dynamic capabilities and contingency theory in a modern context.
Method
We adopt a longitudinal action design research approach centered around a DevOps team working in the IT department of a Dutch public organization. A mixed method design including scientific questionnaires, workshops, expert opinions and semi-structured interviews is employed to build and evaluate the profile.
Results
The resulting profile is characterized by technological complexity, a highly regulated environment, departmental interdependencies and high system relevance. The evaluation phase supports the validity of the artifact and suggests moderately improved coalignment of context and team capabilities after the research period, as well as a positive influence of coalignment on team performance.
Conclusion
It is contended that software teams in public organizations can benefit from improved coalignment between context and DevOps capabilities by following the presented approach. We argue that it is important to create a profile which is internally consistent and views coalignment as a continuous process in order to maximize the positive effect on team performance.",Information and Software Technology,18 Mar 2025,7,"The research focuses on developing a context-capability coalignment profile for DevOps teams in public organizations, providing a model for enhancing team performance through improved alignment, which can be beneficial for startups."
https://www.sciencedirect.com/science/article/pii/S0950584924001903,Improving DevOps team performance through context-capability coalignment: Towards a profile for public sector organizations,February 2025,"Software delivery team performance, DevOps, Team capabilities, Coalignment, Contextual factors, Action design research",Olivia H.=Plant: o.h.plant@utwente.nl; Adina=Aldea: a.i.aldea@utwente.nl; Jos=van Hillegersberg: j.vanhillegersberg@utwente.nl,"Abstract
Context
Many IT organizations turn to agile software delivery approaches such as DevOps in order to reduce the number of IT projects that are running behind schedule and above budget. However, the DevOps paradigm calls for an increased set of capabilities that need to be built and aligned with their context in order to ensure superior team performance.
Objective
This research aims to develop a context-capability coalignment profile for DevOps teams in public organizations. This profile and the corresponding design approach may serve as a model for other software production teams seeking to enhance their performance through improved coalignment. The resulting set of design principles places the traditional information systems theories of dynamic capabilities and contingency theory in a modern context.
Method
We adopt a longitudinal action design research approach centered around a DevOps team working in the IT department of a Dutch public organization. A mixed method design including scientific questionnaires, workshops, expert opinions and semi-structured interviews is employed to build and evaluate the profile.
Results
The resulting profile is characterized by technological complexity, a highly regulated environment, departmental interdependencies and high system relevance. The evaluation phase supports the validity of the artifact and suggests moderately improved coalignment of context and team capabilities after the research period, as well as a positive influence of coalignment on team performance.
Conclusion
It is contended that software teams in public organizations can benefit from improved coalignment between context and DevOps capabilities by following the presented approach. We argue that it is important to create a profile which is internally consistent and views coalignment as a continuous process in order to maximize the positive effect on team performance.",Information and Software Technology,18 Mar 2025,7,"Similar to abstract 24, this research aims to develop a context-capability coalignment profile for DevOps teams, which can benefit startups in improving team performance through alignment."
https://www.sciencedirect.com/science/article/pii/S0950584924002040,Causal reasoning in Software Quality Assurance: A systematic review,February 2025,"Causal reasoning, Causal discovery, Causal inference, Software quality",Luca=Giamattei: luca.giamattei@unina.it; Antonio=Guerriero: Not Found; Roberto=Pietrantuono: Not Found; Stefano=Russo: Not Found,"Abstract
Context:
Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, 
Causal Reasoning
 is gaining increasing interest as a methodology to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies.
Objective:
Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities.
Methods:
A systematic review of the scientific literature on causal reasoning for SQA. The study has found, classified, and analyzed 86 articles, according to established guidelines for software engineering secondary studies.
Results:
Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl’s graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favor their application are appearing at a fast pace — most of them after 2021.
Conclusions:
The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing out exciting possibilities for future research.",Information and Software Technology,18 Mar 2025,7,"The research on causal reasoning for SQA activities presents valuable insights and possibilities for future research, which could benefit early-stage ventures in improving software quality."
https://www.sciencedirect.com/science/article/pii/S0950584924002040,Causal reasoning in Software Quality Assurance: A systematic review,February 2025,"Causal reasoning, Causal discovery, Causal inference, Software quality",Luca=Giamattei: luca.giamattei@unina.it; Antonio=Guerriero: Not Found; Roberto=Pietrantuono: Not Found; Stefano=Russo: Not Found,"Abstract
Context:
Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, 
Causal Reasoning
 is gaining increasing interest as a methodology to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies.
Objective:
Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities.
Methods:
A systematic review of the scientific literature on causal reasoning for SQA. The study has found, classified, and analyzed 86 articles, according to established guidelines for software engineering secondary studies.
Results:
Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl’s graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favor their application are appearing at a fast pace — most of them after 2021.
Conclusions:
The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing out exciting possibilities for future research.",Information and Software Technology,18 Mar 2025,8,"Similar to abstract 26, this research on causal reasoning for SQA activities provides valuable insights and opportunities that can impact early-stage startups in enhancing software quality."
https://www.sciencedirect.com/science/article/pii/S0950584924002295,Evaluating the understandability and user acceptance of Attack-Defense Trees: Original experiment and replication,February 2025,"Security requirements, Attack-Defense Trees, Understandability evaluation, Users acceptance, Empirical user study, Internal replication, Method Evaluation Model",Giovanna=Broccia: giovanna.broccia@isti.cnr.it; Maurice H.=ter Beek: maurice.terbeek@isti.cnr.it; Alberto=Lluch Lafuente: albl@dtu.dk; Paola=Spoletini: pspoleti@kennesaw.edu; Alessandro=Fantechi: alessandro.fantechi@unifi.it; Alessio=Ferrari: alessio.ferrari@isti.cnr.it,"Abstract
Context:
Attack-Defense Trees (ADTs) are a graphical notation used to model and evaluate security requirements. ADTs are popular because they facilitate communication among different stakeholders involved in system security evaluation and are formal enough to be verified using methods like model checking. The understandability and user-friendliness of ADTs are claimed as key factors in their success, but these aspects, along with user acceptance, have not been evaluated empirically.
Objectives:
This paper presents an experiment with 25 subjects designed to assess the understandability and user acceptance of the ADT notation, along with an internal replication involving 49 subjects.
Methods:
The experiments adapt the Method Evaluation Model (MEM) to examine understandability variables (i.e., effectiveness and efficiency in using ADTs) and user acceptance variables (i.e., ease of use, usefulness, and intention to use). The MEM is also used to evaluate the relationships between these dimensions. In addition, a comparative analysis of the results of the two experiments is carried out.
Results:
With some minor differences, the outcomes of the two experiments are aligned. The results demonstrate that ADTs are well understood by participants, with values of understandability variables significantly above established thresholds. They are also highly appreciated, particularly for their ease of use. The results also show that users who are more effective in using the notation tend to evaluate it better in terms of usefulness.
Conclusion:
These studies provide empirical evidence supporting both the understandability and perceived acceptance of ADTs, thus encouraging further adoption of the notation in industrial contexts, and development of supporting tools.",Information and Software Technology,18 Mar 2025,9,"The study on Attack-Defense Trees (ADTs) provides empirical evidence supporting their understandability and acceptance, which could be beneficial for startups needing to evaluate security requirements."
https://www.sciencedirect.com/science/article/pii/S0950584924002295,Evaluating the understandability and user acceptance of Attack-Defense Trees: Original experiment and replication,February 2025,"Security requirements, Attack-Defense Trees, Understandability evaluation, Users acceptance, Empirical user study, Internal replication, Method Evaluation Model",Giovanna=Broccia: giovanna.broccia@isti.cnr.it; Maurice H.=ter Beek: maurice.terbeek@isti.cnr.it; Alberto=Lluch Lafuente: albl@dtu.dk; Paola=Spoletini: pspoleti@kennesaw.edu; Alessandro=Fantechi: alessandro.fantechi@unifi.it; Alessio=Ferrari: alessio.ferrari@isti.cnr.it,"Abstract
Context:
Attack-Defense Trees (ADTs) are a graphical notation used to model and evaluate security requirements. ADTs are popular because they facilitate communication among different stakeholders involved in system security evaluation and are formal enough to be verified using methods like model checking. The understandability and user-friendliness of ADTs are claimed as key factors in their success, but these aspects, along with user acceptance, have not been evaluated empirically.
Objectives:
This paper presents an experiment with 25 subjects designed to assess the understandability and user acceptance of the ADT notation, along with an internal replication involving 49 subjects.
Methods:
The experiments adapt the Method Evaluation Model (MEM) to examine understandability variables (i.e., effectiveness and efficiency in using ADTs) and user acceptance variables (i.e., ease of use, usefulness, and intention to use). The MEM is also used to evaluate the relationships between these dimensions. In addition, a comparative analysis of the results of the two experiments is carried out.
Results:
With some minor differences, the outcomes of the two experiments are aligned. The results demonstrate that ADTs are well understood by participants, with values of understandability variables significantly above established thresholds. They are also highly appreciated, particularly for their ease of use. The results also show that users who are more effective in using the notation tend to evaluate it better in terms of usefulness.
Conclusion:
These studies provide empirical evidence supporting both the understandability and perceived acceptance of ADTs, thus encouraging further adoption of the notation in industrial contexts, and development of supporting tools.",Information and Software Technology,18 Mar 2025,8,"Similar to abstract 28, this study on ADTs offers empirical evidence on their understandability and acceptance, which can be valuable for startups involved in system security evaluation."
https://www.sciencedirect.com/science/article/pii/S0950584924001757,Accessibility of low-code approaches: A systematic literature review,January 2025,"Systematic literature review, Low-code, Visual languages, Block-based programming, Accessibility",Hourieh=Khalajzadeh: hkhalajzadeh@deakin.edu.au; John=Grundy: john.grundy@monash.edu,"Abstract
Context:
Model-driven approaches are increasingly used in different domains, such as education, finance and app development, in order to involve non-developers in the software development process. Such tools are hugely dependent on visual elements and thus might not be accessible for users with specific challenges, 
e.g.
, visual impairments.
Objectives:
To locate and analyse existing literature on the accessibility of low-code approaches, their strengths and weaknesses and key directions for future research.
Methods:
We carried out a systematic literature review and searched through five leading databases for primary studies. We used both quantitative and qualitative methods for data synthesis.
Results:
After reviewing and filtering 918 located studies, and conducting both backward and forward snowballing, we identified 38 primary studies that were included in our analysis. We found most papers focusing on accessibility of visual languages and block-based programming.
Conclusion:
Limited work has been done on improving low code programming environment accessibility. The findings of this systematic literature review will assist researchers and developers in understanding the accessibility issues in low-code approaches and what has been done so far to develop accessible approaches.",Information and Software Technology,18 Mar 2025,5,"While the accessibility of low-code approaches is an important topic, the research findings may not have a direct practical impact on European early-stage ventures and startups compared to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001757,Accessibility of low-code approaches: A systematic literature review,January 2025,"Systematic literature review, Low-code, Visual languages, Block-based programming, Accessibility",Hourieh=Khalajzadeh: hkhalajzadeh@deakin.edu.au; John=Grundy: john.grundy@monash.edu,"Abstract
Context:
Model-driven approaches are increasingly used in different domains, such as education, finance and app development, in order to involve non-developers in the software development process. Such tools are hugely dependent on visual elements and thus might not be accessible for users with specific challenges, 
e.g.
, visual impairments.
Objectives:
To locate and analyse existing literature on the accessibility of low-code approaches, their strengths and weaknesses and key directions for future research.
Methods:
We carried out a systematic literature review and searched through five leading databases for primary studies. We used both quantitative and qualitative methods for data synthesis.
Results:
After reviewing and filtering 918 located studies, and conducting both backward and forward snowballing, we identified 38 primary studies that were included in our analysis. We found most papers focusing on accessibility of visual languages and block-based programming.
Conclusion:
Limited work has been done on improving low code programming environment accessibility. The findings of this systematic literature review will assist researchers and developers in understanding the accessibility issues in low-code approaches and what has been done so far to develop accessible approaches.",Information and Software Technology,18 Mar 2025,6,"The research on accessibility of low-code approaches is important as it addresses a gap in the field. However, the impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584924001782,Testing infrastructures to support mobile application testing: A systematic mapping study,January 2025,"Software testing, Software testing cloud, Software testing crowdsourcing, Testbed, Device farm, Mobile testing, Testing infrastructure, Infrastructure, Systematic mapping study, Mapping study",Pedro Henrique=Kuroishi: phk@ufscar.br; Ana Cristina Ramada=Paiva: apaiva@fe.up.pt; José Carlos=Maldonado: jcmaldon@icmc.usp.br; Auri Marcelo Rizzo=Vincenzi: auri@ufscar.br,"Abstract
Context:
Testing activities are essential for the quality assurance of mobile applications under development. Despite its importance, some studies show that testing is not widely applied in mobile applications. Some characteristics of mobile devices and a varied market of mobile devices with different operating system versions lead to a highly fragmented mobile ecosystem. Thus, researchers put some effort into proposing different solutions to optimize mobile application testing.
Objective:
The main goal of this paper is to provide a categorization and classification of existing testing infrastructures to support mobile application testing.
Methods:
To this aim, the study provides a Systematic Mapping Study of 27 existing primary studies.
Results:
We present a new classification and categorization of existing types of testing infrastructure, the types of supported devices and operating systems, whether the testing infrastructure is available for usage or experimentation, and supported testing types and applications.
Conclusion:
Our findings show a need for mobile testing infrastructures that support multiple phases of the testing process. Moreover, we showed a need for testing infrastructure for context-aware applications and support for both emulators and real devices. Finally, we pinpoint the need to make the research available to the community whenever possible.",Information and Software Technology,18 Mar 2025,7,"The categorization and classification of testing infrastructures for mobile applications can have practical value for startups in Europe, improving the quality of their products."
https://www.sciencedirect.com/science/article/pii/S0950584924001885,Mapping DevOps capabilities to the software life cycle: A systematic literature review,January 2025,"DevOps, Metrics, Performance, Adoption, Software development life cycle, Information system",Ricardo=Amaro: ricardo_amaro@iscte-iul.pt; Rúben=Pereira: ruben.filipe.pereira@iscte-iul.pt; Miguel Mira=da Silva: mms@tecnico.ulisboa.pt,"Abstract
Context:
Many IT organizations are looking towards DevOps to make their software development and delivery processes faster and more reliable, while DevOps revolutionized the industry by emphasizing collaboration between development and operations teams. Nonetheless, there still exist challenges in harmonizing cultural, technical, measurement and process capabilities for its successful adoption.
Objective:
To research improving DevOps adoption, this study explores DevOps Capabilities relevant to the Life Cycle Processes (LCPs) of the IEEE 2675-2021 DevOps standard. Aiming to provide valuable information on increasing efficiency and outcomes by mapping DevOps Capabilities in each phase of the LCPs. Whereas previous research identified and classified 37 DevOps Capabilities, this study aims to determine which capabilities can enhance each of the 30 phases of the LCPs.
Methods:
Out of 102 documents identified in the Systematic Literature Review (SLR), relations among DevOps Capabilities and LCPs have been synthesized and organized. An in-depth analysis of data was conducted over the connections across various categories. The mapping revealed how they relate in terms of their application and impact.
Results:
The SLR shows technical DevOps Capabilities and technical LCPs strongly correlated. DevOps measurement capabilities have a significant impact on agreement processes. Using an impact scale classification, the study identifies eight capabilities that have exceptional impact on LCPs and eleven capabilities that have a very high impact on the supply process, requirements definition, integration process, and validation process.
Conclusion:
The study demonstrates how DevOps Capabilities together with LCPs can improve software delivery, quality, and reliability. It presents a structured approach for improving processes, as well as evidence of DevOps integration in software development and maintenance. The findings help to assess DevOps Capabilities and LCP relations, which is expected to improve successful adoption. Future research should focus on researching practical cases of DevOps integration into LCPs, while overcoming adoption challenges.",Information and Software Technology,18 Mar 2025,9,The study on DevOps capabilities and their impact on software development processes is highly relevant for European early-stage ventures looking to improve efficiency and outcomes.
https://www.sciencedirect.com/science/article/pii/S0950584924001952,Migration of monolithic systems to microservices: A systematic mapping study,January 2025,"Microservices, Monolith, Migration, Architecture, Systematic Mapping Study",Ana=Martínez Saucedo: anmartinez@uade.edu.ar; Guillermo=Rodríguez: guillermo.rodriguez@isistan.unicen.edu.ar; Fabio=Gomes Rocha: gomesrocha@academico.ufs.br; Rodrigo Pereira dos=Santos: rps@uniriotec.br,"Abstract
Context:
The popularity of microservices architecture has grown due to its ability to address monolithic architecture issues, such as limited scalability, hard maintenance, and technological dependence. Nonetheless, the migration of monolith systems to microservices is complex. Therefore, methodologies and techniques are needed to facilitate migration and support practitioners and software architects.
Objective:
The objective of this study is to investigate cases of application migration, microservices identification techniques, tools used during migration, factors that promote migration, as well as issues and benefits of the migration.
Method:
We have conducted this SMS following the guidelines established by Kitchenham and Petersen. The research objective was defined using part of the Goal-Question-Metric model and the Population, Intervention, and Outcome criteria. From 1546 studies that were retrieved from the search execution, 114 were selected and analyzed to answer the research questions.
Results:
This SMS contributes with (i) a migration process proposal based on migration cases, (ii) a characterization of migration techniques based on different criteria, (iii) an analysis of tools to support migration, (iv) the identification of migration drivers, and (v) an exploration of migration issues as well as benefits.
Conclusion:
This SMS sheds light on the complexity and variability of migrating monolithic systems to microservices, as well as the limited number of migration tools. While scalability and maintenance drive migration, few studies assess them. Key challenges include microservices communication and database migration, with most research focusing primarily on monolith decomposition. Despite these difficulties, migration offers benefits, particularly in scalability and maintainability.",Information and Software Technology,18 Mar 2025,8,The investigation into migration from monolithic systems to microservices can provide valuable insights for startups in Europe facing scalability and maintenance challenges.
https://www.sciencedirect.com/science/article/pii/S0950584924001691,Enhancing logic-based testing with EvoDomain: A search-based domain-oriented test suite generation approach,January 2025,"Software testing, Domain-oriented test suite generation, Logic-based testing, Mutation analysis, MC/DC",Akram=Kalaee: a_kalaee@comp.iust.ac.ir; Saeed=Parsa: prasa@iust.ac.ir; Zahra=Mansouri: zahra_mansouri@comp.iust.ac.ir,"Abstract
Context
Effective software testing requires test adequacy criteria. MC/DC, a widely used logic-based testing criterion, struggles to detect domain errors caused by incorrect arithmetic operations. Domain errors occur when test requirement boundaries shift or tilt, causing unpredictable behavior and system crashes.
Objective
To address the inadequacy of MC/DC in detecting domain errors, we present EvoDomain, a search-based testing technique.
Method
EvoDomain uses a memetic algorithm combining genetic and hill-climbing algorithms, along with the DBSCAN clustering algorithm to select diversified boundary test data. The memetic algorithm is designed to efficiently enhance the search process for covering boundary test data. We compared EvoDomain with two logic-based testing approaches, a domain-oriented test suite generation approach, and random testing.
Results
Evaluations on 30 case studies show EvoDomain increases fault detection by 74.44% over MC/DC and 65.06% over RoRG. Additionally, EvoDomain improves support for different fault types by up to 68.89% for MC/DC and 66.33% for RoRG. Compared to COSMOS, which uses static analysis, EvoDomain improves the convergence effectiveness of identifying feasible subdomains by 32%. It offers high accuracy (0.99-1) and F1-score (0.99-1). EvoDomain finds the subdomains in less than 1/3 the time of Random search.
Conclusion
EvoDomain effectively generates domain-oriented test suites, enhancing the accuracy and effectiveness of fault detection.",Information and Software Technology,18 Mar 2025,7,The introduction of EvoDomain as a search-based testing technique with significant improvements over existing methods can benefit European startups aiming to enhance their software testing processes.
https://www.sciencedirect.com/science/article/pii/S0950584924001691,Enhancing logic-based testing with EvoDomain: A search-based domain-oriented test suite generation approach,January 2025,"Software testing, Domain-oriented test suite generation, Logic-based testing, Mutation analysis, MC/DC",Akram=Kalaee: a_kalaee@comp.iust.ac.ir; Saeed=Parsa: prasa@iust.ac.ir; Zahra=Mansouri: zahra_mansouri@comp.iust.ac.ir,"Abstract
Context
Effective software testing requires test adequacy criteria. MC/DC, a widely used logic-based testing criterion, struggles to detect domain errors caused by incorrect arithmetic operations. Domain errors occur when test requirement boundaries shift or tilt, causing unpredictable behavior and system crashes.
Objective
To address the inadequacy of MC/DC in detecting domain errors, we present EvoDomain, a search-based testing technique.
Method
EvoDomain uses a memetic algorithm combining genetic and hill-climbing algorithms, along with the DBSCAN clustering algorithm to select diversified boundary test data. The memetic algorithm is designed to efficiently enhance the search process for covering boundary test data. We compared EvoDomain with two logic-based testing approaches, a domain-oriented test suite generation approach, and random testing.
Results
Evaluations on 30 case studies show EvoDomain increases fault detection by 74.44% over MC/DC and 65.06% over RoRG. Additionally, EvoDomain improves support for different fault types by up to 68.89% for MC/DC and 66.33% for RoRG. Compared to COSMOS, which uses static analysis, EvoDomain improves the convergence effectiveness of identifying feasible subdomains by 32%. It offers high accuracy (0.99-1) and F1-score (0.99-1). EvoDomain finds the subdomains in less than 1/3 the time of Random search.
Conclusion
EvoDomain effectively generates domain-oriented test suites, enhancing the accuracy and effectiveness of fault detection.",Information and Software Technology,18 Mar 2025,9,"The EvoDomain technique significantly improves fault detection in logic-based testing approaches, addressing the inadequacy of current methods. The high fault detection increase and efficiency improvement make it highly valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492400171X,Graph-based explainable vulnerability prediction,January 2025,"Graph neural network, Explanation, Vulnerability",Hong Quy=Nguyen: nguyen.hquy@gmail.com; Thong=Hoang: james.hoang@data61.csiro.au; Hoa Khanh=Dam: hoa@uow.edu.au; Aditya=Ghose: aditya@uow.edu.au,"Abstract
Significant increases in cyberattacks worldwide have threatened the security of organizations, businesses, and individuals. Cyberattacks exploit vulnerabilities in software systems. Recent work has leveraged powerful and complex models, such as deep neural networks, to improve the predictive performance of vulnerability detection models. However, these models are often regarded as “black box” models, making it challenging for software practitioners to understand and interpret their predictions. This lack of explainability has resulted in a reluctance to adopt or deploy these vulnerability prediction models in industry applications. This paper proposes a novel approach, 
G
enetic 
A
lgorithm-based 
Vul
nerability Prediction 
Explainer
, (herein GAVulExplainer), which generates explanations for vulnerability prediction models based on graph neural networks. GAVulExplainer leverages genetic algorithms to construct a subgraph explanation that represents the crucial factor contributing to the vulnerability. Experimental results show that our proposed approach outperforms baselines in providing concrete reasons for a vulnerability prediction.",Information and Software Technology,18 Mar 2025,6,"The GAVulExplainer provides valuable insights into vulnerability prediction models, which can enhance security measures for startups. Although the improvements are noteworthy, the practical application may be limited by the complexity of the approach."
https://www.sciencedirect.com/science/article/pii/S0950584924001861,A dual graph neural networks model using sequence embedding as graph nodes for vulnerability detection,January 2025,"Vulnerability detection, Graph neural network, Sequence embedding",Miaogui=Ling: Not Found; Mingwei=Tang: tang4415@126.com; Deng=Bian: Not Found; Shixuan=Lv: Not Found; Qi=Tang: Not Found,"Abstract
Context:
Detecting critical to ensure software system security. The traditional static vulnerability detection methods are limited by staff expertise and perform poorly with today’s increasingly complex software systems. Researchers have successfully applied the techniques used in NLP to vulnerability detection as deep learning has developed. The existing deep learning-based vulnerability detection models can be divided into sequence-based and graph-based categories. Sequence-based embedding models cannot use structured information embedded in the code, and graph-based embedding models lack effective node representations.
Objective:
To solve these problems, we propose a deep learning-based method, DGVD (Double Graph Neural Network for Vulnerability Detection).
Methods:
We use the sequential neural network approach to extract local semantic features of the code as nodes embedded in the control flow graph. First, we propose a dual graph neural network module (DualGNN) that consists of GCN and GAT. The altered module utilizes two different graph neural networks to obtain the global structural information of the control flow and the relationship between the nodes and fuses the two. Second, we propose a convolution-based feature enhancement module (TC-FE) that uses different convolution kernels of different sizes to capture information at different scales so that subsequent readout layers can better aggregate node information.
Results:
Experiments demonstrate that DGVD outperforms existing models, obtaining 64.23% vulnerability detection accuracy on CodeXGLUE’s real benchmark dataset.
Conclusion:
The proposed DGVD achieves better performance than the state-of-the-art DGVD has a more effective source code feature extraction capability on real-world datasets.",Information and Software Technology,18 Mar 2025,7,"The DGVD method improves vulnerability detection accuracy and feature extraction capabilities, which are crucial for startups dealing with software security. The results show promising advancements in this area."
https://www.sciencedirect.com/science/article/pii/S0950584924001769,Specialized model initialization and architecture optimization for few-shot code search,January 2025,"Code search, Code understanding, Information retrieval, Neural architecture search, AI for software engineering",Fan=Zhang: fanzhang@hnu.edu.cn; Qiang=Wu: wuqiang@hnu.edu.cn; Manman=Peng: pengmanman@hnu.edu.cn; Yuanyuan=Shen: shenyuanyuan@nuc.edu.cn,"Abstract
Context:
Code search aims to find relevant code snippets from a codebase given a natural language query. It not only boosts developer efficiency but also improves the performance of tasks such as code generation and program repair, thus becoming one of the crucial tasks in software engineering.
Objective:
However, recent works are mainly designed for mainstream programming languages with abundant training data. We aim to address the challenges of code search for domain-specific programming languages with limited training data by proposing a novel two-stage, few-shot code search framework named SMIAO.
Method:
SMIAO includes a specialized model initialization and an architecture optimization stage. In the first stage, we first quantitatively identify a mainstream programming language’s dataset that is semantically closest to a target few-shot programming language. Then, we enrich the dataset with hard samples and train an Adapter-GraphCodeBERT model to obtain well-initialized parameters. In the second stage, we first design a search space for the initialized Adapter-GraphCodeBERT model. Then, we employ neural architecture search to optimize the Adapter modules’ positions and quantities in the GraphCodeBERT layers, tailoring for real-world few-shot code search tasks.
Results:
We conduct experiments on a publicly available dataset to demonstrate the effectiveness and rationality of SMIAO. The experimental results show that SMIAO outperforms other state-of-the-art baselines.
Conclusion:
Using mainstream languages’ datasets to initialize Adapter-GraphCodeBERT models, followed by adjusting the quantities and positions of Adapter modules within the GraphCodeBERT layers by neural architecture search, can effectively improve the performance of few-shot code search tasks.",Information and Software Technology,18 Mar 2025,5,"SMIAO tackles the challenges of code search for domain-specific programming languages, which can be beneficial for early-stage ventures with limited resources. However, the impact may be more significant for established companies."
https://www.sciencedirect.com/science/article/pii/S0950584924001368,Extraction and empirical evaluation of GUI-level invariants as GUI Oracles in mobile app testing,January 2025,"Graphical user interface (GUI) testing, Test oracle, GUI invariants, Mobile app testing, Empirical study",Ali Asghar=Yarifard: yarifard@mail.um.ac.ir; Saeed=Araban: araban@um.ac.ir; Samad=Paydar: s-paydar@um.ac.ir; Vahid=Garousi: v.garousi@qub.ac.uk; Maurizio=Morisio: maurizio.morisio@polito.it; Riccardo=Coppola: riccardo.coppola@polito.it,"Abstract
Context
Mobile apps (software) are used in almost all aspects of daily life by billions of people. Given the widespread use of mobile apps in various domains, the demand for systematic testing of their Graphical User Interfaces (GUI) is crucial. Despite the significant advances in automated mobile app testing over the last decade, certain challenges remain, most notably the app-specific GUI test-oracle problem, which can significantly hinder the effective detection of defects in mobile apps. In this study, we introduce the use of GUI-level invariants, referred to as GUI invariants, as app-specific GUI oracles in GUI test cases to address this challenge.
Methods
We propose a semi-automatic solution to extract GUI invariants and use them as app-specific GUI oracles in test cases. We use the mutation testing technique to evaluate the (fault detection) effectiveness of the GUI oracles used. In addition, we evaluate their quality aspects, namely correctness, understandability, and compatibility, from the perspective of human experts using a questionnaire survey.
Results
The empirical results show that the GUI oracles used are effective and helpful, as they improved the fault-detection effectiveness of the empirical test suites ranging from 18% to 32%. These results also highlight the efficacy of GUI oracles used in identifying various defects, including crashing and non-crashing functional issues, and surpassing the performance of existing tools in fault-detection rates. Additionally, the questionnaire survey outcomes indicate that the GUI oracles used are correct, understandable, and compatible.
Conclusions
Based on the empirical results, we can conclude that using GUI invariants as GUI oracles can be useful and effective in mobile app testing.",Information and Software Technology,18 Mar 2025,8,"The use of GUI invariants as oracles in mobile app testing shows significant improvements in fault detection effectiveness. This can be particularly valuable for startups focusing on mobile app development, addressing a crucial testing challenge."
https://www.sciencedirect.com/science/article/pii/S0950584924001733,Software solutions for newcomers’ onboarding in software projects: A systematic literature review,January 2025,"Systematic literature review, Software projects, Open source software, Onboarding, Turnover, Tool, Newcomers, Novices",Italo=Santos: italo_santos@nau.edu; Katia Romero=Felizardo: katiascannavino@utfpr.edu.br; Igor=Steinmacher: igor.steinmacher@nau.edu; Marco A.=Gerosa: marco.gerosa@nau.edu,"Abstract
Context:
Newcomers joining an unfamiliar software project face numerous barriers; therefore, effective onboarding is essential to help them engage with the team and develop the behaviors, attitudes, and skills needed to excel in their roles. However, onboarding can be a lengthy, costly, and error-prone process. Software solutions can help mitigate these barriers and streamline the process without overloading senior members.
Objective:
This study aims to identify the state-of-the-art software solutions for onboarding newcomers.
Methods:
We conducted a systematic literature review (SLR) to answer six research questions.
Results:
We analyzed 32 studies about software solutions for onboarding newcomers and yielded several key findings: (1) a range of strategies exists, with recommendation systems being the most prevalent; (2) most solutions are web-based; (3) solutions target a variety of onboarding aspects, with a focus on process; (4) many onboarding barriers remain unaddressed by existing solutions; (5) laboratory experiments are the most commonly used method for evaluating these solutions; and (6) diversity and inclusion aspects primarily address experience level.
Conclusion:
We shed light on current technological support and identify research opportunities to develop more inclusive software solutions for onboarding. These insights may also guide practitioners in refining existing platforms and onboarding programs to promote smoother integration of newcomers into software projects.",Information and Software Technology,18 Mar 2025,5,"While the study sheds light on technological support for onboarding newcomers in software projects, the impact may be limited to a specific niche within the software industry. The findings may provide insights for practitioners but do not significantly impact a wide range of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001770,DeVAIC: A tool for security assessment of AI-generated code,January 2025,"Static code analysis, Vulnerability detection, AI-code generators, Python",Domenico=Cotroneo: cotroneo@unina.it; Roberta=De Luca: roberta.deluca2@unina.it; Pietro=Liguori: pietro.liguori@unina.it,"Abstract
Context:
AI code generators are revolutionizing code writing and software development, but their training on large datasets, including potentially untrusted source code, raises security concerns. Furthermore, these generators can produce incomplete code snippets that are challenging to evaluate using current solutions.
Objective:
This research work introduces 
DeVAIC
 (Detection of Vulnerabilities in AI-generated Code), a tool to evaluate the security of AI-generated Python code, which overcomes the challenge of examining incomplete code.
Methods:
We followed a methodological approach that involved gathering vulnerable samples, extracting implementation patterns, and creating regular expressions to develop the proposed tool. The implementation of 
DeVAIC
 includes a set of detection rules based on regular expressions that cover 35 Common Weakness Enumerations (CWEs) falling under the OWASP Top 10 vulnerability categories.
Results:
We utilized four popular AI models to generate Python code, which we then used as a foundation to evaluate the effectiveness of our tool. 
DeVAIC
 demonstrated a statistically significant difference in its ability to detect security vulnerabilities compared to the state-of-the-art solutions, showing an 
F
1
 Score and Accuracy of 94% while maintaining a low computational cost of 0.14 s per code snippet, on average.
Conclusions:
The proposed tool provides a lightweight and efficient solution for vulnerability detection even on incomplete code.",Information and Software Technology,18 Mar 2025,9,"The research work on DeVAIC introduces a tool that addresses security concerns in AI-generated code, showcasing significant effectiveness with a high F1 Score and low computational cost. This tool has the potential to benefit a wide range of European startups by providing enhanced security measures during code development."
https://www.sciencedirect.com/science/article/pii/S0950584924001940,An intent-enhanced feedback extension model for code search,January 2025,"Code search, Query expansion, Intentional enhancement, Feedback mechanisms",Haize=Hu: HHZ@gxnu.edu.cn; Mengge=Fang: fmg@stu.gxnu.edu.cn; Jianxun=Liu: Not Found,"Abstract
Context:
Queries and descriptions used for code search not only differ in semantics and syntax, but also in structural features. Therefore, solving the differences between them is of great significance to the study of code search.
Objective:
This study focuses on the improvement of code search accuracy by exploring the expansion of query statements during the search process.
Methods:
To address the disparities between description and query, the paper introduces the Intentional Enhancement and Feedback (QEIEF) query expansion model. QEIEF leverages the written description provided by developers as the source for query expansion. Furthermore, QEIEF incorporates theQEIEF method to enhance the semantic representation of the query. This involves utilizing the query output as the target for intent enhancement and integrating it back into the query.
Results:
To assess the effectiveness of the proposedQEIEF in code search tasks, we conducted experiments using two base models (DeepCS and UNIF) along withQEIEF, as well as baseline models (WordNet and BM25). The experimental results indicate that QEIEF outperforms the baseline models in terms of query expansion accuracy and code search results.
Conclusion:
QEIEF not only enhances the accuracy of query expansion but also substantially improves code search performance. The source code and data associated with our study can be accessed publicly at: The address of our new code and data is 
https://github.com/xiangzheng666/IST-IEFE
.",Information and Software Technology,18 Mar 2025,7,"The QEIEF query expansion model proposed in this study aims to improve code search accuracy, showing promising results in comparison to baseline models. While the impact on European early-stage ventures may not be revolutionary, the enhancement in code search performance could benefit startups in streamlining their development processes."
https://www.sciencedirect.com/science/article/pii/S0950584924001964,DFL: A DOM sample generation oriented fuzzing framework for browser rendering engines,January 2025,"DOM, Browser security, Gray-box fuzzing, Coverage guided fuzzing, Browser rendering engine",Guoyun=Duan: dguoyun@hnu.edu.cn; Hai=Zhao: ha1vk@foxmail.com; Minjie=Cai: caiminjie@hnu.edu.cn; Jianhua=Sun: jhsun@hnu.edu.cn; Hao=Chen: haochen@hnu.edu.cn,"Abstract
The security of web browsers, being fundamental to Internet access infrastructure, has garnered significant attention. Current approaches to identify browser vulnerabilities predominantly rely on code auditing and componentized unit testing. Fuzzing has emerged as an efficient technique for vulnerability discovery. However, adapting this method to browser security testing poses considerable challenges. Recent endeavors in browser vulnerability discovery primarily concentrate on the parsing engine, with limited solutions addressing the rendering engine. Moreover, coverage-guided mutation, a critical aspect, is not prevalent in existing fuzzing frameworks. In this paper, we present a coverage-guided fuzzing framework of DFL, which builds on Freedom and AFL to re-engineer various text generators based on DOM syntax and optimize the efficiency of sample generation. Additionally, serialization and deserialisation methods are developed for the implementation of generator text mutations and the seamless conversion between binary samples and the source DOM tree. When compared with three established DOM fuzzing frameworks in the latest Chromium kernel, DFL has demonstrated an ability to uncover 1.5–3 times more vulnerabilities within a short timeframe. Our research identifies potential avenues for further exploration in browser rendering engine security, specifically focusing on sample generation and path direction.",Information and Software Technology,18 Mar 2025,10,The coverage-guided fuzzing framework of DFL presented in this paper demonstrates a significant improvement in vulnerability discovery within browser rendering engines. The research identifies potential avenues for further exploration in browser security and offers tangible benefits to early-stage ventures by enhancing their security testing processes.
https://www.sciencedirect.com/science/article/pii/S0950584924001873,ENZZ: Effective N-gram coverage assisted fuzzing with nearest neighboring branch estimation,January 2025,"System security, Software testing, Graybox fuzzing, Vulnerability detection, N-gram",Xi=Peng: Not Found; Peng=Jia: pengjia@scu.edu.cn; Ximing=Fan: Not Found; Jiayong=Liu: liujiayong@scu.edu.cn,"Abstract
Fuzzing is a highly effective approach for identifying software bugs and vulnerabilities. Among the various techniques employed, coverage-guided fuzzing stands out as particularly valuable, relying on tracing code coverage information. N-gram coverage is a coverage metric in gray-box fuzz testing, where the value of N determines the sensitivity of the coverage. Block and edge coverage can be represented as 0-gram and 1-gram, respectively. The value of N can range from 0 to infinity. However, the specific methodology for selecting the appropriate value of N is still an area yet to be explored.
This paper proposes an estimation method based on the nearest branch. We initially explained the role of N-gram in the execution paths of programs and elucidated the objective of selecting the value of N, which aims to cover the closest neighboring branches. Subsequently, based on this objective, we proposed a method for calculating N based on the closest neighboring branch and estimated N at the function level. Finally, in this paper, we designed a scheduling mechanism using Adversarial Multi-Armed Bandit model that automatically selects either the seeds generated by N-gram or the original queue seeds for fuzz testing.
We implement our approach in ENZZ based on AFL and compare it with other N-gram coverage fuzzers and the state-of-the-art path coverage-assisted fuzzer PathAFL. We find that ENZZ outperforms other N-gram fuzzers and PathAFL by achieving an average improvement of 5.57% and 4.38% on edge coverage, and it improves the efficiency of path-to-edge discovery by 31.5% and 26.1%, respectively, on 12 Google FuzzBench programs. It also finds more bugs than other N-gram fuzzers and three more real-world bugs than PathAFL.",Information and Software Technology,18 Mar 2025,8,"The proposal of an estimation method for selecting the appropriate value of N in N-gram coverage fuzz testing presents a valuable contribution to the field. The implementation in ENZZ and the comparison with other fuzzers demonstrate superior performance, which can have a positive impact on European startups by improving their bug identification processes."
https://www.sciencedirect.com/science/article/pii/S0950584924001897,A search-and-fill strategy to code generation for complex software requirements,January 2025,"Low-Code Development, Complex software requirements, Search-and-Fill strategy, Code generation, Natural language processing",Yukun=Dong: dongyk@upc.edu.cn; Lingjie=Kong: Not Found; Lulu=Zhang: Not Found; Shuqi=Wang: Not Found; Xiaoshan=Liu: Not Found; Shuai=Liu: Not Found; Mingcheng=Chen: Not Found,"Abstract
Context:
The realm of software development has seen significant transformations with the rise of Low-Code Development (LCD) and the integration of Artificial Intelligence (AI), particularly large language models, into coding practices. The proliferation of open-source software also offers vast resources for developers.
Objective:
We aim to combine the benefits of modifying retrieved code with the use of an extensive code repository to tackle the challenges of complex control structures and multifunctional requirements in software development.
Method:
Our study introduces a Search-and-Fill strategy that utilizes natural language processing (NLP) to dissect complex software requirements. It extracts control structures and identifies atomic function points. By leveraging large-scale pre-trained models, the strategy searches for these elements to fill in the automatically transformed program structures derived from descriptions of control structures. This process generates a code snippet that includes program control structures and the implementations of various function points, thereby facilitating both code reuse and efficient development.
Results:
We have validated the effectiveness of our strategy in generating code snippets. For natural language requirements involving multifunctional complex structures, we constructed two datasets: the Basic Complex Requirements Dataset (BCRD) and the Advanced Complex Requirements Dataset (ACRD). These datasets are based on natural language descriptions and Python code that were randomly extracted and combined. For the code snippets to be generated, we achieved the best results with the ACRD dataset, with BLEU-4 scores reaching up to 0.6326 and TEDS scores peaking at 0.7807.
Conclusion:
The Search-and-Fill strategy successfully generates a comprehensive code snippets, integrating essential control structures and functions to streamline the development process. Experimental results substantiate our strategy’s efficacy in optimizing code reuse by effectively integrating preprocessing and selection optimization approach. Future research will focus on enhancing the recognition of complex software requirements and further refining the code snippets.",Information and Software Technology,18 Mar 2025,8,"The abstract introduces a comprehensive strategy for generating code snippets using natural language processing, large-scale pre-trained models, and datasets. The approach aims to optimize code reuse and facilitate efficient software development, which could potentially impact early-stage ventures by saving time and resources."
https://www.sciencedirect.com/science/article/pii/S0950584924001988,What helps Agile remote teams to be successful in developing software? Empirical evidence,January 2025,"Agile software development project management, Critical success factors, Remote work, TOE framework",Marta=Adzgauskaite: Not Found; Carlos=Tam: carlosvai@novaims.unl.pt; Ricardo=Martins: Not Found,"Abstract
Software development firms have specific goals but today's dynamic business environment, especially regarding the use of remote teams, presents great challenges due to uncertainties and multiple risks. This study investigates the facilitators of the success of Agile software development projects delivered by remote teams. We employ a conceptual research model founded on the technology-organization-environment (TOE) framework. The study contributes to the literature by exploring how remote teams affect the success of Agile software development projects. Partial least squares structural equation modeling (PLS-SEM) analysis of the data collected from 198 IT professionals revealed that perceived pressure from government, job performance, and team satisfaction are significant in explaining these projects’ success.",Information and Software Technology,18 Mar 2025,5,"The study investigates the success factors of Agile software development projects delivered by remote teams, which is relevant in today's dynamic business environment. While the findings contribute to the literature, the practical impact on early-stage ventures might be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002015,Deciphering refactoring branch dynamics in modern code review: An empirical study on Qt,January 2025,"Refactoring, Code review, Developer perception, Software quality",Eman Abdullah=AlOmar: ealomar@stevens.edu,"Abstract
Context:
Modern code review is a widely employed technique in both industrial and open-source projects, serving to enhance software quality, share knowledge, and ensure compliance with coding standards and guidelines. While code review is extensively studied for its general challenges, best practices, outcomes, and socio-technical aspects, little attention has been paid to how refactoring is reviewed and what developers prioritize when reviewing refactored code in the ‘Refactor’ branch.
Objective:
The goal is to understand the review process for refactoring changes in the ‘Refactor’ branch and to identify what developers care about when reviewing code in this branch.
Method:
In this study, we present a quantitative and qualitative examination to understand the main criteria developers use to decide whether to accept or reject refactored code submissions and identify the challenges inherent in this process.
Results:
Analyzing 2154 refactoring and non-refactoring reviews across Qt open-source projects, we find that reviews involving refactoring from the ‘Refactor’ branch take significantly less time to resolve in terms of code review efforts. Additionally, documentation of developer intent is notably sparse within the ‘Refactor’ branch compared to other branches. Furthermore, through thematic analysis of a substantial sample of refactoring code review discussions, we construct a comprehensive taxonomy consisting of 12 refactoring review criteria.
Conclusion:
Our findings underscore the importance of developing precise and efficient tools and techniques to aid developers in the review process amidst refactorings.",Information and Software Technology,18 Mar 2025,6,"The abstract delves into the review process for refactoring changes in software development, providing insights into the challenges developers face. While the findings are valuable for improving the review process, the direct impact on early-stage ventures might not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002027,Better together: Automated app review analysis with deep multi-task learning,January 2025,"App review, Bug classification, Feature extraction, Deep multi-task learning",Yawen=Wang: yawen2018@iscas.ac.cn; Junjie=Wang: junjie@iscas.ac.cn; Hongyu=Zhang: hyzhang@cqu.edu.cn; Xuran=Ming: xuran2020@iscas.ac.cn; Qing=Wang: wq@iscas.ac.cn,"Abstract
Context:
User reviews of mobile apps provide an important communication channel between developers and users. Existing approaches to automated app review analysis mainly focus on one task (e.g., bug classification task, information extraction task, etc.) at a time, and are often constrained by the manually defined patterns and the ignorance of the correlations among the tasks. Recently, multi-task learning (MTL) has been successfully applied in many scenarios, with the potential to address the limitations associated with app review mining tasks.
Objective:
In this paper, we propose 
MABLE
, a deep MTL-based and semantic-aware approach, to improve app review analysis by exploiting task correlations.
Methods:
MABLE
 jointly identifies the types of involved bugs reported in the review and extracts the fine-grained features where bugs might occur. It consists of three main phases: (1) data preparation phase, which prepares data to allow data sharing beyond single task learning; (2) model construction phase, which employs a BERT model as the shared representation layer to capture the semantic meanings of reviews, and task-specific layers to model two tasks in parallel; (3) model training phase, which enables eavesdropping by shared loss function between the two related tasks.
Results:
Evaluation results on six apps show that 
MABLE
 outperforms ten commonly-used and state-of-the-art baselines, with the precision of 79.76% and the recall of 79.24% for classifying bugs, and the precision of 79.83% and the recall of 80.33% for extracting problematic app features. The MTL mechanism improves the F-measure of two tasks by 3.80% and 4.63%, respectively.
Conclusion:
The proposed approach provides a novel and effective way to jointly learn two related review analysis tasks, and sheds light on exploring other review mining tasks.",Information and Software Technology,18 Mar 2025,7,"The proposed approach in this abstract, MABLE, offers a novel way to improve app review analysis through multi-task learning. The high precision and recall rates in classifying bugs and extracting app features could benefit startups by enhancing the quality of their mobile apps."
https://www.sciencedirect.com/science/article/pii/S0950584924001927,Architecture decisions in quantum software systems: An empirical study on Stack Exchange and GitHub,January 2025,"Architecture decision, Quantum software system, Stack Exchange, GitHub, Empirical study",Mst Shamima=Aktar: shamima@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Muhammad=Waseem: muhammad.m.waseem@jyu.fi; Amjed=Tahir: a.tahir@massey.ac.nz; Aakash=Ahmad: a.ahmad13@lancaster.ac.uk; Beiqi=Zhang: zhangbeiqi@whu.edu.cn; Zengyang=Li: zengyangli@ccnu.edu.cn,"Abstract
Context:
Quantum computing provides a new dimension in computation, utilizing the principles of quantum mechanics to potentially solve complex problems that are currently intractable for classical computers. However, little research has been conducted about the architecture decisions made in quantum software development, which have a significant influence on the functionality, performance, scalability, and reliability of these systems.
Objective:
The study aims to empirically investigate and analyze architecture decisions made during the development of quantum software systems, identifying prevalent challenges and limitations by using the posts and issues from Stack Exchange and GitHub.
Methods:
We used a qualitative approach to analyze the obtained data from Stack Exchange Sites and GitHub projects — two prominent platforms in the software development community. Specifically, we collected data from 385 issues (from 87 GitHub projects) and 70 posts (from 3 Stack Exchange sites) related to architecture decisions in quantum software development.
Results:
The results show that in quantum software development (1) architecture decisions are articulated in six linguistic patterns, the most common of which are 
Solution Proposal
 and 
Information Giving
, (2) the two major categories of architectural decisions are 
Implementation Decision
 and 
Technology Decision
, (3) 
Software Development Tools
 are the most common application domain among the twenty application domains identified, (4) 
Maintainability
 is the most frequently considered quality attribute, and (5) 
Design Issues
 and 
High Error Rates
 are the major limitations and challenges that practitioners face when making architecture decisions in quantum software development.
Conclusions:
Our results show that the limitations and challenges encountered in architecture decision-making during the development of quantum software systems are strongly linked to the particular features (e.g., quantum entanglement, superposition, and decoherence) of those systems. These issues mostly pertain to technical aspects and need appropriate measures to address them effectively.",Information and Software Technology,18 Mar 2025,4,"The study investigates architecture decisions in quantum software development, shedding light on challenges and limitations. While the insights are relevant for the research community, the practical impact on European early-stage ventures might be limited due to the specialized nature of quantum computing."
https://www.sciencedirect.com/science/article/pii/S0950584924001927,Architecture decisions in quantum software systems: An empirical study on Stack Exchange and GitHub,January 2025,"Architecture decision, Quantum software system, Stack Exchange, GitHub, Empirical study",Mst Shamima=Aktar: shamima@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Muhammad=Waseem: muhammad.m.waseem@jyu.fi; Amjed=Tahir: a.tahir@massey.ac.nz; Aakash=Ahmad: a.ahmad13@lancaster.ac.uk; Beiqi=Zhang: zhangbeiqi@whu.edu.cn; Zengyang=Li: zengyangli@ccnu.edu.cn,"Abstract
Context:
Quantum computing provides a new dimension in computation, utilizing the principles of quantum mechanics to potentially solve complex problems that are currently intractable for classical computers. However, little research has been conducted about the architecture decisions made in quantum software development, which have a significant influence on the functionality, performance, scalability, and reliability of these systems.
Objective:
The study aims to empirically investigate and analyze architecture decisions made during the development of quantum software systems, identifying prevalent challenges and limitations by using the posts and issues from Stack Exchange and GitHub.
Methods:
We used a qualitative approach to analyze the obtained data from Stack Exchange Sites and GitHub projects — two prominent platforms in the software development community. Specifically, we collected data from 385 issues (from 87 GitHub projects) and 70 posts (from 3 Stack Exchange sites) related to architecture decisions in quantum software development.
Results:
The results show that in quantum software development (1) architecture decisions are articulated in six linguistic patterns, the most common of which are 
Solution Proposal
 and 
Information Giving
, (2) the two major categories of architectural decisions are 
Implementation Decision
 and 
Technology Decision
, (3) 
Software Development Tools
 are the most common application domain among the twenty application domains identified, (4) 
Maintainability
 is the most frequently considered quality attribute, and (5) 
Design Issues
 and 
High Error Rates
 are the major limitations and challenges that practitioners face when making architecture decisions in quantum software development.
Conclusions:
Our results show that the limitations and challenges encountered in architecture decision-making during the development of quantum software systems are strongly linked to the particular features (e.g., quantum entanglement, superposition, and decoherence) of those systems. These issues mostly pertain to technical aspects and need appropriate measures to address them effectively.",Information and Software Technology,18 Mar 2025,8,"The research on architecture decisions in quantum software development provides valuable insights into challenges and limitations faced by practitioners, which can impact the functionality and reliability of systems."
https://www.sciencedirect.com/science/article/pii/S095058492400199X,Strategic digital product management: Nine approaches,January 2025,"Strategic digital product management, DevOps, Data, Artificial intelligence, Digital ecosystems, Digitalization, Digital transformation",Helena Holmström=Olsson: helena.holmstrom.olsson@mau.se; Jan=Bosch: Not Found,"Abstract
Context:
The role of product management (PM) is key for building, implementing and managing software-intensive systems. Whereas engineering is concerned with how to build systems, PM is concerned with ‘what’ to build and ‘why’ we should build the product. The role of PM is recognized as critical for the success of any product. However, few studies explore how the role of PM is changing due to recent trends that come with digitalization and digital transformation.
Objectives:
Although there is prominent research on PM, few studies explore how this role is changing due to the digital transformation of the software-intensive industry. In this paper, we study how trends such as DevOps and short feedback loops, data and artificial intelligence (AI), as well as the emergence of digital ecosystems, are changing current product management practices.
Methods:
This study employs a qualitative approach using multi-case study research as the method. For our research, we selected five case companies in the software-intensive systems domain. Through workshop sessions, frequent meetings and interviews, we explore how DevOps and short feedback loops, data and artificial intelligence (AI), and digital ecosystems challenge current PM practices.
Results:
Our study yielded an in-depth understanding of how digital transformation of the software-intensive systems industry is changing current PM practices. We present empirical results from workshops and from interviews in which case company representatives share their insights on how software, data and AI impact current PM practices. Based on these results, we present a framework organized along two dimensions, i.e. a certainty dimension and an approach dimension. The framework helps structure the approaches product managers can employ to select and prioritize development of new functionality.
Contributions:
The contribution of this paper is a framework for ‘Strategic Digital Product Management’ (SDPM). The framework outlines nine approaches that product managers can employ to maximize the return on investment (RoI) of R&D using new digital technologies.",Information and Software Technology,18 Mar 2025,9,"The study on how digital transformation trends impact product management practices in the software-intensive industry offers practical frameworks to maximize return on investment, aligning with the current industry needs."
https://www.sciencedirect.com/science/article/pii/S095058492400199X,Strategic digital product management: Nine approaches,January 2025,"Strategic digital product management, DevOps, Data, Artificial intelligence, Digital ecosystems, Digitalization, Digital transformation",Helena Holmström=Olsson: helena.holmstrom.olsson@mau.se; Jan=Bosch: Not Found,"Abstract
Context:
The role of product management (PM) is key for building, implementing and managing software-intensive systems. Whereas engineering is concerned with how to build systems, PM is concerned with ‘what’ to build and ‘why’ we should build the product. The role of PM is recognized as critical for the success of any product. However, few studies explore how the role of PM is changing due to recent trends that come with digitalization and digital transformation.
Objectives:
Although there is prominent research on PM, few studies explore how this role is changing due to the digital transformation of the software-intensive industry. In this paper, we study how trends such as DevOps and short feedback loops, data and artificial intelligence (AI), as well as the emergence of digital ecosystems, are changing current product management practices.
Methods:
This study employs a qualitative approach using multi-case study research as the method. For our research, we selected five case companies in the software-intensive systems domain. Through workshop sessions, frequent meetings and interviews, we explore how DevOps and short feedback loops, data and artificial intelligence (AI), and digital ecosystems challenge current PM practices.
Results:
Our study yielded an in-depth understanding of how digital transformation of the software-intensive systems industry is changing current PM practices. We present empirical results from workshops and from interviews in which case company representatives share their insights on how software, data and AI impact current PM practices. Based on these results, we present a framework organized along two dimensions, i.e. a certainty dimension and an approach dimension. The framework helps structure the approaches product managers can employ to select and prioritize development of new functionality.
Contributions:
The contribution of this paper is a framework for ‘Strategic Digital Product Management’ (SDPM). The framework outlines nine approaches that product managers can employ to maximize the return on investment (RoI) of R&D using new digital technologies.",Information and Software Technology,18 Mar 2025,9,"The research on the changing role of product management due to digital transformation trends provides valuable insights and a framework for Strategic Digital Product Management, offering practical guidance for product managers in the industry."
https://www.sciencedirect.com/science/article/pii/S0950584924001915,Constructing the graphical structure of expert-based Bayesian networks in the context of software engineering: A systematic mapping study,January 2025,"Bayesian networks, Bayesian network structure, Expert knowledge, Software engineering, Systematic mapping",Thiago=Rique: thiago.rique@ifpb.edu.br; Mirko=Perkusich: mirko@virtus.ufcg.edu.br; Kyller=Gorgônio: kyller@virtus.ufcg.edu.br; Hyggo=Almeida: hyggo@virtus.ufcg.edu.br; Angelo=Perkusich: perkusic@virtus.ufcg.edu.br,"Abstract
Context:
In scenarios where data availability issues hinder the applications of statistical causal modeling in software engineering (SE), Bayesian networks (BNs) have been widely used due to their flexibility in incorporating expert knowledge. However, the general understanding of how the graphical structure, i.e., the directed acyclic graph (DAG), of these models is built from domain experts is still insufficient.
Objective:
This study aims to characterize the SE landscape of constructing the graphical structure of BNs, including their potential for causal modeling.
Method:
We conducted a systematic mapping study employing a hybrid search strategy that combines a database search with parallel backward and forward snowballing.
Results:
Our mapping included a total of 106 studies. Different methods are commonly combined to construct expert-based BN structures. These methods span across data gathering & analysis (e.g., interviews, focus groups, literature research, grounded theory, and statistical analysis) and reasoning mechanisms (e.g., using idioms combined with the adoption of lifecycle models, risk-centric modeling, and other frameworks to guide BN construction). We found a lack of consensus regarding validation procedures, particularly critical when modeling cause–effect relationships from knowledge. Additionally, expert-based BNs are mainly applied at the tactical level to address problems related to software engineering management and software quality. Challenges in creating expert-based structures include validation procedures, experts’ availability, expertise level, and structure complexity handling. Key recommendations involve empirical validation, participatory involvement, and balance between adaptation to organizational constraints and model construction requirements.
Conclusion:
The construction of expert-based BN structures in SE varies in rigor, with some methods being systematic while others appear ad hoc. To enhance BN application, reducing expert knowledge subjectivity, enhancing methodological rigor, and clearly articulating the construction rationale is essential. Addressing these challenges is crucial for improving the reliability of causal inferences drawn from these models, ultimately leading to better-informed decisions in SE practices.",Information and Software Technology,18 Mar 2025,6,"The study on constructing expert-based BN structures in software engineering offers insights into challenges and recommendations, but the impact on practical applications may vary depending on the rigor of methodology implementation."
https://www.sciencedirect.com/science/article/pii/S0950584924001526,Don’t forget to change these functions! recommending co-changed functions in modern code review,December 2024,"Software quality assurance, Modern code review, Machine learning",Yang=Hong: Not Found; Chakkrit=Tantithamthavorn: chakkrit@monash.edu; Patanamon=Thongtanunam: Not Found; Aldeida=Aleti: Not Found,"Abstract
Context:
Code review is effective and widely used, yet still time-consuming. Especially, in large-scale software systems, developers may forget to change other related functions that must be changed together (aka. co-changes). This may increase the number of review iterations and reviewing time, thus delaying the code review process. Based on our analysis of 66 projects from five open-source systems, we find that there are 16%–33% of code reviews where at least one function must be co-changed, but was not initially changed.
Objectives:
This study aims to propose an approach to recommend co-changed functions in the context of modern code review, which could reduce reviewing time and iterations and help developers identify functions that need to be changed together.
Methods:
We propose 
CoChangeFinder
, a novel method that employs a Graph Neural Network (GNN) to recommend co-changed functions for newly submitted code changes. Then, we conduct a quantitative and qualitative evaluation of 
CoChangeFinder
 with 66 studied large-scale open-source software projects.
Results:
Our evaluation results show that our 
CoChangeFinder
 outperforms the state-of-the-art approach, achieving 3.44% to 40.45% for top-k accuracy, 2.00% to 26.07% for Recall@k, and 0.04 to 0.21 for mean average precision better than the baseline approach. In addition, our 
CoChangeFinder
 demonstrates the capacity to pinpoint the functions related to logic changes.
Conclusion:
Our 
CoChangeFinder
 outperforms the baseline approach (i.e., TARMAQ) in recommending co-changed functions during the code review process. Based on our findings, 
CoChangeFinder
 could help developers save their time and effort, reduce review iterations, and enhance the efficiency of the code review process.",Information and Software Technology,18 Mar 2025,9,"The proposal of CoChangeFinder to recommend co-changed functions in code review processes shows significant performance improvements over existing approaches, providing a practical solution to reduce reviewing time and enhance efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584924001526,Don’t forget to change these functions! recommending co-changed functions in modern code review,December 2024,"Software quality assurance, Modern code review, Machine learning",Yang=Hong: Not Found; Chakkrit=Tantithamthavorn: chakkrit@monash.edu; Patanamon=Thongtanunam: Not Found; Aldeida=Aleti: Not Found,"Abstract
Context:
Code review is effective and widely used, yet still time-consuming. Especially, in large-scale software systems, developers may forget to change other related functions that must be changed together (aka. co-changes). This may increase the number of review iterations and reviewing time, thus delaying the code review process. Based on our analysis of 66 projects from five open-source systems, we find that there are 16%–33% of code reviews where at least one function must be co-changed, but was not initially changed.
Objectives:
This study aims to propose an approach to recommend co-changed functions in the context of modern code review, which could reduce reviewing time and iterations and help developers identify functions that need to be changed together.
Methods:
We propose 
CoChangeFinder
, a novel method that employs a Graph Neural Network (GNN) to recommend co-changed functions for newly submitted code changes. Then, we conduct a quantitative and qualitative evaluation of 
CoChangeFinder
 with 66 studied large-scale open-source software projects.
Results:
Our evaluation results show that our 
CoChangeFinder
 outperforms the state-of-the-art approach, achieving 3.44% to 40.45% for top-k accuracy, 2.00% to 26.07% for Recall@k, and 0.04 to 0.21 for mean average precision better than the baseline approach. In addition, our 
CoChangeFinder
 demonstrates the capacity to pinpoint the functions related to logic changes.
Conclusion:
Our 
CoChangeFinder
 outperforms the baseline approach (i.e., TARMAQ) in recommending co-changed functions during the code review process. Based on our findings, 
CoChangeFinder
 could help developers save their time and effort, reduce review iterations, and enhance the efficiency of the code review process.",Information and Software Technology,18 Mar 2025,6,"The proposed CoChangeFinder method can have a significant impact on reducing code review time and iterations, enhancing efficiency in the code review process for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001514,XDrain: Effective log parsing in log streams using fixed-depth forest,December 2024,Not Found,Changjian=Liu: driz2t@foxmail.com; Yang=Tian: ytian@gxu.edu.cn; Siyu=Yu: gaiusyu6@gmail.com; Donghui=Gao: dhgao1011@gmail.com; Yifan=Wu: yifanwu@pku.edu.cn; Suqun=Huang: huangsuqun2022@163.com; Xiaochun=Hu: huxch999@163.com; Ningjiang=Chen: chnj@gxu.edu.cn,"Abstract
Logs record rich information that can help operators diagnose system failure 
[1]
. Analyzing logs in log streams can expedite the diagnostic process and effectively mitigate the impact of failures. Log parsing is a prerequisite for automated log analysis, which transforms semi-structured logs into structured logs. However, the effectiveness of existing parsers has only been evaluated on a limited set of logs, which lack sufficient log types. After conducting a more comprehensive evaluation of the existing log parser, we identified the following deficiencies: (1) Variable-starting logs can make some log parsers error-prone. (2) The order of logs in a log stream can have a great impact on the effectiveness. We proposes XDrain to satisfy these challenges by using fixed-depth forest. XDrain first shuffles the order of logs and the order of words within each log a few times. Secondly, XDrain will generate parsing forest for all the logs generated after the shuffling. Finally, the final log template is generated by voting. Evaluation results show that XDrain outperforms existing log parsers on two widely-used accuracy metrics and is immune to inappropriate log order. XDrain only takes about 97.89 s to parse one million logs on average.",Information and Software Technology,18 Mar 2025,4,"XDrain addresses log parsing challenges, which can be useful for system failure diagnosis, but the impact on European early-stage ventures is not as direct as other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492400154X,How do software practitioners perceive human-centric defects?,December 2024,"Human-centric defects, Software engineering, Information technology, Defect reporting, Software development lifecycle",Vedant=Chauhan: vedant.chauhan@monash.edu; Chetan=Arora: chetan.arora@monash.edu; Hourieh=Khalajzadeh: hkhalajzadeh@deakin.edu.au; John=Grundy: john.grundy@monash.edu,"Abstract
Context:
Human-centric software design and development prioritises the way users prefer to complete their jobs, rather than expecting users to adapt to the software. Software users can have different genders, ages, cultures, languages, disabilities, socioeconomic statuses, and educational backgrounds, among many other differences. Due to the inherently varied nature of these differences and their impact on software usage, preferences and issues of users can vary, resulting in user-specific defects that we term as 
‘human-centric defects’ (HCDs)
.
Objective:
This research aims to understand the perception and current management practices of such HCDs by software practitioners, identify key challenges in reporting, understanding and fixing them, and provide recommendations to improve HCDs management in software engineering.
Methods:
We conducted a survey and interviews with software engineering practitioners to gauge their knowledge and experience on HCDs and the defect tracking process.
Results:
We analysed fifty (50) survey- and ten (10) interview-responses from SE practitioners and identified that there are multiple gaps in the current management of HCDs in software engineering practice. There is a lack of awareness regarding human-centric aspects, causing them to be lost or under-appreciated during software development. Our results revealed that handling HCDs could be improved by following a better feedback process with end-users, a more descriptive taxonomy, and suitable automation.
Conclusion:
HCDs, given their diverse end-user base, present a major challenge to software practitioners. In the software engineering domain, research on HCDs has been limited and requires effort from research and practice communities to create awareness and support for human-centric aspects.",Information and Software Technology,18 Mar 2025,5,"Understanding and managing human-centric defects can improve user experience for software, but the practical application and impact on early-stage ventures may not be as immediate."
https://www.sciencedirect.com/science/article/pii/S0950584924001654,DCM-GIFT: An Android malware dynamic classification method based on gray-scale image and feature-selection tree,December 2024,Not Found,Jinfu=Chen: Not Found; Zian=Zhao: Not Found; Saihua=Cai: caisaih@ujs.edu.cn; Xiao=Chen: Not Found; Bilal=Ahmad: Not Found; Luo=Song: Not Found; Kun=Wang: Not Found,"Abstract
Context:
The boom of Android market makes mobile products more popular and convenient. However, in the face of the complex Android application market, how to efficiently and accurately identify malware has become one of the focuses of research. Various new types of disguised malware lurk in the web pages, links and major application malls. Therefore, people’s privacy and property security have become a major obstacle to the continued development of mobile devices.
Objective:
Most of the existing malware classification methods are fixed on one or several types of characteristics of Android devices, such as static characteristics, dynamic characteristics and traffic characteristics. Single feature detection or fixed feature fusion models limit the dimension of detection software, and also cause imbalanced classification results. This paper proposes an Android Malware Dynamic Classification Method based on Gray-scale Image and Feature-selection Tree (DCM-GIFT), which aims to improve and stabilize the precision of Android software classification and enhance the robustness of malware classification.
Method:
In this paper, we construct gray-scale images for the original Android traffic to retain the characteristics of the time series and spatial structure of the original network traffic. At the same time, we take the dynamic information and static information of Android software as auxiliary features to build a feature selection tree. The feature-selection algorithm helps the classifier dynamically select the optimal feature fusion scheme, and the resulting fusion feature vector will be trained and predicted using machine learning clusters for model training.
Results:
We evaluate the performance of DCM-GIFT on multiple datasets published at the Canadian Institute for Cybersecurity, the area under the accuracy, precision, recall and 
F
1
m
e
a
s
u
r
e
. The results show that the proposed DCM-GIFT model has significantly better prediction performance compared to other software classification models.
Conclusion:
It can be concluded that: (1) In terms of accuracy, precision, recall and 
F
1
m
e
a
s
u
r
e
, the DCM-GIFT model has a higher average value. (2) The DCM-GIFT model effectively solves the problem of imbalanced classification results in Android software. (3) The DCM-GIFT model achieves the goal of dynamic feature fusion and significantly improves the utilization of system resources.",Information and Software Technology,18 Mar 2025,7,"DCM-GIFT proposes a method to identify Android malware more efficiently, which can significantly benefit early-stage ventures in terms of security and protection of properties."
https://www.sciencedirect.com/science/article/pii/S0950584924001666,A lot of talk and a badge: An exploratory analysis of personal achievements in GitHub,December 2024,"Gamification, Achievements, Badges, Social translucence, Signaling theory, Open source software, Mining software repositories",Fabio=Calefato: fabio.calefato@uniba.it; Luigi=Quaranta: Not Found; Filippo=Lanubile: Not Found,"Abstract
Context:
GitHub
 has introduced a new gamification element through personal achievements, whereby badges are unlocked and displayed on developers’ personal profile pages in recognition of their development activities.
Objective:
In this paper, we present an exploratory analysis using mixed methods to study the diffusion of personal badges in 
GitHub
, in addition to the effects and reactions to their introduction.
Method:
First, we conduct an observational study by mining longitudinal data from more than 6,000 developers and performed correlation and regression analysis. Then, we conduct a survey and analyze over 300 
GitHub
 community discussions on the topic of personal badges to gauge how the community responded to the introduction of the new feature.
Results:
We find that most of the developers sampled own at least a badge, but we also observe an increasing number of users who choose to keep their profile private and opt out of displaying badges. Additionally, badges are generally poorly correlated with developers’ skills and dispositions such as timeliness and desire to collaborate. We also find that, except for the 
Starstruck
 badge (reflecting the number of followers), their introduction does not have an effect. Finally, the reaction of the community has been in general mixed, as developers find them appealing in principle but without a clear purpose and hardly reflecting their abilities in the current form.
Conclusions:
We provide recommendations to the designers of the 
GitHub
platform on how to improve the current implementation of personal badges as both a gamification mechanism and as sources of reliable cues for assessing the abilities of developers.",Information and Software Technology,18 Mar 2025,3,"The study on personal badges in GitHub, while interesting, may not directly impact European early-stage ventures in a practical manner compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001721,REARRANGE: Effort estimation approach for software clustering-based remodularisation,December 2024,"Effort estimation, Software remodularisation, Software clustering, Refactoring",Alvin Jian Jia=Tan: alvin.tan@monash.edu; Chun Yong=Chong: chong.chunyong@monash.edu; Aldeida=Aleti: aldeida.aleti@monash.edu,"Abstract
Context:
Most research in software clustering and remodularisation typically concludes by recommending the refactoring operations without further insight into the practicality of the proposed technique. Developers might be hesitant to follow through with the refactoring suggestions due to the uncertainty in the effort needed.
Objective:
This work aims to address this gap by introducing an effo
R
t 
E
stimation 
A
pp
R
oach fo
R
 softw
A
re clusteri
NG
-based r
E
modularisation (REARRANGE) to close the loop in extant software clustering and remodularisation research by estimating the time required to carry out the suggested refactoring operations based on the history of the evolution of the software. By providing tangible estimates of refactoring effort in person-hours, we can inform developers of complex and time-consuming refactoring operations that will help prioritise refactoring efforts, allowing practitioners to weave in these activities during sprint planning.
Method:
REARRANGE builds a machine learning model to predict effort estimation based on past commit activity which extracts Software Features (lines of code, number of methods), Refactoring Features (refactoring type, source and destination) and Dependency Features (dependencies between classes). REARRANGE is then compared against sanity checks, baseline effort estimation models, and state-of-the-art software estimation models. We also attempt to cross-validate REARRANGE’s effort estimation with software developers.
Results:
Experimented through 25 open-source Java-based projects, the proposed approach estimated the refactoring effort of the test subjects with a Mean Absolute Error (MAE) of 5.47 person-hours against the MAE of the next-best approach of 453.31 person-hours. Based on a survey conducted among software developers, REARRANGE consistently delivers accurate estimates in 93.6% of cases.
Conclusion:
The lack of a direct comparison for REARRANGE highlights the need for a refactoring effort-focused estimation model that provides tangible effort estimates in person-hours for refactoring operations. Only then can developers selectively choose relevant refactoring operations while considering the available time and budget constraints, bridging the gap between software clustering research and real-world application.",Information and Software Technology,18 Mar 2025,8,"The proposed REARRANGE approach provides tangible estimates of refactoring effort in person-hours, which can help developers prioritize refactoring efforts during sprint planning. It addresses a practical need in software clustering research and real-world application."
https://www.sciencedirect.com/science/article/pii/S0950584924001563,Sustainable systematic literature reviews,December 2024,Not Found,Vinicius=dos Santos: vinicius.dos.santos@icmc.usp.br; Anderson Y.=Iwazaki: Not Found; Katia R.=Felizardo: Not Found; Érica F.=de Souza: Not Found; Elisa Y.=Nakagawa: Not Found,"Abstract
Context:
Systematic Literature Reviews (SLR) have been recognized as an important research method for summarizing evidence in Software Engineering (SE). At the same, SLR still presents several problems, such as the high resource consumption (mainly human resources) and lack of effective impact on SE practitioners, although much research has already been done.
Objective:
The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods:
We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results:
We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion:
The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.",Information and Software Technology,18 Mar 2025,6,"The exploration of sustainability in Systematic Literature Reviews (SLR) is valuable in understanding and solving problems, but may have limited immediate impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001538,In memoriam of professor Guenther Ruhe: Contributions to the software product management research and practice,December 2024,"Software product management, Release planning, Requirements engineering, ISPMA",Hans-Bernd=Kittlaus: Not Found; Andrey=Saltan: andrey.saltan@lut.fi,"Abstract
For many years, Guenther Ruhe was a fellow of the International Software Product Management Association, playing an important role in shaping the academic and practical landscapes of Software Product Management (SPM). This editorial note honors Ruhe's enduring impact on the SPM Body of Knowledge, evaluating his extensive contributions to SPM research and practice, and recognizing his legacy in shaping the future trajectory of the field. By examining Ruhe's academic publications and his role in developing the SPM Body of Knowledge, we highlight key areas of his influence, particularly in release planning and requirements engineering. His integration of empirical research into SPM has notably enhanced the discipline's rigor and relevance. Ruhe's contributions to the SPM Body of Knowledge are profound and far-reaching, establishing his work as a cornerstone for ongoing research and practice in SPM.",Information and Software Technology,18 Mar 2025,9,"The editorial note honoring Guenther Ruhe's impact on Software Product Management (SPM) demonstrates his significant contributions to the field, particularly in release planning and requirements engineering. His work serves as a cornerstone for ongoing research and practice in SPM."
https://www.sciencedirect.com/science/article/pii/S0950584924001538,In memoriam of professor Guenther Ruhe: Contributions to the software product management research and practice,December 2024,"Software product management, Release planning, Requirements engineering, ISPMA",Hans-Bernd=Kittlaus: Not Found; Andrey=Saltan: andrey.saltan@lut.fi,"Abstract
For many years, Guenther Ruhe was a fellow of the International Software Product Management Association, playing an important role in shaping the academic and practical landscapes of Software Product Management (SPM). This editorial note honors Ruhe's enduring impact on the SPM Body of Knowledge, evaluating his extensive contributions to SPM research and practice, and recognizing his legacy in shaping the future trajectory of the field. By examining Ruhe's academic publications and his role in developing the SPM Body of Knowledge, we highlight key areas of his influence, particularly in release planning and requirements engineering. His integration of empirical research into SPM has notably enhanced the discipline's rigor and relevance. Ruhe's contributions to the SPM Body of Knowledge are profound and far-reaching, establishing his work as a cornerstone for ongoing research and practice in SPM.",Information and Software Technology,18 Mar 2025,9,"The editorial note honoring Guenther Ruhe's impact on Software Product Management (SPM) demonstrates his significant contributions to the field, particularly in release planning and requirements engineering. His work serves as a cornerstone for ongoing research and practice in SPM."
https://www.sciencedirect.com/science/article/pii/S0950584924001587,A socio-technical perspective on software vulnerabilities: A causal analysis,December 2024,Not Found,Carlos=Paradis: cvas@acm.org; Rick=Kazman: Not Found; Mike=Konrad: Not Found,"Abstract
Context:
Software development organizations are composed of people working together towards a common goal. These people are connected in networks. The effectiveness of these networks seems like it would be an essential consideration for the effectiveness of the organization as a whole, but does network effectiveness actually matter?
Objective:
In this paper, we seek to understand whether causal relationships exist between the maintenance effort spent on files implicated in software vulnerabilities and suboptimal social behaviors – social smells – within that project’s developer community.
Methods:
To gain insight into this question, we chose to study OpenSSL and over 100 of its published vulnerabilities. We performed a socio-technical analysis on OpenSSL to understand whether social smells could be causally linked to the effort to maintain files implicated in vulnerabilities.
Results:
Our results indicate that this is the case: Social smells are, in fact, causally linked to the maintenance effort surrounding files implicated in software vulnerabilities.
Conclusion:
This result has significant implications for the management of software projects. These insights may motivate and help to guide project managers and architects to also focus on team communications, and not merely on technical quality measures such as bug rates or feature velocity. Social interactions among a project’s team members matter, and smells can be measured and monitored.",Information and Software Technology,18 Mar 2025,7,"The study on social smells and maintenance effort in software development projects provides insights into team dynamics and their impact on project management. While relevant, the practical application to early-stage ventures may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001587,A socio-technical perspective on software vulnerabilities: A causal analysis,December 2024,Not Found,Carlos=Paradis: cvas@acm.org; Rick=Kazman: Not Found; Mike=Konrad: Not Found,"Abstract
Context:
Software development organizations are composed of people working together towards a common goal. These people are connected in networks. The effectiveness of these networks seems like it would be an essential consideration for the effectiveness of the organization as a whole, but does network effectiveness actually matter?
Objective:
In this paper, we seek to understand whether causal relationships exist between the maintenance effort spent on files implicated in software vulnerabilities and suboptimal social behaviors – social smells – within that project’s developer community.
Methods:
To gain insight into this question, we chose to study OpenSSL and over 100 of its published vulnerabilities. We performed a socio-technical analysis on OpenSSL to understand whether social smells could be causally linked to the effort to maintain files implicated in vulnerabilities.
Results:
Our results indicate that this is the case: Social smells are, in fact, causally linked to the maintenance effort surrounding files implicated in software vulnerabilities.
Conclusion:
This result has significant implications for the management of software projects. These insights may motivate and help to guide project managers and architects to also focus on team communications, and not merely on technical quality measures such as bug rates or feature velocity. Social interactions among a project’s team members matter, and smells can be measured and monitored.",Information and Software Technology,18 Mar 2025,8,"The study provides insights into the impact of social interactions on software project management, offering practical implications for project managers and architects to improve team communications."
https://www.sciencedirect.com/science/article/pii/S0950584924001319,A PRISMA-driven systematic mapping study on system assurance weakeners,November 2024,"Assurance cases, Assurance deficits, Uncertainty, Logical fallacies, PRISMA, GSN, SACM, Systematic mapping study, Cyber–physical systems, Safety, Reliability, Defeaters",Kimya Khakzad=Shahandashti: Not Found; Alvine B.=Belle: alvine.belle@lassonde.yorku.ca; Timothy C.=Lethbridge: Not Found; Oluwafemi=Odu: Not Found; Mithila=Sivakumar: Not Found,"Abstract
Context:
An assurance case is a structured hierarchy of claims aiming at demonstrating that a mission-critical system supports specific requirements (e.g., safety, security, privacy). The presence of assurance weakeners (i.e., assurance deficits, logical fallacies) in assurance cases reflects insufficient evidence, knowledge, or gaps in reasoning. These weakeners can undermine confidence in assurance arguments, potentially hindering the verification of mission-critical system capabilities which could result in catastrophic outcomes (e.g., loss of lives). Given the growing interest in employing assurance cases to ensure that systems are developed to meet their requirements, exploring the management of assurance weakeners becomes beneficial.
Objective:
As a stepping stone for future research on assurance weakeners, we aim to initiate the first comprehensive systematic mapping study on this subject.
Methods:
We followed the well-established PRISMA 2020 and SEGRESS guidelines to conduct our systematic mapping study. We searched for primary studies in five digital libraries and focused on the 2012–2023 publication year range. Our selection criteria focused on studies addressing assurance weakeners from a qualitative standpoint, resulting in the inclusion of 39 primary studies in our systematic review.
Results:
Our systematic mapping study reports a taxonomy (map) that provides a uniform categorization of assurance weakeners and approaches proposed to manage them from a qualitative perspective. The taxonomy classifies weakeners in four categories: aleatory, epistemic, ontological, and argument uncertainty. Additionally, it classifies approaches supporting the management of weakeners in three main categories: representation, identification and mitigation approaches.
Conclusion:
Our study findings suggest that the SACM (Structured Assurance Case Metamodel) – a standard specified by the OMG (Object Management Group) – offers a comprehensive range of capabilities to capture structured arguments and reason about their potential assurance weakeners. Our findings also suggest novel assurance weakener management approaches should be proposed to better assure mission-critical systems.",Information and Software Technology,18 Mar 2025,7,"The systematic mapping study on assurance weakeners offers a comprehensive taxonomy and management approaches, which can benefit mission-critical system development, but lacks direct relevance to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001319,A PRISMA-driven systematic mapping study on system assurance weakeners,November 2024,"Assurance cases, Assurance deficits, Uncertainty, Logical fallacies, PRISMA, GSN, SACM, Systematic mapping study, Cyber–physical systems, Safety, Reliability, Defeaters",Kimya Khakzad=Shahandashti: Not Found; Alvine B.=Belle: alvine.belle@lassonde.yorku.ca; Timothy C.=Lethbridge: Not Found; Oluwafemi=Odu: Not Found; Mithila=Sivakumar: Not Found,"Abstract
Context:
An assurance case is a structured hierarchy of claims aiming at demonstrating that a mission-critical system supports specific requirements (e.g., safety, security, privacy). The presence of assurance weakeners (i.e., assurance deficits, logical fallacies) in assurance cases reflects insufficient evidence, knowledge, or gaps in reasoning. These weakeners can undermine confidence in assurance arguments, potentially hindering the verification of mission-critical system capabilities which could result in catastrophic outcomes (e.g., loss of lives). Given the growing interest in employing assurance cases to ensure that systems are developed to meet their requirements, exploring the management of assurance weakeners becomes beneficial.
Objective:
As a stepping stone for future research on assurance weakeners, we aim to initiate the first comprehensive systematic mapping study on this subject.
Methods:
We followed the well-established PRISMA 2020 and SEGRESS guidelines to conduct our systematic mapping study. We searched for primary studies in five digital libraries and focused on the 2012–2023 publication year range. Our selection criteria focused on studies addressing assurance weakeners from a qualitative standpoint, resulting in the inclusion of 39 primary studies in our systematic review.
Results:
Our systematic mapping study reports a taxonomy (map) that provides a uniform categorization of assurance weakeners and approaches proposed to manage them from a qualitative perspective. The taxonomy classifies weakeners in four categories: aleatory, epistemic, ontological, and argument uncertainty. Additionally, it classifies approaches supporting the management of weakeners in three main categories: representation, identification and mitigation approaches.
Conclusion:
Our study findings suggest that the SACM (Structured Assurance Case Metamodel) – a standard specified by the OMG (Object Management Group) – offers a comprehensive range of capabilities to capture structured arguments and reason about their potential assurance weakeners. Our findings also suggest novel assurance weakener management approaches should be proposed to better assure mission-critical systems.",Information and Software Technology,18 Mar 2025,7,"The systematic mapping study on assurance weakeners offers a comprehensive taxonomy and management approaches, which can benefit mission-critical system development, but lacks direct relevance to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001460,On meetings involving remote software teams: A systematic literature review,November 2024,Not Found,Anielle S.L.=de Andrade: anielle.lisboa@edu.pucrs; Victoria=Jackson: Not Found; Rafael=Prikladnicki: Not Found; André=van der Hoek: Not Found,"Abstract
Context:
The adoption of remote work models and the global nature of software projects have significantly transformed collaboration and communication within the software development industry. Remote meetings have become a common means of collaboration for software development teams.
Objective:
This study seeks to enhance our understanding of remote meeting practices in software teams. It identifies the benefits of remote meetings, the problems associated with remote meetings, tools used to facilitate remote meetings and provides recommended good practices. The study employs a systematic literature review to assist remote teams in improving their meeting practices and identifying areas for future research.
Methods:
We conducted a systematic literature review that involved searching multiple databases and employing quantitative and qualitative analysis techniques on the identified set of studies to answer our research questions.
Results:
The search yielded 30 papers offering valuable insights into remote meeting practices in software teams. Remote meetings offer advantages over traditional in-person meetings such as increased effectiveness and ease of attendance. However, challenges exist such as technological issues, ineffective collaboration, and reduced team socialization. Identified good practices to mitigate the challenges include inserting breaks in longer meetings, catch-up time at the start of meeting, communicating goals in advance of the meeting, and pre-recording demos.
Conclusion:
The study explored remote meetings in software teams. We identified advantages that remote meetings have in comparison to in-person meetings, challenges to remote meetings, and good practices along with supportive tooling. While the practices help in promoting effective meetings, additional research is required to further improve remote meeting experiences. Researching topics such as investigating different types of meetings common to software development teams along with the potential for novel tools to better support meetings will help identify additional practices and tools that can benefit remote teams.",Information and Software Technology,18 Mar 2025,9,The study on remote meeting practices in software teams provides practical insights and good practices that can directly benefit early-stage ventures by improving collaboration in remote work settings.
https://www.sciencedirect.com/science/article/pii/S0950584924001289,Fine-tuning and prompt engineering for large language models-based code review automation,November 2024,"Modern code review, Code review automation, Large language models, GPT-3.5, Few-shot learning, Persona",Chanathip=Pornprasit: chanathip.pornprasit@monash.edu; Chakkrit=Tantithamthavorn: chakkrit@monash.edu,"Abstract
Context:
The rapid evolution of Large Language Models (LLMs) has sparked significant interest in leveraging their capabilities for automating code review processes. Prior studies often focus on developing LLMs for code review automation, yet require expensive resources, which is infeasible for organizations with limited budgets and resources. Thus, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation.
Objective:
We aim to investigate the performance of LLMs-based code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting. Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model’s generation process without requiring a specific code review dataset.
Methods:
We leverage model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona) on LLMs-based code review automation. In total, we investigate 12 variations of two LLMs-based code review automation (i.e., GPT-3.5 and Magicoder), and compare them with the Guo et al.’s approach and three existing code review automation approaches (i.e., CodeReviewer, TufanoT5 and D-ACT).
Results:
The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17%–74.23% higher EM than the Guo et al.’s approach. In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with few-shot learning achieves 46.38%–659.09% higher EM than GPT-3.5 with zero-shot learning.
Conclusions:
Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance.; and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation. Our findings contribute valuable insights into the practical recommendations and trade-offs associated with deploying LLMs for code review automation.",Information and Software Technology,18 Mar 2025,8,"The investigation into leveraging LLMs for code review automation offers practical recommendations and trade-offs for improving code review processes, which can be valuable for early-stage ventures looking to optimize development workflows."
https://www.sciencedirect.com/science/article/pii/S0950584924001289,Fine-tuning and prompt engineering for large language models-based code review automation,November 2024,"Modern code review, Code review automation, Large language models, GPT-3.5, Few-shot learning, Persona",Chanathip=Pornprasit: chanathip.pornprasit@monash.edu; Chakkrit=Tantithamthavorn: chakkrit@monash.edu,"Abstract
Context:
The rapid evolution of Large Language Models (LLMs) has sparked significant interest in leveraging their capabilities for automating code review processes. Prior studies often focus on developing LLMs for code review automation, yet require expensive resources, which is infeasible for organizations with limited budgets and resources. Thus, fine-tuning and prompt engineering are the two common approaches to leveraging LLMs for code review automation.
Objective:
We aim to investigate the performance of LLMs-based code review automation based on two contexts, i.e., when LLMs are leveraged by fine-tuning and prompting. Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model’s generation process without requiring a specific code review dataset.
Methods:
We leverage model fine-tuning and inference techniques (i.e., zero-shot learning, few-shot learning and persona) on LLMs-based code review automation. In total, we investigate 12 variations of two LLMs-based code review automation (i.e., GPT-3.5 and Magicoder), and compare them with the Guo et al.’s approach and three existing code review automation approaches (i.e., CodeReviewer, TufanoT5 and D-ACT).
Results:
The fine-tuning of GPT 3.5 with zero-shot learning helps GPT-3.5 to achieve 73.17%–74.23% higher EM than the Guo et al.’s approach. In addition, when GPT-3.5 is not fine-tuned, GPT-3.5 with few-shot learning achieves 46.38%–659.09% higher EM than GPT-3.5 with zero-shot learning.
Conclusions:
Based on our results, we recommend that (1) LLMs for code review automation should be fine-tuned to achieve the highest performance.; and (2) when data is not sufficient for model fine-tuning (e.g., a cold-start problem), few-shot learning without a persona should be used for LLMs for code review automation. Our findings contribute valuable insights into the practical recommendations and trade-offs associated with deploying LLMs for code review automation.",Information and Software Technology,18 Mar 2025,8,"The study provides practical recommendations for leveraging LLMs for code review automation, which can be valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001290,An exploratory study on just-in-time multi-programming-language bug prediction,November 2024,Not Found,Zengyang=Li: zengyangli@ccnu.edu.cn; Jiabao=Ji: jjb_coder@mails.ccnu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Ran=Mo: moran@ccnu.edu.cn; Hui=Liu: hliu@hust.edu.cn,"Abstract
Context:
An increasing number of software systems are written in multiple programming languages (PLs), which are called multi-programming-language (MPL) systems. MPL bugs (MPLBs) refers to the bugs whose resolution involves multiple PLs. Despite high complexity of MPLB resolution, there lacks MPLB prediction methods.
Objective:
This work aims to construct just-in-time (JIT) MPLB prediction models with selected prediction metrics, analyze the significance of the metrics, and then evaluate the performance of cross-project JIT MPLB prediction.
Methods:
We develop JIT MPLB prediction models with the selected metrics using machine learning algorithms and evaluate the models in within-project and cross-project contexts with our constructed dataset based on 18 Apache MPL projects.
Results:
Random Forest is appropriate for JIT MPLB prediction. Changed LOC of all files, added LOC of all files, and the total number of lines of all files of the project currently are the most crucial metrics in JIT MPLB prediction. The prediction models can be simplified using a few top-ranked metrics. Training on the dataset from multiple projects can yield significantly higherAUC than training on the dataset from a single project for cross-project JIT MPLB prediction.
Conclusions:
JIT MPLB prediction models can be constructed with the selected set of metrics, which can be reduced to build simplified JIT MPLB prediction models, and cross-project JIT MPLB prediction is feasible.",Information and Software Technology,18 Mar 2025,6,"The research addresses an important issue in bug prediction for multi-programming-language systems, which can be useful for startups dealing with such complexities."
https://www.sciencedirect.com/science/article/pii/S0950584924001320,Cross-Modal Retrieval-enhanced code Summarization based on joint learning for retrieval and generation,November 2024,Not Found,Lixuan=Li: 20224227057@stu.suda.edu.cn; Bin=Liang: bin.liang@cuhk.edu.hk; Lin=Chen: lchen@nju.edu.cn; Xiaofang=Zhang: xfzhang@suda.edu.cn,"Abstract
Context:
Code summarization refers to a task that automatically generates a natural language description of a code snippet to facilitate code comprehension. Existing methods have achieved satisfactory results by incorporating information retrieval into generative deep-learning models for reusing summaries of existing code. However, most of these existing methods employed non-learnable generic retrieval methods for content-based retrieval, resulting in a lack of diversity in the retrieved results during training, thereby making the model over-reliant on retrieved results and reducing the generative model’s ability to generalize to unknown samples.
Objective:
To address this issue, this paper introduces CMR-Sum: a novel Cross-Modal Retrieval-enhanced code Summarization framework based on joint learning for generation and retrieval tasks, where both two tasks are allowed to be optimized simultaneously.
Method:
Specifically, we use a cross-modal retrieval module to dynamically alter retrieval results during training, which enhances the diversity of the retrieved results and maintains a relative balance between the two tasks. Furthermore, in the summary generation phase, we employ a cross-attention mechanism to generate code summaries based on the alignment between retrieved and generated summaries. We conducted experiments on three real-world datasets, comparing the performance of our method with baseline models. Additionally, we performed extensive qualitative analysis.
Result:
Results from qualitative and quantitative experiments indicate that our approach effectively enhances the performance of code summarization. Our method outperforms both the generation-based and the retrieval-enhanced baselines. Further ablation experiments demonstrate the effectiveness of each component of our method. Results from sensitivity analysis experiments suggest that our approach achieves good performance without requiring extensive hyper-parameter search.
Conclusion:
The direction of utilizing retrieval-enhanced generation tasks shows great potential. It is essential to increase the diversity of retrieval results during the training process, which is crucial for improving the generality and the performance of the model.",Information and Software Technology,18 Mar 2025,9,"The proposed CMR-Sum framework enhances code summarization performance significantly, which can have a positive impact on early-stage ventures in improving code comprehension."
https://www.sciencedirect.com/science/article/pii/S0950584924001332,SFIDMT-ART: A metamorphic group generation method based on Adaptive Random Testing applied to source and follow-up input domains,November 2024,"Metamorphic testing, Metamorphic group, Metamorphic relation, Adaptive random testing, Input domain",Zhihao=Ying: zhihao.ying2@nottingham.edu.cn; Dave=Towey: Dave.Towey@nottingham.edu.cn; Anthony Graham=Bellotti: Anthony-Graham.Bellotti@nottingham.edu.cn; Tsong Yueh=Chen: tychen@swin.edu.au; Zhi Quan=Zhou: zhiquan.zhou@gmail.com,"Abstract
Context:
The performance of metamorphic testing relates strongly to the quality of test cases. However, most related research has only focused on source test cases, ignoring follow-up test cases to some extent. In this paper, we identify a potential problem that may be encountered with existing metamorphic group generation algorithms. We then propose a possible solution to address this problem. Based on this solution, we design a new algorithm for generating effective source and follow-up test cases.
Objective:
To improve the performance (test effectiveness and efficiency) of metamorphic testing.
Methods:
We introduce the concept of the input-domain difference problem, which is likely to affect the performance of metamorphic group generation algorithms. We propose a new test-case distribution criterion for metamorphic testing to address this problem. Based on our proposed criterion, we further present a new metamorphic group generation algorithm, from a black-box perspective, with new distance metrics to facilitate this algorithm.
Results:
Our algorithm performs significantly better than existing algorithms, in terms of test effectiveness, efficiency and test-case diversity.
Conclusions:
Through experiments, we find that the input-domain difference problem is likely to affect the performance of metamorphic group generation algorithms. The experimental results demonstrate that our algorithm can achieve good test efficiency, effectiveness, and test-case diversity.",Information and Software Technology,18 Mar 2025,7,The development of a new algorithm for generating effective test cases in metamorphic testing can benefit startups in improving test effectiveness and efficiency.
https://www.sciencedirect.com/science/article/pii/S0950584924001502,FCTree: Visualization of function calls in execution,November 2024,Not Found,Fangfang=Zhou: Not Found; Yilun=Fan: Not Found; Shenglan=Lv: Not Found; Lijia=Jiang: Not Found; Zhuo=Chen: Not Found; Jian=Yuan: Not Found; Feijiang=Han: Not Found; Haojin=Jiang: Not Found; Genghuai=Bai: Not Found; Ying=Zhao: zhaoying@csu.edu.cn,"Abstract
Function calls in execution contain rich bivariate, hierarchical, and chronological information. Many visualizations have been adopted to analyze function calls in execution for program testing, vulnerability locating, and malware detection. However, we conducted a pilot study and revealed that existing single-viewed function call visualizations fail to present the bivariate, hierarchical, and chronological information comprehensively. A new function call visualization named FCTree is proposed in this work to deal with this situation. Learned from advantages of existing visualizations and iterative discussions with actual users, FCTree uses a compact and aligned hierarchical layout design to present the bivariate and hierarchical information and adopts a glyph design to present the chronological information. Subjective and objective experiments in the laboratory and a field study in a real-world scenario were conducted to evaluate the effectiveness of FCTree.",Information and Software Technology,18 Mar 2025,5,"While the FCTree visualization is innovative, its impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001356,An empirical study on compatibility issues in Android API field evolution,November 2024,Not Found,Tarek=Mahmud: tarek_mahmud@txstate.edu; Meiru=Che: m.che@cqu.edu.au; Guowei=Yang: guowei.yang@uq.edu.au,"Abstract
Context:
The continuous evolution of the Android operating system requires regular API updates, which may affect the functionality of Android apps. This is becoming increasingly common due to the frequent evolution of the Android platform, which introduces new APIs and deprecates existing ones. Recent studies investigated API evolution to ensure the reliability of Android apps; however, they focused on API methods alone.
Objectives:
This study aims to understand how API fields evolve and how this affects API compatibility in real-world Android apps and their development.
Method:
We perform an empirical study on compatibility issues in Android API field evolution by analyzing the nature and resolution of these issues across 681 open-source Android apps.
Results:
Our experimental results yield interesting findings: (1) On average two API field compatibility issues exist per app in each tag; (2) Although API method evolution and API field evolution are related, current API method-level analysis techniques may fail to detect numerous API field compatibility issues; (3) Different types of checks are preferred when addressing different types of compatibility issues; (4) It takes on average three and a half months for an API field compatibility issue to get fixed since when it is introduced; (5) Developers pay proper attention to API field compatibility issues and address them soon after becoming aware of them in the apps.
Conclusion:
These findings highlight the significance of including API fields in future research on API evolution and can assist developers and researchers in understanding, detecting, and handling compatibility issues in API field evolution.",Information and Software Technology,18 Mar 2025,7,"The study provides valuable insights into API field evolution in Android apps, which can help developers in understanding and addressing compatibility issues."
https://www.sciencedirect.com/science/article/pii/S0950584924001472,A rule-based decision model to support technical debt decisions: A multiple case study of web and mobile app startups,November 2024,Not Found,Abdullah=Aldaeej: aaaldaeej@iau.edu.sa; Carolyn=Seaman: Not Found,"Abstract
Context
Software startups are immature software organizations that focus on the development of a single software product or service. This organizational context accumulates a lot of technical debt to cope with constraints such as limited resources and product-market fit uncertainty. While some research has explored technical debt in startups, there is no study that investigates how software startups should make technical debt decisions throughout the startup evolution stages.
Objective
The objective of this study is to understand how technical debt decisions are made, and how such decisions should have been made in hindsight.
Method
We conducted a multiple embedded case study to investigate technical debt decisions in five web/mobile app startups. For each case, we interviewed the case founder and developer (a total of 17 participants across cases). In addition, we collected some public documents about the five startups. The data were analyzed using qualitative data analysis techniques.
Results
We developed a rule-based decision model that summarizes the logic to effectively make technical debt decisions throughout the startup evolution stages. In addition, we evaluated the model by conducting follow-up interviews with three participants.
Conclusion
The study provides a decision model that reflects actual practice, and is designed to help software teams in startups when making technical debt decisions throughout the startup evolution stages.",Information and Software Technology,18 Mar 2025,8,"The decision model developed in this study can assist software teams in startups in making technical debt decisions throughout the startup evolution stages, providing practical guidance."
https://www.sciencedirect.com/science/article/pii/S0950584924001344,Qubernetes: Towards a unified cloud-native execution platform for hybrid classic-quantum computing,November 2024,"Quantum software, Hybrid classical-quantum software, Containers, Quantum software development lifecycle, Cloud-native computing",Vlad=Stirbu: vlad.a.stirbu@jyu.fi; Otso=Kinanen: Not Found; Majid=Haghparast: Not Found; Tommi=Mikkonen: Not Found,"Abstract
Context:
The emergence of quantum computing proposes a revolutionary paradigm that can radically transform numerous scientific and industrial application domains. The ability of quantum computers to scale computations beyond what the current computers are capable of implies better performance and efficiency for certain algorithmic tasks.
Objective:
However, to benefit from such improvement, quantum computers must be integrated with existing software systems, a process that is not straightforward. In this paper, we propose a unified execution model that addresses the challenges that emerge from building hybrid classical-quantum applications at scale.
Method:
Following the Design Science Research methodology, we proposed a convention for mapping quantum resources and artifacts to Kubernetes concepts. Then, in an experimental Kubernetes cluster, we conducted experiments for scheduling and executing quantum tasks on both quantum simulators and hardware.
Results:
The experimental results demonstrate that the proposed platform Qubernetes (or Kubernetes for quantum) exposes the quantum computation tasks and hardware capabilities following established cloud-native principles, allowing seamless integration into the larger Kubernetes ecosystem.
Conclusion:
The quantum computing potential cannot be realized without seamless integration into classical computing. By validating that it is practical to execute quantum tasks in a Kubernetes infrastructure, we pave the way for leveraging the existing Kubernetes ecosystem as an enabler for hybrid classical-quantum computing.",Information and Software Technology,18 Mar 2025,9,"The proposed unified execution model for hybrid classical-quantum applications in Kubernetes demonstrates a significant step towards realizing the potential of quantum computing, which can have a transformative impact on various industries."
https://www.sciencedirect.com/science/article/pii/S0950584924001277,GitHub marketplace for automation and innovation in software production,November 2024,"Software engineering, Platform-mediated, GitHub marketplace, Automation, GitHub actions, Production",SK. Golam=Saroar: saroar@yorku.ca; Waseefa=Ahmed: waseefa@yorku.ca; Elmira=Onagh: eonagh@yorku.ca; Maleknaz=Nayebi: mnayebi@yorku.ca,"Abstract
Context:
GitHub, renowned for facilitating collaborative code version control and software production in software teams, expanded its services in 2017 by introducing GitHub Marketplace. This online platform hosts automation tools to assist developers with the production of their GitHub-hosted projects, and it has become a valuable source of information on the tools used in the 
Open Source Software
 (OSS) community.
Objective:
In this 
exploratory study
, we introduce GitHub Marketplace as a software marketplace by exploring the Characteristics, Features, and Policies of the platform comprehensively, identifying common themes in production automation. Further, we explore popular tools among practitioners and researchers and highlight disparities in the approach to these tools between industry and academia.
Method:
We adopted the conceptual framework of software app stores from previous studies and used that to examine 8,318 automated production tools (440 Apps and 7,878 Actions) across 32 categories on GitHub Marketplace. We explored and described the policies of this marketplace as a unique platform where developers share production tools for the use of other developers. Furthermore, we conducted a 
systematic mapping
 of 515 research papers published from 2000 to 2021 and compared open-source academic production tools with those available in the marketplace.
Results:
We found that although some of the automation topics in literature are widely used in practice, they have yet to align with the state-of-practice for automated production. We discovered that practitioners often use automation tools for tasks like “Continuous Integration” and “Utilities”, while researchers tend to focus more on “Code Quality” and “Testing”.
Conclusion:
Our study illuminates the landscape of open-source tools for automation production. We also explored the disparities between industry trends and researchers’ priorities. Recognizing these distinctions can empower researchers to build on existing work and guide practitioners in selecting tools that meet their specific needs. Bridging this gap between industry and academia helps with further innovation in the field and ensures that research remains pertinent to the evolving challenges in software production.",Information and Software Technology,18 Mar 2025,6,"The research sheds light on the disparities between industry and academia in using automation tools, which can be informative for both researchers and practitioners in selecting tools aligned with their needs."
https://www.sciencedirect.com/science/article/pii/S0950584924001277,GitHub marketplace for automation and innovation in software production,November 2024,"Software engineering, Platform-mediated, GitHub marketplace, Automation, GitHub actions, Production",SK. Golam=Saroar: saroar@yorku.ca; Waseefa=Ahmed: waseefa@yorku.ca; Elmira=Onagh: eonagh@yorku.ca; Maleknaz=Nayebi: mnayebi@yorku.ca,"Abstract
Context:
GitHub, renowned for facilitating collaborative code version control and software production in software teams, expanded its services in 2017 by introducing GitHub Marketplace. This online platform hosts automation tools to assist developers with the production of their GitHub-hosted projects, and it has become a valuable source of information on the tools used in the 
Open Source Software
 (OSS) community.
Objective:
In this 
exploratory study
, we introduce GitHub Marketplace as a software marketplace by exploring the Characteristics, Features, and Policies of the platform comprehensively, identifying common themes in production automation. Further, we explore popular tools among practitioners and researchers and highlight disparities in the approach to these tools between industry and academia.
Method:
We adopted the conceptual framework of software app stores from previous studies and used that to examine 8,318 automated production tools (440 Apps and 7,878 Actions) across 32 categories on GitHub Marketplace. We explored and described the policies of this marketplace as a unique platform where developers share production tools for the use of other developers. Furthermore, we conducted a 
systematic mapping
 of 515 research papers published from 2000 to 2021 and compared open-source academic production tools with those available in the marketplace.
Results:
We found that although some of the automation topics in literature are widely used in practice, they have yet to align with the state-of-practice for automated production. We discovered that practitioners often use automation tools for tasks like “Continuous Integration” and “Utilities”, while researchers tend to focus more on “Code Quality” and “Testing”.
Conclusion:
Our study illuminates the landscape of open-source tools for automation production. We also explored the disparities between industry trends and researchers’ priorities. Recognizing these distinctions can empower researchers to build on existing work and guide practitioners in selecting tools that meet their specific needs. Bridging this gap between industry and academia helps with further innovation in the field and ensures that research remains pertinent to the evolving challenges in software production.",Information and Software Technology,18 Mar 2025,6,"While the study provides insights into GitHub Marketplace and automation tools, the practical implications for early-stage ventures might be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924001083,Analysing the synergies between Multi-agent Systems and Digital Twins: A systematic literature review,October 2024,"Digital twin, Multi-agent system, MAS, Literature review, Internet of Things",Elena=Pretel: Not Found; Alejandro=Moya: Not Found; Elena=Navarro: elena.navarro@uclm.es; Víctor=López-Jaquero: Not Found; Pascual=González: Not Found,"Abstract
Context
Digital Twins
 (DTs) are used to augment physical entities by exploiting assorted 
computational approaches
 applied to the virtual twin counterpart. A DT is generally described as a physical entity, its virtual counterpart, and the data connections between them. Multi-Agent Systems (MAS) paradigm is alike DTs in many ways. Agents of MAS are entities operating and interacting in a specific environment, while exploring and 
collecting data
 to solve some tasks.
Objective
This paper presents the results of a systematic literature review (SLR) focused on the analysis of current proposals exploiting the synergies of DTs and MAS. This research aims to synthesize studies that focus on the use of MAS to support DTs development and MAS that exploit DTs, paving the way for future research.
Method
A SLR methodology was used to conduct a detailed study analysis of 64 primary studies out of a total of 220 studies that were initially identified. This SLR analyses three research questions related to the synergies between MAS and DT.
Results
The most relevant findings of this SLR and their implications for further research are the following: i) most of the analyzed proposals design digital shadows rather than DT; ii) they do not fully support the properties expected from a DT; iii) most of the MAS properties have not fully exploited for the development of DT; iv) ontologies are frequently used for specifying semantic models of the physical twin.
Conclusions
Based on the results of this SLR, our conclusions for the community are presented in a 
research agenda
 that highlights the need of innovative theoretical proposals and design frameworks that guide the development of DT. They should be defined exploiting the properties of MAS to unleash the full potential of DT. Finally, ontologies for 
machine learning
 models should be designed for its use in DT.",Information and Software Technology,18 Mar 2025,7,"The research addresses the synergies between DTs and MAS, providing insights and future research directions. It can be valuable for early-stage ventures looking to innovate in these areas."
https://www.sciencedirect.com/science/article/pii/S0950584924001083,Analysing the synergies between Multi-agent Systems and Digital Twins: A systematic literature review,October 2024,"Digital twin, Multi-agent system, MAS, Literature review, Internet of Things",Elena=Pretel: Not Found; Alejandro=Moya: Not Found; Elena=Navarro: elena.navarro@uclm.es; Víctor=López-Jaquero: Not Found; Pascual=González: Not Found,"Abstract
Context
Digital Twins
 (DTs) are used to augment physical entities by exploiting assorted 
computational approaches
 applied to the virtual twin counterpart. A DT is generally described as a physical entity, its virtual counterpart, and the data connections between them. Multi-Agent Systems (MAS) paradigm is alike DTs in many ways. Agents of MAS are entities operating and interacting in a specific environment, while exploring and 
collecting data
 to solve some tasks.
Objective
This paper presents the results of a systematic literature review (SLR) focused on the analysis of current proposals exploiting the synergies of DTs and MAS. This research aims to synthesize studies that focus on the use of MAS to support DTs development and MAS that exploit DTs, paving the way for future research.
Method
A SLR methodology was used to conduct a detailed study analysis of 64 primary studies out of a total of 220 studies that were initially identified. This SLR analyses three research questions related to the synergies between MAS and DT.
Results
The most relevant findings of this SLR and their implications for further research are the following: i) most of the analyzed proposals design digital shadows rather than DT; ii) they do not fully support the properties expected from a DT; iii) most of the MAS properties have not fully exploited for the development of DT; iv) ontologies are frequently used for specifying semantic models of the physical twin.
Conclusions
Based on the results of this SLR, our conclusions for the community are presented in a 
research agenda
 that highlights the need of innovative theoretical proposals and design frameworks that guide the development of DT. They should be defined exploiting the properties of MAS to unleash the full potential of DT. Finally, ontologies for 
machine learning
 models should be designed for its use in DT.",Information and Software Technology,18 Mar 2025,7,"Similar to abstract 81, this research also focuses on the synergies between DTs and MAS, offering valuable insights and a research agenda for future developments."
https://www.sciencedirect.com/science/article/pii/S095058492400106X,Reporting case studies in systematic literature studies—An evidential problem,October 2024,"Systematic mapping study, Systematic review, Systematic literature review, Case study, Credible evidence",Austen=Rainer: Not Found; Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Context:
The term and label, “case study”, is not used consistently by authors of primary studies in 
software engineering
 research. It is not clear whether this problem also occurs for systematic literature studies (SLSs).
Objective:
To investigate the extent to which SLSs in/correctly use the term and label, “case study”, when classifying primary studies.
Methods:
We systematically collect two sub-samples (2010–2021 & 2022) comprising a total of eleven SLSs and 79 primary studies. We examine the designs of these SLSs, and then analyse whether the SLS authors and the primary-study authors correctly label the respective primary study as a “case study”.
Results:
76% of the 79 primary studies are misclassified by SLSs (with the two sub-samples having 60% and 81% 
misclassification
, respectively). For 39% of the 79 studies, the SLSs propagate a mislabelling by the original authors, whilst for 37%, the SLSs introduce a new mislabel, thus making the problem worse. SLSs rarely present explicit definitions for “case study” and when they do, the definition is not consistent with established definitions.
Conclusions:
SLSs are both propagating and exacerbating the problem of the mislabelling of primary studies as “case studies”, rather than – as we should expect of SLSs – correcting the labelling of primary studies, and thus improving the body of credible evidence. Propagating and exacerbating mislabelling undermines the credibility of evidence in terms of its quantity, quality and relevance to both practice and research.",Information and Software Technology,18 Mar 2025,5,"The study on mislabeling of case studies in SLSs may have limited immediate practical impact on early-stage ventures in Europe, but could contribute to the reliability of software engineering research."
https://www.sciencedirect.com/science/article/pii/S0950584924001058,Multi-objective model transformation chain exploration with MOMoT,October 2024,"Model-driven optimization, Transformation chain, Multi-objective graph traversal, Model repository",Martin=Eisenberg: eisenberg.martin@jku.at; Apurvanand=Sahay: a_sahay@blr.amrita.edu; Davide=Di Ruscio: davide.diruscio@univaq.it; Ludovico=Iovino: ludovico.iovino@gssi.it; Manuel=Wimmer: manuel.wimmer@jku.at; Alfonso=Pierantonio: alfonso.pierantonio@univaq.it,"Abstract
Context:
The increasing complexity of modern systems leads to an increasing amount of artifacts that are used along the model-based software and systems 
development lifecycle
. This also includes model transformations, which serve for mapping models between representations, e.g., for verification and validation purposes.
Objectives:
Model repositories
 manage this variety of artifacts and promote 
reusability
, but should also enable the bundling of compatible artifacts. Therefore, model transformations should be reused and arranged into 
transformation chains
 to support more complex transformation scenarios. The resulting transformation should correspond to the user’s interest in terms of quality criteria such as model coverage, transformation coverage, and number of transformation steps, thus assembling such chains becomes a multi-objective problem.
Methods:
A novel multi-objective approach for exploring possible transformation chains residing in model repositories is presented. MOMoT, a model-driven optimization framework, is leveraged to explore the transformation space spanned by the repository. For demonstration, three differently populated repositories are considered.
Results:
We have extended MOMoT with an exhaustive, multi-objective search that explores the entire model transformation space defined by graph 
transformation rules
, allowing all possible transformation chains to be considered as solution. Accordingly, the optimal solutions were identified in the demonstration cases with negligible 
computation time
.
Conclusion:
The approach assists modelers when there are multiple chains for transforming an input model to a specified output model to consider. Our evaluation shows that the approach elicits all legitimate transformation chains, thus enabling the modelers to consider trade-offs in view of multiple criteria selection.",Information and Software Technology,18 Mar 2025,6,"The research on multi-objective model transformation chains could be beneficial for startups involved in model-based software development, offering a new approach for managing complexity."
https://www.sciencedirect.com/science/article/pii/S0950584924001174,SENEM: A software engineering-enabled educational metaverse,October 2024,"Metaverse engineering, Virtual learning environments, Human-centered studies, Software engineering in practice",Viviana=Pentangelo: vpentangelo@unisa.it; Dario=Di Dario: ddidario@unisa.it; Stefano=Lambiase: slambiase@unisa.it; Filomena=Ferrucci: fferrucci@unisa.it; Carmine=Gravino: gravino@unisa.it; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
The term metaverse refers to a persistent, virtual, three-dimensional environment where individuals may communicate, engage, and collaborate. One of the most multifaceted and challenging use cases of the metaverse is education, where educators and learners may require multiple technical, social, psychological, and interaction instruments to accomplish their learning objectives. While the characteristics of the metaverse might nicely fit the problem’s needs, our research points out a noticeable lack of knowledge into (1) the specific requirements that an educational metaverse should actually fulfill to let educators and learners successfully interact towards their objectives and (2) how to design an appropriate educational metaverse for both educators and learners.
Objective:
In this paper, we aim to bridge this knowledge gap by proposing 
SENEM
, a novel software engineering-enabled educational metaverse. We first elicit a set of functional requirements that an educational metaverse should fulfill.
Method:
In this respect, we conduct a literature survey to extract the currently available knowledge on the matter discussed by the research community, and afterward, we assess and complement such knowledge through semi-structured interviews with educators and learners. Upon completing the 
requirements elicitation
 stage, we then build our prototype implementation of 
SENEM
, a metaverse that makes available to educators and learners the features identified in the previous stage. Finally, we evaluate the tool in terms of learnability, efficiency, and satisfaction through a Rapid Iterative Testing and Evaluation research approach, leading us to the iterative refinement of our prototype.
Results:
Through our survey strategy, we extracted nine requirements that guided the tool development that the study participants positively evaluated.
Conclusion:
Our study reveals that the target audience appreciates the elicited design strategy. Our work has the potential to form a solid contribution that other researchers can use as a basis for further improvements.",Information and Software Technology,18 Mar 2025,8,"The proposal of SENEM, an educational metaverse, has high practical value for startups in the education technology space. The detailed methodology and positive evaluation results make it a promising innovation."
https://www.sciencedirect.com/science/article/pii/S0950584924001071,On current limitations of online eye-tracking to study the visual processing of source code,October 2024,"Eye-tracking, Code comprehension, Webcam, Online experiment, Fixation algorithm",Eva=Thilderkvist: evth1400@student.miun.se; Felix=Dobslaw: felix.dobslaw@miun.se,"Abstract
Context:
Eye-tracking is an increasingly popular instrument to study how programmers process and comprehend 
source code
. While most studies are conducted in controlled environments with lab-grade hardware, it would be desirable to simplify and scale participation in experiments for users sitting remotely, leveraging home equipment.
Objective:
This study investigates the possibility of performing eye-tracking studies remotely using open-source algorithms and consumer-grade webcams. It establishes the technology’s current limitations and evaluates the quality of the 
data collected
 by it. We conclude by recommending ways forward to address the shortcomings and make remote code-reading studies in support of eye-tracking feasible in the future.
Method:
We gathered eye-gaze data remotely from 40 participants performing a code reading experiment on a purpose-built web application. The utilized eye-tracker worked client-side and used ridge regression to generate x- and y-coordinates in real-time predicting the participants’ on-screen gaze points without the need to collect and save video footage. We processed and analysed the collected data according to 
common practices
 for isolating eye-movement events and deriving metrics used in 
software engineering
 eye-tracking studies. In response to the lack of an algorithm explicitly developed for detecting oculomotor fixation events in low-frequency webcam data, we also introduced a dispersion 
threshold algorithm
 for that purpose. The quality of the collected data was subsequently assessed to determine the adequacy and validity of the methodology for eye-tracking.
Results:
The collected data was found to be of varying quality despite extensive calibration and graphical user guidance. We present our results highlighting both the negative and positive observations from which the community hopefully can learn. Both accuracy and precision were low and ultimately deemed insufficient for drawing valid conclusions in a high-precision empirical study. We nonetheless contribute to identifying critical limitations to be addressed in future research. Apart from the overall challenge of vastly diverse equipment, setup, and configuration, we found two main problems with the current webcam eye-tracking technology. The first was the 
absence
 of a validated algorithm to isolate fixations in low-frequency data, compromising the assurance of the accuracy of the data derived from it. The second problem was the lack of algorithmic support for head movements when predicting gaze location. Unsupervised participants do not always keep their heads still, even if instructed to do so. Consequently, we frequently observed spatial shifts that corrupted many collected datasets. Three encouraging observations resulted from the study. Even when shifted, gaze points were consistently dispersed in patterns resembling both the shape and size of the stimuli without extreme deviations. We could also distinguish recognizable reading patterns. Linearity was significantly different when participants were reading source code compared to natural text, and we could detect the expected left-to-right and top-to-bottom reading directions for participants reading natural text snippets.
Conclusion:
The accuracy and precision levels were not sufficient for a word-by-word analysis of code reading but could be adequate for a broader, coarse-grained precision study. Additionally we identified two main issues compromising the collected data validity and contributed a fixation 
detection algorithm
 to approach one of these issues. With suitable solutions to the identified issues, remote eye-tracking studies with webcams on code reading could eventually be feasible.",Information and Software Technology,18 Mar 2025,7,"This abstract presents a study on remote eye-tracking using consumer-grade webcams for code reading experiments, highlighting limitations and potential solutions. While the findings have practical implications for software engineering research, the impact on early-stage ventures is moderate."
https://www.sciencedirect.com/science/article/pii/S0950584924001162,Measuring and improving software testability at the design level,October 2024,Not Found,Morteza=Zakeri-Nasrabadi: Not Found; Saeed=Parsa: parsa@iust.ac.ir; Sadegh=Jafari: Not Found,"Abstract
Context
The quality of software systems is significantly influenced by design testability, an aspect often overlooked during the initial phases of software development. The implementation may deviate from its design, resulting in decreased testability at the integration and unit levels.
Objective
The objective of this study is to automatically identify low-testable parts in object-orientated design and enhance them by refactoring to 
design patterns
. The impact of various design metrics mainly coupling (
e.g.
, fan-in and fan-out) and inheritance (
e.g.
, depth of inheritance tree and number of subclasses) metrics on design testability is measured to select the most appropriate refactoring candidates.
Method
The methodology involves creating a machine learning model for design testability prediction using a large dataset of Java classes, followed by developing an automated refactoring tool. The design classes are vectorized by ten design metrics and labeled with testability scores calculated from a mathematical model. The model computes testability based on code coverage and test suite size of classes that have already been tested via automatic tools. A voting 
regressor
 model is trained to predict the design testability of any class diagram based on these design metrics. The proposed refactoring tool for dependency injection and factory method is applied to various open-source Java projects, and its impact on design testability is assessed.
Results
The proposed design testability model demonstrates its effectiveness by satisfactorily predicting design testability, as indicated by a mean squared error of 0.04 and an R
2
 score of 0.53. The automated refactoring tool has been successfully evaluated on six open-source Java projects, revealing an enhancement in design testability by up to 19.11 %.
Conclusion
The proposed automated approach offers software developers the means to continuously evaluate and enhance design testability throughout the entire software development life cycle, mitigating the risk of testability issues stemming from design-to-implementation discrepancies.",Information and Software Technology,18 Mar 2025,9,"This abstract introduces an automated approach to identify and refactor low-testable parts in object-oriented design, significantly enhancing design testability. The practical value for early-stage ventures and startups is high as it offers a solution to mitigate risks arising from design-to-implementation discrepancies."
https://www.sciencedirect.com/science/article/pii/S0950584924001228,A vulnerability detection framework by focusing on critical execution paths,October 2024,Not Found,Jianxin=Cheng: jianxin@stu.pku.edu.cn; Yizhou=Chen: Not Found; Yongzhi=Cao: Not Found; Hanpin=Wang: whpxhy@pku.edu.cn,"Abstract
Context:
Vulnerability detection
 is critical to ensure software security, and detecting vulnerabilities in 
smart contract
 code is currently gaining massive attention. Existing deep learning-based vulnerability detection methods represent the code as a code structure graph and eliminate vulnerability-irrelevant nodes. Then, they learn vulnerability-related code features from the simplified graph for vulnerability detection. However, this simplified graph struggles to represent relatively complete structural information of code, which may affect the performance of existing vulnerability detection methods.
Objective:
In this paper, we present a novel 
V
ulnerability 
D
etection framework based on 
C
ritical 
E
xecution 
P
aths (VDCEP), which aims to improve 
smart contract
 vulnerability detection.
Method:
Firstly, given a code structure graph, we deconstruct it into multiple execution paths that reflect rich structural information of code. To reduce irrelevant code information, a path selection strategy is employed to identify critical execution paths that may contain vulnerable code information. Secondly, a feature extraction module is adopted to learn feature representations of critical paths. Finally, we feed all path feature representations into a classifier for vulnerability detection. Also, the feature weights of paths are provided to measure their importance in vulnerability detection.
Results:
We evaluate VDCEP on a large dataset with four types of smart contract vulnerabilities. Results show that VDCEP outperforms 14 representative vulnerability detection methods by 5.34%–60.88% in F1-score. The ablation studies analyze the effects of our path selection strategy and feature extraction module on VDCEP. Moreover, VDCEP still outperforms 
ChatGPT
 by 34.46% in F1-score.
Conclusion:
Compared to existing vulnerability detection methods, VDCEP is more effective in detecting smart contract vulnerabilities by utilizing critical execution paths. Besides, we can provide interpretable details about vulnerability detection by analyzing the path feature weights.",Information and Software Technology,18 Mar 2025,8,"This abstract presents a novel vulnerability detection framework for smart contract code based on critical execution paths, outperforming existing methods significantly. The impact on European early-stage ventures is substantial as it addresses a critical aspect of software security in the context of blockchain technology."
https://www.sciencedirect.com/science/article/pii/S0950584924001150,SeDPGK: Semi-supervised software defect prediction with graph representation learning and knowledge distillation,October 2024,Not Found,Wangshu=Liu: liuws0707@gmail.com; Ye=Yue: Not Found; Xiang=Chen: Not Found; Qing=Gu: Not Found; Pengzhan=Zhao: Not Found; Xuejun=Liu: Not Found; Jianjun=Zhao: Not Found,"Abstract
Context:
Constructing an effective 
defect prediction
 model relies on a substantial number of labeled program modules. Unfortunately, program module labeling is often time-consuming and error-prone. Semi-supervised 
software defect
 prediction (SSDP) can alleviate this issue by incorporating some labeled modules and the remaining unlabeled modules from the same project.
Objective:
However, previous SSDP methods ignore the significant influence of dependencies between software modules. The potential of 
knowledge distillation
 in leveraging labeled instances to guide the learning process and effectively utilizing information from unlabeled instances to improve SSDP performance has not been fully investigated.
Method:
We propose a novel approach SeDPGK. Specifically, to exploit the graph-structured knowledge, we first construct the program 
dependence graph
 to extract control and 
data dependencies
 among modules. Then we use 
graph neural networks
 (GNNs) to learn the 
graph representation
 of the module relationships and encode with the statement semantics of abstract syntax 
tree
 and traditional static features for diversity. Second, we integrate multiple GNNs jointly trained as teacher models to ensemble various styles of graph-based networks and generate trustworthy labels for unlabeled modules. Further, to preserve the teacher model’s sufficient structure and 
semantic knowledge
, we adopt a trainable label propagation and multi-layer perception as the student model and mitigate the differences between the teacher and student models using two widespread 
knowledge distillation
 functions.
Results:
We conducted our experiments on 17 real-world projects. The experimental results show that SeDPGK outperforms semi-supervised baselines with an average improvement of 16.9% for 
PD
, 42.5% for 
FAR
, and 8.9% for AUC, respectively. Moreover, the 
performance improvement
 is consistently significant across multiple statistical tests.
Conclusion:
The effectiveness of SeDPGK comes from the aggregation of the different GNNs with heterogeneity. Moreover, the graph structure and 
semantic features
 hidden behind the 
source code
 play a crucial role in the distillation framework.",Information and Software Technology,18 Mar 2025,9,"This abstract introduces a novel approach SeDPGK for semi-supervised software defect prediction, demonstrating significant performance improvement across real-world projects. The practical value for early-stage ventures is high as it offers an effective solution to improve software quality and reduce manual labeling effort."
https://www.sciencedirect.com/science/article/pii/S095058492400123X,Are your apps accessible? A GCN-based accessibility checker for low vision users,October 2024,Not Found,Mengxi=Zhang: zmx19@mails.jlu.edu.cn; Huaxiao=Liu: liuhuaxiao@jlu.edu.cn; Shenning=Song: songsz22@mails.jlu.edu.cn; Chunyang=Chen: chun-yang.chen@tum.de; Pei=Huang: huangpei@stanford.edu; Jian=Zhao: zhaojian@ccu.edu.cn,"Abstract
Context:
Accessibility issues (e.g., small size and narrow interval) in mobile applications (apps) lead to obstacles for billions of low vision users in interacting with Graphical User Interfaces (GUIs). Although GUI accessibility scanning tools exist, most of them perform rule-based check relying on complex GUI hierarchies. This might make them detect invisible redundant information, cannot handle small deviations, omit similar components, and is hard to extend.
Objective:
In this paper, we propose a novel approach, named ALVIN (Accessibility Checker for Low Vision), which represents the GUI as a graph and adopts the 
Graph Convolutional Neural Networks
 (GCN) to label inaccessible components.
Method:
ALVIN removes invisible views to prevent detecting redundancy and uses annotations from low vision users to handle small deviations. Also, the 
GCN model
 could consider the relations between GUI components, connecting similar components and reducing the possibility of omission. ALVIN only requires users to annotate the relevant dataset when detecting new kinds of issues.
Results:
Our experiments on 48 apps demonstrate the effectiveness of ALVIN, with precision of 83.5%, recall of 78.9%, and F1-score of 81.2%, outperforming 
baseline methods
. In RQ2, the usefulness is verified through 20 issues submitted to open-source apps. The RQ3 also illustrates the 
GCN model
 is better than other models.
Conclusion:
To summarize, our proposed approach can effectively detect accessibility issues in GUIs for low vision users, thereby guiding developers in fixing them efficiently.",Information and Software Technology,18 Mar 2025,6,"This abstract presents ALVIN, a novel approach for detecting accessibility issues in GUIs for low vision users using GCN. While the findings have implications for mobile app development, the direct impact on European early-stage ventures is moderate."
https://www.sciencedirect.com/science/article/pii/S0950584924001204,Feature envy detection based on cross-graph local semantics matching,October 2024,Not Found,Quanxin=Yang: Not Found; Dongjin=Yu: yudj@hdu.edu.cn; Xin=Chen: Not Found; Yihang=Xu: Not Found; Wangliang=Yan: Not Found; Bin=Hu: Not Found,"Abstract
Context:
As a typical code smell, feature envy occurs when a method exhibits excessive reliance and usage on specific functionalities of another class, which can lead to issues with the 
maintainability
 and extensibility of the code. As such, detecting and avoiding feature envy is critical for software development. Previous research on detecting feature envy has demonstrated significant advantages of deep learning-based approaches over 
static code analysis tools
. However, current deep learning-based approaches still suffer from two limitations: (1) They focus on the functional or overall semantics of the code, which ignores the opportunities for local code semantics matching, making it challenging to identify some more complex cases; (2) Existing feature envy datasets are collected or synthesized using 
static code analysis tools
, which limits feature envy cases to fixed rules and makes it challenging to cover other complex cases in real projects.
Objective:
We are motivated to propose a Siamese 
graph neural network
 based on code local semantics matching and collect feature envy refactoring cases from real projects for experimental evaluation.
Method:
To address the first issue, we propose a cross-graph local semantics 
matching network
, which aims to simulate human intuition or experience to detect feature envy by analyzing the local semantics matching between code graphs. To address the second one, we manually review and collect commits for refactoring feature envy cases on GitHub. Then, we refer to image 
data augmentation
 technology to construct two datasets for identifying feature envy and recommending 
Move Method
 refactorings, respectively.
Results:
Extensive experiments show that our approach outperforms state-of-the-art baselines regarding both tasks’ comprehensive metrics, F1-score and AUC.
Conclusion:
The experimental results indicate that the proposed Siamese 
graph neural network
 based on code local semantics matching is effective. In addition, the provided 
data augmentation
 algorithms can significantly improve model performance.",Information and Software Technology,18 Mar 2025,7,"The proposed Siamese graph neural network addresses existing limitations in detecting feature envy, providing significant advantages over current approaches. The data augmentation techniques also contribute to improving model performance, offering practical value to software development in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001216,Product managers in software startups: A grounded theory,October 2024,"Product manager, Software startup, Product management, Agile software development, Socio-technical systems, Grounded theory",Jorge=Melegati: jorge.melegati@unibz.it; Igor=Wiese: Not Found; Eduardo=Guerra: Not Found; Rafael=Chanin: Not Found; Abdullah=Aldaeej: Not Found; Tommi=Mikkonen: Not Found; Rafael=Prikladnicki: Not Found; Xiaofeng=Wang: Not Found,"Abstract
Context:
Defining and designing a software product is not merely a technical endeavor, but also a socio-technical journey. As such, its success is associated with human-related aspects, such as the value users perceive. To handle this issue, the product manager role has become more evident in software-intensive companies. A unique, challenging context for these professionals is constituted by software startups, emerging companies developing novel solutions looking for sustainable and scalable business models.
Objective:
This study aims to describe the role of product managers in the context of software startups.
Method:
We performed a Socio-Technical 
Grounded Theory
 study using data from blog posts and interviews.
Results:
The results describe the product manager as a multidisciplinary, general role, not only guiding the product by developing its vision but also as a connector that emerges in a growing company, enabling communication of software development with other areas, mainly business and 
user experience
. The professional performing this role has a background in one of these areas but a broad knowledge and understanding of key concepts of the other areas is needed. We also describe how differences of this role to other lead roles are perceived in practice.
Conclusions:
Our findings represent several implications for research, such as better understanding of the role transformation in growing software startups, practice, e.g., identifying the points to which a professional migrating to this role should pay attention, and the education of future software developers, by suggesting the inclusion of related topics in the education and training of future software engineers.",Information and Software Technology,18 Mar 2025,5,"While describing the role of product managers in software startups is informative, the practical implications for early-stage ventures are limited compared to other abstracts. It offers insights into the multidisciplinary nature of the role but may not directly impact or benefit startups' growth and development significantly."
https://www.sciencedirect.com/science/article/pii/S0950584924001216,Product managers in software startups: A grounded theory,October 2024,"Product manager, Software startup, Product management, Agile software development, Socio-technical systems, Grounded theory",Jorge=Melegati: jorge.melegati@unibz.it; Igor=Wiese: Not Found; Eduardo=Guerra: Not Found; Rafael=Chanin: Not Found; Abdullah=Aldaeej: Not Found; Tommi=Mikkonen: Not Found; Rafael=Prikladnicki: Not Found; Xiaofeng=Wang: Not Found,"Abstract
Context:
Defining and designing a software product is not merely a technical endeavor, but also a socio-technical journey. As such, its success is associated with human-related aspects, such as the value users perceive. To handle this issue, the product manager role has become more evident in software-intensive companies. A unique, challenging context for these professionals is constituted by software startups, emerging companies developing novel solutions looking for sustainable and scalable business models.
Objective:
This study aims to describe the role of product managers in the context of software startups.
Method:
We performed a Socio-Technical 
Grounded Theory
 study using data from blog posts and interviews.
Results:
The results describe the product manager as a multidisciplinary, general role, not only guiding the product by developing its vision but also as a connector that emerges in a growing company, enabling communication of software development with other areas, mainly business and 
user experience
. The professional performing this role has a background in one of these areas but a broad knowledge and understanding of key concepts of the other areas is needed. We also describe how differences of this role to other lead roles are perceived in practice.
Conclusions:
Our findings represent several implications for research, such as better understanding of the role transformation in growing software startups, practice, e.g., identifying the points to which a professional migrating to this role should pay attention, and the education of future software developers, by suggesting the inclusion of related topics in the education and training of future software engineers.",Information and Software Technology,18 Mar 2025,5,"Similar to Abstract 92, the description of product managers in software startups, while insightful, may not have a direct impact on the practical aspects of early-stage ventures. The study's implications for research and education are valuable but may not offer immediate benefits to startups."
https://www.sciencedirect.com/science/article/pii/S0950584924001253,CausalOps — Towards an industrial lifecycle for causal probabilistic graphical models,October 2024,"Causal engineering, Model lifecycle, MLOps, Causal graphical models",Robert=Maier: robert.maier@othr.de; Andreas=Schlattl: Andreas.Schlattl@efs-techhub.com; Thomas=Guess: Thomas.Guess@efs-techhub.com; Jürgen=Mottok: juergen.mottok@othr.de,"Abstract
Context:
Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as safety analysis of complex systems, 
software engineering
, and 
machine learning
, the need for an integrated lifecycle framework akin to 
DevOps
 and MLOps has emerged. Currently, such a reference for organizations interested in employing causal engineering is missing. This lack of guidance hinders the incorporation and maturation of causal methods in the context of real-life applications.
Objective:
This work contextualizes causal model usage across different stages and stakeholders and outlines a holistic view of creating and maintaining them within the process landscape of an organization.
Methods:
A novel lifecycle framework for causal model development and application called CausalOps is proposed. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, a consistent vocabulary and workflow model to guide organizations in adopting causal methods are established.
Results:
Based on the early adoption of the discussed methodology to a real-life problem within the automotive domain, an experience report underlining the practicability and challenges of the proposed approach is discussed.
Conclusion:
It is concluded that besides current technical advancements in various aspects of causal engineering, an overarching lifecycle framework that integrates these methods into organizational practices is missing. Although diverse skills from adjacent disciplines are widely available, guidance on how to transfer these assets into causality-driven practices still need to be addressed in the published literature. CausalOps’ aim is to set a baseline for the adoption of causal methods in practical applications within interested organizations and the causality community.",Information and Software Technology,18 Mar 2025,8,"The development of a novel lifecycle framework, CausalOps, for causal model usage provides a practical guide for organizations interested in employing causal engineering. The framework's aim to integrate causal methods into organizational practices fills a gap in the field, offering significant value to startups seeking to incorporate causal methods in their processes."
https://www.sciencedirect.com/science/article/pii/S0950584924001253,CausalOps — Towards an industrial lifecycle for causal probabilistic graphical models,October 2024,"Causal engineering, Model lifecycle, MLOps, Causal graphical models",Robert=Maier: robert.maier@othr.de; Andreas=Schlattl: Andreas.Schlattl@efs-techhub.com; Thomas=Guess: Thomas.Guess@efs-techhub.com; Jürgen=Mottok: juergen.mottok@othr.de,"Abstract
Context:
Causal probabilistic graph-based models have gained widespread utility, enabling the modeling of cause-and-effect relationships across diverse domains. With their rising adoption in new areas, such as safety analysis of complex systems, 
software engineering
, and 
machine learning
, the need for an integrated lifecycle framework akin to 
DevOps
 and MLOps has emerged. Currently, such a reference for organizations interested in employing causal engineering is missing. This lack of guidance hinders the incorporation and maturation of causal methods in the context of real-life applications.
Objective:
This work contextualizes causal model usage across different stages and stakeholders and outlines a holistic view of creating and maintaining them within the process landscape of an organization.
Methods:
A novel lifecycle framework for causal model development and application called CausalOps is proposed. By defining key entities, dependencies, and intermediate artifacts generated during causal engineering, a consistent vocabulary and workflow model to guide organizations in adopting causal methods are established.
Results:
Based on the early adoption of the discussed methodology to a real-life problem within the automotive domain, an experience report underlining the practicability and challenges of the proposed approach is discussed.
Conclusion:
It is concluded that besides current technical advancements in various aspects of causal engineering, an overarching lifecycle framework that integrates these methods into organizational practices is missing. Although diverse skills from adjacent disciplines are widely available, guidance on how to transfer these assets into causality-driven practices still need to be addressed in the published literature. CausalOps’ aim is to set a baseline for the adoption of causal methods in practical applications within interested organizations and the causality community.",Information and Software Technology,18 Mar 2025,8,"Similar to Abstract 94, the proposal of CausalOps as a lifecycle framework for causal model development and application is highly valuable for organizations looking to adopt causal methods. The emphasis on practical applications within interested organizations and the causality community demonstrates a clear practical impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000946,The impact of human aspects on the interactions between software developers and end-users in software engineering: A systematic literature review,September 2024,"Systematic literature review, Human aspects, Software developers, Software users, Software engineering",Hashini=Gunatilake: hashini.gunatilake@monash.edu; John=Grundy: john.grundy@monash.edu; Rashina=Hoda: rashina.hoda@monash.edu; Ingo=Mueller: ingo.mueller@monash.edu,"Abstract
Context:
Research on human aspects within the field of 
software engineering
 (SE) has been steadily gaining prominence in recent years. These human aspects have a significant impact on SE due to the inherently interactive and 
collaborative nature
 of the discipline.
Objective:
In this paper, we present a systematic literature review (SLR) on human aspects affecting developer-user interactions. The objective of this SLR is to plot the current landscape of primary studies by examining the human aspects that influence developer-user interactions, their implications, interrelationships, and how existing studies address these implications.
Method:
We conducted this SLR following the guidelines proposed by Kitchenham et al. We performed a comprehensive search in six digital databases, and an exhaustive backward and forward snowballing process. We selected 46 primary studies for data extraction.
Results:
We identified various human aspects affecting developer-user interactions in SE, assessed their interrelationships, identified their positive impacts and 
mitigation strategies
 for negative effects. We present specific recommendations derived from the identified research gaps.
Conclusion:
Our findings suggest the importance of leveraging positive effects and addressing negative effects in developer-user interactions through the implementation of effective mitigation strategies. These insights may benefit software practitioners for effective user interactions, and the recommendations proposed by this SLR may aid the research community in further human aspects related studies.",Information and Software Technology,18 Mar 2025,6,"The systematic literature review on human aspects affecting developer-user interactions provides insights that may benefit software practitioners, but the practical application for early-stage ventures is limited."
https://www.sciencedirect.com/science/article/pii/S0950584924000971,Understanding the landscape of software modelling assistants for MDSE tools: A systematic mapping,September 2024,"Modelling assistance, Model-driven development, Systematic mapping, State of the practice, Low code, No-code",David=Mosquera: mosq@zhaw.ch; Marcela=Ruiz: Not Found; Oscar=Pastor: Not Found; Jürgen=Spielberger: Not Found,"Abstract
Context
Model Driven Software Engineering (MDSE) and low-code/no-code software development tools promise to increase quality and productivity by modelling instead of coding software. One of the major advantages of modelling software is the increased possibility of involving diverse stakeholders since it removes the barrier of being IT experts to actively participate in software production processes. From an academic and industry point of view, the main question remains: What has been proposed to assist humans in software modelling tasks?
Objective
In this paper, we systematically elucidate the state of the art in assistants for software modelling and their use in MDSE and low-code/no-code tools.
Method
We conducted a systematic mapping to review the state of the art and answer the following research questions: i) how is software modelling assisted? ii) what goals and limitations do existing modelling assistance proposals report? iii) which evaluation metrics and target users do existing modelling assistance proposals consider? For this purpose, we selected 58 proposals from 3.176 screened records and reviewed 17 MDSE and low-code/no-code tools from main market players published by the Gartner Magic Quadrant.
Result
We clustered existing proposals regarding their modelling assistance strategies, goals, limitations, evaluation metrics, and target users, both in research and practice.
Conclusions
We found that both academic and industry proposals recognise the value of assisting software modelling. However, documentation about MDSE assistants’ limitations, evaluation metrics, and target users is scarce or non-existent. With the advent of artificial intelligence, we expect more assistants for MDSE and low-code/no-code software development will emerge, making imperative the need for well-founded frameworks for designing modelling assistants focused on addressing target users’ needs and advancing the state of the art.",Information and Software Technology,18 Mar 2025,7,"The exploration of assistants for software modelling in MDSE and low-code/no-code tools can bring increased quality and productivity, which could be valuable for European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000958,Automating modern code review processes with code similarity measurement,September 2024,Not Found,Yusuf=Kartal: ykartal@ogu.edu.tr; E. Kaan=Akdeniz: Not Found; Kemal=Özkan: Not Found,"Abstract
Context:
Modern code review is a critical component in software development processes, as it ensures security, detects errors early and improves code quality. However, manual reviews can be time-consuming and unreliable. Automated code review can address these issues. Although deep-learning methods have been used to recommend code review comments, they are expensive to train and employ. Instead, information retrieval (IR)-based methods for automatic code review are showing promising results in efficiency, effectiveness, and flexibility.
Objective:
Our main objective is to determine the optimal combination of the 
vectorization
 method and similarity to measure what gives the best results in an automatic code review, thereby improving the performance of IR-based methods.
Method:
Specifically, we investigate different 
vectorization
 methods (Word2Vec, Doc2Vec, Code2Vec, and Transformer) that differ from previous research (TF-IDF and Bag-of-Words), and similarity measures (Cosine, Euclidean, and Manhattan) to capture the semantic similarities between code texts. We evaluate the performance of these methods using standard metrics, such as Blue, Meteor, and Rouge-L, and include the run-time of the models in our results.
Results:
Our results demonstrate that the Transformer model outperforms the state-of-the-art method in all standard metrics and similarity measurements, achieving a 19.1% improvement in providing exact matches and a 6.2% improvement in recommending reviews closer to human reviews.
Conclusion:
Our findings suggest that the Transformer model is a highly effective and efficient approach for recommending code review comments that closely resemble those written by humans, providing valuable insight for developing more efficient and effective automated code review systems.",Information and Software Technology,18 Mar 2025,8,"The investigation into automatic code review using different vectorization methods and similarity measures, with a focus on improving effectiveness and efficiency, has practical implications for software development processes and early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000958,Automating modern code review processes with code similarity measurement,September 2024,Not Found,Yusuf=Kartal: ykartal@ogu.edu.tr; E. Kaan=Akdeniz: Not Found; Kemal=Özkan: Not Found,"Abstract
Context:
Modern code review is a critical component in software development processes, as it ensures security, detects errors early and improves code quality. However, manual reviews can be time-consuming and unreliable. Automated code review can address these issues. Although deep-learning methods have been used to recommend code review comments, they are expensive to train and employ. Instead, information retrieval (IR)-based methods for automatic code review are showing promising results in efficiency, effectiveness, and flexibility.
Objective:
Our main objective is to determine the optimal combination of the 
vectorization
 method and similarity to measure what gives the best results in an automatic code review, thereby improving the performance of IR-based methods.
Method:
Specifically, we investigate different 
vectorization
 methods (Word2Vec, Doc2Vec, Code2Vec, and Transformer) that differ from previous research (TF-IDF and Bag-of-Words), and similarity measures (Cosine, Euclidean, and Manhattan) to capture the semantic similarities between code texts. We evaluate the performance of these methods using standard metrics, such as Blue, Meteor, and Rouge-L, and include the run-time of the models in our results.
Results:
Our results demonstrate that the Transformer model outperforms the state-of-the-art method in all standard metrics and similarity measurements, achieving a 19.1% improvement in providing exact matches and a 6.2% improvement in recommending reviews closer to human reviews.
Conclusion:
Our findings suggest that the Transformer model is a highly effective and efficient approach for recommending code review comments that closely resemble those written by humans, providing valuable insight for developing more efficient and effective automated code review systems.",Information and Software Technology,18 Mar 2025,8,"The study on the influence of human factors like personality and self-efficacy on domain modeling productivity provides valuable insights for enhancing quality and productivity in software development, which can be beneficial for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492400096X,The impact of personality and self-efficacy on domain modeling productivity in graphical and textual notations,September 2024,"Human factors, Model productivity, Personality, Self-efficacy, Domain modelling, Empirical software engineering, Model-driven engineering, Domain-driven design",Santiago=Meliá: santi@dlsi.ua.es; Raymari=Reyes: Not Found; Cristina=Cachero: Not Found,"Abstract
Context
Software development is a complex and human-intensive activity, where human factors can have a significant impact on productivity and quality of results. To address the complexity of software, domain modeling has gained much importance, mainly due to software methodologies such as Model-Driven Engineering and Domain-Driven Design. In particular, domain modeling is an essential task that allows developers to understand and effectively represent the problem domain. However, domain modeling productivity can be affected by several human factors, including developers' personality and self-efficacy.
Objective
The study aims to explore the influence of human factors, specifically developers' personality and self-efficacy, on domain modeling productivity in graphical and textual notations.
Method
An empirical controlled study was conducted with 134 third-year computer science students from the University of Alicante, guided by the definition of a theoretical model based on previous studies. The participants were tasked with creating domain models in both graphical and textual notations. The order in which the notations were used was randomized, and the participants were given different system specifications to model. After modeling, 98 participants completed questionnaires assessing their personality, self-efficacy, and notation satisfaction. The design and evaluation of the experiment employed the Goal, Question, and Metrics framework. Data analysis was performed using a stepwise selection method to select the most appropriate regression model.
Results
The study indicates that personality and self-efficacy have a significant impact on the performance of junior domain model developers. Specifically, it was discovered that while 
neuroticism
 had a 
negative impact
 on efficiency in both notations, developers' ability belief and use of 
graphical notation
 had a positive influence on effectiveness and efficiency in creating domain models.
Conclusions
These findings highlight the importance of considering human factors and notation choice in software development. Developers' personality and self-efficacy emerge as critical considerations for enhancing both productivity and quality in domain modeling.",Information and Software Technology,18 Mar 2025,7,"The empirical study on the influence of human factors on domain modeling productivity, specifically personality and self-efficacy, offers practical implications for improving productivity and quality, which could be valuable for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001046,Boosting fault localization of statements by combining topic modeling and Ochiai,September 2024,Not Found,Romain=Vacheret: romain.vacheret@lip6.fr; Francisca=Pérez: mfperez@usj.es; Tewfik=Ziadi: tewfik.ziadi@lip6.fr; Lom=Hillah: lom.hillah@newco-partners.com,"Abstract
Context:
Reducing the cost of maintenance tasks by fixing bugs automatically is the cornerstone of Automated Program Repair (APR). To do this, automated 
Fault Localization
 (FL) is essential. Two families of FL techniques are Spectrum-based 
Fault Localization
 (SBFL) and Information Retrieval Fault Localization (IRFL). In SBFL, the coverage information and execution results of test cases are utilized. Ochiai is one of the most effective and used SBFL strategies. In IRFL, the 
bug report
 information is utilized as well as the identifier names and comments in 
source code files
. 
Latent Dirichlet Allocation
 (LDA) is a generative statistical model and one of the most popular 
topic modeling
 methods. However, LDA has been used at the method level of 
granularity
 as IRFL technique, whereas most existing APR tools are focused on the statement level.
Objective:
This paper presents our approach that combines 
topic modeling
 and Ochiai to boost FL at the statement level.
Method:
We evaluate our approach considering five different projects in Defects4J benchmark. We report the performance of our approach in terms of hit@k and MRR. To study the impact on the results, we compare our approach against five baselines: two SBFL approaches (Ochiai and Dstar), two IRFL approaches (LDA and Blues), and one hybrid approach (SBIR). In addition, we compare the number of bugs that are found by our approach with the baselines.
Results:
Our approach significantly outperforms the baselines in all metrics. Especially, when hit@1, hit@3 and hit@5 are compared. Also, our approach locates more bugs than Ochiai and Blues.
Conclusion:
The results of our approach indicate that the integration of topic modeling with Ochiai boosts FL. This uncovers the potential of topic modeling for FL at statement level, which is valuable for the APR community.",Information and Software Technology,18 Mar 2025,8,"The integration of topic modeling with Ochiai to boost Fault Localization at the statement level has significant practical value for automated program repair, showcasing potential for advancing the APR community."
https://www.sciencedirect.com/science/article/pii/S0950584924000910,"A brief note, with thanks, on the contributions of Guenther Ruhe",September 2024,Not Found,Tim=Menzies: timm@ieee.org,"Abstract
Someone once told me that the best we can hope for as academics is to be a footnote in some as-yet-unwritten textbook. Guenther Ruhe’s footnote will surely be very large.",Information and Software Technology,18 Mar 2025,1,The abstract lacks relevance and practical value for European early-stage ventures or startups in the context of technology adoption or software engineering.
https://www.sciencedirect.com/science/article/pii/S0950584924000910,"A brief note, with thanks, on the contributions of Guenther Ruhe",September 2024,Not Found,Tim=Menzies: timm@ieee.org,"Abstract
Someone once told me that the best we can hope for as academics is to be a footnote in some as-yet-unwritten textbook. Guenther Ruhe’s footnote will surely be very large.",Information and Software Technology,18 Mar 2025,1,The abstract lacks relevance and practical value for European early-stage ventures or startups in the context of technology adoption or software engineering.
https://www.sciencedirect.com/science/article/pii/S0950584924000740,Acceptance behavior theories and models in software engineering — A mapping study,August 2024,"Acceptance behavior, Technology adoption, Theory use in software engineering, TAM, TPB, TRA, Fitness, Innovation diffusion",Jürgen=Börstler: jurgen.borstler@bth.se; Nauman bin=Ali: Not Found; Kai=Petersen: Not Found; Emelie=Engström: Not Found,"Abstract
Context:
The adoption or acceptance of new technologies or ways of working in software development activities is a recurrent topic in the 
software engineering
 literature. The topic has, therefore, been empirically investigated extensively. It is, however, unclear which theoretical frames of reference are used in this research to explain acceptance behaviors.
Objective:
In this study, we explore how major theories and models of acceptance behavior have been used in the software engineering literature to empirically investigate acceptance behavior.
Method:
We conduct a 
systematic mapping study
 of empirical studies using acceptance behavior theories in software engineering.
Results:
We identified 47 primary studies covering 56 theory uses. The theories were categorized into six groups. Technology acceptance models (TAM and its extensions) were used in 29 of the 47 primary studies, innovation theories in 10, and the theories of planned behavior/ reasoned action (TPB/TRA) in six. All other theories were used in at most two of the primary studies. The usage and operationalization of the theories were, in many cases, inconsistent with the underlying theories. Furthermore, we identified 77 constructs used by these studies of which many lack clear definitions.
Conclusions:
Our results show that software engineering researchers are aware of some of the leading theories and models of acceptance behavior, which indicates an attempt to have more theoretical foundations. However, we identified issues related to theory usage that make it difficult to aggregate and synthesize results across studies. We propose 
mitigation actions
 that encourage the consistent use of theories and emphasize the measurement of key constructs.",Information and Software Technology,18 Mar 2025,6,"The systematic mapping study on acceptance behavior theories in software engineering provides insights into the theoretical foundations of technology adoption, offering potential value for startups looking to understand user acceptance of new technologies."
https://www.sciencedirect.com/science/article/pii/S0950584924000740,Acceptance behavior theories and models in software engineering — A mapping study,August 2024,"Acceptance behavior, Technology adoption, Theory use in software engineering, TAM, TPB, TRA, Fitness, Innovation diffusion",Jürgen=Börstler: jurgen.borstler@bth.se; Nauman bin=Ali: Not Found; Kai=Petersen: Not Found; Emelie=Engström: Not Found,"Abstract
Context:
The adoption or acceptance of new technologies or ways of working in software development activities is a recurrent topic in the 
software engineering
 literature. The topic has, therefore, been empirically investigated extensively. It is, however, unclear which theoretical frames of reference are used in this research to explain acceptance behaviors.
Objective:
In this study, we explore how major theories and models of acceptance behavior have been used in the software engineering literature to empirically investigate acceptance behavior.
Method:
We conduct a 
systematic mapping study
 of empirical studies using acceptance behavior theories in software engineering.
Results:
We identified 47 primary studies covering 56 theory uses. The theories were categorized into six groups. Technology acceptance models (TAM and its extensions) were used in 29 of the 47 primary studies, innovation theories in 10, and the theories of planned behavior/ reasoned action (TPB/TRA) in six. All other theories were used in at most two of the primary studies. The usage and operationalization of the theories were, in many cases, inconsistent with the underlying theories. Furthermore, we identified 77 constructs used by these studies of which many lack clear definitions.
Conclusions:
Our results show that software engineering researchers are aware of some of the leading theories and models of acceptance behavior, which indicates an attempt to have more theoretical foundations. However, we identified issues related to theory usage that make it difficult to aggregate and synthesize results across studies. We propose 
mitigation actions
 that encourage the consistent use of theories and emphasize the measurement of key constructs.",Information and Software Technology,18 Mar 2025,6,"The systematic mapping study on acceptance behavior theories in software engineering provides insights into the theoretical foundations of technology adoption, offering potential value for startups looking to understand user acceptance of new technologies."
https://www.sciencedirect.com/science/article/pii/S0950584924000806,A Systematic Literature Review on Software Maintenance Offshoring Decisions,August 2024,Not Found,Hanif Ur=Rahman: Not Found; Alberto Rodrigues=da Silva: Not Found; Asaad=Alzayed: Not Found; Mushtaq=Raza: Not Found,"Abstract
Context
Over the last decades, the rapid expansion of the internet has prompted an increasing number of organizations that have taken their work global and have outsourced their information technology (IT) activities to specialized suppliers. The longest part of the 
software life cycle
 includes software maintenance, which consumes 60-70% of the total IT budget. Therefore, organizations have adopted offshoring strategies to reduce maintenance costs and free up resources to focus on their 
core competencies
. Offshore outsourcing decision-making involves technical, social, and other influencing factors; however, there is a limited understanding of the key factors associated with offshoring software maintenance within the 
global software development
 context.
Objective
This work presents the factors that have influenced the decision-making process of offshoring software maintenance. Further, this research sheds light on decision-making by identifying the models, frameworks, and 
software tools
 used within this context.
Method
A systematic literature review is conducted, delving into the factors related to the decision-making and analyzing the models, frameworks and tools supporting offshoring software maintenance.
Results
This study identifies the top 10 key factors concerning the decision-making process, namely human communication, cost reduction, organizational and employee maturity, 
project management practices
, IT infrastructure support, language constraints, knowledge-based support, changes in requirements, legal issues and cultural diversity. In addition, the models, frameworks, and tools used in the decision-making process of software maintenance are analyzed, and research gaps are identified.
Conclusion
The findings reveal that the software industry lacks effective and efficient models tailored explicitly for software offshoring within the 
global software development
 landscape. Overall, this study provides valuable insights into the decision-making dynamics of software maintenance offshoring by identifying key factors and research gaps that can pave the way for developing more effective decision 
support systems
.",Information and Software Technology,18 Mar 2025,8,"The study provides valuable insights into decision-making dynamics of software maintenance offshoring, identifying key factors and research gaps that can lead to more effective decision support systems."
https://www.sciencedirect.com/science/article/pii/S0950584924000727,Automatic test cases generation from formal contracts,August 2024,"Automatic test cases generation, Software testing, Formal methods, Software verification",Samuel Jiménez=Gil: Samuel.Gil@satixfy.com; Manuel I.=Capel: manuelcapel@ugr.es; Gabriel Olea=Olea: gabrieloo@correo.ugr.es,"Abstract
Context:
Software verification
 for 
critical systems
 is facing an unprecedented cost increase due to the large amount of software packed in 
multicore platforms
 generally. A substantial amount of the verification efforts are dedicated to testing. Spark/Ada is a language often employed in safety-critical systems due to its high reliability. Formal contracts are often inserted in Spark’s program specification to be used by a static 
theorem prover
 that checks whether the specification conforms with the implementation. However, this 
static analysis
 has its limitations as certain bugs can only be spotted through software testing.
Objective:
The main goal of our work is to use these formal contracts in Spark as input for a test oracle – whose method we describe – to generate test cases. Subsequent objectives consist of a) arguing about the traceability to comply with safety-critical software standards such as DO-178C for civil avionics and b) embracing the best-established software testing methods for these systems.
Method:
Our test generation method reads Spark formal contracts and applies Equivalence Class Partitioning with Boundary Analysis as a software testing method generating traceable test cases.
Results:
The evaluation, which uses an array of open-source examples of Spark contracts, shows a high level of passed test cases and 
statement coverage
. The results are also compared against a 
random test
 generator.
Conclusion:
The proposed method is very effective at achieving a high number of passed test cases and coverage. We make the case that the effort to create formal specifications for Spark can be used both for proof and (automatic) testing. Lastly, we noticed that some formal contracts are more suitable than others for our test generation.",Information and Software Technology,18 Mar 2025,9,"The proposed method effectively achieves a high number of passed test cases and coverage, making it valuable for improving software testing methods and complying with safety-critical software standards."
https://www.sciencedirect.com/science/article/pii/S0950584924000727,Automatic test cases generation from formal contracts,August 2024,"Automatic test cases generation, Software testing, Formal methods, Software verification",Samuel Jiménez=Gil: Samuel.Gil@satixfy.com; Manuel I.=Capel: manuelcapel@ugr.es; Gabriel Olea=Olea: gabrieloo@correo.ugr.es,"Abstract
Context:
Software verification
 for 
critical systems
 is facing an unprecedented cost increase due to the large amount of software packed in 
multicore platforms
 generally. A substantial amount of the verification efforts are dedicated to testing. Spark/Ada is a language often employed in safety-critical systems due to its high reliability. Formal contracts are often inserted in Spark’s program specification to be used by a static 
theorem prover
 that checks whether the specification conforms with the implementation. However, this 
static analysis
 has its limitations as certain bugs can only be spotted through software testing.
Objective:
The main goal of our work is to use these formal contracts in Spark as input for a test oracle – whose method we describe – to generate test cases. Subsequent objectives consist of a) arguing about the traceability to comply with safety-critical software standards such as DO-178C for civil avionics and b) embracing the best-established software testing methods for these systems.
Method:
Our test generation method reads Spark formal contracts and applies Equivalence Class Partitioning with Boundary Analysis as a software testing method generating traceable test cases.
Results:
The evaluation, which uses an array of open-source examples of Spark contracts, shows a high level of passed test cases and 
statement coverage
. The results are also compared against a 
random test
 generator.
Conclusion:
The proposed method is very effective at achieving a high number of passed test cases and coverage. We make the case that the effort to create formal specifications for Spark can be used both for proof and (automatic) testing. Lastly, we noticed that some formal contracts are more suitable than others for our test generation.",Information and Software Technology,18 Mar 2025,9,"The proposed method effectively achieves a high number of passed test cases and coverage, making it valuable for improving software testing methods and complying with safety-critical software standards."
https://www.sciencedirect.com/science/article/pii/S0950584924000788,Automatic build repair for test cases using incompatible Java versions,August 2024,Not Found,Ching Hang=Mak: chmakac@connect.ust.hk; Shing-Chi=Cheung: scc@cse.ust.hk,"Abstract
Context:
Bug bisection is a common technique used to identify a revision that introduces a bug or indirectly fixes a bug, and often involves executing multiple revisions of a project to determine whether the bug is present within the revision. However, many legacy revisions often cannot be successfully compiled due to changes in the programming language or tools used in the compilation process, adding complexity and preventing automation in the bisection process.
Objective:
In this paper, we introduce an approach to repair test cases of Java projects by performing dependency minimization. Our approach aims to remove classes and methods that are not required for the execution of one or more test cases. Unlike existing state-of-the-art techniques, our approach performs minimization at source-level, which allows compile-time errors to be fixed.
Methods:
A standalone Java tool implementing our technique was developed, and we evaluated our technique using subjects from Defects4J retargeted against Java 8 and 17.
Results:
Our evaluation showed that a majority of subjects can be repaired solely by performing minimization, including replicating the test results of the original version. Furthermore, our technique is also shown to achieve accurate minimized results, while only adding a small overhead to the bisection process.
Conclusion:
Our proposed technique is shown to be effective for repairing build failures with minimal overhead, making it suitable for use in automated bug bisection. Our tool can also be adapted for use cases such as bug corpus creation and refactoring.",Information and Software Technology,18 Mar 2025,7,"The approach presented is effective for repairing build failures with minimal overhead, suitable for use in automated bug bisection, which can be beneficial for startups dealing with legacy codebases."
https://www.sciencedirect.com/science/article/pii/S0950584924000818,CriticalFuzz: A critical neuron coverage-guided fuzz testing framework for deep neural networks,August 2024,Not Found,Tongtong=Bai: btt070619@163.com; Song=Huang: huangsong@aeu.edu.cn; Yifan=Huang: yifan005@e.ntu.edu.so; Xingya=Wang: xingyawang@outlook.com; Chunyan=Xia: xiachunyan@mdjnu.edu.cn; Yubin=Qu: yubinqu@icloud.com; Zhen=Yang: yangzhen@aeu.edu.cn,"Abstract
Context:
Deep neural networks
 (DNN) have been widely deployed in safety-critical domains, such as autonomous cars and healthcare, where error behaviors can lead to serious accidents, testing DNN is extremely important. Neuron coverage-guided 
fuzz testing
 (NCFT) has become an effective whitebox testing approach for testing DNN, which iteratively generates new test cases with the guidance of neuron coverage to explore different logics of DNN, and has found numerous defects. However, existing NCFT approaches ignore that the role of neurons is distinct for the final output of DNN. Given an input, only a fraction of neurons determines the final output of the DNN. These neurons hold the essential logic of the DNN.
Objective:
To ensure the quality of DNN and improve testing efficiency, NCFT should first cover neurons containing major logic of DNN.
Method:
In this paper, we propose the critical neurons that hold essential logic of DNN. In order to prioritize the detection of potential defects of critical neurons, we propose a 
fuzz testing
 framework, named CriticalFuzz, which mainly contains the energy-based test case generation and the critical neuron coverage criteria. The energy-based test case generation has the capability to produce test cases that are more likely to cover critical neurons and involves energy-based seed selection, power schedule, and seed mutation. The critical neuron coverage as a mechanism for providing feedback to guide the CriticalFuzz in prioritizing the coverage of critical neurons. To evaluate the significance of critical neurons and the performance of CriticalFuzz, we conducted experiments on popular DNNs and datasets.
Results:
The experiment results show that (1) the critical neurons have a 100% impact on the output of models, while the non-critical neurons have a lesser effect; (2) CriticalFuzz is effective in achieving 100% coverage of critical neurons and covering 10 classes of critical neurons, outperforming both DeepHunter and TensorFuzz. (3) CriticalFuzz exhibits exceptional 
error detection
 capabilities, successfully identifying thousands of errors across 10 diverse error classes within DNN.
Conclusion:
The critical neurons defined in this paper hold more significant logic of DNN than non-critical neurons. CriticalFuzz can preferentially cover critical neurons, thereby improving the efficiency of the NCFT process. Additionally, CriticalFuzz is capable of identifying a 
greater number
 of errors, thus enhancing the reliability and effectiveness of the NCFT.",Information and Software Technology,18 Mar 2025,10,"The CriticalFuzz framework demonstrates exceptional error detection capabilities and significantly enhances the efficiency and reliability of testing DNN, which can have a high impact on startups operating in safety-critical domains."
https://www.sciencedirect.com/science/article/pii/S0950584924000570,Case study identification with GPT-4 and implications for mapping studies,July 2024,"Systematic mapping studies, Data extraction, Case study, GPT-4",Kai=Petersen: kai.petersen@hs-flensburg.de,"Abstract
Context:
Rainer and Wohlin showed that 
case studies
 are not well understood by reviewers and authors and thus they say that a given research is a case study when it is not.
Objective:
Rainer and Wohlin proposed a smell indicator (inspired by code smells) to identify 
case studies
 based on the frequency of occurrences of words, which performed better than human classifiers. With the emergence of 
ChatGPT
, we evaluate ChatGPT to assess its performance in accurately identifying case studies. We also reflect on the results’ implications for mapping studies, specifically data extraction.
Method:
We used ChatGPT with the model GPT-4 to identify case studies and compared the result with the smell indicator for precision, recall, and accuracy.
Results:
GPT-4 and the smell indicator perform similarly, with GPT-4 performing slightly better in some instances and the smell indicator (SI) in others. The advantage of GPT-4 is that it is based on the definition of case studies and provides traceability on how it reaches its conclusions.
Conclusion:
As GPT-4 performed well on the task and provides traceability, we should use and, with that, evaluate it on data extraction tasks, supporting us as authors.",Information and Software Technology,18 Mar 2025,8,"The study evaluates the performance of ChatGPT in identifying case studies, which could have practical implications for mapping studies. The use of GPT-4 and traceability are valuable for authors in tasks like data extraction."
https://www.sciencedirect.com/science/article/pii/S0950584924000570,Case study identification with GPT-4 and implications for mapping studies,July 2024,"Systematic mapping studies, Data extraction, Case study, GPT-4",Kai=Petersen: kai.petersen@hs-flensburg.de,"Abstract
Context:
Rainer and Wohlin showed that 
case studies
 are not well understood by reviewers and authors and thus they say that a given research is a case study when it is not.
Objective:
Rainer and Wohlin proposed a smell indicator (inspired by code smells) to identify 
case studies
 based on the frequency of occurrences of words, which performed better than human classifiers. With the emergence of 
ChatGPT
, we evaluate ChatGPT to assess its performance in accurately identifying case studies. We also reflect on the results’ implications for mapping studies, specifically data extraction.
Method:
We used ChatGPT with the model GPT-4 to identify case studies and compared the result with the smell indicator for precision, recall, and accuracy.
Results:
GPT-4 and the smell indicator perform similarly, with GPT-4 performing slightly better in some instances and the smell indicator (SI) in others. The advantage of GPT-4 is that it is based on the definition of case studies and provides traceability on how it reaches its conclusions.
Conclusion:
As GPT-4 performed well on the task and provides traceability, we should use and, with that, evaluate it on data extraction tasks, supporting us as authors.",Information and Software Technology,18 Mar 2025,8,"The study evaluates the performance of ChatGPT in identifying case studies, which could have practical implications for mapping studies. The use of GPT-4 and traceability are valuable for authors in tasks like data extraction."
https://www.sciencedirect.com/science/article/pii/S0950584924000569,Understanding and evaluating software reuse costs and benefits from industrial cases—A systematic literature review,July 2024,"Software reuse, Software reuse costs, Software reuse benefits, Systematic literature review",Xingru=Chen: xingru.chen@bth.se; Muhammad=Usman: muhammad.usman@bth.se; Deepika=Badampudi: deepika.badampudi@bth.se,"Abstract
Context:
Software reuse
 costs and benefits have been investigated in several primary studies, which have been aggregated in multiple secondary studies as well. However, existing secondary studies on software reuse have not critically appraised the evidence in primary studies. Moreover, there has been relatively less focus on how software reuse costs and benefits were measured in the primary studies, and the aggregated evidence focuses more on software reuse benefits than reuse costs.
Objective:
This study aims to cover the gaps mentioned in the context above by synthesizing and critically appraising the evidence reported on software reuse costs and benefits from industrial cases.
Method:
We used a systematic literature review (SLR) to conduct this study. The results of this SLR are based on a final set of 30 primary studies.
Results:
We identified nine software reuse benefits and six software reuse costs, in which better quality and improved productivity were investigated the most. The primary studies mostly used defect-based and development time-based metrics to measure reuse benefits and costs. Regarding the reuse practices, the results show that software product lines, verbatim reuse, and systematic reuse were the top investigated ones, contributing to more reuse benefits. The quality assessment of the primary studies showed that most of them are either of low (20%) or moderate (67%) quality.
Conclusion:
Based on the number and quality of the studies, we conclude that the strength of evidence for better quality and improved productivity as reuse benefits is high. There is a need to conduct more high quality studies to investigate, not only other reuse costs and benefits, but also how relatively new reuse-related practices, such as InnerSource and 
microservices architecture
, impact software reuse.",Information and Software Technology,18 Mar 2025,7,"The study synthesizes evidence on software reuse costs and benefits from industrial cases, highlighting the need for more high-quality studies in the field. The findings contribute to understanding the impact of new reuse-related practices."
https://www.sciencedirect.com/science/article/pii/S0950584924000569,Understanding and evaluating software reuse costs and benefits from industrial cases—A systematic literature review,July 2024,"Software reuse, Software reuse costs, Software reuse benefits, Systematic literature review",Xingru=Chen: xingru.chen@bth.se; Muhammad=Usman: muhammad.usman@bth.se; Deepika=Badampudi: deepika.badampudi@bth.se,"Abstract
Context:
Software reuse
 costs and benefits have been investigated in several primary studies, which have been aggregated in multiple secondary studies as well. However, existing secondary studies on software reuse have not critically appraised the evidence in primary studies. Moreover, there has been relatively less focus on how software reuse costs and benefits were measured in the primary studies, and the aggregated evidence focuses more on software reuse benefits than reuse costs.
Objective:
This study aims to cover the gaps mentioned in the context above by synthesizing and critically appraising the evidence reported on software reuse costs and benefits from industrial cases.
Method:
We used a systematic literature review (SLR) to conduct this study. The results of this SLR are based on a final set of 30 primary studies.
Results:
We identified nine software reuse benefits and six software reuse costs, in which better quality and improved productivity were investigated the most. The primary studies mostly used defect-based and development time-based metrics to measure reuse benefits and costs. Regarding the reuse practices, the results show that software product lines, verbatim reuse, and systematic reuse were the top investigated ones, contributing to more reuse benefits. The quality assessment of the primary studies showed that most of them are either of low (20%) or moderate (67%) quality.
Conclusion:
Based on the number and quality of the studies, we conclude that the strength of evidence for better quality and improved productivity as reuse benefits is high. There is a need to conduct more high quality studies to investigate, not only other reuse costs and benefits, but also how relatively new reuse-related practices, such as InnerSource and 
microservices architecture
, impact software reuse.",Information and Software Technology,18 Mar 2025,7,"The study synthesizes evidence on software reuse costs and benefits from industrial cases, highlighting the need for more high-quality studies in the field. The findings contribute to understanding the impact of new reuse-related practices."
https://www.sciencedirect.com/science/article/pii/S0950584924000612,The need for more informative defect prediction: A systematic literature review,July 2024,"Systematic literature review, Software quality, Software defect prediction, Explainable AI, Machine learning",Natalie=Grattan: natalie.grattan@postgrad.otago.ac.nz; Daniel=Alencar da Costa: danielcalencar@otago.ac.nz; Nigel=Stanger: nigel.stanger@otago.ac.nz,"Abstract
Context:
Software defect
 prediction is crucial for prioritising quality assurance tasks, however, there are still limitations to the use of defect models. For example, the outputs often do not provide the defect type, severity, or the cause of the defect. Current models are also often complex in implementation (they use low transparency classifiers such as 
random forest
 or support vector machines) and primarily output binary predictions. They lack directly 
actionable outputs
, that is, outputs that provide additional information (e.g., defect severity or defect type) to aid in fixing the defect. One approach is to utilise tools of 
explainable AI
.
Objective:
In order to improve current models and plan the direction for explainability in software 
defect prediction
, we need to understand how explainable current models are.
Methods:
Starting from 861 papers from multiple databases, we investigated a sample of 132 papers in a systematic literature review. We extracted the following information to answer our research questions: (i) information about the outputs (e.g., how informative they were) and explainability methods used, (ii) how explainability and performance is measured and (iii) explainability in future research. Our results were summarised by manually labelling the data so that trends could be analysed across selected papers, along with a thematic analysis.
Results:
We found that 71% of current models used binary outputs, while 68% of models were not yet utilising any explainability techniques. Only 7% of studies considered explainability in their future research suggestions.
Conclusion:
There is still a lack of awareness among researchers for the need for explainability and motivation to invest further research into more explainable and more informative software defect prediction models.",Information and Software Technology,18 Mar 2025,6,"The study focuses on the lack of explainability in software defect prediction models, highlighting the need for more informative outputs. The findings could lead to improvements in the understandability of defect prediction models."
https://www.sciencedirect.com/science/article/pii/S0950584924000776,Knowledge and research mapping of the data and database forensics domains: A bibliometric analysis,July 2024,Not Found,Georgios=Chorozidis: gchorozidis@gmail.com; Konstantinos=Georgiou: konsgeor@csd.auth.gr; Nikolaos=Mittas: nmittas@chem.ihu.gr; Lefteris=Angelis: lef@csd.auth.gr,"Abstract
The field of 
digital forensics
 has undergone rapid development alongside the technological advancements of the latest century. This study focuses in two of its subdomains, namely 
database forensics
 and data forensics. Though the concept of a database is relatively old, there is an academic void when it comes to its research compared to different domains in 
digital forensics
. Data forensics has a myriad of applications, however there appears to be a lack of standardization in regards to the field itself throughout the different disciplines of the forensic field. Our main objectives with this study were to identify the prominent trends, uncover research gaps or further research necessity and to provide a high level outline of the selected domains. To fulfill the objectives, we designed and executed a protocol with predefined phases, steps, and activities that all stem from the principles of 
bibliometric analysis
. The findings of the methodological procedure are presented and the research questions are answered in a concise manner. The two domains have considerable growth, given how recently they emerged in literature. However, there are issues present in the current literature that might hinder the future research and might repulse not only the aspiring but also the current professionals of the forensic field. These issues must be resolved in order to make the selected domains less elusive when it comes to cross-domain applications and when new practitioners are concerned.",Information and Software Technology,18 Mar 2025,5,"While the study addresses important issues in digital forensics, the practical impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584924000545,Technical risk model of machine learning based software project development - A multinational empirical study using modified Delphi-AHP method,July 2024,Not Found,Ching-Te=Lin: Not Found; Sun-Jen=Huang: huangsj@mail.ntust.edu.tw,"Abstract
Context
The development of 
machine learning
 (ML) based software projects has increased significantly over the past decade, introducing new technical risks that rarely or never appear in traditional software development projects.
Objective
This research aims to identify and prioritize the technical risk factors that may lead to the failure of ML-based software development projects.
Method
First, a literature review was conducted to compile a preliminary list of technical risk factors for ML-based software project development. Then, two rounds of the modified Delphi process were conducted with 17 ML experts to review and verify the completeness and appropriateness of the preliminary technical risk factors. A hierarchy of five technical risk categories with 22 technical risk factors was concluded for the 
analytic hierarchy process
 (AHP). Then, three rounds of online AHP questionnaires were administered. The consistency ratio (CR) was used to check the respondents’ answers, and the quartile deviation (QD) was applied to assess the consensus on all 96 questions. Finally, we prioritized the technical risk categories and associated technical risk factors.
Results
We found that ""data availability and quality"" ranked as the top technical risk category in terms of severity, probability, and impact rankings of the five technical risk categories. Furthermore, all four technical risk factors within this category also occupied the top four positions of impact ranking.
Conclusion
The 
research results
 highlight the crucial role of the four data availability and quality risk factors for the failure of ML-based software project development. The proposed technical risk model of ML-based software project development with the identified severity and probability priorities may provide practitioners and research community with a clear overview, highlighting areas demanding priority attention to effectively mitigate project failure risks. These findings have broader implications for improving the success rates of ML-based software projects across various domains.",Information and Software Technology,18 Mar 2025,8,"The research on technical risk factors in ML-based software projects has direct relevance to startups and can help improve success rates, making it valuable for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000545,Technical risk model of machine learning based software project development - A multinational empirical study using modified Delphi-AHP method,July 2024,Not Found,Ching-Te=Lin: Not Found; Sun-Jen=Huang: huangsj@mail.ntust.edu.tw,"Abstract
Context
The development of 
machine learning
 (ML) based software projects has increased significantly over the past decade, introducing new technical risks that rarely or never appear in traditional software development projects.
Objective
This research aims to identify and prioritize the technical risk factors that may lead to the failure of ML-based software development projects.
Method
First, a literature review was conducted to compile a preliminary list of technical risk factors for ML-based software project development. Then, two rounds of the modified Delphi process were conducted with 17 ML experts to review and verify the completeness and appropriateness of the preliminary technical risk factors. A hierarchy of five technical risk categories with 22 technical risk factors was concluded for the 
analytic hierarchy process
 (AHP). Then, three rounds of online AHP questionnaires were administered. The consistency ratio (CR) was used to check the respondents’ answers, and the quartile deviation (QD) was applied to assess the consensus on all 96 questions. Finally, we prioritized the technical risk categories and associated technical risk factors.
Results
We found that ""data availability and quality"" ranked as the top technical risk category in terms of severity, probability, and impact rankings of the five technical risk categories. Furthermore, all four technical risk factors within this category also occupied the top four positions of impact ranking.
Conclusion
The 
research results
 highlight the crucial role of the four data availability and quality risk factors for the failure of ML-based software project development. The proposed technical risk model of ML-based software project development with the identified severity and probability priorities may provide practitioners and research community with a clear overview, highlighting areas demanding priority attention to effectively mitigate project failure risks. These findings have broader implications for improving the success rates of ML-based software projects across various domains.",Information and Software Technology,18 Mar 2025,8,"Similar to abstract 117, this research on technical risk factors in ML-based software projects is highly practical and valuable for startups in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584924000636,"Making vulnerability prediction more practical: Prediction, categorization, and localization",July 2024,Not Found,Chongyang=Liu: lcyyy@mail.ustc.edu.cn; Xiang=Chen: xchencs@ntu.edu.cn; Xiangwei=Li: lixw20@mail.ustc.edu.cn; Yinxing=Xue: yxxue@ustc.edu.cn,"Abstract
Context:
Due to the prevalence of software vulnerabilities, 
vulnerability detection
 becomes a fundamental problem in system security.
Objective:
To solve this problem, academics and industries have made great efforts to propose deep-learning-based (DL-based) approaches but these attempts have three main limitations: (1) perform poorly on real-world projects (e.g., Accuracy below 74.33% and F1 below 73.55%); (2) perform poorly in catching vulnerable patterns due to incomplete code representations; (3) mostly perform coarse-grained function-level prediction and lack 
interpretability
 analysis.
Methods:
In this paper, we propose 
VulPCL
, a BLSTM and CodeBERT based approach, which makes the first attempt to perform vulnerability prediction, categorization, and localization automatically within a framework. To alleviate the above-mentioned limitations, our 
VulPCL
 considers multi-dimension (i.e., text-based, sequence-based, and graph-based) representations to catch latent vulnerable patterns and multi-model training to learn high-level semantics.
Results:
Through experiments on four real-world datasets containing 114+ CWE (Common Weakness Enumeration) types spanning from 2005 to 2022, we find that our 
VulPCL
 outperforms the baselines by (1) 13.51%
∼
60.64% and 14.34%
∼
180.23% on Accuracy, and F1 respectively on vulnerability prediction; (2) 10.32%
∼
46.79%, and 10.71%
∼
127.80% on Accuracy, and macro-F1 respectively on vulnerability categorization; (3) 9.23%
∼
36.54% on Top-10 Accuracy on vulnerability localization.
Conclusion:
These results indicate that our 
VulPCL
 is considerably more accurate, effective, fine-grained, and practical than previous studies. Besides, our further analyses show that 
VulPCL
 is indeed capable of capturing all vulnerability lines, and the result of line-level vulnerability localization is consistent with the function-level vulnerability prediction as the increase of predicted lines. Thus making 
VulPCL
 more interpretable than previous studies. Our additional investigation also shows that 
VulPCL
 effectively detects the Most Dangerous 25 CWEs in 2022, which is instructive for security researchers.",Information and Software Technology,18 Mar 2025,7,The development of VulPCL and its effectiveness in vulnerability prediction can benefit European early-stage ventures by enhancing security measures in software development.
https://www.sciencedirect.com/science/article/pii/S0950584924000624,A declarative approach to detecting design patterns from Java execution traces and source code,July 2024,Not Found,Aswathy=Mohan: Not Found; Swaminathan=Jayaraman: swaminathanj@am.amrita.edu; Bharat=Jayaraman: Not Found,"Abstract
Design patterns
 are invaluable for software engineers because they help obtain well-structured and reusable object-oriented software components and contribute towards ease of software comprehension, maintenance, and modification. However, identifying design patterns from an inspection of the 
source code
 is not easy because, in most cases, there are no 
syntactic
 cues that signal their presence. This topic has therefore elicited considerable interest in the field. The novel aspect of our work is that we propose a set of primitives using which we can declaratively specify design patterns based on a combination of static and dynamic information. Our experimental work is carried out in the context of Java: static information is extracted from Java 
source code
, and dynamic information from an execution trace of a program. Each declarative pattern specification is automatically translated into an 
SQL
 query which retrieves all instances of the design pattern present in the program. We illustrate our approach with the well-known Gang-of-Four design patterns and the approach extends to other such design patterns. The experimental results show the efficacy of our approach for representative programs for all GoF patterns in addition to more extensive 
case studies
, JHotDraw, Junit, and QuickUML.",Information and Software Technology,18 Mar 2025,5,"While the research on design patterns is valuable for software engineers, its direct impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924000582,Hybrid semantics-based vulnerability detection incorporating a Temporal Convolutional Network and Self-attention Mechanism,July 2024,Not Found,Jinfu=Chen: jinfuchen@ujs.edu.cn; Weijia=Wang: Not Found; Bo=Liu: Not Found; Saihua=Cai: Not Found; Dave=Towey: Not Found; Shengran=Wang: Not Found,"Abstract
Context:
Desirable characteristics in vulnerability-detection (VD) systems (VDSs) include both good detection capability (high accuracy, low false positive rate, low 
false negative
 rate, etc.) and low time overheads. The widely used VDSs based on models such as 
Recurrent Neural Networks
 (RNNs) have some problems, such as low time efficiency, failing to learn the vulnerability features better, and insufficient amounts of vulnerability features. Therefore, it is very important to construct an automatic detection model with high detection accuracy.
Objective:
This paper reports on training based on the 
source code
 to analyze and learn from the code’s patterns and structures by deep-learning techniques to generate an efficient VD model that does not require manual feature design.
Method:
We propose a software VD model based on multi-feature fusion and 
deep neural networks
 called AIdetectorX-SP. It first uses a 
Temporal Convolutional Network
 (TCN) and adds a Self-attention Mechanism (SaM) to the TCN to build a model for extracting vulnerability logic features, then transforms the 
source code
 into an image input to a 
Convolutional Neural Network
 (CNN) to extract structural and semantic information. Finally, we use feature-fusion technology to design and implement an improved deep-learning-based VDS, called AIdetectorX Sequence with Picturization (AIdetectorX-SP).
Results:
We report on experiments conducted using publicly-available and widely-used datasets to evaluate the effectiveness of AIdetectorX-SP, with results indicating that AIdetectorX-SP is an effective VDS; that the combination of TCN and SaM can effectively extract vulnerability logic features; and that the pictorial code can extract code structure features, which can further improve the VD capability.
Conclusion:
In this paper, we propose a novel detection model for software vulnerability based on TCNs, SaM, and software picturization. The proposed model solves some shortcomings and limitations of existing VDSs, and obtains a high software-VD accuracy with a high degree of stability.",Information and Software Technology,18 Mar 2025,8,"The proposed VD model addresses shortcomings of existing systems and achieves high accuracy in vulnerability detection, which can significantly impact the security of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000715,Search-based co-creation of software models: The case of particle systems for video games,July 2024,Not Found,Jorge=Chueca: jchueca@usj.es; Carlos=Cetina: Not Found; Oscar=Pastor: Not Found; Jaime=Font: Not Found,"Abstract
Context:
The 
video game
 industry is one of the fastest-growing industries in the world. However, the creation of content is the bottleneck of the industry nowadays.
Objective:
In this paper, we propose a new approach for co-creating content by means of combining an 
evolutionary algorithm
 Map-Elites, and software models. Our approach involves generating a large number of software models and selecting the best ones based on a fitness function. This fitness function is guided by the human, who chooses which content fits their interests best.
Method:
We evaluated this approach in the domain of Particle Systems (PS). PS are a popular type of content used to create visual effects such as explosions, fire, smoke, or rain. Our evaluation also involves industry experts of different roles in the 
video game
 
development process
. Using our approach, they were tasked to create PS for their games. Then, they compared the generated models with handmade ones.
Results:
Our results show that practitioners chose the generated models four out of five times over handmade ones as a better fit for their projects. Furthermore, models created with our approach by non-experts in five minutes are similar in quality to the ones hand-made by an expert in 15 min.
Conclusion:
In conclusion, using human artistic taste to guide the algorithm renders positive results in creative tasks such as content generation for video games. With minor adjustments, the 
generated content
 can be game-ready, accelerating development.",Information and Software Technology,18 Mar 2025,7,"The approach of combining evolutionary algorithms and human guidance for content generation in video games shows promise in accelerating development, which can benefit startups in the industry."
https://www.sciencedirect.com/science/article/pii/S0950584924000752,Guiding the way: A systematic literature review on mentoring practices in open source software projects,July 2024,Not Found,Zixuan=Feng: fengzi@oregonstate.edu; Katie=Kimura: kimuraka@oregonstate.edu; Bianca=Trinkenreich: bianca.trinkenreich@oregonstate.edu; Anita=Sarma: Anita.Sarma@oregonstate.edu; Igor=Steinmacher: Igor.Steinmacher@nau.edu,"Abstract
Context:
Mentoring in 
Open Source Software
 (OSS) is important to its project’s growth and sustainability. Mentoring allows contributors to improve their technical skills and learn about the protocols and cultural norms of the project. However, mentoring has its challenges: mentors sometimes feel unappreciated, and mentees may have mismatched interests or lack interpersonal skills. Existing research has investigated the different challenges of mentoring in different OSS contexts, but we lack a holistic understanding.
Objective:
A comprehensive understanding of the current practices and challenges of mentoring in OSS is needed to implement appropriate strategies to facilitate mentoring.
Method:
This study presents a 
systematic literature review
 investigating how literature has characterized mentoring practices in OSS, including their challenges and the strategies to mitigate them. We retrieved 232 studies from four digital libraries. Out of these, 21 were primary studies. Using this, we performed backward and author snowballing, adding another 27 studies. We conducted a 
completeness check
 by reviewing the references of the 4 most relevant primary studies, which resulted in us adding 1 additional study. We then conducted a full-text review and evaluated the studies using a set of criteria; as a result, 10 papers were excluded. We then employed an open-coding approach to analyze, aggregate, and synthesize the selected studies.
Results:
We reviewed 39 studies to investigate the different facets of mentoring in OSS, encompassing motivations, goals, channels, and contributor dynamics. We then identified 13 challenges associated with mentoring in OSS, which fall into three categories: social, process, and technical. We also present a quick-reference strategy catalog to map these strategies to challenges for mitigation.
Conclusions:
Our study serves as a guideline for researchers and practitioners about mentoring challenges and potential strategies to mitigate these challenges.",Information and Software Technology,18 Mar 2025,6,"The study on mentoring challenges and strategies in OSS provides valuable insights for project growth and sustainability, offering guidance to startups on fostering talent within their teams."
https://www.sciencedirect.com/science/article/pii/S0950584924000491,On the use of contextual information for machine learning based test case prioritization in continuous integration development,July 2024,Not Found,Enrique A. da=Roza: enriqueaugroza@gmail.com; Jackson A. do=Prado Lima: japlima@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br,"Abstract
Context:
In most software organizations, 
Continuous Integration (CI)
 is a 
common practice
 usually subject to some budgets. Consequently, prioritizing test cases to be executed in the CI cycle is fundamental. The idea is first to execute test cases with higher failure-proneness to provide rapid feedback and decrease costs. To perform this task approaches in the literature adopt failure history and 
Machine Learning
 (ML)
. However, in addition to the failure history, it is also important to consider information from the CI context of the organizations and the application domain.
Objective:
For this end, we introduce a contextual information approach for 
ML algorithms
. Such an approach considers information from the testing activity that can be easily collected, such as test case 
execution time
, size, and complexity. We implement the approach by introducing two contextual versions of the algorithms: Multi-Armed Bandit (MAB) and 
Random Forest
 (RF).
Method:
Six systems are used to compare both contextual algorithms and to evaluate their performance regarding their corresponding non-contextual versions, considering three different budgets.
Results:
Contextual algorithms perform better when indicators related to test time reduction are considered, as the contextual information they use is related to 
execution time
. Regarding NAPFD and APFDc, the non-contextual algorithms have better general performance, but both contextual versions obtain competitive results.
Conclusions:
The contextual versions implemented can capture the desired context information in the prioritization without negatively impacting their performance regarding fault-detection.",Information and Software Technology,18 Mar 2025,5,The contextual ML approach for test case prioritization in CI introduces efficiency but may have limited immediate impact on European early-stage ventures compared to other abstracts.
https://www.sciencedirect.com/science/article/pii/S0950584924000739,Effective test generation using pre-trained Large Language Models and mutation testing,July 2024,Not Found,Arghavan Moradi=Dakhel: arghavan.moradi-dakhel@polymtl.ca; Amin=Nikanjam: amin.nikanjam@polymtl.ca; Vahid=Majdinasab: vahid.majdinasab@polymtl.ca; Foutse=Khomh: foutse.khomh@polymtl.ca; Michel C.=Desmarais: michel.desmarais@polymtl.ca,"Abstract
Context:
One of the critical phases in the software development 
life cycle
 is software testing. Testing helps with identifying potential bugs and reducing maintenance costs. The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests. Recently, researchers have leveraged 
Large Language Models
 (LLMs) of code to generate unit tests. While the code coverage of generated tests was usually assessed, the literature has acknowledged that the coverage is weakly correlated with the efficiency of tests in bug detection.
Objective:
To improve over this limitation, in this paper, we introduce 
MuTAP
 (
Mu
tation 
T
est case generation using 
A
ugmented 
P
rompt) for improving the effectiveness of 
test cases generated
 by LLMs in terms of revealing bugs by leveraging mutation testing.
Methods:
Our goal is achieved by augmenting prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs. 
MuTAP
 is capable of generating effective test cases in the absence of natural language descriptions of the Program Under Test (PUTs). We employ different LLMs within 
MuTAP
 and evaluate their performance on different benchmarks.
Results:
Our results show that our proposed method is able to detect up to 28% more faulty human-written code snippets. Among these, 17% remained undetected by both the current state-of-the-art fully-automated test generation tool (i.e., Pynguin) and zero-shot/few-shot 
learning approaches
 on LLMs. Furthermore, 
MuTAP
 achieves a Mutation Score (MS) of 93.57% on synthetic buggy code, outperforming all other approaches in our evaluation.
Conclusion:
Our findings suggest that although LLMs can serve as a useful tool to generate test cases, they require specific post-processing steps to enhance the effectiveness of the generated test cases which may suffer from 
syntactic
 or functional errors and may be ineffective in detecting certain types of bugs and testing corner cases in 
PUT
s.",Information and Software Technology,18 Mar 2025,9,"The MuTAP approach significantly improves the effectiveness of test cases generated by LLMs, enhancing bug detection capabilities, which could be crucial for startup software development."
https://www.sciencedirect.com/science/article/pii/S095058492400079X,Forward-Oriented Programming: A meta-DSL for fast development of component libraries,July 2024,Not Found,Emmanouil=Krasanakis: manios.krasanakis@issel.ee.auth.gr; Andreas=Symeonidis: symeonid@ece.auth.gr,"Abstract
Libraries that implement Domain-Specific Language (DSL) components keep gaining traction when it comes to developing software for 
specific application domains
. However, creating components that can be organically weaved into use cases is an extremely complex task. In this work, we introduce a meta-DSL to assist library development, called Forward-Oriented Programming (FOP). This combines lazy evaluation and aspect-oriented programming principles to align crosscutting 
component configurations
 and alter their execution outcomes depending on usage in subsequent code. Theoretical analysis shows that FOP simplifies component development and makes their combination logic learnable by library users. We realize the paradigm with a Python package, called 
pyfop
, and conduct a 
case study
 that compares it with purely functional and object-oriented library implementations. In the study, 
source code
 quality metrics demonstrate reduced time and effort to write library components, and increased 
comprehensibility
. Configurations are shared without modifying distant code segments.",Information and Software Technology,18 Mar 2025,8,"The introduction of Forward-Oriented Programming (FOP) and pyfop for component development in libraries can significantly reduce time and effort, improving source code quality and comprehensibility in software development."
https://www.sciencedirect.com/science/article/pii/S0950584924000600,Quantum aided efficient resource control for connected support in IRS assisted networks,July 2024,Not Found,Ashu=Taneja: ashu.taneja@chitkara.edu.in; Shalli=Rani: shalli.rani@chitkara.edu.in; Meshal=Alharbi: Mg.alharbi@psau.edu.sa; Muhammad=Zohaib: Muhammad.zohaib@lut.fi,"Abstract
Context:
To achieve the vision of all connected world with uninterrupted communication support, 6G technology plays an important role. But the scarce radio spectrum and limited network resources is the main challenge in delivering its promised performance.
Objectives:
This paper presents an IRS-aided cell free 
NOMA
 network model that aims to provide uniform network coverage. The future 6G technology envisions for serving billions of interconnected devices with seamless communication support, data handling capabilities and computational accuracy. But the scarcity of network resources is the main limitation. Thus, the need is to design quantum enabled intelligent and dynamic networks capable of offering extended network capabilities. Proposed work is on intelligent network framework that provides uniform network coverage through efficient resource management for a 6G enabled expanded 
IoT
 network.
Methods:
To enable efficient resource management, a quantum enabled 
resource control
 algorithm is proposed that creates user clusters and associates each AP-IRS pair to each cluster. Each 
AP
 transmits the superimposed signals of its intended cluster against all the user clusters as in conventional 
NOMA
 system. The nodes in each cluster have been assigned unique pilots so as to avoid intracluster interference. The use of 
IRS
 enables desired 
NOMA
 
beamforming
 such that the effect of unfavourable wireless environment is mitigated.
Results:
The performance of the IRS-aided cell-free 
NOMA
 network is evaluated for average sum rate with different 
AP
 transmit power, cluster sizes, 
IRS
 reflecting elements and IRS phase shifts. It is shown that at transmit power per AP of 30dBm, the average sum rate of the system improves by 7.52% with the proposed algorithm using equal power allocation scheme. Further, the comparative performance analysis of three different communication systems is carried out to validate the proposed communication model.
Conclusion:
It is observed that with more number of users per cluster, the average sum rate of the system initially increases for small cluster sizes and then it becomes constant for large cluster sizes. The proposed clustering method outperforms the random clustering approach achieving sum rate of 12.9 bits/s/Hz with 
N
 =300 and 
M
= 8. The comparison of different communication scenarios reveals that the maximum sum rate of 12.2 bits/s/Hz is achieved with the proposed model incorporating proposed clustering mechanism. Further, the energy efficiency analysis suggests that energy efficiency improves with 
N
 and 
P
c
 with proposed clustering approach. The use case scenarios for the integration of 
quantum computing
 with IRS technology are also presented.",Information and Software Technology,18 Mar 2025,6,"The IRS-aided cell-free NOMA network model for 6G technology presents a novel approach to resource management and communication support, but the impact on early-stage ventures may be limited due to the specificity of network technology."
https://www.sciencedirect.com/science/article/pii/S095058492400048X,Prioritisation of code clones using a genetic algorithm,June 2024,Not Found,Umberto=Azadi: u.azadi@campus.unimib.it; Bartosz=Walter: bartosz.walter@cs.put.poznan.pl; Francesca Arcelli=Fontana: francesca.arcelli@unimib.it,"Abstract
Context:
Code clones are prevalent, and due to their diverse impact on projects’ quality they require a proper management strategy.
Objectives:
Develop GA-based Refactoring-Aware Detection (RAD) approach for prioritisation of code clones.
Method:
A genetic algorithm (GA) that balances estimated gain and cost/risk of refactoring to select the optimal clone candidate to refactor.
Results:
GA converges on a solution, with diverse variance. The value of fitness function is higher for multi-objective approaches, but they also exhibit higher variance.
Conclusion:
GA can be effectively applied for clone prioritising.",Information and Software Technology,18 Mar 2025,5,"The GA-based Refactoring-Aware Detection (RAD) approach for code clone prioritization provides a useful tool, but its practical impact on European early-stage ventures may be less significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492400048X,Prioritisation of code clones using a genetic algorithm,June 2024,Not Found,Umberto=Azadi: u.azadi@campus.unimib.it; Bartosz=Walter: bartosz.walter@cs.put.poznan.pl; Francesca Arcelli=Fontana: francesca.arcelli@unimib.it,"Abstract
Context:
Code clones are prevalent, and due to their diverse impact on projects’ quality they require a proper management strategy.
Objectives:
Develop GA-based Refactoring-Aware Detection (RAD) approach for prioritisation of code clones.
Method:
A genetic algorithm (GA) that balances estimated gain and cost/risk of refactoring to select the optimal clone candidate to refactor.
Results:
GA converges on a solution, with diverse variance. The value of fitness function is higher for multi-objective approaches, but they also exhibit higher variance.
Conclusion:
GA can be effectively applied for clone prioritising.",Information and Software Technology,18 Mar 2025,5,"Similar to Abstract 128, the GA-based Refactoring-Aware Detection (RAD) approach for code clone prioritization is valuable but may have less immediate impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000272,A method of multidimensional software aging prediction based on ensemble learning: A case of Android OS,June 2024,Not Found,Yuge=Nie: Not Found; Yulei=Chen: Not Found; Yujia=Jiang: Not Found; Huayao=Wu: Not Found; Beibei=Yin: yinbeibei@buaa.edu.cn; Kai-Yuan=Cai: Not Found,"Abstract
Context:
Software aging refers to the phenomenon of 
performance degradation
, increasing failure rate, or system crash due to resource consumption and error accumulation in software systems running for a long time. It has become the key factor affecting software systems’ 
sustainability
. Due to its complex formation reasons, precisely predicting the aging state in actual execution is hard but crucial for enabling proactive measures before a catastrophic situation. 
Machine learning
 (ML) has been employed on this issue.
Objective:
However, previous ML-based prediction methods are single-threaded in the whole process, posing challenges in delivering the desired performance facing diverse user scenarios. To alleviate this problem, we propose a multidimensional software aging prediction method based on 
ensemble learning
 (MSAP).
Method:
In the framework of MSAP, five dimensions, including datasets, labeling metrics, labeling thresholds, algorithms, and model decisions, are extracted and diversified according to aging characteristics and application situations.
Results:
Plenty of experiments have been conducted on 
Android
 devices from three distinct vendors. When subjected to identical workloads, MSAP demonstrates comparable performance to most unidimensional models. While under varied workloads, MSAP outperforms unidimensional models whose performance drops dramatically, demonstrating enhanced adaptability and 
predictive accuracy
.
Conclusion:
MSAP shows exceptional stability while concurrently upholding outstanding prediction precision across a spectrum of user scenarios. It has better generalization characteristics and application prospects.",Information and Software Technology,18 Mar 2025,7,"The multidimensional software aging prediction method based on ensemble learning (MSAP) addresses a crucial issue in software sustainability and demonstrates enhanced adaptability and predictive accuracy, making it valuable for startups dealing with software systems."
https://www.sciencedirect.com/science/article/pii/S0950584924000399,Inclusion of individuals with autism spectrum disorder in Software Engineering,June 2024,Not Found,Gastón=Márquez: gmarquez@ubiobio.cl; Michelle=Pacheco: Not Found; Hernán=Astudillo: Not Found; Carla=Taramasco: Not Found; Esteban=Calvo: Not Found,"Abstract
Context:
Software Engineering
 is dedicated to the systematic and efficient development of software, which necessitates the active participation of all team members and a recognition of their unique skills and abilities, including those with 
autism spectrum disorders
 (ASD). The inclusion of individuals with ASD presents new perspectives, yet there is a lack of systematic evidence regarding the primary obstacles and 
potential benefits
 associated with their inclusion.
Objective:
This paper aims to identify, characterize, and describe barriers, facilitators, and methodological proposals described by the community to include individuals with ASD in the discipline of 
Software Engineering
.
Methods:
We conducted a comprehensive systematic multivocal mapping study to evaluate the existing evidence on the inclusion of individuals with ASD in Software Engineering.
Results:
We obtained 34 primary studies from which we identified the main facilitators of motivation to learn new skills, attention to detail, and the ability to report and visualize patterns. In contrast, the main barriers detected were communication, a lack of neurodivergent computational thinking, and sensory integration. Additionally, we identified and classified four categories of proposals that allowed the inclusion of individuals with ASD: (i) using virtual reality, (ii) creating more inclusive workspaces, (iii) encouraging neurodivergent computational thinking, and (iv) improving social skills.
Conclusions:
This study identifies the principal elements that ought to be taken into consideration when allocating tasks and roles to individuals with ASD in software development.",Information and Software Technology,18 Mar 2025,8,"This study provides valuable insights into the inclusion of individuals with autism spectrum disorders in Software Engineering, offering new perspectives and proposing methodological proposals for their successful integration."
https://www.sciencedirect.com/science/article/pii/S0950584924000363,UX Research practices related to Long-Term UX: A Systematic Literature Review,June 2024,Not Found,Suéllen=Martinelli: suellen.martinelli@estudante.ufscar.br; Larissa=Lopes: larii.albano@gmail.com; Luciana=Zaina: lzaina@ufscar.br,"Abstract
Context:
The software industry has sought to apply 
User eXperience
 (UX) practices that can help maintain a sustainable business. UX practices make it possible to conduct research and make evaluations with users through the application of methods and techniques. But few studies in the literature discuss UX Research practices with Long-Term UX.
Objective:
The objective of this paper is to identify in the literature what UX Research practices are employed by the software industry and their relationship with Long-Term UX.
Methods:
We conducted a 
Systematic Literature Review
 with string applied in search engines, besides selection criteria and quality assessment applied in the papers. We selected 45 papers that were submitted for the qualitative analysis carried out with coding techniques.
Results:
38 UX Research practices were identified and classified between formal and informal practices. We also identified 52 UX methods, techniques, and tools that are used for these UX Research practices. Our findings show that 15 out of 38 practices are related to Long-Term UX.
Conclusion:
We drew up 14 guidelines for conducting UX Research in the software industry, which includes goals, UX methods, and the means of putting them into practice.",Information and Software Technology,18 Mar 2025,6,"The research on UX Research practices and Long-Term UX in the software industry is relevant, but the practical implications might be limited for early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000533,VALIDATE: A deep dive into vulnerability prediction datasets,June 2024,"Security, Replicability, Vulnerability, Machine learning, Repository, Dataset",Matteo=Esposito: m.esposito@ing.uniroma2.it; Davide=Falessi: falessi@ing.uniroma2.it,"Abstract
Context:
Vulnerabilities are an essential issue today, as they cause economic damage to the industry and endanger our daily life by threatening critical national security infrastructures. Vulnerability prediction supports software engineers in preventing the use of vulnerabilities by malicious attackers, thus improving the security and reliability of software. Datasets are vital to vulnerability prediction studies, as 
machine learning
 models require a dataset. Dataset creation is time-consuming, error-prone, and difficult to validate.
Objectives:
This study aims to characterise the datasets of prediction studies in terms of availability and features. Moreover, to support researchers in finding and sharing datasets, we provide the first VulnerAbiLty predIction DatAseT rEpository (
VALIDATE
).
Methods:
We perform a 
systematic literature review
 of the datasets of vulnerability prediction studies.
Results:
Our results show that out of 50 primary studies, only 22 studies (i.e., 38%) provide a reachable dataset. Of these 22 studies, only one study provides a dataset in a stable repository.
Conclusions:
Our repository of 31 datasets, 22 reachable plus nine datasets provided by authors via email, supports researchers in finding datasets of interest, hence avoiding reinventing the wheel; this translates into less effort, more reliability, and more reproducibility in dataset creation and use.",Information and Software Technology,18 Mar 2025,7,"The creation of a repository for vulnerability prediction datasets can benefit software engineers by providing easier access to valuable datasets, contributing to improved security and reliability of software."
https://www.sciencedirect.com/science/article/pii/S0950584924000478,MSGVUL: Multi-semantic integration vulnerability detection based on relational graph convolutional neural networks,June 2024,"Vulnerability detection, Code representation, Program slicing, Graph convolutional neural networks",Wei=Xiao: Not Found; Zhengzhang=Hou: Not Found; Tao=Wang: Not Found; Chengxian=Zhou: Not Found; Chao=Pan: kerwinpc@ccut.edu.cn,"Abstract
Software security has drawn extensive attention as software projects have grown increasingly large and complex. Since the traditional manual or equipment 
vulnerability detection
 technology cannot meet today's software development needs, there is a recognized need to create more effective techniques to address security issues. Although various vulnerability detection systems have been proposed, most are based only on serialization or 
graph representation
, to inadequate effect. We propose a system, MSGVUL, that provides superior vulnerability detection using a new multi-semantic approach. MSGVUL uses versatile and efficient code slicing employing a search algorithm based on sensitive data and functions and innovatively constructs an SSVEC model to fully integrate the semantic and structural information into the code. We also developed a novel BAG model, made up of BAP and PAG frameworks, that enables the hierarchical extraction of code vulnerability representations from the graph and sequence levels. The MSGVUL model is evaluated on slice-level and function-level vulnerability datasets, and the results demonstrate that the MSGVUL method outperforms other state-of-the-art methods.",Information and Software Technology,18 Mar 2025,9,"The proposed MSGVUL system for vulnerability detection offers a new, effective approach to addressing security issues in software projects, which can have a significant impact on improving software security for early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000508,Source code expert identification: Models and application,June 2024,Not Found,Otávio=Cury: otaviocury@ufpi.edu.br; Guilherme=Avelino: gaa@ufpi.edu.br; Pedro Santos=Neto: pasn@ufpi.edu.br; Marco Túlio=Valente: mtov@dcc.ufmg.br; Ricardo=Britto: rbr@bth.se,"Abstract
Context:
Identifying source code expertise is useful in several situations. Activities like bug fixing and helping newcomers are best performed by knowledgeable developers. Some studies have proposed repository-mining techniques to identify source code experts. However, there is a gap in understanding which variables are most related to code knowledge and how they can be used for identifying expertise.
Objective:
This study explores models of expertise identification and how these models can be used to improve a Truck Factor algorithm.
Methods:
First, we built an oracle with the knowledge of developers from software projects. Then, we use this oracle to analyze the correlation between measures from the development history and source code knowledge. We investigate the use of linear and machine-learning models to identify file experts. Finally, we use the proposed models to improve a Truck Factor algorithm and analyze their performance using data from public and private repositories.
Results:
First Authorship
 and 
Recency of Modification
 have the highest positive and negative correlations with source code knowledge, respectively. Machine learning classifiers outperformed the linear techniques (
F-Score
 = 71% to 73%) in the largest 
analyzed dataset
, but this advantage is unclear in the smallest one. The Truck Factor algorithm using the proposed models could handle developers missed by the previous expertise model with the best average 
F-Score
 of 74%. It was perceived as more accurate in computing the Truck Factor of an industrial project.
Conclusion:
If we analyze 
F-Score
, the studied models have similar performance. However, 
machine learning
 classifiers get higher 
Precision
 while linear models obtained the highest 
Recall
. Therefore, choosing the best technique depends on the user’s tolerance to 
false positives
 and negatives. Additionally, the proposed models significantly improved the accuracy of a Truck Factor algorithm, affirming their effectiveness in precisely identifying the key developers within software projects.",Information and Software Technology,18 Mar 2025,8,The study on expertise identification in source code and the improvement of the Truck Factor algorithm can be beneficial for startups in enhancing their development processes and identifying key developers within their projects.
https://www.sciencedirect.com/science/article/pii/S0950584924000326,Evaluating the effectiveness of a security flaws prevention tool,June 2024,Not Found,Itzhak=Gershfeld: gershitz@post.bgu.ac.il; Arnon=Sturm: sturm@bgu.ac.il,"Abstract
Context:
Securing code is crucial for all software stakeholders. Nevertheless, state-of-the-art tools are imperfect and tend to miss critical errors, resulting in zero-day vulnerabilities. Thus, there is a need for alternatives to mitigate such issues.
Objective:
We aim to facilitate an effective identification mechanism of security flaws in the early stages of development.
Method:
Following our analysis of the root causes of vulnerabilities and examining existing code analyzers, we devise a new Rule-Based Security Flaws Prevention (RbSFP) tool. The tool is based on a set of allow-list rules and consists of the following stages: (1) 
AST
 creation based on the source code and marking critical code areas; (2) Context-based code analysis that further validates the code; (3) Results’ normalization to suggest alerts and warnings. To evaluate the RbSFP tool, we utilized two complementary evaluations. The first refers to the tool’s ability to detect security flaws compared to competing tools by executing them on open-source projects. The second refers to evaluating the tool’s usability and efficiency via a controlled experiment.
Results:
We found that the outcomes were of better quality when using the RbSFP tool, and the differences were statistically significant. Thus, utilizing the new approach and tool has a significant impact as it can eliminate root causes for security flaws at the early stages of development.
Conclusion:
Using an allow-list-based approach can reduce security flaws in the code. However, further analysis and evaluation are needed to provide a more comprehensive solution.",Information and Software Technology,18 Mar 2025,8,The development of a new Rule-Based Security Flaws Prevention tool that can eliminate root causes for security flaws at the early stages of development has significant practical value and impact on European early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S095058492400034X,Prevalence and severity of design anti-patterns in open source programs—A large-scale study,June 2024,Not Found,Alan=Liu: alanl200511@gmail.com; Jason=Lefever: Not Found; Yi=Han: Not Found; Yuanfang=Cai: Not Found,"Abstract
Context:
Design anti-patterns can be symptoms of problems that lead to long-term maintenance difficulty. How should development teams prioritize their treatment? Which ones are more severe and deserve more attention? Does the impact of anti-patterns and general maintenance efforts differ with different programming languages?
Objective:
In this study, we assess the prevalence and severity of anti-patterns in different programming languages and the impact of 
dynamic typing
 in Python, as well as the impact scopes of prevalent anti-patterns that manifest the violation of design principles.
Method:
We conducted a large-scale study of anti-patterns using 1717 open-source projects written in Java, C/C++, and Python. For the 288 Python projects, we extracted both explicit and dynamic dependencies and compared how the detected anti-patterns and maintenance costs changed. Finally, we removed anti-patterns involving five or fewer files to assess the impact of trivial anti-patterns.
Results:
The results reveal that 99.55% of these projects contain anti-patterns. Modularity Violation – frequent co-changes among seemingly unrelated files – is most prevalent (detected in 83.54% of all projects) and costly (incurred 61.55% of maintenance effort on average). Unstable Interface and Crossing, caused by influential but unstable files, although not as prevalent, tend to incur severe maintenance costs. Duck typing in Python incurs more anti-patterns, and the churn spent on Python files multiplies that of C/C++ and Java files. Several prevalent anti-patterns have a large portion of trivial instances, meaning that these common symptoms are usually not harmful.
Conclusion:
Implicit and visible dependencies are the most expensive to maintain, and 
dynamic typing
 in Python exacerbates the issue. Influential but unstable files need to be monitored and rectified early to prevent the accumulation of high maintenance costs. The violations of design principles are widespread, but many are not high-maintenance.",Information and Software Technology,18 Mar 2025,7,"The study on anti-patterns in different programming languages and their impact on maintenance costs provides valuable insights, but the direct practical application on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924000314,Can serious gaming tactics bolster spear-phishing and phishing resilience? : Securing the human hacking in Information Security,June 2024,"Human factor in security, Phishing attack, Scam, Education, Serious game, Information security",Affan=Yasin: affan.yasin@outlook.com; Rubia=Fatima: rubiafatima91@hotmail.com; Zheng=JiangBin: zhengjb@nwpu.edu.cn; Wasif=Afzal: wasif.afzal@mdu.se; Shahid=Raza: shahid.raza@ri.se,"Abstract
Context:
In the digital age, there is a notable increase in fraudulent activities perpetrated by social engineers who exploit individuals’ limited knowledge of digital devices. These actors strategically manipulate human psychology, targeting IT devices to gain unauthorized access to sensitive data.
Objectives:
Our study is centered around two distinct objectives to be accomplished through the utilization of a serious game: (i) The 
primary objective
 entails delivering training and educational content to participants with a focus on 
phishing attacks
; (ii) The secondary objective aims to heighten participants’ awareness regarding the perils associated with divulging excessive information online.
Methodology:
To address these objectives, we have employed the following techniques and methods: (i) A comprehensive literature review was conducted to establish foundational knowledge in areas such as social engineering, game design, learning principles, 
human interaction
, and game-based learning; (ii) We meticulously aligned the game design with the philosophical concept of 
social engineering attacks
; (iii) We devised and crafted an advanced hybrid version of the game, incorporating the use of QR codes to generate game card data; (iv) We conducted an empirical evaluation encompassing surveys, observations, discussions, and URL assessments to assess the effectiveness of the proposed hybrid game version.
Results:
Quantitative data
 and qualitative observations suggest the “PhishDefend Quest” game successfully improved players’ comprehension of phishing threats and how to detect them through an interactive 
learning experience
. The results highlight the potential of serious games to educate people about social engineering risks.
Conclusion:
Through the evaluation, we can readily arrive at the following conclusions: (i) Game-based learning proves to be a viable approach for educating participants about phishing awareness and the associated risks tied to the unnecessary disclosure of 
sensitive information
 online; (ii) Furthermore, game-based learning serves as an effective means of disseminating awareness among participants and players concerning prevalent phishing attacks.",Information and Software Technology,18 Mar 2025,9,The use of serious games to educate participants about phishing awareness and risks presents an innovative approach with high potential impact on European startups dealing with cybersecurity issues.
https://www.sciencedirect.com/science/article/pii/S0950584924000387,Multi-objective optimization and integrated indicator-driven two-stage project recommendation in time-dependent software ecosystem,June 2024,Not Found,Xin=Shen: shenxinpassion@163.com; Xiangjuan=Yao: yaoxj@cumt.edu.cn; Dunwei=Gong: dwgong@vip.163.com; Huijie=Tu: tb19080008b4@cumt.edu.cn,"Abstract
Context:
Time-dependent software ecosystem is a complex system, where there are many projects and developers. Recommending projects to developers in a time-dependent software ecosystem can improve their quality and development speeds. However, the time-dependence of projects and developers results in an increased difficulty of project recommendation.
Objective:
To better recommend projects to developers in a time-dependent software ecosystem, we propose a method of multi-objective optimization and integrated indicator-driven two-stage project recommendation, which is fulfilled according to the change of developer communities and their projects.
Method:
According to the change of developer communities and their projects, a method of multi-objective optimization and integrated indicator-driven two-stage project recommendation is fulfilled. In the first stage, a constrained multi-objective optimization model for project recommendation to developer communities is established, and an improved NSGA-II algorithm is adopted to solve this model, with the purpose of obtaining the recommended projects to a developer community. For the second stage, an integrated indicator for project recommendation to developers is built to determine the developers of a project.
Results:
The proposed method is applied to project recommendation for nine time-dependent software ecosystems in GitHub, and compared with six state-of-the-art ones. The experimental results show that our method has significant advantages in recommendation accuracy and efficiency.
Conclusion:
According to the experimental results, we conclude that the proposed method can timely and accurately recommend projects to developers in a time-dependent software ecosystem, which reduces the difficulty of solving the problem of project recommendation.",Information and Software Technology,18 Mar 2025,7,"The method of multi-objective optimization for project recommendation in time-dependent software ecosystems is useful, but the direct practical application and impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924000351,Objectivity by design: The impact of AI-driven approach on employees' soft skills evaluation,June 2024,Not Found,Ruti=Gafni: Not Found; Itzhak=Aviv: itzhakav@mta.ac.il; Boris=Kantsepolsky: Not Found; Sofia=Sherman: Not Found; Havana=Rika: Not Found; Yariv=Itzkovich: Not Found; Artem=Barger: Not Found,"Abstract
Engineers’ team collaboration skills are among software development's most important 
success factors
. Existing 
Artificial Intelligence
 practices for the engineers' soft skills assessment mainly rely on evaluations of subjective data gathered through surveys, interviews, or observations. As a result, the insights gained by these methods are biased because of the subjective data people report. To overcome the challenge of subjectivity, we offer a novel objectivity-by-design approach for continuous AI-driven team collaboration skills analytics. The method analyzes the data from workstreams gathered from data repositories like Jira. Based on the study results, we conclude that this approach enables a continuous assessment of employees' team collaboration skills, provides more accurate insights, eliminates subjective biases, and helps uncover trends and deficits on individual and team levels. Understanding and recognizing employees' strengths and weaknesses can foster an organizational culture of growth and development. An improved organizational climate is expected to result in work satisfaction, engagement, and motivation, thus positively impacting employees, businesses, and society.",Information and Software Technology,18 Mar 2025,8,"The novel objectivity-by-design approach for continuous AI-driven team collaboration skills analytics can provide valuable insights for European early-stage ventures, impacting their team collaboration and success factors."
https://www.sciencedirect.com/science/article/pii/S0950584924000375,Agile software development projects–Unveiling the human-related critical success factors,June 2024,"Agile methodologies, Agile projects, Software development, Critical success factors, People factors, Psychological safety",Leonor=Barros: Not Found; Carlos=Tam: carlosvai@novaims.unl.pt; João=Varajão: Not Found,"Abstract
Context
Investment in information technology is associated with better business performance when its implementation is successful, but it has high costs in case of failure, especially for large software development projects, which typically have the highest failure rates. Agile methodologies emerged with the expectation of reducing the risk of software development project failure.
Objective
This research aims to answer the following question: What are the human-related critical success factors for agile software development projects to succeed? The research model comprises four explanatory variables (team capability, customer involvement, psychological safety, and team autonomy) and one dependent variable (success of agile software development projects).
Method
A questionnaire-based survey was carried out, resulting in 177 valid responses. A Partial Least Squares (PLS) analysis was performed to test the theoretical model.
Results
The findings indicate that team capability and customer involvement have the strongest effects on the success of agile software development projects. The results also show that psychological safety is a significant indirect success factor and that team autonomy appears to have a competing dynamic with psychological safety on the other two factors.
Conclusion
To the best of our knowledge, this is the first study to examine the direct impact of psychological safety and its indirect effect, mediated by team capability and customer involvement, on the success of agile software development projects. The mediation, moderation, and direct effects are studied, offering theoretical and practical insights.",Information and Software Technology,18 Mar 2025,8,The research on human-related critical success factors for agile software development projects has practical insights for European startups aiming to enhance project success and team dynamics.
https://www.sciencedirect.com/science/article/pii/S0950584924000521,Ensemble effort estimation for novice agile teams,June 2024,Not Found,Bashaer=Alsaadi: b.alsaadi@seu.edu.sa; Kawther=Saeedi: Not Found,"Abstract
CONTEXT
To establish a reliable 
development plan
, developers should investigate the software being developed. One main challenge for developers is estimating the effort required to develop the software. Agile teams deliver the software in a set of iterations, with each iteration containing user stories. Therefore, unlike traditional development, software development effort estimation (SDEE) in agile should focus on the user stories level. An inaccurate estimation has detrimental consequences for software development such as poor 
resource allocation
 or the delivery of low-quality software. However, limited works have developed new estimation methods for agile projects compared to traditional ones.
OBJECTIVES
This study introduces an ensemble model for estimating efforts in agile user stories development. It also creates a new dataset with 140 user stories, aiming for future research use.
METHODS
This research followed the 
Design Science Research
 methodology (DSR). Six individual models were examined to build the ensemble model. The top three models — Extra Trees, K-Nearest Neighbors, and Multi-Layer 
Perceptron
 — were employed. The model's performance was assessed through 
Mean Absolute Error
 (MAE), 
Mean Squared Error
 (MSE), and 
Root Mean Squared Error
 (RMSE). Additionally, an experiment tested the model's efficacy on real software projects by novice teams.
RESULTS
The results show that the ensemble model outperformed individual models, as it scored 0.78 in MAE, 1.62 in MSE, and 1.15 in RMSE. The experiment results showed that the model outperformed human estimation and proved its effectiveness in improving the accuracy of human estimation.
CONCLUSION
The findings demonstrate the model's success in refining effort estimates for novice Agile teams, leading to fewer errors. Practically, it means enhanced project planning and resource management. Additionally, developers' estimation confidence improved, indicating a positive impact on team dynamics and decision-making.",Information and Software Technology,18 Mar 2025,9,"The ensemble model for estimating effort in agile user stories development proves to be effective for novice Agile teams, leading to enhanced project planning and resource management, which can greatly benefit European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000557,Studying logging practice in machine learning-based applications,June 2024,"Logging practices, ML-based applications, Mining software repositories, Source code analysis",Patrick Loic=Foalem: patrick-loic.foalem@polymtl.ca; Foutse=Khomh: foutse.khomh@polymtl.ca; Heng=Li: heng.li@polymtl.ca,"Abstract
Context:
Logging is a 
common practice
 in traditional software development. There have been multiple studies on the characteristics of logging in traditional software systems such as C/C++, Java, and 
Android applications
. However, logging practices in Machine Learning-based (ML-based) applications are still not well understood. The size and complexity of data and models used in ML-based applications present unique challenges for logging.
Objective:
In this paper, we aim to bridge this knowledge gap and provide insight into the logging practices in ML-based applications, making the first attempt to characterize current logging practices within a large number of open-source ML-based applications.
Method:
We conducted an empirical study on 502 open-source ML applications to understand their logging practices, combining quantitative and qualitative analyses and a survey involving 31 practitioners.
Results:
Our quantitative analysis reveals that logging in ML applications is less common than in traditional software, with info and warn log levels being popular. Top ML-specific logging libraries include MLflow, Tensorboard, Neptune, and W&B. Qualitatively, logging is used for data and model management, especially in model training. Our survey reinforces the importance of logging in experiment tracking, complementing our qualitative findings.
Conclusion:
Our research carries significant implications. It reveals distinctive ML logging practices compared to traditional software. We have highlighted the prevalence of general-purpose logging libraries in ML code, indicating a potential gap in awareness regarding ML-specific logging tools. This insight benefits researchers and developers aiming to enhance ML project reproducibility and sets the stage for exploring ML-specific logging tools’ impact on machine learning system quality and trustworthiness.",Information and Software Technology,18 Mar 2025,7,"Insights into ML-based applications' logging practices can be valuable for European startups involved in machine learning projects, offering a basis for enhancing project reproducibility and quality."
https://www.sciencedirect.com/science/article/pii/S0950584924000594,Role of quantum computing in shaping the future of 6 G technology,June 2024,"6 G technology, Quantum computing, Challenges, Opportunities",Muhammad Azeem=Akbar: azeem.akbar@ymail.com; Arif Ali=Khan: Not Found; Sami=Hyrynsalmi: Not Found,"Abstract
Context
The emergence of 6 G technology heralds a groundbreaking era in digital connectivity, envisaging universal and seamless links. To address the intricate computational and security requirements of this revolution, the integration of 
quantum computing
 (QC) into these networks is perceived as a promising solution.
Objective
The objective this study presents a comprehensive investigation into the potential roles and implications of QC within the context of 6 G technology.
Methodology
To address the objectives of this study, firstly, we have conducted literature survey to identify the key applications of using QC in 6 G technology. Secondly, we performed interview study with industry experts to identify the best practices related to the key application of QC in 6 G technology.
Results
Our study unfolds in two distinct stages: firstly, we identify 15 key applications of QC in 6 G technology and segmented into 4 core areas. Secondly, the literature findings were empirically validated by conducting interview study and identified 49 best practices related to one of the identified key applications of QC in 6 G technology.
Conclusion
The outcomes of this research lay a solid foundation for understanding both the pivotal applications of QC in 6 G technology and the effective practices for its implementation, thus providing valuable insights to both academics and industry practitioners.",Information and Software Technology,18 Mar 2025,6,The investigation into the potential roles of quantum computing in 6 G technology provides valuable insights for European early-stage ventures looking to stay at the forefront of technological advancements.
https://www.sciencedirect.com/science/article/pii/S0950584924000193,What does matter in the success of a decentralized application? From idea to development,May 2024,Not Found,Elvira-Maria=Arvanitou: Not Found; Dimitrios=Gagoutis: Not Found; Apostolos=Ampatzoglou: apostolos.ampatzoglou@gmail.com; Nikolaos=Mittas: Not Found; Ignatios=Deligiannis: Not Found; Alexander=Chatzigeorgiou: Not Found,"Abstract
Context
With the rise of 
blockchain
, various applications are running in a decentralized manner, covering the needs of various end-users. Decentralized Applications (DApps) are becoming popular in numerous application domains, ranging from 
finance
 to games, and from Non-Fungible Tokens to security mechanisms. The success of a DApp, from a financial perspective, can be perceived as the market fragment that it captures, and the volume of transactions it generates.
Objective
The goal of this study is to investigate the factors that are important for safeguarding (as much as possible) the financial success of a Decentralized Application. In this study, we focus on four management factors that could influence financial success: the context of the DApp (e.g., focusing on 
finance
, games, entertainment), the intensity of development activities (e.g., number of: commits, forks, or branches of the repository), the size of the development team and the existence of project documentation.
Method
We performed a 
case study
 on 122 DApps that were available through an open repository of 
smart contracts
, namely State-of-the-DApps. By mining the repository, we recorded two metrics that capture the financial success of the application (number of users and volume of transactions) and explored their relation to the aforementioned factors.
Results
The findings of the study suggest that the intensity of development activities is the most important factor for its financial success. Similarly, the context (i.e., the application domain) of the decentralized application is also a key-factor since it influences the number of users that the DApp will reach.
Conclusions
Based on the findings, we suggest businesses that want to enter the market of decentralized applications to balance properly between technical and business parameters. For an application to be successful, it requires both an intensive 
development process
, but also a careful consideration of the application domain.",Information and Software Technology,18 Mar 2025,8,The study on factors influencing the financial success of Decentralized Applications could provide European startups in the blockchain space with valuable guidance on balancing technical and business parameters for success.
https://www.sciencedirect.com/science/article/pii/S0950584924000193,What does matter in the success of a decentralized application? From idea to development,May 2024,Not Found,Elvira-Maria=Arvanitou: Not Found; Dimitrios=Gagoutis: Not Found; Apostolos=Ampatzoglou: apostolos.ampatzoglou@gmail.com; Nikolaos=Mittas: Not Found; Ignatios=Deligiannis: Not Found; Alexander=Chatzigeorgiou: Not Found,"Abstract
Context
With the rise of 
blockchain
, various applications are running in a decentralized manner, covering the needs of various end-users. Decentralized Applications (DApps) are becoming popular in numerous application domains, ranging from 
finance
 to games, and from Non-Fungible Tokens to security mechanisms. The success of a DApp, from a financial perspective, can be perceived as the market fragment that it captures, and the volume of transactions it generates.
Objective
The goal of this study is to investigate the factors that are important for safeguarding (as much as possible) the financial success of a Decentralized Application. In this study, we focus on four management factors that could influence financial success: the context of the DApp (e.g., focusing on 
finance
, games, entertainment), the intensity of development activities (e.g., number of: commits, forks, or branches of the repository), the size of the development team and the existence of project documentation.
Method
We performed a 
case study
 on 122 DApps that were available through an open repository of 
smart contracts
, namely State-of-the-DApps. By mining the repository, we recorded two metrics that capture the financial success of the application (number of users and volume of transactions) and explored their relation to the aforementioned factors.
Results
The findings of the study suggest that the intensity of development activities is the most important factor for its financial success. Similarly, the context (i.e., the application domain) of the decentralized application is also a key-factor since it influences the number of users that the DApp will reach.
Conclusions
Based on the findings, we suggest businesses that want to enter the market of decentralized applications to balance properly between technical and business parameters. For an application to be successful, it requires both an intensive 
development process
, but also a careful consideration of the application domain.",Information and Software Technology,18 Mar 2025,8,"The study focuses on factors important for the financial success of Decentralized Applications, providing insight for businesses entering this market."
https://www.sciencedirect.com/science/article/pii/S0950584924000284,Model driven engineering for machine learning components: A systematic literature review,May 2024,"Model driven engineering, Software engineering, Artificial intelligence, Machine learning, Systematic literature review",Hira=Naveed: hira.naveed@monash.edu; Chetan=Arora: chetan.arora@monash.edu; Hourieh=Khalajzadeh: hourieh.khalajzadeh@deakin.edu.au; John=Grundy: john.grundy@monash.edu; Omar=Haggag: omar.haggag@monash.edu,"Abstract
Context:
Machine Learning
 (ML) has become widely adopted as a component in many modern 
software applications
. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, 
anomaly detection
, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and 
software engineering
. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as 
modeling languages
, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.",Information and Software Technology,18 Mar 2025,9,"The intersection of Model-Driven Engineering with Machine Learning is explored through a systematic literature review, benefiting both researchers and practitioners."
https://www.sciencedirect.com/science/article/pii/S0950584924000156,An empirical study on metamorphic testing for recommender systems,May 2024,Not Found,Chengying=Mao: maochy@yeah.net; Jifu=Chen: chenjifu1989@sina.com; Xiaorong=Yi: 757927904@qq.com; Linlin=Wen: wenll97@foxmail.com,"Abstract
Context:
Recommender systems
 are widely used in various fields because they can provide decision-making guidance to users facing an overwhelming set of choices. In previous studies, the accuracy of recommendations has been the focus and has significantly improved. However, the quality issues of these systems have been overlooked. In practical applications, the reliability of recommender systems plays an important role in their acceptance by users.
Objective:
This paper aims to develop a solution for performing metamorphic testing on recommender systems, and then to evaluate their reliability based on the test results.
Methods:
A metamorphic testing framework for recommender systems is first proposed to effectively alleviate the difficulty of the test oracle (i.e., the construction of the expected output of a program). Meanwhile, a set of specific metamorphic relations for recommender systems is also designed, and an empirical analysis is conducted using three open-source recommender libraries: LibRec, PREA, and Surprise.
Results:
The effectiveness of the proposed metamorphic testing solution is confirmed through the experiments, and the comparison analysis of the designed metamorphic relations and the three recommender libraries is also conducted, yielding the rankings of both the metamorphic relations and the program libraries, respectively.
Conclusion:
The study suggests that metamorphic testing is effective in automatically revealing the reliability problems in recommender systems, without requiring test oracles.",Information and Software Technology,18 Mar 2025,6,"The study proposes a solution for evaluating the reliability of recommender systems through metamorphic testing, which can have practical implications for system developers."
https://www.sciencedirect.com/science/article/pii/S0950584924000156,An empirical study on metamorphic testing for recommender systems,May 2024,Not Found,Chengying=Mao: maochy@yeah.net; Jifu=Chen: chenjifu1989@sina.com; Xiaorong=Yi: 757927904@qq.com; Linlin=Wen: wenll97@foxmail.com,"Abstract
Context:
Recommender systems
 are widely used in various fields because they can provide decision-making guidance to users facing an overwhelming set of choices. In previous studies, the accuracy of recommendations has been the focus and has significantly improved. However, the quality issues of these systems have been overlooked. In practical applications, the reliability of recommender systems plays an important role in their acceptance by users.
Objective:
This paper aims to develop a solution for performing metamorphic testing on recommender systems, and then to evaluate their reliability based on the test results.
Methods:
A metamorphic testing framework for recommender systems is first proposed to effectively alleviate the difficulty of the test oracle (i.e., the construction of the expected output of a program). Meanwhile, a set of specific metamorphic relations for recommender systems is also designed, and an empirical analysis is conducted using three open-source recommender libraries: LibRec, PREA, and Surprise.
Results:
The effectiveness of the proposed metamorphic testing solution is confirmed through the experiments, and the comparison analysis of the designed metamorphic relations and the three recommender libraries is also conducted, yielding the rankings of both the metamorphic relations and the program libraries, respectively.
Conclusion:
The study suggests that metamorphic testing is effective in automatically revealing the reliability problems in recommender systems, without requiring test oracles.",Information and Software Technology,18 Mar 2025,6,"Similar to Abstract 148, the study proposes a solution for evaluating the reliability of recommender systems through metamorphic testing."
https://www.sciencedirect.com/science/article/pii/S0950584924000168,Towards sustainable software systems: A software sustainability analysis framework,May 2024,Not Found,Hira=Noman: hira.noman@faculty.muet.edu.pk; Naeem=Mahoto: naeem.mahoto@faculty.muet.edu.pk; Sania=Bhatti: sania.bhatti@faculty.muet.edu.pk; Adel=Rajab: adrajab@nu.edu.sa; Asadullah=Shaikh: asshaikh@nu.edu.sa,"Abstract
Context:
In today’s rapidly evolving technological landscape designing sustainable software systems requires considering the software impacts and its long-term viability. For professionals, a significant barrier lies in the need for 
practical guidelines
 and tangible frameworks for effectively incorporating sustainability considerations during 
software design
 and development.
Objective:
The study aims to help software practitioners consider sustainability during 
software design
 and development by providing systematic guidelines. We proposed a framework that enables professionals to do so by allowing them to foresee how and how much impact the software has on sustainability and its dimensions.
Methods:
The study presented a software sustainability analysis framework that helps to derive sustainability goals and extract sustainability improvement features for software systems based on their impacts on sustainability dimensions. The framework’s application is exemplified through two 
Software applications
, and sustainability guidelines have been provided.
Results:
Industry professionals with diverse roles were engaged to validate the framework’s relevance and practicality. Their feedback was collected through discussion, and direct excerpts were reported. Validation results revealed that the framework effectively addresses the key challenges professionals face in integrating sustainability into their practices.
Conclusion:
Professionals acknowledged the importance of considering sustainability aspects in software design and development and appreciated the structured approach provided by the framework.",Information and Software Technology,18 Mar 2025,7,"The study provides systematic guidelines for incorporating sustainability considerations into software design, addressing a significant barrier for professionals in the field."
https://www.sciencedirect.com/science/article/pii/S095058492400017X,Co-evolving scenarios and simulated players to locate bugs that arise from the interaction of software models of video games,May 2024,"Bug localization, Model interaction, Game software engineering, Search-based software engineering, Model-driven engineering",Isis=Roca: iroca@usj.es; Óscar=Pastor: opastor@pros.upv.es; Carlos=Cetina: ccetina@usj.es; Lorena=Arcega: larcega@usj.es,"Abstract
Context:
Game 
Software Engineering
 (GSE) is a field that focuses on developing and maintaining the software part of 
video games
. A key component of video game development is the utilization of game engines, with many engines using software models to capture various aspects of the game.
Objective:
A challenge that GSE faces is the localization of bugs, mainly when working with large and intricated software models. Additionally, the interaction between software models (i.e. bosses, enemies, or environmental elements) during gameplay is often a significant source of bugs. In response to this challenge, we propose a co-evolution approach for bug localization in the software models of video games, called CoEBA.
Methods:
The CoEBA approach leverages Search-Based Software Engineering (SBSE) techniques to locate bugs in software models while considering their interactions. We conducted an evaluation in which we applied our approach to a commercial video game, Kromaia. We compared our approach with a state-of-the-art baseline approach that relied on the bug localization approach used by Kromaia’s developers and a random search used as a sanity check.
Results:
Our co-evolution approach outperforms the baseline approach in precision, recall, and F-measure. In addition, to provide evidence of the significance of our results, we conducted a statistical analysis. that shows significant differences in precision and recall values.
Conclusion:
The proposed approach, CoEBA, which considers the interaction between software models, can identify and locate bugs that other bug localization approaches may have overlooked.",Information and Software Technology,18 Mar 2025,8,"The proposed CoEBA approach addresses a significant challenge in game software engineering, showing better performance than baseline approaches, which can have a positive impact on early-stage ventures in the gaming industry."
https://www.sciencedirect.com/science/article/pii/S0950584924000120,Investigating the relationship between personalities and agile team climate: A replicated study,May 2024,Not Found,Gleyser=Guimarães: Not Found; Icaro=Costa: Not Found; Mirko=Perkusich: mirko@virtus.ufcg.edu.br; Emilia=Mendes: Not Found; Danilo=Santos: Not Found; Hyggo=Almeida: Not Found; Angelo=Perkusich: Not Found,"Abstract
Context
A study in 2020 (S1) explored the relationship between 
personality traits
 and team climate perceptions of software professionals working in agile teams. S1 surveyed 43 software professionals from a large 
telecom company
 in Sweden and found that a person's ability to get along with team members (
Agreeableness
) influences significantly and positively the perceived level of team climate. Further, they observed that 
personality traits
 accounted for less than 15 % of the variance in team climate.
Objective
The study described herein replicates S1 using data gathered from 148 software professionals from an industrial partner in Brazil.
Method
We used the same research methods as S1. We employed a survey to gather the personality and climate data, which was later analyzed using correlation and 
regression analyses
. The former aimed to measure the level of association between 
personality traits
 and climate and the latter to estimate team climate factors using 
personality traits
 as predictors.
Results
The results for the correlation analyses showed statistically significant and positive associations between two personality traits - 
Agreeableness
 and 
Conscientiousness
, and all five team climate factors. There was also a significant and positive association between 
Openness
 and 
Team Vision.
 Our results corroborate those from S1, with respect to two personality traits – 
Openness
 and 
Agreeableness
; however, in S1, 
Openness
 was significantly and positively associated with 
Support for Innovation
 (not 
Team Vision
). In regard to 
Agreeableness
, in S1 it was also significantly and positively associated with 
perceived team climate
. Furthermore, our regression models also support S1’s findings - personality traits accounted for less than 15 % of the variance in team climate.
Conclusion
Despite variances in location, sample size, and 
operational domain
, our study confirmed S1′s results on the limited influence of personality traits. 
Agreeableness
 and 
Openness
 were significant predictors for team climate, although the predictive factors differed. These discrepancies highlight the necessity for further research, incorporating larger samples and additional 
predictor variables
, to better comprehend the intricate relationship between personality traits and team climate across diverse cultural and professional settings.",Information and Software Technology,18 Mar 2025,7,"The study replicates and confirms previous research findings, providing valuable insights into the relationship between personality traits and team climate in agile software development teams, which can be beneficial for early-stage startups looking to build effective teams."
https://www.sciencedirect.com/science/article/pii/S0950584924000181,A random forest model for early-stage software effort estimation for the SEERA dataset,May 2024,Not Found,Emtinan I.=Mustafa: Not Found; Rasha=Osman: rosman@ieee.org,"Abstract
Context
Publicly available 
software cost estimation
 datasets are outdated and may not represent current industrial environments. Thus most research has concentrated on the development and evaluation of estimation models with limited evidence of their applicability to industrial practice. Moreover, these datasets and models may not be applicable in (under-represented) technically and economically constrained environments such as the software development environment in Sudan.
Objective
This paper aims to develop a 
machine learning
 model that is suitable for the Sudanese software industry. To demonstrate the suitability of our approach, we evaluate our model using the publicly available SEERA (
S
oftware engin
EER
ing in Sud
A
n) dataset, which is a 
software cost estimation
 dataset from organizations in Sudan.
Method
We demonstrated the suitability of the SEERA dataset for effort estimation by comparing the attributes that had a high correlation with 
actual effort
 and 
actual duration
 to the cost factors identified by (Sudanese) experts. In addition, we developed an early-stage 
Random Forest model
 to estimate project effort and duration from the SEERA dataset. Early-stage estimation is in-line with current Sudanese industrial practice. We investigated the impact of oversampling, feature selection, heterogeneity and local environmental factors on model accuracy.
Results
Our experimental results showed that the 
Random Forest model
 with oversampling and feature selection provided accurate estimates that were better than random guessing (standardized accuracy > 70 %). Our results were similar to accuracies reported in the literature. In addition, we demonstrated that our random forest model provided estimations that were more accurate than (Sudanese) 
expert judgement
.
Conclusion
This study has demonstrated the feasibility of our random forest model for early-stage effort and duration estimation for Sudanese software projects. The results demonstrate the importance of representative models and datasets for non-traditional technical environments. Further research is required to investigate the impact of local environmental factors on software cost estimation.",Information and Software Technology,18 Mar 2025,6,"The development of a machine learning model for software cost estimation in Sudanese software industry shows promise, but further research is needed to fully assess its impact on early-stage ventures in this specific region."
https://www.sciencedirect.com/science/article/pii/S0950584924000302,An exploratory study of software artifacts on GitHub from the lens of documentation,May 2024,Not Found,Akhila Sri Manasa=Venigalla: cs19d504@iittp.ac.in; Sridhar=Chimalakonda: ch@iittp.ac.in,"Abstract
Context:
The abundance of software artifacts in open-source repositories has been analyzed by researchers from many perspectives, to address challenges in downstream tasks such as bug localization, code clone detection and so on. However, there is limited exploration of artifacts such as pull-requests and issues from a documentation perspective.
Objective:
We aim to explore the presence of information useful for documentation in different sources within the software projects. We present an exploratory analysis of 1.38M artifacts extracted from 950 GitHub repositories that analyses the content present in multiple software artifacts from a documentation perspective.
Method:
We arrive at a list of documentation types and sources through card-sorting and a developer survey. We apply 
topic modeling
 on the data extracted from 1.38M software artifacts based on these lists and study the extent of documentation-related information present in the software artifacts. The exploratory analysis of the artifacts listed is consolidated into the ‘
DocMine
’ dataset that comprises 50.63M textual sentences spanning across repositories written in four different programming languages.
Results:
We observe that about 28.1% of content extracted from the artifacts contains information related to features and modifications of the project at a higher level, and that 
pull-requests
 and 
issues
 comprise 18.26% and 17.85% of the extracted information.
Conclusion:
The presence of information about the projects in 
pull-requests
 and 
issues
 indicates immense scope in analyzing and processing multiple software artifacts for the purposes of generating 
software documentation
 and beyond. We envision that this study could open up a new line of research in 
software documentation
.",Information and Software Technology,18 Mar 2025,9,"The 'DocMine' dataset and exploratory analysis provide valuable insights into extracting documentation-related information from software artifacts, potentially opening up new research avenues for software documentation, which could benefit early-stage ventures in software development."
https://www.sciencedirect.com/science/article/pii/S0950584924000338,Mashup-oriented API recommendation via pre-trained heterogeneous information networks,May 2024,Not Found,Mingdong=Tang: Not Found; Fenfang=Xie: Not Found; Sixian=Lian: Not Found; Jiajin=Mai: Not Found; Shuangyin=Li: shuangyinli@scnu.edu.cn,"Abstract
Combining different Web APIs to create Mashups has become very popular nowadays. Choosing suitable ones from massive Web APIs is of vital importance for efficient Mashup creations. A number of Mashup-oriented API recommendation methods have been proposed to address this issue, but they have limitations in their ability to exploit the rich attributes and connection data of Web APIs, which impedes their performance. By modeling the API-related data as a heterogeneous information network and using pre-training technology, this paper proposes an accurate API recommendation method, named PHRec. In this method, the meta paths of APIs in the heterogeneous information network are exploited to obtain their 
context semantics
; the method adopts an 
attention mechanism
. Extensive experiments have been conducted with a real Web API dataset to evaluate the proposed method. The experimental results demonstrate that it significantly outperforms the state-of-the-art methods in the Web API recommendation task.",Information and Software Technology,18 Mar 2025,8,"The PHRec method for API recommendation shows significant improvements over existing methods, which can be beneficial for startups looking to efficiently create Mashups using Web APIs, improving their product development process."
https://www.sciencedirect.com/science/article/pii/S0950584923002501,Skills development for software engineers: Systematic literature review,April 2024,Not Found,Giovana Giardini=Borges: giovana.giardini@unesp.br; Rogéria Cristiane=Gratão de Souza: Not Found,"Abstract
Context
A good software professional must have technical and non-technical skills, that is, hard and soft skills, to deal with the diverse challenges they will encounter throughout their career. To make this possible, such professional must develop these abilities from the undergraduate.
Objective
This research aims to identify the necessary soft skills for future Software Engineers and the teaching methodologies that contribute to developing such skills from the undergraduate, keeping the students motivated. In addition, this study proposes a framework to help educators conduct a teaching-learning process that includes hard and soft skills during the undergraduate of future Software Engineers.
Methodology
A Systematic Literature Review was performed on six databases, resulting in 56 selected articles identifying the soft skills and the teaching methodologies desired to train Software Engineers. These were the base for the proposed framework.
Results
We proposed a grouping of soft skills found in the literature totaling 33 soft skills. Furthermore, since were found, in the literature, definitions for only 23, this study also defined the other ten soft skills addressed. Regarding the most used and indicated methodologies for developing soft skills in undergraduate students, it was possible to organize them by the principal and auxiliary methodologies. Finally, a framework was proposed to assist in the development of hard and soft skills in undergraduate students, focused on Software Engineering, the FraSSD - Framework for Soft Skills Development.
Conclusion
The proposed framework can contribute to educators’ critical thinking about applying the most effective teaching methodologies for developing hard and soft skills in an undergraduate class, improving the teaching-learning process. This study also evidences the most relevant soft skills for Software Engineers, encouraging the constant search to improve their soft skills aligned with their hard skills since graduation.",Information and Software Technology,18 Mar 2025,6,"The proposed framework for developing soft skills in Software Engineers could have a positive impact on early-stage ventures by enhancing the capabilities of team members. However, the focus on undergraduate education may limit its direct applicability to startups."
https://www.sciencedirect.com/science/article/pii/S0950584923002501,Skills development for software engineers: Systematic literature review,April 2024,Not Found,Giovana Giardini=Borges: giovana.giardini@unesp.br; Rogéria Cristiane=Gratão de Souza: Not Found,"Abstract
Context
A good software professional must have technical and non-technical skills, that is, hard and soft skills, to deal with the diverse challenges they will encounter throughout their career. To make this possible, such professional must develop these abilities from the undergraduate.
Objective
This research aims to identify the necessary soft skills for future Software Engineers and the teaching methodologies that contribute to developing such skills from the undergraduate, keeping the students motivated. In addition, this study proposes a framework to help educators conduct a teaching-learning process that includes hard and soft skills during the undergraduate of future Software Engineers.
Methodology
A Systematic Literature Review was performed on six databases, resulting in 56 selected articles identifying the soft skills and the teaching methodologies desired to train Software Engineers. These were the base for the proposed framework.
Results
We proposed a grouping of soft skills found in the literature totaling 33 soft skills. Furthermore, since were found, in the literature, definitions for only 23, this study also defined the other ten soft skills addressed. Regarding the most used and indicated methodologies for developing soft skills in undergraduate students, it was possible to organize them by the principal and auxiliary methodologies. Finally, a framework was proposed to assist in the development of hard and soft skills in undergraduate students, focused on Software Engineering, the FraSSD - Framework for Soft Skills Development.
Conclusion
The proposed framework can contribute to educators’ critical thinking about applying the most effective teaching methodologies for developing hard and soft skills in an undergraduate class, improving the teaching-learning process. This study also evidences the most relevant soft skills for Software Engineers, encouraging the constant search to improve their soft skills aligned with their hard skills since graduation.",Information and Software Technology,18 Mar 2025,6,"Similar to abstract 156, the framework proposed for developing soft skills in Software Engineers has potential value for early-stage ventures. However, its emphasis on undergraduate education may limit its immediate impact on startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000016,Towards a taxonomy of privacy requirements based on the LGPD and ISO/IEC 29100,April 2024,Not Found,Sâmmara=Éllen Renner Ferrão: Not Found; Geovana=Ramos Sousa Silva: Not Found; Edna=Dias Canedo: ednacanedo@unb.br; Fabiana=Freitas Mendes: Not Found,"Abstract
Context:
Ensuring compliance with current data privacy legislation poses a significant challenge for software development teams, demanding adaptations to processes in order to align with legal requirements.
Objective:
This study proposes a comprehensive taxonomy of privacy requirements, drawing from the Brazilian General 
Data Protection Law
 (LGPD) and ISO/IEC 29100. The aim is to assist software development teams in navigating the complexities of legal compliance.
Method:
To define the research gap, we conducted a systematic literature review (SLR) initially, identifying existing taxonomies of privacy requirements. Subsequently, we applied the Goal-Based Requirements Analysis Method (GBRAM) to extract privacy requirements from LGPD and ISO/IEC 29000. Finally, we implemented the proposed taxonomy in the privacy policies of Brazil’s three largest banks.
Results:
The taxonomy comprises 129 requirements, categorized into 10 distinct groups across 5 contexts. In applying the taxonomy to ISO/IEC 29100, analysis of 63 statements for GDPR+ISO/IEC 29100 yielded 33 requirements, whereas for LGPD+ISO/IEC 29100, 58 statements resulted in 57 requirements. Application of the taxonomy revealed adherence percentages ranging from 40% to 71% concerning the evaluated solutions.
Conclusions:
The outcomes strongly suggest that major corporations are yet to achieve full LGPD compliance. We posit that the proposed taxonomy offers a valuable industry tool for validating LGPD compliance within implemented systems, as exemplified by our successful use case with Brazilian banks.",Information and Software Technology,18 Mar 2025,8,The comprehensive taxonomy of privacy requirements proposed in this study could directly benefit European early-stage ventures by assisting software development teams in achieving compliance with data privacy legislation. The practical implications and clear industry application make this abstract highly valuable.
https://www.sciencedirect.com/science/article/pii/S0950584923002495,Test Code Flakiness in Mobile Apps: The Developer’s Perspective,April 2024,"Test Code Flakiness, Software Testing, Mobile Apps Development, Mixed-Method Research",Valeria=Pontillo: valeria.pontillo@vub.be; Fabio=Palomba: fpalomba@unisa.it; Filomena=Ferrucci: fferrucci@unisa.it,"Abstract
Context:
Test flakiness arises when test cases have a non-deterministic, intermittent behavior that leads them to either pass or fail when run against the same code. While researchers have been contributing to the detection, classification, and removal of flaky tests with several empirical studies and automated techniques, little is known about how the problem of test flakiness arises in mobile applications.
Objective:
We point out a lack of knowledge on: (1) The prominence and harmfulness of the problem; (2) The most frequent root causes inducing flakiness; and (3) The strategies applied by practitioners to deal with it in practice. An improved understanding of these matters may lead the 
software engineering
 research community to assess the need for tailoring existing instruments to the mobile context or for brand-new approaches that focus on the peculiarities identified.
Methods:
We address this gap of knowledge by means of an empirical study into the mobile developer’s perception of test flakiness. We first perform a systematic grey literature review to elicit how developers discuss and deal with the problem of test flakiness in the wild. Then, we complement the systematic review through a survey study that involves 130 mobile developers and that aims at analyzing their experience on the matter.
Results:
The results of the grey literature review indicate that developers are often concerned with flakiness connected to 
user interface elements
. In addition, our survey study reveals that flaky tests are perceived as critical by mobile developers, who pointed out major production code- and source code design-related root causes of flakiness, other than the long-term effects of recurrent flaky tests. Furthermore, our study lets the diagnosing and fixing processes currently adopted by developers and their limitations emerge.
Conclusion:
We conclude by distilling lessons learned, implications, and future research directions.",Information and Software Technology,18 Mar 2025,3,"While the study on test flakiness in mobile applications addresses important issues, such as flaky tests perception by developers, the direct impact on European early-stage ventures may be limited as the focus is more specific to mobile development rather than general software startups."
https://www.sciencedirect.com/science/article/pii/S0950584923002471,Process mining software engineering practices: A case study for deployment pipelines,April 2024,"ci/cd, DevOps, Process mining, Temporal profiling, Continuous improvement, Software engineering",Ana Filipa=Nogueira: afnog@dei.uc.pt; Mário=Zenha-Rela: mzrela@dei.uc.pt,"Abstract
Context:
In mature software development organizations the 
ci/cd
 pipeline is the only route to deploy software into production. While the workflow of this process seems straightforward, the reality is different since exceptions and deviations are the norm in actual industry practice. In this context, Process Mining appears as a promising technique to uncover deviations and check compliance with standardized 
DevOps
 processes, and highlight bottlenecks and potential improvement areas.
Objective:
This paper presents a 
case study
 designed to assess the potential of using 
Process Mining techniques
 to provide visibility into the deployment pipeline.
Method:
This research uses raw event data extracted from the continuous practices toolchain, which is then used to compute a comprehensive set of DevOps-specific metrics, thus supporting objective monitoring of the quality and efficiency of the deployment workflow. The study focuses on different development units in the Engineering team, each working in a distinct 
business context
 but sharing standard practices.
Results:
We verified that even though there are standards for the deployment pipelines, each team’s workflow denotes local variations with unique points for improvement that are highly coupled to their business unit context. We observed that each team’s pipeline has different temporal profiles that reflect their context and work practices. Additionally, we identified a set of deployment pipeline metrics focusing on process compliance, efficiency, and deployment stability.
Conclusion:
The main contributions of this paper include (1) the description of an actual application of Process Mining to the deployment pipeline of a highly complex e-commerce platform, (2) how this approach provided an objective understanding of the efficiency and quality of the development workflow, (3) how this process-centric view, combined with domain-specific DevOps metrics, supports continuous practices, and (4) how Developers can analyse their workflows by applying Process Mining while using standard tools like GitLab and PM4Py.",Information and Software Technology,18 Mar 2025,8,"The case study on using Process Mining techniques to assess the deployment pipeline offers valuable insights for software development organizations, including early-stage ventures. The potential for uncovering improvements and bottlenecks aligns with the needs of startups looking to streamline their development processes."
https://www.sciencedirect.com/science/article/pii/S0950584924000090,A longitudinal study on the temporal validity of software samples,April 2024,Not Found,Juan Andrés=Carruthers: jacarruthers@exa.unne.edu.ar; Jorge Andrés=Diaz-Pace: Not Found; Emanuel=Irrazábal: Not Found,"Abstract
Context
In Empirical 
Software Engineering
, it is crucial to work with 
representative samples
 that reflect the current state of the software 
industry
. An important consideration, especially in rapidly changing fields like software development, is that if we use a sample collected years ago, it should continue to represent the same population in the present day to produce generalizable results. However, it is seldom the case in which a software sample built several years ago accurately depicts the current state of the development 
industry
. Nevertheless, many recent studies rely on rather old datasets (seven or more years of age) to conduct their investigations.
Objective
To analyze the evolution of a population of open-source projects, determine the likelihood of detecting significant differences over time, and study the activity history of the projects.
Method
We performed a longitudinal study with 72 snapshots of quality projects from Github, covering the period between July 1
st
 2017 and June 1
st
 2023. We recorded monthly values of seven repository metrics (contributors, commits, closed pull-requests, merged pull-requests, closed issues, number of stars and forks), encompassing data from a total of 1991 repositories.
Results
We observed significant changes in all the metrics evaluated, with most cases showing negligible to small 
effect sizes
. Notably, merged pull-requests registered medium 
effect sizes
. The evolution was not equal in all the metrics, however, after five years it was unlikely that a sample of projects remained representative for any of the analyzed metrics, showing probabilities below 25%.
Conclusion
Although the temporal validity of a sample depends on the specific data being studied, employing datasets created several years ago does not appear to be a sound strategy if the aim is to produce results that can be extrapolated to the current state of the population.",Information and Software Technology,18 Mar 2025,7,"The study provides valuable insights into the temporal validity of software development datasets, highlighting the importance of up-to-date samples for accurate results, which can impact early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000107,Automatic smart contract comment generation via large language models and in-context learning,April 2024,Not Found,Junjie=Zhao: zhaojunjie225@gmail.com; Xiang=Chen: xchencs@ntu.edu.cn; Guang=Yang: novelyg@outlook.com; Yiheng=Shen: yiheng.s@outlook.com,"Abstract
Context:
Designing effective automatic 
smart contract
 comment generation approaches can facilitate developers’ comprehension, boosting 
smart contract
 development and improving 
vulnerability detection
. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches.
Objective:
However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in 
large language models
 (LLMs) to alleviate the disadvantages of these two types of approaches.
Method:
In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top-
k
 code snippets from the historical corpus by considering syntax, semantics, and 
lexical information
. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs.
Results:
We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies.
Conclusion:
Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies.",Information and Software Technology,18 Mar 2025,9,"The proposed SCCLLM approach using LLMs and in-context learning shows promising results for automatic smart contract comment generation, offering practical value for startups in the blockchain industry."
https://www.sciencedirect.com/science/article/pii/S0950584924000144,A deep semantics-aware data augmentation method for fault localization,April 2024,Not Found,Jian=Hu: jianhu@cqu.edu.cn; Yan=Lei: yanlei@cqu.edu.cn,"Abstract
Context:
Fault localization
 (FL) techniques are employed to identify the relationship between program statements and failures by analyzing runtime information. They rely on the statistics of input data to explore the underlying correlation rooted in it. Consequently, the quality of input data is of 
utmost importance
 for FL. However, in practice, passing tests significantly outnumber failing tests regarding a fault. This leads to a 
class imbalance
 challenge that can adversely affect the effectiveness of FL.
Objective:
To tackle the issue of 
imbalanced data
 in fault localization, we propose 
PRAM
: a dee
P
 semantic-awa
R
e d
A
ta augmentation 
M
ethod to improve the effectiveness of 
FL methods
.
Method:
PRAM
 utilizes program dependencies to enhance the 
semantic context
, thus showing how a failure is caused. Then, 
PRAM
 employs mixup method to synthesize new failing test samples by merging two real 
failing test cases
 with a random ratio to balance the input data. Finally, 
PRAM
 feeds the balanced data consisting of synthesized 
failing test cases
 and original test cases to FL techniques. To evaluate the effectiveness of 
PRAM
, we conducted large-scale experiments on 330 versions of nine large-sized real programs for six state-of-the-art 
FL methods
, two data optimization methods and two 
data augmentation
 methods.
Results:
Our experimental results show that 
PRAM
 outperforms in most cases for Top-K metrics and reduces the number of checked statements from 40.38% to 80.04% compared with the original FL methods. Furthermore, 
PRAM
 reduces the checked statements from 16.92% to 56.98% for data optimization methods and from 12.48% to 26.82% for 
data augmentation
 methods.
Conclusion:
The experimental results show that 
PRAM
 is not only more effective than the original FL methods but also more effective than two representative data optimization methods and two data augmentation methods, which indicates that 
PRAM
 is a universal effective data augmentation method for various FL methods.",Information and Software Technology,18 Mar 2025,10,"PRAM demonstrates significant improvements in fault localization by addressing the imbalanced data challenge, providing a universal effective data augmentation method that can benefit early-stage ventures in software development."
https://www.sciencedirect.com/science/article/pii/S0950584923002185,Towards the definition of a research agenda on mobile application testing based on a tertiary study,March 2024,Not Found,Pedro Henrique=Kuroishi: phk@ufscar.br; José Carlos=Maldonado: jcmaldon@icmc.usp.br; Auri Marcelo Rizzo=Vincenzi: auri@ufscar.br,"Abstract
Context:
Mobile application testing has gained considerable attention in recent years since mobile devices have become increasingly present in our lives. Unlike traditional software, mobile application testing has to deal with peculiarities, such as screen size and densities, different operating systems, and multiple sensors that increase the complexity of testing.
Objective:
This paper summarizes and analyzes the current secondary studies on mobile application testing through a tertiary study.
Method:
We selected and analyzed 21 secondary studies related to mobile application testing.
Results:
We categorized 21 secondary studies according to their main and specific 
research topics
, test objectives, and testing platforms. Furthermore, we analyze 87 gaps and challenges identified by the secondary studies to understand which gaps have already been addressed and which gaps are still uncovered.
Conclusion:
Based on the results, we propose a 
research agenda
 with 15 open challenges related to mobile application testing to help future research.",Information and Software Technology,18 Mar 2025,5,"While the research agenda proposed in the study is useful for defining future research directions, the practical impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923002185,Towards the definition of a research agenda on mobile application testing based on a tertiary study,March 2024,Not Found,Pedro Henrique=Kuroishi: phk@ufscar.br; José Carlos=Maldonado: jcmaldon@icmc.usp.br; Auri Marcelo Rizzo=Vincenzi: auri@ufscar.br,"Abstract
Context:
Mobile application testing has gained considerable attention in recent years since mobile devices have become increasingly present in our lives. Unlike traditional software, mobile application testing has to deal with peculiarities, such as screen size and densities, different operating systems, and multiple sensors that increase the complexity of testing.
Objective:
This paper summarizes and analyzes the current secondary studies on mobile application testing through a tertiary study.
Method:
We selected and analyzed 21 secondary studies related to mobile application testing.
Results:
We categorized 21 secondary studies according to their main and specific 
research topics
, test objectives, and testing platforms. Furthermore, we analyze 87 gaps and challenges identified by the secondary studies to understand which gaps have already been addressed and which gaps are still uncovered.
Conclusion:
Based on the results, we propose a 
research agenda
 with 15 open challenges related to mobile application testing to help future research.",Information and Software Technology,18 Mar 2025,5,"Similar to abstract 164, while summarizing secondary studies on mobile application testing is valuable, the direct impact on European early-stage ventures may not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923002264,Vulnerability detection based on federated learning,March 2024,Not Found,Chunyong=Zhang: Not Found; Tianxiang=Yu: Not Found; Bin=Liu: Not Found; Yang=Xin: yangxin@bupt.edu.cn,"Abstract
Context:
Detecting 
potential vulnerabilities
 is a key step in defending against network attacks. However, manual detection is time-consuming and requires expertise. Therefore, 
vulnerability detection
 must require automated techniques.
Objective:
Vulnerability detection methods based on 
deep learning
 need to rely on sufficient vulnerable code samples. However, the problem of code islands has not been extensively researched. For example, in the case of multi-party vulnerability data, how to securely combine multi-party data to improve 
vulnerability detection
 performance. From the perspectives of 
data augmentation
 and data security, we propose a 
v
ulnerability 
d
etection framework 
b
ased on 
f
ederated 
l
earning (VDBFL). VDBFL is a new model for vulnerability code detection that combines multi-party data.
Method:
Firstly, VDBFL utilizes the code property graph as a code representation. The code property graph contains various 
semantic dependencies
 of the code. Secondly, VDBFL utilizes 
graph neural networks
 and 
convolutional neural networks
 as the code feature extractor. VDBFL utilizes the jump-structured graph 
attention network
 to aggregate node information of important neighbors. Finally, VDBFL utilizes horizontal 
federated learning
 to train a local vulnerability detection model for the client.
Result:
In the real world, VDBFL improves F1-Score by 37.4% compared to the vulnerability detection method Reveal. Among the 5401 vulnerability samples, VDBFL detected 11.8 times more vulnerabilities than Reveal.
Conclusion:
Under different datasets, VDBFL has shown better performance than advanced vulnerability detection methods in multiple metrics. In addition, the 
federated learning
 stage of VDBFL can be expanded on top of the feature extraction stage of any vulnerable detection method.",Information and Software Technology,18 Mar 2025,8,The proposed Vulnerability Detection Framework based on Federated Learning shows significant improvement in detecting vulnerabilities. This has practical value for early-stage ventures dealing with network security.
https://www.sciencedirect.com/science/article/pii/S0950584923002264,Vulnerability detection based on federated learning,March 2024,Not Found,Chunyong=Zhang: Not Found; Tianxiang=Yu: Not Found; Bin=Liu: Not Found; Yang=Xin: yangxin@bupt.edu.cn,"Abstract
Context:
Detecting 
potential vulnerabilities
 is a key step in defending against network attacks. However, manual detection is time-consuming and requires expertise. Therefore, 
vulnerability detection
 must require automated techniques.
Objective:
Vulnerability detection methods based on 
deep learning
 need to rely on sufficient vulnerable code samples. However, the problem of code islands has not been extensively researched. For example, in the case of multi-party vulnerability data, how to securely combine multi-party data to improve 
vulnerability detection
 performance. From the perspectives of 
data augmentation
 and data security, we propose a 
v
ulnerability 
d
etection framework 
b
ased on 
f
ederated 
l
earning (VDBFL). VDBFL is a new model for vulnerability code detection that combines multi-party data.
Method:
Firstly, VDBFL utilizes the code property graph as a code representation. The code property graph contains various 
semantic dependencies
 of the code. Secondly, VDBFL utilizes 
graph neural networks
 and 
convolutional neural networks
 as the code feature extractor. VDBFL utilizes the jump-structured graph 
attention network
 to aggregate node information of important neighbors. Finally, VDBFL utilizes horizontal 
federated learning
 to train a local vulnerability detection model for the client.
Result:
In the real world, VDBFL improves F1-Score by 37.4% compared to the vulnerability detection method Reveal. Among the 5401 vulnerability samples, VDBFL detected 11.8 times more vulnerabilities than Reveal.
Conclusion:
Under different datasets, VDBFL has shown better performance than advanced vulnerability detection methods in multiple metrics. In addition, the 
federated learning
 stage of VDBFL can be expanded on top of the feature extraction stage of any vulnerable detection method.",Information and Software Technology,18 Mar 2025,7,"Studying group dynamics in student software development projects can provide insights into team performance, which can be valuable for startups aiming to enhance their team collaboration and productivity."
https://www.sciencedirect.com/science/article/pii/S095058492300232X,Relating team atmosphere and group dynamics to student software development teams’ performance,March 2024,"Team atmosphere, Group dynamics, Software development projects, Team performance",Sherlock A.=Licorish: sherlock.licorish@otago.ac.nz; Daniel Alencar=da Costa: Not Found; Elijah=Zolduoarrati: Not Found; Natalie=Grattan: Not Found,"Abstract
Context
While the 
software engineering
 community (i.e., those involved with engineering software) is constantly in search of insights into team atmosphere and group dynamics and the way these issues impact team performance, little opportunities typically exist to explore this issue. Student projects offer an opportunity for us to understand these issues, and particularly if these students are on the verge of leaving university for post-study work and using similar practices to those used in 
industry
.
Objective
We explore a range of student software development projects’ data and students’ open-ended responses to five group dynamics categories: communication, 
time management
, commitment, problem analysis and solving, and initiative and involvement.
Method
We analyse both quantitative and qualitative data to study the variation in group dynamics across teams developing different software and how these variations correlated with team satisfaction. We also explore the group dynamics themes that evolve from students’ open responses in relation to the five categories. Furthermore, we relate the prevalence of the themes to various software development performance metrics, before exploring the opportunity of predicting an optimum team dynamics.
Results
We observe variations in the way different teams work, but higher performing teams also committed more to their projects. Various group dynamics themes were evident among functional teams, and specific patterns were more pronounced when teams were productive. Further, while there is no specific group dynamics pattern that predicts project success, successful teams were most organised and reflective.
Conclusion
Competence may set the tone for positive group dynamics and team performance. Also, an achievement-driven orientation is as important as the soft skills and interpersonal aspects.",Information and Software Technology,18 Mar 2025,6,"While the study on group dynamics is insightful, it may have limited direct impact on European early-stage ventures or startups as it mainly focuses on student projects."
https://www.sciencedirect.com/science/article/pii/S0950584923002331,"BIGOWL4DQ: Ontology-driven approach for Big Data quality meta-modelling, selection and reasoning",March 2024,"Data quality evaluation and measurement, Data quality information model, Big Data, Ontology, Decision model and notation",Cristóbal=Barba-González: cbarba@uma.es; Ismael=Caballero: ismael.caballero@uclm.es; Ángel Jesús=Varela-Vaca: ajvarela@us.es; José A.=Cruz-Lemus: joseantonio.cruz@uclm.es; María Teresa=Gómez-López: maytegomez@us.es; Ismael=Navas-Delgado: ismael@uma.es,"Abstract
Context:
Data quality should be at the core of many 
Artificial Intelligence
 initiatives from the very first moment in which data is required for a successful analysis. Measurement and evaluation of the level of quality are crucial to determining whether data can be used for the tasks at hand. Conscientious of this importance, industry and academia have proposed several 
data quality measurements
 and assessment frameworks over the last two decades. Unfortunately, there is no common and shared vocabulary for data quality terms. Thus, it is difficult and time-consuming to integrate data quality analysis within a (Big) Data workflow for performing Artificial Intelligence tasks. One of the main reasons is that, except for a reduced number of proposals, the presented vocabularies are neither machine-readable nor processable, needing human processing to be incorporated.
Objective:
This paper proposes a unified data quality measurement and assessment information model. This model can be used in different environments and contexts to describe data quality measurement and evaluation concerns.
Method:
The model has been developed as an ontology to make it interoperable and machine-readable. For better interoperability and applicability, this ontology, BIGOWL4DQ, has been developed as an extension of a previously developed ontology for describing 
knowledge management
 in 
Big Data analytics
.
Conclusions:
This extended ontology provides a data quality measurement and assessment framework required when designing Artificial Intelligence workflows and integrated reasoning capacities. Thus, BIGOWL4DQ can be used to describe 
Big Data
 analysis and assess the data quality before the analysis.
Result:
Our proposal has been validated with two use cases. First, the semantic proposal has been assessed using an academic use case. And second, a real-world 
case study
 within an Artificial Intelligence workflow has been conducted to endorse our work.",Information and Software Technology,18 Mar 2025,5,"The proposal of a unified data quality measurement and assessment information model is relevant for AI initiatives, but its immediate impact on early-stage ventures may be less pronounced."
https://www.sciencedirect.com/science/article/pii/S0950584923002288,Flakiness goes live: Insights from an In Vivo testing simulation study,March 2024,"68-04, 68-U01",Morena=Barboni: morena.barboni@unicam.it; Antonia=Bertolino: antonia.bertolino@isti.cnr.it; Guglielmo=De Angelis: guglielmo.deangelis@iasi.cnr.it,"Abstract
Context:
Test flakiness is a topmost concern in software test automation. While conducting pre-deployment testing, those tests that are flagged as flaky are put aside for being either repaired or discarded.
Objective:
We hypothesise that some flaky tests could provide useful insights if run in the field, i.e., they could help identify failures that manifest themselves sporadically during In House testing, but are later experienced in operation.
Method:
We present the first simulation study to investigate the behaviour of flaky tests when moved to the field. The work compares the behaviour of known flaky tests from an open-source library when executed in the development environment vs. when executed in a simulation of the field.
Results:
Our experimentation over 52 test methods labelled as flaky provides a first confirmation that moving from the development environment to the field, the behaviour of tests changes. In particular, the failure frequency of intermittently failing tests can increase, and we could also identify few cases of field failures that would have been hardly detected during In House testing due to the numerous combinations of inputs and states. In most cases, such flakiness was rooted in the design of the test method itself, however we could also identify an actual bug.
Conclusion:
The results of our study suggest that the identification of an intermittently failing behaviour could be a valuable hint for a test engineer, and hence flaky tests should not be dismissed right away.",Information and Software Technology,18 Mar 2025,4,"Investigating test flakiness behavior may be relevant for software testing practices, but the direct application to early-stage ventures or startups may not be as significant."
https://www.sciencedirect.com/science/article/pii/S0950584923002069,Privacy-Compliant Software Reuse in Early Development Phases: A Systematic Literature Review,March 2024,Not Found,Jenny=Guber: jguber@campus.haifa.ac.il; Iris=Reinhartz-Berger: Not Found,"Abstract
Context
Privacy-compliant software development has received substantial attention in recent years, especially with the growth of digital services and the emergence of privacy regulations and standards. The increasing popularity of open-source software repositories and reuse practices challenges privacy-compliant software development.
Objective
This paper aims to present the state-of-the-art in privacy-compliant 
software reuse
, focusing on early development phases of 
requirements engineering
, domain analysis and 
software design
, as well as to discuss the current challenges that identify directions for future research.
Method
We conducted a Systematic Literature Reviews (SLR) and analyzed 61 papers published in the last two decades, in terms of their business and technological domains, followed reuse approaches, applied privacy strategies, and utilized evaluation approaches.
Results
The reviewed studies vary in terms of business domains (e.g., healthcare, smart objects and finance) and technological domains (e.g., 
IoT
, mobile, cloud and microservices). Most of the studies do not refer to a specific regulation and if so – to 
GDPR
. Their common purpose is to support benign reuse, most notably through patterns, components & libraries and model-driven engineering, but malicious reuse is also researched to a lesser extent. A 
strong emphasis
 is put on integrating privacy strategies whose goal is building trust and transparency (in particular, inform and demonstrate), while other strategies are studied to a limited extent in 
software reuse
 context. Evaluation is commonly performed through analytical, observational and experimental approaches.
Conclusions
The operationalization of privacy compliance practices for existing software artifacts is still challenging. The challenges encompass improving trustworthiness of reused artifacts, ensuring privacy compliance in distributed architectures, bridging the gap between legal regulations and software requirements, enhancing privacy analysis and 
vulnerability detection
, supporting late application of privacy strategies, and developing objective assessments for privacy-compliant software reuse.",Information and Software Technology,18 Mar 2025,7,"The paper addresses the current challenges in privacy-compliant software development, which is a relevant topic for early-stage ventures dealing with data privacy and security."
https://www.sciencedirect.com/science/article/pii/S095058492300246X,License recommendation for open source projects in the power industry,March 2024,Not Found,Ximing=Zhang: Not Found; Huan=Xu: Not Found; Qiuling=Yu: Not Found; Shipei=Zeng: Not Found; Shan=Dai: Not Found; Haowen=Yang: Not Found; Shuhan=Wu: shuhanwu@sribd.cn,"Abstract
Context:
Establishing secure and appropriate licensing procedures for open-source software is essential in the development of a decentralized renewable energy system within the smart grid industry. Nonetheless, software developers in the power industry encounter obstacles in comprehending and electing licenses on account of factors such as resemblances in terms, intricacies of the law, compatibility of licenses, and the slow development of the open source movement in the power industry.
Objective:
This paper aims to comprehensively examine the licenses of 
open source projects
 in the power industry, which is essential for the completion and popularity of projects. A novel framework consisting of two stages (i.e. data processing and recommendation) is proposed to analyze the current situation of 
open source license
 selection in the power industry.
Method:
By analyzing 274,442 open source repositories related to 40 electricity-related keywords from GitHub, we developed a machine learning-powered license recommendation methodology. We first employed the K-means method to cluster the selected repositories and identified 6 major clusters. Next, we utilized the 
random forest
 method to predict licenses for new repositories based on the 
clustering results
. We evaluated the accuracy of the model by testing it on training and testing datasets and achieved 96% accuracy.
Results:
We found that open source repository clusters in the power industry have distinct licensing preferences reflecting their unique objectives, with MIT being the most popular due to its permissiveness, and GPL-3.0, Apache-2.0, and BSD-3-Clause being favored by clusters valuing copyleft principles, closed-source derivatives protection, and control over software use, respectively. In addition, the study recognizes the content of 
open source projects
 as a meaningful indicator for license recommendation.
Conclusion:
These insights substantially enhance comprehension of the distribution and the selection of open source licenses in the power industry, potentially aiding future research on license recommendation in this field.",Information and Software Technology,18 Mar 2025,8,"The research offers a practical framework and methodology for analyzing open-source licenses in the power industry, providing valuable insights for startups in renewable energy ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923002343,Making ethics practical: User stories as a way of implementing ethical consideration in Software Engineering,March 2024,Not Found,Erika=Halme: erika.halme@rejlers.fi; Marianna=Jantunen: Not Found; Ville=Vakkuri: Not Found; Kai-Kristian=Kemell: Not Found; Pekka=Abrahamsson: Not Found,"Abstract
Context:
Shortcomings of 
AI
 systems have recently brought ethics into the spotlight in 
Software Engineering
 (SE) in the form of 
AI
 ethics. However, actually implementing ethics into practice remains a challenge in both AI ethics and SE at large. Translating abstract ethical principles into requirements and features is difficult and lacks established processes, as well as practices and methods.
Objective:
In this study, we explored user stories as a way of implementing ethics in SE. Initially, we simply investigated whether user stories could be utilized for this purpose. After we began to consider this possible, we began to develop the concept of ethical user stories (EUSs) as a specific practice for this purpose.
Method:
We utilized a 
design science research
 (DSR) approach to first explore the use of user stories in implementing ethics, and then to develop the concept of EUS. This process featured three DSR phases through which the concept of EUS was iteratively developed with empirical data.
Results:
Over three DSR iterations, we studied 689 user stories produced in different contexts including both student and industry settings. Based on the data, we defined the concept of EUS and provided empirical validation for it.
Conclusions:
The concept of EUS provides a novel way of tackling ethics in SE. This paper presents the concept in-depth, along with practical suggestions for utilizing EUS.",Information and Software Technology,18 Mar 2025,6,"The study introduces a novel concept of ethical user stories for implementing ethics in software engineering practice, which could be beneficial for startups aiming to incorporate ethical considerations in their development process."
https://www.sciencedirect.com/science/article/pii/S0950584923002483,Multi-grained contextual code representation learning for commit message generation,March 2024,Not Found,Chuangwei=Wang: 20215227033@stu.suda.edu.cn; Li=Zhang: zhangliml@suda.edu.cn; Xiaofang=Zhang: xfzhang@suda.edu.cn,"Abstract
Commit messages, precisely describing the code changes for each commit in natural language, makes it possible for developers and succeeding reviewers to understand the code changes without digging into implementation details. However, the semantic and structural gap between code and natural language poses a significant challenge for commit message generation. Several researchers have proposed automated techniques to generate commit messages. Nevertheless, the information about the code is not sufficiently exploited. In this paper, we propose multi-grained contextual code 
representation learning
 for commit message generation (COMU). We extract multi-grained information from the changed code at the line and 
AST
 levels (i.e., Code_Diff and AST_Diff). In Code_Diff, we construct global contextual 
semantic information
 about the changed code, and mark whether a line of code has changed with three different tokens. In AST_Diff, we extract the code structure from 
source code
 changes and combine the extracted structure with four types of editing operations to explicitly focus on the detailed information of the changed part. In addition, we build the 
experimental datasets
, since there is still no publicly sufficient dataset for this task. The release of this dataset would contribute to advancing research in this field. We perform an extensive experiment to evaluate the effectiveness of COMU. The experimental evaluation and human study show that our model outperforms the 
baseline model
.",Information and Software Technology,18 Mar 2025,9,"The paper proposes a cutting-edge technique for commit message generation, which can greatly benefit developers in startups by improving code understanding and collaboration."
https://www.sciencedirect.com/science/article/pii/S0950584923002458,Diversity-aware fairness testing of machine learning classifiers through hashing-based sampling,March 2024,Not Found,Zhenjiang=Zhao: zhenjiang@disc.lab.uec.ac.jp; Takahisa=Toda: toda@disc.lab.uec.ac.jp; Takashi=Kitamura: t.kitamura@aist.go.jp,"Abstract
Context:
There are growing concerns about algorithmic fairness, as some machine learning (ML)-based algorithms have been found to exhibit biases against protected attributes such as gender, race, age and so on. Individual fairness requires an ML classifier to produce similar outputs for similar individuals. Verification Based Testing (
Vbt
) is a state-of-the-art black-box testing algorithm for individual fairness that leverages constraint solving to generate test cases.
Objective:
Generating diverse test cases is expected to facilitate efficient detection of diverse discriminatory data instances (i.
 
e., cases that violate individual fairness). Hashing-based sampling techniques draw a sample approximately uniformly at random from the set of solutions of given Boolean constraints. We propose 
Vbt
-X, which improves 
Vbt
 with hashing-based sampling, aiming to improve its testing performance.
Method:
We realize hashing-based sampling for 
Vbt
. The challenge is that the off-the-shelf hashing-based sampling techniques cannot be integrated in a straightforward manner because the constraints in 
Vbt
 are generally not Boolean. Moreover, we propose several enhancement techniques to make 
Vbt
-X more efficient.
Results:
To evaluate our method, we conduct experiments, where 
Vbt
-X is compared to 
Vbt
, 
Sg
 and ExpGA (other well-known fairness testing algorithms) over a set of configurations consisting of several datasets, protected attributes, and ML classifiers. The results show that, with each configuration, 
Vbt
-X detects more discriminatory data instances with higher diversity than 
Vbt
 and 
Sg
. 
Vbt
-X detects discriminatory data instances with higher diversity than ExpGA, though the number of discriminatory data instances detected by 
Vbt
-X is lesser than ExpGA.
Conclusion:
Our proposed method performs better than other state-of-the-art black-box fairness testing algorithms, particularly in terms of diversity. Our method can serve to efficiently identify flaws in ML classifiers with respect to individual fairness for subsequent improvements of an ML classifier. On the other hand, although our method is specific to individual fairness, it could work for testing other aspects of a software system such as security and counterfactual explanations with some technical adaptations, which remains for future work.",Information and Software Technology,18 Mar 2025,8,"The research introduces an improved fairness testing algorithm, which is crucial for startups utilizing machine learning algorithms to ensure fairness and non-discrimination in their products or services."
https://www.sciencedirect.com/science/article/pii/S0950584923002355,Unraveling quantum computing system architectures: An extensive survey of cutting-edge paradigms,March 2024,Not Found,Xudong=Zhao: Not Found; Xiaolong=Xu: njuxlxu@gmail.com; Lianyong=Qi: lianyongqi@gmail.com; Xiaoyu=Xia: Not Found; Muhammad=Bilal: Not Found; Wenwen=Gong: Not Found; Huaizhen=Kou: Not Found,"Abstract
Context:
The convergence of 
physics
 and computer science in the realm of 
quantum computing
 systems has sparked a profound revolution within the computer industry. However, despite such promise, the existing focus on quantum software systems primarily centers on the generation of quantum 
source code
, inadvertently overlooking the 
pivotal role
 of the overall software architecture.
Objectives:
In order to provide comprehensive guidance to researchers and practitioners engaged in quantum software development, employing an architecture-centered development model, an extensive literature review was conducted pertaining to existing research on quantum software architecture. The analysis encompasses a detailed examination of the characteristics exhibited by these studies and the identification of prospective challenges that lie ahead in the field of quantum software architecture.
Methods:
We have closely examined instances of quantum 
software engineering
, quantum 
modeling languages
, quantum 
design patterns
, and 
quantum communication
 security to gain insights into the distinctive attributes associated with various software architecture approaches.
Results:
Our findings underscore the critical significance of prioritizing software architecture in the development of robust and efficient quantum software systems. Through the synthesis of these multifaceted aspects, both researchers and practitioners can devise quantum software solutions that are inherently architecture-centric.
Conclusion:
The software architecture of 
quantum computing
 systems plays a 
pivotal role
 in determining their ultimate success and usability. Given the ongoing advancements in 
quantum computing
 technology, the migration of traditional software architecture development methods to the domain of quantum software development holds significant importance.",Information and Software Technology,18 Mar 2025,7,"The abstract provides comprehensive insights into the importance of software architecture in quantum computing, which can have significant implications for early-stage ventures in the tech industry."
https://www.sciencedirect.com/science/article/pii/S0950584923002355,Unraveling quantum computing system architectures: An extensive survey of cutting-edge paradigms,March 2024,Not Found,Xudong=Zhao: Not Found; Xiaolong=Xu: njuxlxu@gmail.com; Lianyong=Qi: lianyongqi@gmail.com; Xiaoyu=Xia: Not Found; Muhammad=Bilal: Not Found; Wenwen=Gong: Not Found; Huaizhen=Kou: Not Found,"Abstract
Context:
The convergence of 
physics
 and computer science in the realm of 
quantum computing
 systems has sparked a profound revolution within the computer industry. However, despite such promise, the existing focus on quantum software systems primarily centers on the generation of quantum 
source code
, inadvertently overlooking the 
pivotal role
 of the overall software architecture.
Objectives:
In order to provide comprehensive guidance to researchers and practitioners engaged in quantum software development, employing an architecture-centered development model, an extensive literature review was conducted pertaining to existing research on quantum software architecture. The analysis encompasses a detailed examination of the characteristics exhibited by these studies and the identification of prospective challenges that lie ahead in the field of quantum software architecture.
Methods:
We have closely examined instances of quantum 
software engineering
, quantum 
modeling languages
, quantum 
design patterns
, and 
quantum communication
 security to gain insights into the distinctive attributes associated with various software architecture approaches.
Results:
Our findings underscore the critical significance of prioritizing software architecture in the development of robust and efficient quantum software systems. Through the synthesis of these multifaceted aspects, both researchers and practitioners can devise quantum software solutions that are inherently architecture-centric.
Conclusion:
The software architecture of 
quantum computing
 systems plays a 
pivotal role
 in determining their ultimate success and usability. Given the ongoing advancements in 
quantum computing
 technology, the migration of traditional software architecture development methods to the domain of quantum software development holds significant importance.",Information and Software Technology,18 Mar 2025,7,"Similar to abstract 176, this abstract highlights the critical role of software architecture in quantum computing, which can be beneficial for startups working in the quantum technology space."
https://www.sciencedirect.com/science/article/pii/S0950584923002057,Deep learning-based software bug classification,February 2024,Not Found,Jyoti Prakash=Meher: jpmeher.iitkgp@gmail.com; Sourav=Biswas: bsws.sourav@gmail.com; Rajib=Mall: rajib@cse.iitkgp.ac.in,"Abstract
Context:
Accurate classification of bugs can help accelerate the bug triage process, code inspection, and repair activities. In this context, many 
machine learning techniques
 have been proposed to classify bugs. The 
expressive power
 of 
deep learning
 could be used to further improve classification.
Objective:
We propose a novel deep learning-based bug 
classification approach
.
Methods:
We first build a bug taxonomy with eight bug classes, each characterized by a set of keywords. Subsequently, we heuristically annotate a moderately large set (
∼
1.36M) of software bug resolution reports using an earth-mover distance technique based on the keywords. Finally, we use four attention-based 
classification techniques
 to classify these curated bugs.
Results:
Our experiments on a carefully collected dataset indicate that our proposed technique achieved a mean F1-Score of 84.78% and a mean macro-average ROC of 98.25%.
Conclusion:
Our proposed approach was observed to outperform the existing techniques by 16.88% on an average in terms of F1-Score for the considered dataset.",Information and Software Technology,18 Mar 2025,10,"The abstract presents a novel deep learning-based bug classification approach with impressive results, which can greatly benefit startups and early-stage ventures involved in software development and bug fixing."
https://www.sciencedirect.com/science/article/pii/S0950584923002057,Deep learning-based software bug classification,February 2024,Not Found,Jyoti Prakash=Meher: jpmeher.iitkgp@gmail.com; Sourav=Biswas: bsws.sourav@gmail.com; Rajib=Mall: rajib@cse.iitkgp.ac.in,"Abstract
Context:
Accurate classification of bugs can help accelerate the bug triage process, code inspection, and repair activities. In this context, many 
machine learning techniques
 have been proposed to classify bugs. The 
expressive power
 of 
deep learning
 could be used to further improve classification.
Objective:
We propose a novel deep learning-based bug 
classification approach
.
Methods:
We first build a bug taxonomy with eight bug classes, each characterized by a set of keywords. Subsequently, we heuristically annotate a moderately large set (
∼
1.36M) of software bug resolution reports using an earth-mover distance technique based on the keywords. Finally, we use four attention-based 
classification techniques
 to classify these curated bugs.
Results:
Our experiments on a carefully collected dataset indicate that our proposed technique achieved a mean F1-Score of 84.78% and a mean macro-average ROC of 98.25%.
Conclusion:
Our proposed approach was observed to outperform the existing techniques by 16.88% on an average in terms of F1-Score for the considered dataset.",Information and Software Technology,18 Mar 2025,10,"Similar to abstract 178, this abstract proposes a deep learning-based bug classification approach with significant performance improvements, offering practical value to startups and tech ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923002094,Why and how bug blocking relations are breakable: An empirical study on breakable blocking bugs,February 2024,Not Found,Hao=Ren: Not Found; Yanhui=Li: yanhuili@nju.edu.cn; Lin=Chen: Not Found; Yuming=Zhou: Not Found; Changhai=Nie: Not Found,"Abstract
Context:
Blocking bugs prevents other bugs from being fixed, which is difficult to repair and negatively impacts software quality. During software maintenance, developers usually try to break the blocking relationship between blocking and blocked bugs, e.g., propose a temporary fix.
Object:
However, to our knowledge, no studies have investigated why and how blocking relations between bugs are breakable. In this study, we aim to construct an empirical analysis to explore breakable blocking bugs (BBBs).
Method:
Specifically, we employ quantitative and qualitative analysis to study these BBBs from two aspects. One is to investigate the characteristics of these bugs, and the other is to explore why and how developers break the blocking relationship between bugs during software maintenance. We build a dataset on five large-scale open-source projects and classify bugs into three types (BBBs, normal blocking bugs, and other bugs) to compare the differences between BBBs and other types of bugs.
Results:
We observe that BBBs have higher levels of involvement, take longer to fix, and involve more complex 
source code
 than other bugs. Moreover, we summarize four reasons blocking relationships between bugs are broken, i.e., partial association (41.87%), serious influence (26.40%), time pressure (19.73%), and flawed blocking (12.21%), and three measures developers adopt to break these blocking relationships, i.e., quick patch for blocking bugs (41.33%), quick patch for blocked bugs (38.67%), and ignore the blocking relation and fix blocked bugs directly (20.00%).
Conclusion:
Through these analyses, it is meaningful for software maintainers to have a 
deeper understanding
 of the characteristics and repair practices of BBBs, which will help solve these BBBs effectively in the future.",Information and Software Technology,18 Mar 2025,7,"The abstract offers valuable insights into breakable blocking bugs during software maintenance, providing useful information for startups and early-stage ventures involved in software development and bug resolution."
https://www.sciencedirect.com/science/article/pii/S0950584923002203,Improving domain-specific neural code generation with few-shot meta-learning,February 2024,Not Found,Zhen=Yang: zhenyang@sdu.edu.cn; Jacky Wai=Keung: Jacky.Keung@cityu.edu.hk; Zeyu=Sun: szy_@pku.edu.cn; Yunfei=Zhao: zhaoyunfei@pku.edu.cn; Ge=Li: lige@pku.edu.cn; Zhi=Jin: zhijin@pku.edu.cn; Shuo=Liu: sliu273-c@my.cityu.edu.hk; Yishu=Li: yishuli5-c@my.cityu.edu.hk,"Abstract
Context:
Neural code generation aims to automatically generate code snippets guided by Natural Language Descriptions (NLDs). In recent years, various neural code generation models for mainstream Programming Languages (PLs), such as Java and Python, have been proposed and demonostrated significant success in prior studies. Nonetheless, due to the scarcity of available training examples for some domain-specific PLs, such as Solidity, Bash, and Clojure, simply adopting previous 
neural models
 may lead to overfitting and inadequate learning.
Objective:
To overcome this challenge, we propose MetaCoder, a novel meta-learning code generation approach that efficiently extracts general-purpose knowledge from a large-scale source language and rapidly adapts to domain-specific scenarios, even with relatively few samples.
Method:
MetaCoder employs MAML, a powerful few-shot meta-learning method, to construct a 
transfer learning
 framework. This framework learns general-purpose knowledge from large-scale source languages and applies it in domain-specific target languages. To acquire more general-purpose knowledge, heterogeneous sub-tasks are constructed from the source language during the pre-training phase of MAML. As such, combining with CodeBERT and K-means, we design an unsupervised category assignment method for code generation samples, thereby exploiting the 
n
-way 
k
-shot rule to construct the heterogeneous sub-tasks. Consequently, MetaCoder can be applied to the code generation field.
Results:
We evaluate MetaCoder with both tree-based (e.g., TreeGen) and sequence-based (e.g., CodeGPT) backbones on two domain-specific PLs, including Solidity and Bash. Extensive experiments demonstrate the superior performance of our approach compared to baselines and verified its capability of code generation visually in practice.
Conclusion:
MetaCoder effectively extracts general-purpose knowledge from large-scale source languages, thereby enhancing model performance. Therefore, we highly recommend MetaCoder as a code generation approach for domain-specific PLs.",Information and Software Technology,18 Mar 2025,9,"MetaCoder introduces a novel approach to code generation that addresses a specific challenge in domain-specific programming languages, showing superior performance in extensive experiments."
https://www.sciencedirect.com/science/article/pii/S0950584923002227,Understanding the implementation issues when using deep learning frameworks,February 2024,Not Found,Chao=Liu: liu.chao@cqu.edu.cn; Runfeng=Cai: crf@cqu.edu.cn; Yiqun=Zhou: zhouyiqun@cqu.edu.cn; Xin=Chen: xinchen@cqu.edu.cn; Haibo=Hu: haibo.hu@cqu.edu.cn; Meng=Yan: mengy@cqu.edu.cn,"Abstract
Context:
Deep Learning
 (DL) frameworks like TensorFlow can help developers implement DL applications (e.g., computer vision) faster and easier. When using DL frameworks, developers encountered a large number of questions and posted them on Stack Overflow (SO).
Objective:
The goal of this paper is to conduct a comprehensive empirical study on the SO questions, summarize the implementation issues, and suggest future opportunities.
Methods:
This paper focuses on three DL frameworks (i.e., TensorFlow, PyTorch, and Theano), groups 2,401 relevant SO questions into various implementation issues, and constructs a taxonomy. We also analyze the popularity and difficulty of these issues under the taxonomy.
Results:
For the identified various implementation issues, we constructed a taxonomy consisting of seven major categories with 63 subcategories. Our analysis reveals that 91.7% of questions are related to the implementation categories of 
data processing
, model setting, model training, and model prediction. Developers frequently address the remaining three categories (i.e., Model evaluation, 
runtime environment
, and visualization), where 
runtime environment
 is the most difficult category. Based on empirical findings, we provide some suggestions for future research.
Conclusion:
In this paper, we summarized the issues of DL implementation and proposed corresponding opportunities for future study. We expect this paper to help developers and researchers understand these issues and design better tools to improve the productivity of DL implementation.",Information and Software Technology,18 Mar 2025,7,"The study on Deep Learning frameworks implementation issues provides valuable insights for developers and researchers, suggesting future research directions."
https://www.sciencedirect.com/science/article/pii/S0950584923002240,Collaborative software design and modeling in virtual reality,February 2024,"Virtual reality, Collaboration, Immersion, Software development, Software modeling",Martin=Stancek: Not Found; Ivan=Polasek: ivan.polasek@fmph.uniba.sk; Tibor=Zalabai: Not Found; Juraj=Vincur: Not Found; Rodi=Jolak: Not Found; Michel=Chaudron: Not Found,"Abstract
Context:
Software engineering
 is becoming more and more distributed. Developers and other stakeholders are often located in different locations, departments, and countries and operating within different time zones. Most online 
software design
 and modeling tools are not adequate for distributed collaboration since they do not support awareness and lack features for effective communication.
Objective:
The aim of our research is to support distributed software design activities in Virtual Reality (VR).
Method:
Using design science research methodology, we design and evaluate a tool for 
collaborative design
 in VR. We evaluate the collaboration efficiency and recall of design information when using the VR software design environment compared to a non-VR software design environment. Moreover, we collect the perceptions and preferences of users to explore the opportunities and challenges that were incurred by using the VR software design environment.
Results:
We find that there is no significant difference in the efficiency and recall of design information when using the VR compared to the non-VR environment. Furthermore, we find that developers are more satisfied with collaboration in VR.
Conclusion:
The results of our research and similar studies show that working in VR is not yet faster or more efficient than working on standard desktops. It is very important to improve the interface in VR (gestures with haptics, keyboard and voice input), as confirmed by the difference in results between the first and second evaluation.",Information and Software Technology,18 Mar 2025,6,"The research on distributed software design in Virtual Reality highlights user perceptions and preferences, but does not show significant efficiency improvements compared to non-VR environments."
https://www.sciencedirect.com/science/article/pii/S0950584923002276,Automated code-based test case reuse for software product line testing,February 2024,Not Found,Pilsu=Jung: Not Found; Seonah=Lee: saleese@gnu.ac.kr; Uicheon=Lee: Not Found,"Abstract
Context
A software product line (SPL) grows in size as a new product is developed. A new product in an SPL should be tested extensively for quality assurance. For the efficient testing, previous studies suggested reusing the existing test cases of a 
product family
. However, either their methods were not efficient because interventions from human experts, specifications, architecture and/or traceabilities for test cases were required.
Objective
To address these limitations, we propose an Automated Code-based Test case reuse for SPLs (ActSPL). ActSPL automatically identifies reusable test cases for new products of a 
product family
 using 
source code
 and test cases.
Method
ActSPL automatically constructs a hash-based 
traceability links
 between test cases and 
source code
 of a product family. Using the 
traceability links
, ActSPL selects reusable test cases for a given new product from existing test cases of the product family.
Results
We evaluated ActSPL in terms of the effectiveness and cost reduction of reusing test cases with five open-source SPLs. The evaluation results showed that ActSPL, on average, achieved 100 % precision and 62 % recall. In addition, ActSPL, on average, saved 47.5 % of time required for testing a new product from scratch.
Conclusion
Our study shows the feasibility of ActSPL reusing SPL test cases based on source code and test cases. Our results can be a basis for successive studies for automated code-based SPL testing.",Information and Software Technology,18 Mar 2025,8,"ActSPL proposes an automated test case reuse method for software product lines, demonstrating effectiveness and significant time savings in testing new products."
https://www.sciencedirect.com/science/article/pii/S0950584923002252,An empirical study on the performance and energy costs of ads and analytics in mobile web apps,February 2024,"Mobile web, Empirical study, Controlled experiment, Energy efficiency, Performance",Christos=Petalotis: c.petalotis@student.vu.nl; Luka=Krumpak: l.krumpak@student.vu.nl; Maximilian Stefan=Floroiu: m.floroiu@student.vu.nl; Laréb Fatima=Ahmad: l.f.ahmad@student.vu.nl; Shashank=Athreya: s.m.athreya@student.vu.nl; Ivano=Malavolta: i.malavolta@vu.nl,"Abstract
Context:
As the use of mobile devices has increased immensely through the years, the presence of analytics and advertisements on web and native applications has become prevalent. However, serving ads and analytics comes with costs, as they are associated with additional code and network requests to execute properly. Subsequently, more computing resources are used, having an impact on the energy consumption and the performance of web applications. Previous work has focused only on native Android applications, has used different metrics for performance, or has focused on other aspects of web applications.
Goal:
This paper aims to investigate the costs of including advertisements and analytics in web applications. This is done in terms of energy consumption and performance. For energy, the consumption is measured in Joules. For performance, the following metrics are used: 
first contentful paint
 and 
full page load time
. The results of this study could influence the decisions of web developers and web browser vendors related to ads and analytics usage, while providing the foundation for further research on this topic.
Method:
To collect reliable and population-representative results, the research focused on 9 popular web applications included in the Tranco list. Energy consumption and performance metrics were gathered for 3 versions of each web application — original version with ads and analytics, without ads, and without analytics. A cross-over paired comparison design is conducted. Multiple executions of each run were performed in random order to ascertain rigorous measures. The experiment is carried out on an Android tablet using two browsers, Google Chrome and Opera.
Results:
Ads significantly impact the energy consumption of mobile web apps for both browsers, with a large effect size; analytics have a significant impact on the energy consumption of Chrome (with a medium effect size), but not on Opera. In terms of performance, both ads and analytics do not significantly impact the first contentful paint metric on both browsers; differently, both ads and analytics significantly impact the full page load time of the mobile web apps on both browsers, but with a small effect size.
Conclusions:
This study provides evidence that both ads and analytics can have a significant impact on the energy consumption and performance of mobile web apps loaded either on Opera or Chrome. Depending on the requirements of the mobile web app, it is advisable to limit both ads and analytics in a mobile web app in order to reduce its energy consumption and improve its full page load time. Special attention should be paid to the presence of ads since they resulted to be the most impactful in terms of energy consumption.",Information and Software Technology,18 Mar 2025,7,The investigation on the impact of ads and analytics on energy consumption and performance of mobile web apps provides practical insights for web developers and browser vendors.
https://www.sciencedirect.com/science/article/pii/S0950584923002215,Local polynomial software reliability models and their application,February 2024,Not Found,Tadashi=Dohi: dohi@hiroshima-u.ac.jp; Siqiao=Li: rel-siqiao@hiroshima-u.ac.jp; Okamura=Hiroyuki: okamu@hiroshima-u.ac.jp,"Abstract
In this paper, we propose local polynomial 
software reliability
 models (SRMs), which can be categorized into a semi-parametric modeling framework. Our models belong to the common non-homogeneous 
Poisson process
 (NHPP)-based SRMs, but possess a flexible structure to approximate an arbitrary mean value function by controlling the polynomial degree. More specifically, we develop two types of local polynomial NHPP-based SRMs; finite-failure (type-I) and infinite-failure (type-II) SRMs, which are substantial extensions of the existing NHPP-based SRMs in the similar categories. We also develop two 
maximum likelihood estimation
 algorithms in both estimation and prediction phases, where the former is used for the testing period experienced in the past, and the latter for the prediction in the future. In numerical experiments with actual 8 software fault count time-interval data sets, we compare our local polynomial NHPP-based SRMs with the well-known existing parametric NHPP-based SRMs in terms of goodness-of-fit and predictive performances. Finally, it can be concluded that our local polynomial NHPP-based SRMs with lower polynomial degrees could outperform the existing NHPP-based SRMs in several cases and should be listed as candidates for the representative NHPP-based SRMs in 
software reliability
 analysis.",Information and Software Technology,18 Mar 2025,5,"The abstract discusses advanced software reliability models, but the practical value for early-stage European ventures or startups is limited."
https://www.sciencedirect.com/science/article/pii/S0950584923002215,Local polynomial software reliability models and their application,February 2024,Not Found,Tadashi=Dohi: dohi@hiroshima-u.ac.jp; Siqiao=Li: rel-siqiao@hiroshima-u.ac.jp; Okamura=Hiroyuki: okamu@hiroshima-u.ac.jp,"Abstract
In this paper, we propose local polynomial 
software reliability
 models (SRMs), which can be categorized into a semi-parametric modeling framework. Our models belong to the common non-homogeneous 
Poisson process
 (NHPP)-based SRMs, but possess a flexible structure to approximate an arbitrary mean value function by controlling the polynomial degree. More specifically, we develop two types of local polynomial NHPP-based SRMs; finite-failure (type-I) and infinite-failure (type-II) SRMs, which are substantial extensions of the existing NHPP-based SRMs in the similar categories. We also develop two 
maximum likelihood estimation
 algorithms in both estimation and prediction phases, where the former is used for the testing period experienced in the past, and the latter for the prediction in the future. In numerical experiments with actual 8 software fault count time-interval data sets, we compare our local polynomial NHPP-based SRMs with the well-known existing parametric NHPP-based SRMs in terms of goodness-of-fit and predictive performances. Finally, it can be concluded that our local polynomial NHPP-based SRMs with lower polynomial degrees could outperform the existing NHPP-based SRMs in several cases and should be listed as candidates for the representative NHPP-based SRMs in 
software reliability
 analysis.",Information and Software Technology,18 Mar 2025,6,"The abstract presents local polynomial software reliability models, which could have some potential impact on software development in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001854,The consolidation of game software engineering: A systematic literature review of software engineering for industry-scale computer games,January 2024,Not Found,Jorge=Chueca: jchueca@usj.es; Javier=Verón: jveron@usj.es; Jaime=Font: jfont@usj.es; Francisca=Pérez: mfperez@usj.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Game 
Software Engineering
 (GSE) is a branch of 
Software Engineering
 (SE) that focuses on the development of 
video game
 applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE.
Objective:
This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field.
Method:
A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE.
Results:
The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work.
Conclusion:
The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge.",Information and Software Technology,18 Mar 2025,8,"The abstract analyzes the state of game software engineering, highlighting gaps and future research directions. This information could be valuable for European early-stage ventures in the gaming industry."
https://www.sciencedirect.com/science/article/pii/S0950584923001854,The consolidation of game software engineering: A systematic literature review of software engineering for industry-scale computer games,January 2024,Not Found,Jorge=Chueca: jchueca@usj.es; Javier=Verón: jveron@usj.es; Jaime=Font: jfont@usj.es; Francisca=Pérez: mfperez@usj.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Game 
Software Engineering
 (GSE) is a branch of 
Software Engineering
 (SE) that focuses on the development of 
video game
 applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE.
Objective:
This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field.
Method:
A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE.
Results:
The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work.
Conclusion:
The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge.",Information and Software Technology,18 Mar 2025,9,"Similar to abstract 188, this abstract evaluates game software engineering, providing insights and future research directions. This information is highly relevant and valuable for European startups in the gaming sector."
https://www.sciencedirect.com/science/article/pii/S0950584923002033,A tertiary study on links between source code metrics and external quality attributes,January 2024,"Product quality, Quality models, Code quality, Evidence, Tertiary study, Tertiary review",Umar=Iftikhar: umar.iftikhar@bth.se; Nauman Bin=Ali: nauman.ali@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Muhammad=Usman: muhammad.usman@bth.se,"Abstract
Context:
Several secondary studies have investigated the relationship between internal quality attributes, source 
code metrics
 and external quality attributes. Sometimes they have contradictory results.
Objective:
We synthesize evidence of the link between internal quality attributes, source code metrics and external quality attributes along with the efficacy of the prediction models used.
Method:
We conducted a tertiary review to identify, evaluate and synthesize secondary studies. We used several characteristics of secondary studies as indicators for the strength of evidence and considered them when synthesizing the results.
Results:
From 711 secondary studies, we identified 15 secondary studies that have investigated the link between source code and external quality. Our results show : (1) primarily, the focus has been on object-oriented systems, (2) 
maintainability
 and reliability are most often linked to internal quality attributes and source code metrics, with only one secondary study reporting evidence for security, (3) only a small set of complexity, coupling, and size-related source code metrics report a consistent positive link with maintainability and reliability, and (4) group method of data handling (GMDH) based prediction models have performed better than other prediction models for maintainability prediction.
Conclusions:
Based on our results, lines of code, coupling, complexity and the cohesion metrics from Chidamber & Kemerer (CK) metrics are good indicators of maintainability with consistent evidence from high and moderate-quality secondary studies. Similarly, four CK metrics related to coupling, complexity and cohesion are good indicators of reliability, while inheritance and certain cohesion metrics show no consistent evidence of links to maintainability and reliability. Further empirical studies are needed to explore the link between internal quality attributes, source code metrics and other external quality attributes, including functionality, portability, and usability. The results will help researchers and practitioners understand the body of knowledge on the subject and identify future research directions.",Information and Software Technology,18 Mar 2025,7,"The abstract synthesizes evidence on the link between internal quality attributes, source code metrics, and external quality attributes. While insightful, the direct impact on early-stage European ventures may be more indirect."
https://www.sciencedirect.com/science/article/pii/S0950584923001866,Stratified random sampling for neural network test input selection,January 2024,Not Found,Zhuo=Wu: wuzhuo@tju.edu.cn; Zan=Wang: wangzan@tju.edu.cn; Junjie=Chen: junjiechen@tju.edu.cn; Hanmo=You: youhanmo@tju.edu.cn; Ming=Yan: yanming@tju.edu.cn; Lanjun=Wang: wang.lanjun@outlook.com,"Abstract
Context:
Testing techniques to ensure the quality of deep 
neural networks
 (DNNs) are essential and crucial. However, the testing process can be inefficient due to a large number of test cases and the manual effort of labeling them. Recent work tackles the above challenge by selecting a small but representative subset of the tests. Such an approach allows us to quickly estimate the accuracy of a DNN with reduced effort, because only a small set of tests are to be manually labeled. However, existing approaches cannot guarantee unbiased results or provide an accurate estimation.
Objectives:
In this work, we leverage a statistical perspective on providing an unbiased estimation of the model accuracy with the smallest estimation variance, named 
Stratified random
 Sampling with Optimum Allocation (SSOA).
Methods:
Our approach first divides the unlabeled test set into strata based on predictive confidences. Then, we design two stratum accuracy variance estimation methods to allocate the given budget assigned to each stratum based on the optimum allocation strategy. Finally, we conduct multiple experiments to evaluate the effectiveness and stability of SSOA by comparing it with 
baseline methods
.
Results:
The results show that SSOA significantly outperforms all compared approaches with average improvements over 26.14% in terms of 
Mean Squared Errors
 (MSE) of estimated accuracy. In addition, the MSE shows a steady downward trend as the budget grows.
Conclusion:
SSOA can assist testers in estimating the accuracy of DNNs, lowering labeling costs, and enhancing the efficiency of DNN testing.",Information and Software Technology,18 Mar 2025,8,"The proposed approach SSOA significantly improves the accuracy estimation for DNNs with reduced effort, which can be valuable for early-stage ventures dealing with deep learning models."
https://www.sciencedirect.com/science/article/pii/S0950584923001866,Stratified random sampling for neural network test input selection,January 2024,Not Found,Zhuo=Wu: wuzhuo@tju.edu.cn; Zan=Wang: wangzan@tju.edu.cn; Junjie=Chen: junjiechen@tju.edu.cn; Hanmo=You: youhanmo@tju.edu.cn; Ming=Yan: yanming@tju.edu.cn; Lanjun=Wang: wang.lanjun@outlook.com,"Abstract
Context:
Testing techniques to ensure the quality of deep 
neural networks
 (DNNs) are essential and crucial. However, the testing process can be inefficient due to a large number of test cases and the manual effort of labeling them. Recent work tackles the above challenge by selecting a small but representative subset of the tests. Such an approach allows us to quickly estimate the accuracy of a DNN with reduced effort, because only a small set of tests are to be manually labeled. However, existing approaches cannot guarantee unbiased results or provide an accurate estimation.
Objectives:
In this work, we leverage a statistical perspective on providing an unbiased estimation of the model accuracy with the smallest estimation variance, named 
Stratified random
 Sampling with Optimum Allocation (SSOA).
Methods:
Our approach first divides the unlabeled test set into strata based on predictive confidences. Then, we design two stratum accuracy variance estimation methods to allocate the given budget assigned to each stratum based on the optimum allocation strategy. Finally, we conduct multiple experiments to evaluate the effectiveness and stability of SSOA by comparing it with 
baseline methods
.
Results:
The results show that SSOA significantly outperforms all compared approaches with average improvements over 26.14% in terms of 
Mean Squared Errors
 (MSE) of estimated accuracy. In addition, the MSE shows a steady downward trend as the budget grows.
Conclusion:
SSOA can assist testers in estimating the accuracy of DNNs, lowering labeling costs, and enhancing the efficiency of DNN testing.",Information and Software Technology,18 Mar 2025,8,"The proposed approach SSOA significantly improves the accuracy estimation for DNNs with reduced effort, which can be valuable for early-stage ventures dealing with deep learning models."
https://www.sciencedirect.com/science/article/pii/S0950584923001921,The impact of knowledge inertia on process tailoring in the dynamic development of software projects in Chinese industries,January 2024,Not Found,Jung-Chieh=Lee: Not Found; Chung-Yang=Chen: cychen@mgt.ncu.edu.tw,"Abstract
Context
This paper stresses the importance of continual planning in the dynamic development of software projects and highlights a decisional situation: should teams follow process standards and 
past experiences
 for safety, or should they take different learning paths and innovate the development despite the risk of experimenting with new process solutions.
Objective
To address this situation, we emphasize software process tailoring (SPT), a team-based planning practice in software projects, and utilize knowledge inertia theory to investigate how experience and learning inertia influence the efficiency and effectiveness of conducting SPT given autonomous and knowledge-diverse team environments.
Method
This study employed a split questionnaire design to collect samples. A total of 88 Chinese software teams from 45 firms with software development functions in various 
industries
 participated in the research. In particular, a software team delegated at least two team members who have SPT experience to participate in the survey. The partial least squares (PLS) approach was adopted to analyze the data.
Results
Software teams are found to conveniently adopt and accommodate previous similar process solutions when familiar learning routes are formed (learning inertia), leading teams to make efficient but ineffective tailoring decisions. Experience inertia is found to facilitate effective 
process redesign
. Surprisingly, autonomous software teams tend to follow familiar ways when tailoring development. Knowledge diversity diminishes the effect of learning inertia on SPT effectiveness, reflecting that knowledge-diverse teams rely less on past learning routes and can develop more creative tailoring solutions.
Conclusion
Experience and learning inertia have distinct effects on SPT performance. Team 
autonomy
 and knowledge diversity contextually affect teams’ tailoring experience and learning routines, which subsequently determine SPT outcomes.",Information and Software Technology,18 Mar 2025,6,"The study on software process tailoring provides insights into how experience and learning inertia affect team decisions, which can be beneficial for startups in improving development processes."
https://www.sciencedirect.com/science/article/pii/S0950584923001945,Cluster-based adaptive test case prioritization,January 2024,Not Found,Xiaolin=Wang: wangxiaolin@zjxu.edu.cn; Sulan=Zhang: Not Found,"Abstract
In order to enhance the efficiency of regression testing, test case prioritization (TCP) has been widely implemented, wherein a higher priority test case is executed earlier. Traditional TCP methods focus on improving the prioritization algorithm's efficacy. However, the majority of TCP approaches are characterized by a predetermined sequence of test cases prior to execution. Once established, this sequence remains consistent throughout the entire test execution process. As a result, any execution information generated during current test execution (such as fault-detected information) is unavailable for use in current round of test case prioritization and can only be utilized in subsequent regression testing. To address the issue of lagging utilization of fault-detected information, a cluster-based adaptive test case prioritization approach is proposed, which adds the new adaptive adjustment content in pre-prioritization. First, a new 
clustering criterion
 is defined and designed, by which produces test-case clusters in advance. Second, an adaptive TCP algorithm is proposed, which utilizes fault-detected information to adaptively adjust the order of test cases during the execution process based on the test-case clusters. Finally, one open-source Java program and three industrial-grade Java programs were selected for empirical evaluation. The experimental results demonstrate that the proposed technique not only serves as an enhanced version of pre-prioritization to improve the performance of the corresponding pre-prioritization technique, but also functions as an independent approach that outperforms other TCP techniques, including cluster-based TCPs, and another adaptive TCP. Specifically, when 
step=2
 is applied using our cluster-based adaptive TCP approach, the results are significantly better than those obtained with 
step=1.
 For instance, in 
CT-14
, the median APFD improvement rate for 
step=2
 reaches 17.08 %, which is substantially higher than that achieved with 
step=1
 (5.48 %).",Information and Software Technology,18 Mar 2025,7,"The proposed adaptive test case prioritization approach improves the efficiency of regression testing, which can be useful for early-stage ventures seeking to enhance their testing processes."
https://www.sciencedirect.com/science/article/pii/S095058492300191X,Syntax-aware on-the-fly code completion,January 2024,"Code completion, Multi-task learning",Wannita=Takerngsaksiri: Not Found; Chakkrit=Tantithamthavorn: chakkrit@monash.edu; Yuan-Fang=Li: Not Found,"Abstract
Context:
Code completion aims to help improve developers’ productivity by suggesting the next code tokens from a given context. Various approaches have been proposed to incorporate 
abstract syntax tree
 (AST) information for model training, ensuring that code completion is aware of the syntax of the programming languages. However, existing syntax-aware code completion approaches are not on-the-fly, as we found that for every two-thirds of characters that developers type, AST fails to be extracted because it requires the syntactically correct source code, limiting its practicality in real-world scenarios. On the other hand, existing on-the-fly code completion does not consider 
syntactic information
 yet.
Objective:
In this paper, we propose PyCoder to leverage token types, a kind of lightweight syntactic information, which is readily available and aligns with the natural order of source code.
Method:
Our PyCoder is trained in a multi-task training manner so that by learning the supporting task of predicting token types during the training phase, the models achieve better performance on predicting tokens and lines of code without the need for token types in the inference phase.
Results:
Comprehensive experiments show that PyCoder achieves the first rank on the CodeXGLUE leaderboard with an accuracy of 77.12% for the token-level predictions, which is 0.43%–24.25% more accurate than baselines. In addition, PyCoder achieves an exact match of 43.37% for the line-level predictions, which is 3.63%–84.73% more accurate than baselines.
Conclusions:
These results lead us to conclude that token type information (an alternative to syntactic information) that is rarely used in the past can greatly improve the performance of code completion approaches, without requiring the syntactically correct source code like AST-based approaches do. Our PyCoder is publicly available on HuggingFace and GitHub.",Information and Software Technology,18 Mar 2025,7,"PyCoder's approach to code completion using token types shows significant performance improvements, which can be valuable for startups looking to enhance developer productivity in coding tasks."
https://www.sciencedirect.com/science/article/pii/S0950584923001957,Understanding how early-stage researchers leverage socio-technical affordances for distributed research support,January 2024,"Crowdsourcing, Empirical study, Research skills development, Socio-technical systems",Yuchao=Jiang: yuchao.jiang@unsw.edu.au; Boualem=Benatallah: boualem.benatallah@dcu.ie; Marcos=Báez: marcos.baez@hsbi.de,"Abstract
Early-stage researchers (ESRs) are often challenged to learn research skills with sufficient support from a small circle of advisors and colleagues. Meanwhile, emerging socio-technical systems (STSs) are now available for social interactions among the general public and people in particular interest topics, such as research. However, how STSs can effectively support ESRs in developing research skills is not yet well understood. In this paper, we report on a series of interviews and surveys with ESRs. We found that online research communities held the potentials for ESRs to learn from diverse perspectives and experience. But the adoption of research communities for learning was still limited. We identified unmet needs in the design of these systems limiting the adoption. We then provide design implications for future STSs to support learning research skills with socio-technical affordances.",Information and Software Technology,18 Mar 2025,7,"The study addresses the issue of supporting early-stage researchers in developing research skills using online research communities, providing design implications for future systems. This could have a positive impact on European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001040,Improving effort-aware defect prediction by directly learning to rank software modules,January 2024,Not Found,Xiao=Yu: xiaoyu@whut.edu.cn; Jiqing=Rao: jqrao@whut.edu.cn; Lei=Liu: leiliu_cs@hotmail.com; Guancheng=Lin: gclin@whut.edu.cn; Wenhua=Hu: whu10@whut.edu.cn; Jacky Wai=Keung: jacky.keung@cityu.edu.hk; Junwei=Zhou: junweizhou@whut.edu.cn; Jianwen=Xiang: jwxiang@whut.edu.cn,"Abstract
Context:
E
ffort-
A
ware 
D
efect 
P
rediction (EADP) ranks software modules according to the 
defect density
 of software modules, which allows testers to find more bugs while reviewing a certain amount of 
L
ines 
O
f 
C
ode (LOC). Most existing methods regard the EADP task as a regression or classification problem. Optimizing the regression loss or 
classification accuracy
 might result in poor effort-aware performance.
Objective:
Therefore, we propose a method called EALTR to improve the EADP performance by directly maximizing the 
P
roportion 
of
 the found 
B
ugs (PofB@20%) value when inspecting the top 20% LOC.
Method:
EALTR uses the linear regression model to build the EADP model, and then employs the composite 
differential evolution algorithm
 to generate a set of coefficient vectors for the linear regression model. Finally, EALTR selects the coefficient vector that achieves the highest PofB@20% value on the training dataset to construct the EADP model. To further reduce the 
I
nitial 
F
alse 
A
larms (IFA) value of EALTR, we propose a re-ranking strategy in the 
prediction phase
.
Results:
Our experimental results on eleven project datasets with 41 releases show that EALTR can find 5.83%–54.47% more bugs than the 
baseline methods
 whose IFA values are less than 10 and the re-ranking strategy significantly reduces the IFA value by 16.95%.
Conclusion:
Our study verifies the effectiveness of directly optimizing the effort-aware metric (i.e., PofB@20%) to build the EADP model. EALTR is recommended as an effective EADP method, since it can help software testers find more bugs.",Information and Software Technology,18 Mar 2025,8,"The proposed method EALTR improves effort-aware defect prediction by maximizing the proportion of found bugs, which can have practical value for software testers in early-stage ventures. The results show significant improvements over baseline methods."
https://www.sciencedirect.com/science/article/pii/S0950584923002045,QLSN: Quantum key distribution for large scale networks,January 2024,Not Found,Cherry=Mangla: Not Found; Shalli=Rani: shallir79@gmail.com; Ahmed=Abdelsalam: ahmed.abdelsalam@lut.fi,"Abstract
Context:
Key management among large-scale networks is still a challenging issue considering the limited information about adversaries (whether quantum or classical). One solution for such an issue is to use quantum-inspired to which refers to a technology, an algorithm, or a strategy that uses conventional computers to execute 
quantum physics
 concepts. These techniques make an effort to imitate particular 
quantum computing
 properties, such as superposition, 
entanglement
, or 
quantum parallelism
, in order to more effectively or creatively handle particular issues. It can provide security against all types of intruders.
Objectives:
In this article, inspired by 
Quantum Key Distribution
, we proposed a key-management protocol for enhancing the security and reliability of the network. Two major objectives have been taken into consideration during the proposal of QLSN (proposed protocol), (i) reduced risk if the network needs to transfer the long keys and (ii) a reliable network to transmit the information safely.
Methods:
(i) QLSN is proposed in the place of the classical Diffie–Hellman key exchange algorithm, which makes it secure against attacks during the transmission of data through the IPSec tunnel. (ii) 
Risk Analysis
 is performed on 1-bit, 2 bits, 15 bits, and 50 bits sizes of data. 
Data transmission
 is tested for risk analysis and the probability of attacks is checked under different scenarios.
Results:
The simulation results illustrated the better performance of the proposed protocol in different scenarios with large-scale networks compared to the existing Diffie–Hellman key exchange used in the IPSec protocol proposed by CISCO. Using 
quantum algorithms
 and processing advantages, quantum adversaries may be able to take advantage of weaknesses in conventional security systems.
Conclusion:
An essential step in determining and validating the viability of proposed protocol inside a 
quantum computing
 framework is the integration of a QLSN protocol into the IBM Qiskit simulator. By addressing pressing concerns, this validation approach clarifies the performance and security consequences of large-scale networks. Looking ahead, the fusion of 
artificial intelligence
 and 
quantum computing
 promises fresh methods for key management that will improve 
network security
. In addition, Turing machines–classical computing tools that bridge the gap between the classical and 
quantum computing
 paradigms–will continue to play crucial roles in the complex world of large-scale networks.",Information and Software Technology,18 Mar 2025,6,"The proposed Quantum Key Management protocol aims to enhance network security and reliability. While the technology is promising, its impact on European early-stage ventures may not be immediate but could be beneficial in the long run."
https://www.sciencedirect.com/science/article/pii/S0950584923002070,Genetic model-based success probability prediction of quantum software development projects,January 2024,"Quantum computing (QC), Quantum software development (QSD), Variables, Prediction model, Genetic algorithm",Muhammad Azeem=Akbar: azeem.akbar@lut.fi; Arif Ali=Khan: Not Found; Mohammad=Shameem: Not Found; Mohammad=Nadeem: Not Found,"Abstract
Context
Quantum computing
 (QC) holds the potential to revolutionize computing by solving complex problems exponentially faster than classical computers, transforming fields such as cryptography, optimization, and scientific simulations. To unlock the 
potential benefits
 of QC, quantum software development (QSD) enables harnessing its power, further driving innovation across diverse domains. To ensure successful QSD projects, it is crucial to concentrate on key variables.
Objective
This study aims to identify key variables in QSD and develop a model for predicting the success probability of QSD projects.
Methodology
We identified key QSD variables from existing literature to achieve these objectives and collected expert insights using a survey instrument. We then analyzed these variables using an optimization model, i.e., 
Genetic Algorithm
 (GA), with two different prediction methods the Naïve 
Bayes Classifier
 (NBC) and 
Logistic Regression
 (LR).
Results
The results of success probability prediction models indicate that as the QSD process matures, project success probability significantly increases, and costs are notably reduced. Furthermore, the best fitness rankings for each QSD project variable determined using NBC and LR indicated a strong 
positive correlation
 (rs=0.945). The 
t
-test results (
t
 = 0.851, 
p
 = 0.402>0.05) show no significant differences between the rankings calculated by the two methods (NBC and LR).
Conclusion
The results reveal that the developed success probability prediction model, based on 14 identified QSD project variables, highlights the areas where practitioners need to focus more in order to facilitate the cost-effective and 
successful implementation
 of QSD projects.",Information and Software Technology,18 Mar 2025,9,The study aims to predict the success probability of Quantum Software Development projects by identifying key variables. This can be highly valuable for European early-stage ventures in the emerging field of quantum computing.
https://www.sciencedirect.com/science/article/pii/S0950584923001908,Software Engineering for Systems-of-Systems and Software Ecosystems,January 2024,Not Found,Rodrigo=Santos: Not Found; Eleni=Constantinou: Not Found; Pablo=Antonino: Not Found; Jan=Bosch: Not Found,"Abstract
Software Engineering has faced several challenges in the last decade, especially those related to aspects beyond the technical side. As such, technological, organizational and social aspects should be considered altogether in research and practice in the field so that complexity could be handled in order to provide solution to the existing problems from the software industry demands. In this context, systems-of-systems (SoS) and software ecosystems (SECO) emerged as topics that joined researchers and practitioners interested in understanding how to manage and engineer software-intensive systems within modern, complex, distributed, dynamic, and open environments. An SoS comprises independent constituent systems which work together to fulfill missions driven by architectural concerns. In turn, a SECO consists of a set of actors and artifacts, as well as their relationships, to produce value over a common technology platform driven by external contributions. Both classes of systems have a distributed nature, focus on optimizing the cost-benefit trade-off, and aim to reach global markets. In this special section, we introduce extended versions of two papers selected from the 10th IEEE/ACM International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems (SESoS 2022). These articles provide researchers and practitioners with advances on the development and evolution of complex software-intensive systems.",Information and Software Technology,18 Mar 2025,5,"The abstract introduces systems-of-systems and software ecosystems, which are important topics but may not directly impact European early-stage ventures or startups. The focus on complex systems engineering is valuable but more indirect in practical application."
https://www.sciencedirect.com/science/article/pii/S0950584923001908,Software Engineering for Systems-of-Systems and Software Ecosystems,January 2024,Not Found,Rodrigo=Santos: Not Found; Eleni=Constantinou: Not Found; Pablo=Antonino: Not Found; Jan=Bosch: Not Found,"Abstract
Software Engineering has faced several challenges in the last decade, especially those related to aspects beyond the technical side. As such, technological, organizational and social aspects should be considered altogether in research and practice in the field so that complexity could be handled in order to provide solution to the existing problems from the software industry demands. In this context, systems-of-systems (SoS) and software ecosystems (SECO) emerged as topics that joined researchers and practitioners interested in understanding how to manage and engineer software-intensive systems within modern, complex, distributed, dynamic, and open environments. An SoS comprises independent constituent systems which work together to fulfill missions driven by architectural concerns. In turn, a SECO consists of a set of actors and artifacts, as well as their relationships, to produce value over a common technology platform driven by external contributions. Both classes of systems have a distributed nature, focus on optimizing the cost-benefit trade-off, and aim to reach global markets. In this special section, we introduce extended versions of two papers selected from the 10th IEEE/ACM International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems (SESoS 2022). These articles provide researchers and practitioners with advances on the development and evolution of complex software-intensive systems.",Information and Software Technology,18 Mar 2025,8,The abstract addresses modern challenges in software engineering and provides valuable insights for researchers and practitioners working on complex software-intensive systems.
https://www.sciencedirect.com/science/article/pii/S0950584923001842,Threats to validity in software engineering research: A critical reflection,December 2023,Not Found,Roberto=Verdecchia: roberto.verdecchia@unifi.it; Emelie=Engström: emelie.engstrom@cs.lth.se; Patricia=Lago: p.lago@vu.nl; Per=Runeson: per.runeson@cs.lth.se; Qunying=Song: qunying.song@cs.lth.se,"Abstract
Context:
In the contemporary body of 
software engineering
 literature, some recurrent shortcomings characterize how threats to validity (TTV) are considered in studies.
Objective:
With this position paper, we aim to open a discourse on the current use of TTV sections. The goal of our position is to jointly reflect and systematically improve how we, as a research community, consider TTV in our studies.
Methods:
Based on our personal experience as researchers, authors, reviewers, and editors, we critically reflect on the treatment of TTV in current empirical 
software engineering
 literature.
Results:
We discuss the key shortcomings of TTV consideration, including the failure to acknowledge different types of validity categorizations and the tendency to treat threats just as an afterthought. For each identified problem, we propose a vision for an improved state, intending to catalyze thoughtful engagement and improvements the way our community addresses TTV.
Conclusion:
We posit there is an urgent need to reconsider how we approach, document, and evaluate TTV in software engineering research.",Information and Software Technology,18 Mar 2025,6,"The abstract highlights the importance of reconsidering TTV in software engineering research, but the practical implications for early-stage ventures may not be as direct."
https://www.sciencedirect.com/science/article/pii/S0950584923001842,Threats to validity in software engineering research: A critical reflection,December 2023,Not Found,Roberto=Verdecchia: roberto.verdecchia@unifi.it; Emelie=Engström: emelie.engstrom@cs.lth.se; Patricia=Lago: p.lago@vu.nl; Per=Runeson: per.runeson@cs.lth.se; Qunying=Song: qunying.song@cs.lth.se,"Abstract
Context:
In the contemporary body of 
software engineering
 literature, some recurrent shortcomings characterize how threats to validity (TTV) are considered in studies.
Objective:
With this position paper, we aim to open a discourse on the current use of TTV sections. The goal of our position is to jointly reflect and systematically improve how we, as a research community, consider TTV in our studies.
Methods:
Based on our personal experience as researchers, authors, reviewers, and editors, we critically reflect on the treatment of TTV in current empirical 
software engineering
 literature.
Results:
We discuss the key shortcomings of TTV consideration, including the failure to acknowledge different types of validity categorizations and the tendency to treat threats just as an afterthought. For each identified problem, we propose a vision for an improved state, intending to catalyze thoughtful engagement and improvements the way our community addresses TTV.
Conclusion:
We posit there is an urgent need to reconsider how we approach, document, and evaluate TTV in software engineering research.",Information and Software Technology,18 Mar 2025,6,"The abstract focuses on TTV in software engineering research, which is essential but may not have immediate practical implications for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001544,"Synthesizing research on programmers’ mental models of programs, tasks and concepts — A systematic literature review",December 2023,"Mental model, Mental representation, Human factors, Program comprehension, Psychology of programming, Programmer, Software developer, Software development, Systematic literature review, Empirical software engineering",Ava=Heinonen: ava.heinonen@aalto.fi; Bettina=Lehtelä: Not Found; Arto=Hellas: Not Found; Fabian=Fagerholm: Not Found,"Abstract
Context:
Programmers’ 
mental models
 represent their knowledge and understanding of programs, programming concepts, and programming in general. They guide programmers’ work and influence their task performance. Understanding mental models is important for designing work systems and practices that support programmers.
Objective:
Although the importance of programmers’ mental models is widely acknowledged, research on mental models has decreased over the years. The results are scattered and do not take into account recent developments in 
software engineering
. In this article, we analyze the state of research on programmers’ mental models and provide an overview of existing research. We connect results on mental models from different strands of research to form a more unified knowledge base on the topic.
Method:
We conducted a systematic literature review on programmers’ mental models. We analyzed literature addressing mental models in different contexts, including mental models of programs, programming tasks, and programming concepts. Using nine search engines, we found 3678 articles (excluding duplicates). Of these, 84 were selected for further analysis. Using the snowballing technique, starting from these 84, we obtained a final result set containing 187 articles.
Results:
We show that the literature shares a kernel of shared understanding of mental models. By collating and connecting results on mental models from different fields of research, we provide a comprehensive synthesis of results related to programmers’ mental models.
Conclusion:
The research field on programmers’ mental 
models faces
 many challenges arising from a lack of a shared knowledge base and poorly defined constructs. By creating a unified knowledge base on the topic, this work provides a basis for future work on mental models. We also point to directions for future studies. In particular, we call for studies that examine programmers working with modern practices and tools.",Information and Software Technology,18 Mar 2025,8,"The abstract provides a comprehensive analysis of programmers' mental models, which can be valuable for designing work systems and practices to support software development in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001544,"Synthesizing research on programmers’ mental models of programs, tasks and concepts — A systematic literature review",December 2023,"Mental model, Mental representation, Human factors, Program comprehension, Psychology of programming, Programmer, Software developer, Software development, Systematic literature review, Empirical software engineering",Ava=Heinonen: ava.heinonen@aalto.fi; Bettina=Lehtelä: Not Found; Arto=Hellas: Not Found; Fabian=Fagerholm: Not Found,"Abstract
Context:
Programmers’ 
mental models
 represent their knowledge and understanding of programs, programming concepts, and programming in general. They guide programmers’ work and influence their task performance. Understanding mental models is important for designing work systems and practices that support programmers.
Objective:
Although the importance of programmers’ mental models is widely acknowledged, research on mental models has decreased over the years. The results are scattered and do not take into account recent developments in 
software engineering
. In this article, we analyze the state of research on programmers’ mental models and provide an overview of existing research. We connect results on mental models from different strands of research to form a more unified knowledge base on the topic.
Method:
We conducted a systematic literature review on programmers’ mental models. We analyzed literature addressing mental models in different contexts, including mental models of programs, programming tasks, and programming concepts. Using nine search engines, we found 3678 articles (excluding duplicates). Of these, 84 were selected for further analysis. Using the snowballing technique, starting from these 84, we obtained a final result set containing 187 articles.
Results:
We show that the literature shares a kernel of shared understanding of mental models. By collating and connecting results on mental models from different fields of research, we provide a comprehensive synthesis of results related to programmers’ mental models.
Conclusion:
The research field on programmers’ mental 
models faces
 many challenges arising from a lack of a shared knowledge base and poorly defined constructs. By creating a unified knowledge base on the topic, this work provides a basis for future work on mental models. We also point to directions for future studies. In particular, we call for studies that examine programmers working with modern practices and tools.",Information and Software Technology,18 Mar 2025,7,"The abstract offers insights into programmers' mental models, but the direct impact on European early-stage ventures may not be as pronounced."
https://www.sciencedirect.com/science/article/pii/S095058492300157X,Software vulnerability prediction: A systematic mapping study,December 2023,Not Found,Ilias=Kalouptsoglou: iliaskaloup@uom.edu.gr; Miltiadis=Siavvas: siavvasm@iti.gr; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Dionysios=Kehagias: diok@iti.gr; Alexander=Chatzigeorgiou: achat@uom.edu.gr,"Abstract
Context:
Software security is considered a major aspect 
of software quality
 as the number of discovered vulnerabilities in software products is growing. Vulnerability prediction is a mechanism that helps engineers to prioritize their inspection efforts focusing on vulnerable parts. Despite the recent advancements, current literature lacks a 
systematic mapping study
 on vulnerability prediction.
Objective:
This paper aims to analyze the state-of-the-art of vulnerability prediction focusing on: (a) the goals of vulnerability prediction-related studies; (b) the data collection processes and the types of datasets that exist in the literature; (c) the mostly examined techniques for the construction of the prediction models and their input features; and (d) the utilized evaluation techniques.
Method:
We collected 180 primary studies following a broad search methodology across four popular digital libraries. We mapped these studies to the variables of interest and we identified trends and relationships between the studies.
Results:
The main findings suggest that: (i) there are two major study types, prediction of vulnerable software components and forecasting of the evolution of vulnerabilities in software; (ii) most studies construct their own vulnerability-related dataset retrieving information from 
vulnerability databases
 for real-world software; (iii) there is a growing interest for 
deep learning models
 along with a trend on textual source code representation; and (iv) 
F
1
-score was found to be the most widely used 
evaluation metric
.
Conclusions:
The results of our study indicate that there are several open challenges in the domain of vulnerability prediction. One of the major conclusions, is the fact that most studies focus on within-project prediction, neglecting the real-world scenario of cross-project prediction.",Information and Software Technology,18 Mar 2025,7,"The study provides insights into vulnerability prediction, which is crucial for software security, but lacks immediate practical application for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001817,Model-based security testing in IoT systems: A Rapid Review,December 2023,"Internet of Things, Model-based testing, Security testing",Francesca=Lonetti: francesca.lonetti@isti.cnr.it; Antonia=Bertolino: antonia.bertolino@isti.cnr.it; Felicita=Di Giandomenico: felicita.digiandomenico@isti.cnr.it,"Abstract
Context:
Security testing is a challenging and effort-demanding task in 
IoT
 scenarios. The heterogeneous devices expose different vulnerabilities that can influence the methods and cost of security testing. Model-based security testing techniques support the systematic generation of test cases for the assessment of security requirements by leveraging the specifications of the IoT system model and of the attack templates.
Objective:
This paper aims to review the adoption of model-based security testing in the context of IoT, and then provides the first systematic and up-to-date comprehensive classification and analysis of research studies in this topic.
Method:
We conducted a systematic literature review analyzing 803 publications and finally selecting 17 primary studies, which satisfied our 
inclusion criteria
 and were classified according to a set of relevant analysis dimensions.
Results:
We report the state-of-the-art about the used formalisms, the test techniques, the objectives, the target applications and domains; we also identify the targeted security attacks, and discuss the challenges, gaps and future research directions.
Conclusion:
Our review represents the first attempt to systematically analyze and classify existing studies on model-based security testing for IoT. According to the results, model-based security testing has been applied in core IoT domains. Models complexity and the need of modeling evolving scenarios that include heterogeneous 
open software
 and hardware components remain the most important shortcomings. Our study shows that model-based security testing of IoT applications is a promising research direction. The principal future research directions deal with: extending the existing modeling formalisms in order to capture all peculiarities and constraints of complex and large scale IoT networks; the definition of context-aware and dynamic evolution modeling approaches of IoT entities; and the combination of model-based testing techniques with other security test strategies such as 
penetration testing
 or learning techniques for model inference.",Information and Software Technology,18 Mar 2025,8,The systematic review of model-based security testing in IoT offers valuable insights and practical implications for startups operating in this industry.
https://www.sciencedirect.com/science/article/pii/S0950584923001829,Application of knowledge graph in software engineering field: A systematic literature review,December 2023,Not Found,Lu=Wang: wanglu@xidian.edu.cn; Chenhan=Sun: 21031221792@stu.xidian.edu.cn; Chongyang=Zhang: 21031211699@stu.xidian.edu.cn; Weikun=Nie: 21031211743@stu.xidian.edu.cn; Kaiyuan=Huang: 21031211704@stu.xidian.edu.cn,"Abstract
Context:
Knowledge graphs describe knowledge resources and their carriers through visualization. Moreover, they mine, analyze, construct, draw, and display knowledge and their interrelationships to reveal the dynamic development law of the knowledge field. Furthermore, knowledge graphs provide practical and valuable references for subject research. With the development of 
software engineering
, powerful 
semantic processing
 and organizational interconnection capabilities of knowledge graphs are gradually required. Current research suggests using knowledge graphs for code or 
API
 recommendation, vulnerability mining, and positioning to improve the efficiency and accuracy of development and design. However, 
software engineering
 lacks a 
systematic analysis
 of the knowledge graphs application.
Objective:
This paper explores the construction techniques and application status of knowledge graphs in the field of software engineering, broadens the application prospects of knowledge graphs in this field, and facilitates the subsequent research of researchers.
Methods:
We collected over 100 documents from 2017 to date and selected 55 directly 
related documents
 for 
systematic analysis
. Then, we analyzed the organized knowledge mainly stored in software engineering knowledge graphs, including software architecture, code details, and security reports.
Results:
We studied the emerging research methods in ontology modeling, 
named entity recognition
, and knowledge fusion in 
graph construction
 and found that current knowledge graphs are mainly used in intelligent software development, software vulnerability mining, security testing, and 
API
 recommendation.
Conclusion:
Our research on the innovation of knowledge graph in software engineering and the future construction of integrating open-source community software and developer recommendations with knowledge-driven 
microservice
 O&M aspects can inspire more scholars and knowledge workers to use knowledge graph technology, which is important to solve software engineering problems and promote the development of both fields.",Information and Software Technology,18 Mar 2025,9,"The exploration of knowledge graphs in software engineering presents a promising approach with practical applications for startups in code recommendation, vulnerability mining, and API recommendation."
https://www.sciencedirect.com/science/article/pii/S0950584923001593,A data-driven approach for understanding invalid bug reports: An industrial case study,December 2023,"Software maintenance, Invalid bug reports, Bug management, Topic modeling, LDA, Bug classification, Software analytics",Muhammad=Laiq: muhammad.laiq@bth.se; Nauman bin=Ali: nauman.ali@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Emelie=Engström: emelie.engstrom@cs.lth.se,"Abstract
Context:
Bug reports
 created during software development and maintenance do not always describe deviations from a system’s valid behavior. Such invalid bug reports may consume significant resources and adversely affect the prioritization and resolution of valid bug reports. There is a need to identify 
preventive actions
 to reduce the inflow of invalid bug reports. Existing research has shown that manually analyzing invalid bug report descriptions provides cues regarding preventive actions. However, such a manual approach is not cost-effective due to the time required to analyze a sufficiently large number of bug reports needed to identify useful patterns. Furthermore, the analysis needs to be repeated as the underlying causes of invalid bug reports change over time.
Objective:
In this study, we propose and evaluate the use of 
Latent Dirichlet Allocation
 (LDA), a 
topic modeling
 approach, to support practitioners in suggesting preventive actions to avoid the creation of similar invalid bug reports in the future.
Method:
In an industrial 
case study
, we first manually analyzed descriptions of invalid bug reports to identify common patterns in their descriptions. We further investigated to what extent LDA can support this manual process. We used expert-based validation to evaluate the relevance of identified common patterns and their usefulness in suggesting preventive measures.
Results:
We found that invalid bug reports have common patterns that are perceived as relevant, and they can be used to devise preventive measures. Furthermore, the identification of common patterns can be supported with automation.
Conclusion:
Using LDA, practitioners can effectively identify representative groups of bug reports (i.e., relevant common patterns) from a large number of bug reports and analyze them further to devise preventive measures.",Information and Software Technology,18 Mar 2025,8,"The use of LDA for identifying common patterns in bug reports is beneficial for reducing invalid bug reports, offering practical value for startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584923001593,A data-driven approach for understanding invalid bug reports: An industrial case study,December 2023,"Software maintenance, Invalid bug reports, Bug management, Topic modeling, LDA, Bug classification, Software analytics",Muhammad=Laiq: muhammad.laiq@bth.se; Nauman bin=Ali: nauman.ali@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Emelie=Engström: emelie.engstrom@cs.lth.se,"Abstract
Context:
Bug reports
 created during software development and maintenance do not always describe deviations from a system’s valid behavior. Such invalid bug reports may consume significant resources and adversely affect the prioritization and resolution of valid bug reports. There is a need to identify 
preventive actions
 to reduce the inflow of invalid bug reports. Existing research has shown that manually analyzing invalid bug report descriptions provides cues regarding preventive actions. However, such a manual approach is not cost-effective due to the time required to analyze a sufficiently large number of bug reports needed to identify useful patterns. Furthermore, the analysis needs to be repeated as the underlying causes of invalid bug reports change over time.
Objective:
In this study, we propose and evaluate the use of 
Latent Dirichlet Allocation
 (LDA), a 
topic modeling
 approach, to support practitioners in suggesting preventive actions to avoid the creation of similar invalid bug reports in the future.
Method:
In an industrial 
case study
, we first manually analyzed descriptions of invalid bug reports to identify common patterns in their descriptions. We further investigated to what extent LDA can support this manual process. We used expert-based validation to evaluate the relevance of identified common patterns and their usefulness in suggesting preventive measures.
Results:
We found that invalid bug reports have common patterns that are perceived as relevant, and they can be used to devise preventive measures. Furthermore, the identification of common patterns can be supported with automation.
Conclusion:
Using LDA, practitioners can effectively identify representative groups of bug reports (i.e., relevant common patterns) from a large number of bug reports and analyze them further to devise preventive measures.",Information and Software Technology,18 Mar 2025,8,"Similar to abstract 209, the study on LDA for preventive actions in bug reports has practical implications for startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584923001581,Can an old fashioned feature extraction and a light-weight model improve vulnerability type identification performance?,December 2023,Not Found,Hieu Dinh=Vo: hieuvd@vnu.edu.vn; Son=Nguyen: sonnguyen@vnu.edu.vn,"Abstract
Recent advances in automated 
vulnerability detection
 have achieved potential results in helping developers determine vulnerable components. However, after detecting vulnerabilities, investigating to fix vulnerable code is a non-trivial task. In fact, the types of vulnerability, such as 
buffer overflow
 or 
memory corruption
, could help developers quickly understand the nature of the weaknesses and localize vulnerabilities for security analysis. In this work, we investigate the problem of vulnerability type identification (VTI). The problem is modeled as the multi-label 
classification task
, which could be effectively addressed by “
pre-training, then fine-tuning
” framework with deep pre-trained embedding models. We evaluate the performance of the well-known and advanced pre-trained models for VTI on a large set of vulnerabilities. Surprisingly, their performance is not much better than that of the classical baseline approach with an old-fashioned bag-of-word, TF-IDF. Meanwhile, these deep 
neural network approaches
 cost much more resources and require GPU. We also introduce a lightweight independent component to refine the predictions of the baseline approach. Our idea is that the types of vulnerabilities could strongly correlate to certain code tokens (distinguishing tokens) in several crucial parts of programs. The distinguishing tokens for each vulnerability type are statistically identified based on their prevalence in the type versus the others. Our results show that the baseline approach enhanced by our component can outperform the state-of-the-art deep pre-trained approaches while retaining very high efficiency. Furthermore, the proposed component could also improve the 
neural network
 approaches by up to 92.8% in macro-average F1.",Information and Software Technology,18 Mar 2025,8,The research on vulnerability type identification and the proposed lightweight independent component show practical value for early-stage ventures by improving security analysis efficiently.
https://www.sciencedirect.com/science/article/pii/S0950584923001556,Work and career-related features of technology: A grounded theory study of software professionals,December 2023,Not Found,Gunjan=Tomer: Not Found; Sushanta Kumar=Mishra: sushantam@iimb.ac.in,"Abstract
For software professionals, work and technology are inseparable. Their work and careers are intertwined with different features of technology. For instance, features such as uncertainty and market dominance influence the career prospects of Software professionals. Despite the 
criticality
 of technology in professionals' lives, studies exploring technology features that influence Software professionals' work and life are, at best, limited. Based on an exploratory approach, we found that Software professionals evaluate technology based on their perceptions and career expectations. The positive or negative evaluation captures the perceived fit/match between their expectations, preferences from technology, and the features of a given technology. Based on the findings, we conceptualized technology from the career perspective of software professionals. We highlighted the implications of the fit/match between the software professionals' preferences and the features/characteristics of a given technology.",Information and Software Technology,18 Mar 2025,4,"While exploring technology features in software professionals' work is insightful, the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001568,CLeBPI: Contrastive Learning for Bug Priority Inference,December 2023,Not Found,Wen-Yao=Wang: wenyaowang108@gmail.com; Chen-Hao=Wu: wuchenhao78@gmail.com; Jie=He: hejie1213@126.com,"Abstract
Context:
Automated bug priority inference (BPI) can reduce the time overhead of bug triagers for priority assignments, improving the efficiency of software maintenance.
Objective:
There are two orthogonal lines for this task, i.e., traditional 
machine learning
 based (TML-based) and 
neural network
 based (NN-based) approaches. Although these approaches achieve competitive performance, our observation finds that existing approaches 
face
 the following two issues: 1) TML-based approaches require much manual feature engineering and cannot learn the 
semantic information
 of 
bug reports
; 2) Both TML-based and NN-based approaches cannot effectively address the label imbalance problem because they are difficult to distinguish the semantic difference between 
bug reports
 with different priorities.
Method:
We propose CLeBPI (
C
ontrastive 
Le
arning for 
B
ug 
P
riority 
I
nference), which leverages pre-trained 
language model
 and 
contrastive learning
 to tackle the above-mentioned two issues. Specifically, CLeBPI is first pre-trained on a large-scale bug report corpus in a self-supervised way, thus it can automatically learn contextual representations of bug reports without manual feature engineering. Afterward, it is further pre-trained by a 
contrastive learning
 objective, which enables it to distinguish semantic differences between bug reports, learning more precise contextual representations for each bug report. When finishing pre-training, we can connect a classification layer to CLeBPI and fine-tune it for BPI in a supervised way.
Results:
We choose four baseline approaches and conduct comparison experiments on a public dataset. The experimental results show that CLeBPI outperforms all baseline approaches by 23.86%–77.80% in terms of weighted average F1-score, showing its effectiveness.
Conclusion:
This paper propose CLeBPI, a pre-trained model combining contrastive learning that can automatically predict bug priority. Experimental results show that It achieves new result in BPI and can effectively alleviate label imbalance problem.",Information and Software Technology,18 Mar 2025,9,"The proposed CLeBPI model for bug priority inference addresses efficiency issues in software maintenance, which can have a significant impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492300174X,Warnings: Violation symptoms indicating architecture erosion,December 2023,Not Found,Ruiyin=Li: ryli_cs@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
As a software system evolves, its architecture tends to degrade, and gradually impedes software maintenance and evolution activities and negatively impacts the quality attributes of the system. The main root cause behind architecture erosion phenomenon derives from violation symptoms (i.e., various architecturally-relevant violations, such as violations of architecture pattern). Previous studies focus on detecting violations in software systems using architecture conformance checking approaches. However, code review comments are also rich sources that may contain extensive discussions regarding architecture violations, while there is a limited understanding of violation symptoms from the viewpoint of developers.
Objective:
In this work, we investigated the characteristics of architecture violation symptoms in code review comments from the developers’ perspective.
Methods:
We employed a set of keywords Related to violation symptoms to collect 606 (out of 21,583) code review comments from four popular OSS projects in the openStack and qt communities. We manually analyzed the collected 606 review comments to provide the categories and linguistic patterns of violation symptoms, as well as the reactions how developers addressed them.
Results:
Our findings show that: (1) three main categories of violation symptoms are discussed by developers during the 
code review process
; (2) The frequently-used terms of expressing violation symptoms are “
inconsistent
” and “
violate
”, and the most common linguistic pattern is 
Problem Discovery
; (3) Refactoring and removing code are the major measures (90%) to tackle violation symptoms, while a few violation symptoms were ignored by developers.
Conclusions:
Our findings suggest that the investigation of violation symptoms can help researchers better understand the characteristics of architecture erosion and facilitate the development and maintenance activities, and developers should explicitly manage violation symptoms, not only for addressing the existing architecture violations but also preventing future violations.",Information and Software Technology,18 Mar 2025,6,"The investigation of architecture violation symptoms in code review comments provides useful insights, but the direct impact on early-stage ventures may be moderate."
https://www.sciencedirect.com/science/article/pii/S0950584923001623,FrMi: Fault-revealing Mutant Identification using killability severity,December 2023,Not Found,Taha=Rostami: taha.rostami@modares.ac.ir; Saeed=Jalili: sjalili@modares.ac.ir,"Abstract
Context:
Mutation testing is a powerful method used in software testing for various activities, such as guidance for test case generation and test suite quality assessment. However, a vast number of mutants, most unrelated to real faults, threaten the scalability and validity of the method. Over the decades, researchers have proposed various approaches to alleviate these problems, most of which have almost the same performance in practice. To overcome this issue, recently predicting a category of mutants named fault-revealing mutants has been proposed, which outperforms other methods in terms of real-fault revelation ability. Although recent research shows the usefulness of targeting this type of mutant, they are scarce, which makes predictions of them with higher accuracy challenging.
Objective:
This paper aims to propose a method that can predict fault-revealing mutants with higher accuracy compared to the state-of-the-art method.
Methods:
To tackle this challenge, a feature representing the difficulty of killing a mutant is added as a new feature to complement the state-of-the-art feature set. Then a method based on 
ensemble learning
 is proposed that uses this feature for fault-revealing mutants’ prediction.
Results:
According to our experimental results, the proposed method outperforms the state-of-the-art method regarding area under a receiver operating characteristic curve (AUC) value on the Codeflaws and CoRBench data sets by 7.09% and 8.97%, respectively.
Conclusion:
It is concluded that the proposed method, which includes a new feature and an ensemble-learning approach, enhances the accuracy of predicting fault-revealing mutants in software testing. This is achieved by incorporating the difficulty of killing a mutant as a feature, which complements the existing feature set used in state-of-the-art methods. The experimental results demonstrate that the proposed method outperforms the state-of-the-art method on two datasets, Codeflaws and CoRBench, indicating that it has the potential to be applied in practical software testing scenarios.",Information and Software Technology,18 Mar 2025,7,"The proposed method for predicting fault-revealing mutants in software testing shows promise, but the impact on early-stage ventures may be slightly limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001751,Leveraging a combination of machine learning and formal concept analysis to locate the implementation of features in software variants,December 2023,Not Found,Hamzeh=Eyal Salman: hamzehmu@mutah.edu.jo,"Abstract
Context:
Recently, software variants are adopted to build software product lines in the industry. In this adoption, the available assets (features, 
source code
, design documents, etc.) are reused to build a software product line rather than building it from scratch. The feature location is the first step in this adoption process. In the literature, numerous approaches were proposed to locate the implementations of features in the 
source code
.
Objective:
However, these approaches are guided using feature-specific information, which is not always available, especially in legacy applications. In this study, a feature location approach is proposed without predefined feature-specific information.
Method:
The proposed approach incorporates a mathematical research technique called formal concept analysis with other proposed algorithms. This combination is empirically evaluated using a benchmark 
case study
.
Results:
The obtained results demonstrate that this combination achieves promising results in terms of well-known used metrics in this area: Recall, Precision, and F-measure.
Conclusion:
Also, the results show that the approach effectively finds features implementation across software variants.",Information and Software Technology,18 Mar 2025,7,"The proposed approach addresses a practical need in software development by improving feature location without predefined information, which can benefit European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001799,An empirical experiment of a usability requirements elicitation method to design GUIs based on interviews,December 2023,"Usability requirements elicitation, Interviews, Empirical experiment, Guidelines",Yeshica Isela=Ormeño: Not Found; José Ignacio=Panach: joigpana@uv.es; Oscar=Pastor: Not Found,"Abstract
Context
The usability 
requirements elicitation
 process is a difficult task that lacks methods to guide and help analysts, who are usually not experts at usability.
Objective
This paper conducts an experiment with two replications to evaluate a method that elicits 
usability requirements
 based on structured interviews named 
UREM
 versus an unstructured method. The method consists of guided interviews by the analyst using 
decision trees
. The tree is composed of questions and possible answers. Each question appears when there are different possible design alternatives, and each answer represents one of these alternatives. The tree also recommends the alternative that enhances the usability based on existing usability guidelines.
Method
We have conducted an experiment with two replications with 22 and 26 subjects playing two different roles in a within-subjects design. The analysts used a tree to guide the interview and elicit the requirements while the end users had to explain to the analyst the type of system to develop. During the interview, the analyst must design a 
paper prototype
 to be validated by the end user. For the analyst, the experiment measures the effectiveness of usability requirements elicitation, the effectiveness of the use of the usability guidelines, the efficiency of the 
elicitation process
, and the satisfaction with the entire elicitation process. For the end user, the experiment measures the satisfaction with the designed prototype at the end of the interview.
Results
UREM yielded significantly better results for the effectiveness in the usability requirements elicitation process and for the effectiveness in the use of usability guidelines when compared to unstructured interviews. The use of UREM did not reduce the analysts’ efficiency and both analyst and end user remained the same satisfaction.
Conclusions
Eliciting usability requirements is a difficult task if it is done with unstructured interviews and without usability recommendations.",Information and Software Technology,18 Mar 2025,8,"The UREM method demonstrates significant improvements in usability requirements elicitation, which can have a direct impact on startups developing user-centered products in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584923001787,Systematic reviews in mobile app software engineering: A tertiary study,December 2023,Not Found,Samer=Zein: szain@birzeit.edu; Norsaremah=Salleh: Not Found; John=Grundy: Not Found,"Abstract
Context:
A number of secondary studies in the form of systematic reviews and 
systematic mapping studies
 exist in the area of mobile 
application software
 engineering.
Objective:
The focus of this paper is to provide an overview and analysis of these secondary studies of mobile app 
software engineering
 for researchers and practitioners.
Method:
We conducted a systematic tertiary study following the guidelines by Kitchenham et al. to classify and analyze secondary studies in this area.
Results:
After going through several filtration steps, we identified 24 secondary studies addressing major 
software engineering
 phases, such as initiation, 
requirements engineering
, design, development and testing. The majority of the secondary studies focused on testing and design phases. Specific 
research topics
 addressed by the included studies were: 
usability evaluation
, test automation, context-aware testing, cloud-based development, 
architectural models
, effort and size estimation models, 
defect prediction
, and GUI testing. We found that the trend in secondary studies is towards more specific areas of mobile 
application software
 engineering such as 
architectural design
 models, context-aware testing, testing of non-functional requirements, 
mobile cloud computing
, and intelligent mobile applications. Research directions and some identified practices for practitioners were also identified.
Conclusions:
Mobile application software engineering is an active research area. The area can benefit from additional research in terms of secondary studies targeting evolution, maintenance, 
requirements engineering
, and cross-platform mobile application development. Additionally, some of the secondary studies identify some useful practices for practitioners.",Information and Software Technology,18 Mar 2025,6,"The overview of secondary studies in mobile app software engineering provides valuable insights for researchers and practitioners, but the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001763,A large-scale exploratory study of android sports apps in the google play store,December 2023,Not Found,Bhagya=Chembakottu: bhagya.c@polymtl.ca; Heng=Li: Not Found; Foutse=Khomh: Not Found,"Abstract
Context
Prior studies on mobile app analysis often analyze apps across different categories or focus on a small set of apps within a category. These studies either provide general insights for an entire app store which consists of millions of apps, or provide specific insights for a small set of apps. However, a single app category can often contain tens of thousands to hundreds of thousands of apps. For example, according to AppBrain, there are 46,625 apps in the “Sports” category of Google Play apps. Analyzing such a targeted category of apps can provide more specific insights than analyzing apps across categories while still benefiting many app developers interested in the category.
Objective
This work aims to study a large number of apps from a single category (i.e., the 
sports
 category). Our work can provide two folds contributions: 1) identifying insights that are specific to tens of thousands of sports apps, and 2) providing empirical evidence on the benefits of analyzing apps in a specific category.
Method
We perform an empirical study on over two thousand sports apps in the Google Play Store. We study the characteristics of these apps (e.g., their targeted sports types and main functionalities) through manual analysis, the topics in the user review through 
topic modeling
, as well as the aspects that contribute to the negative opinions of users through analysis of user ratings and sentiment.
Results
We identified sports apps that cover 16 sports types (e.g., Football, Cricket, Baseball) and 15 main functionalities (e.g., Betting, Betting Tips, Training, Tracking). We also extracted 14 topics from the user reviews, among which three are specific to sports apps (
accuracy of prediction, up-to-dateness
, and 
precision of tools
). Finally, we observed that users are mainly complaining about the advertisements and quality (e.g., bugs, content quality, streaming quality) of sports apps.
Conclusion
It is concluded that analyzing a targeted category of apps (e.g., sports apps) can provide more specific insights than analyzing apps across different categories while still being relevant for a large number (e.g., tens of thousands) of apps. Besides, as a rapid-growing and competitive market, sports apps provide rich opportunities for future research, for example, to study the integration of data science or 
machine learning techniques
 in 
software applications
 or to study the factors that influence the competitiveness of the apps.",Information and Software Technology,18 Mar 2025,9,"Studying a targeted category of apps like sports apps can provide specific insights for app developers, which can be valuable for startups in Europe looking to enter this market."
https://www.sciencedirect.com/science/article/pii/S0950584923001830,A software vulnerability detection method based on deep learning with complex network analysis and subgraph partition,December 2023,Not Found,Wenjing=Cai: Not Found; Junlin=Chen: Not Found; Jiaping=Yu: Not Found; Lipeng=Gao: gaolipeng@nwpu.edu.cn,"Abstract
The increasing size and complexity of software programs have made them an integral part of modern society’s infrastructure, making software vulnerabilities a major threat to 
computer security
. To address this issue, the use of deep learning-based software 
vulnerability detection
 methods has become increasingly popular. Although the effectiveness of the deep learning-based methods has been demonstrated, these methods have faced challenges in scalability and detection performance. To tackle this challenge, we propose a new 
vulnerability detection
 method based on 
deep learning
 with complex 
network analysis
 and subgraph partition that enhances detection accuracy while maintaining scalability. The method uses complex 
network analysis
 theory to convert the CPG into an image-like matrix, and then utilizes TextCNN for vulnerability detection. As a result, our method shows a 6% improvement in accuracy and a 10% reduction in false positive rates compared to state-of-the-art methods. In addition, our approach is able to detect some of the vulnerabilities recently released by CVE.",Information and Software Technology,18 Mar 2025,7,"The proposed vulnerability detection method using deep learning shows improvement in accuracy and scalability, which can benefit European startups in enhancing their software security measures."
https://www.sciencedirect.com/science/article/pii/S095058492300188X,Business-driven technical debt management using Continuous Debt Valuation Approach (CoDVA),December 2023,Not Found,Marek G.=Stochel: marek.stochel@motorolasolutions.com; Tomasz=Borek: tomasz.borek@motorolasolutions.com; Mariusz R.=Wawrowski: mariusz.wawrowski@motorolasolutions.com; Piotr=Chołda: piotr.cholda@agh.edu.pl,"Abstract
Context:
Despite the increasing research on Technical Debt Management (TDM), there is still a need for empirical studies that take a comprehensive approach to managing Technical Debt (TD) and consider the business perspective.
Objectives:
We introduce an empirically evaluated methodology called Continuous Debt Valuation Approach (CoDVA), which improves business value, team productivity, and developer morale.
Methods:
CoDVA prioritizes TD against a predicted 
product roadmap
, quantifying 
potential benefits
 based on their impact on the future state of the product and profitability of the investments. This approach enables a relative comparison among TD items and justifies a budget for 
TD repayment
. The methodology was validated through a survey and a three-year-long 
case study
 on TDM practices driven by an engineering team responsible for development and maintenance of a telecommunication software.
Results:
The results of the study show that the CoDVA approach has a positive impact, as the perceived business value from TD refactorings grew by 27%, the engineering team velocity improved by 39%, predictability of the engineering team increased by 60%, the effort spent on product release stabilization decreased by 50%, and overall developer satisfaction increased in 86% of the cases. The majority of the developers found that the applied TDM strategy was beneficial in terms of technical decisions made, the ability to develop a new functionality, and experience while working with the product code.
Conclusion:
The results prove that the approach is expedient across realized business value, 
software engineering team
 productivity, and satisfaction of the engineers responsible for development and maintenance of the software product.",Information and Software Technology,18 Mar 2025,8,"The abstract presents an empirically evaluated methodology (CoDVA) that shows positive impact on business value, team productivity, and developer satisfaction, which are critical aspects for early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001726,A reflection on the impact of model mining from GitHub,December 2023,"Modeling, UML, Mining software repositories, Empirical research",Gregorio=Robles: gregorio.robles@urjc.es; Michel R.V.=Chaudron: Not Found; Rodi=Jolak: Not Found; Regina=Hebig: Not Found,"Abstract
Context:
Since 1998, the ACM/IEEE 25th International Conference on 
Model Driven Engineering
 Languages and Systems (MODELS) has been studying all aspects surrounding modeling in 
software engineering
, from languages and methods to tools and applications. In order to enable empirical studies, the MODELS community developed a need for having examples of models, especially of models used in real software development projects. Such models may be used for a range of purposes, but mostly related to domain analysis and 
software design
 (at various levels of abstraction). However, finding such models was very difficult. The most used ones had their origin in academic books or student projects, which addressed “artificial” applications, i.e., were not base on real-case scenarios. To address this issue, the authors of this reflection paper, members of the modeling and of the 
mining software repositories
 fields, came together with the aim of creating a dataset with an abundance of modeling projects by mining GitHub. As a scoping of our effort we targeted models represented using the UML notation because this is the 
lingua franca
 in practice for software modeling. As a result, almost 100k models from 22k projects were made publicly available, known as the Lindholmen dataset.
Objective:
In this paper, we analyze the impact of our research, and compare this to what we envisioned in 2016. We draw practical lessons gained from this effort, reflect on the perils and pitfalls of the dataset, and point out promising avenues of research.
Method:
We base our reflection on the 
systematic analysis
 of recent research literature, and especially those papers citing our dataset and its associated publications.
Results:
What we envisioned in the original research when making the dataset available has to a major extent not come true; however, fellow researchers have found alternative uses of the dataset.
Conclusions:
By understanding the possibilities and shortcomings of the current dataset, we aim to offer the research community i) future research avenues of how the data can be used; and ii) raise awareness of the limitations, not only to point out threats to validity of research, but also to encourage fellow researchers to find ideas to overcome them. Our reflections can also be helpful to researchers who want to perform similar mining efforts.",Information and Software Technology,18 Mar 2025,5,The abstract discusses the creation of a dataset for modeling projects but does not directly address practical value for European early-stage ventures or startups. It focuses more on research impact and lessons learned.
https://www.sciencedirect.com/science/article/pii/S0950584923001726,A reflection on the impact of model mining from GitHub,December 2023,"Modeling, UML, Mining software repositories, Empirical research",Gregorio=Robles: gregorio.robles@urjc.es; Michel R.V.=Chaudron: Not Found; Rodi=Jolak: Not Found; Regina=Hebig: Not Found,"Abstract
Context:
Since 1998, the ACM/IEEE 25th International Conference on 
Model Driven Engineering
 Languages and Systems (MODELS) has been studying all aspects surrounding modeling in 
software engineering
, from languages and methods to tools and applications. In order to enable empirical studies, the MODELS community developed a need for having examples of models, especially of models used in real software development projects. Such models may be used for a range of purposes, but mostly related to domain analysis and 
software design
 (at various levels of abstraction). However, finding such models was very difficult. The most used ones had their origin in academic books or student projects, which addressed “artificial” applications, i.e., were not base on real-case scenarios. To address this issue, the authors of this reflection paper, members of the modeling and of the 
mining software repositories
 fields, came together with the aim of creating a dataset with an abundance of modeling projects by mining GitHub. As a scoping of our effort we targeted models represented using the UML notation because this is the 
lingua franca
 in practice for software modeling. As a result, almost 100k models from 22k projects were made publicly available, known as the Lindholmen dataset.
Objective:
In this paper, we analyze the impact of our research, and compare this to what we envisioned in 2016. We draw practical lessons gained from this effort, reflect on the perils and pitfalls of the dataset, and point out promising avenues of research.
Method:
We base our reflection on the 
systematic analysis
 of recent research literature, and especially those papers citing our dataset and its associated publications.
Results:
What we envisioned in the original research when making the dataset available has to a major extent not come true; however, fellow researchers have found alternative uses of the dataset.
Conclusions:
By understanding the possibilities and shortcomings of the current dataset, we aim to offer the research community i) future research avenues of how the data can be used; and ii) raise awareness of the limitations, not only to point out threats to validity of research, but also to encourage fellow researchers to find ideas to overcome them. Our reflections can also be helpful to researchers who want to perform similar mining efforts.",Information and Software Technology,18 Mar 2025,5,"Similar to abstract 222, it focuses on the creation of a dataset without direct implications for early-stage ventures or startups. The emphasis is more on research impact and future research avenues."
https://www.sciencedirect.com/science/article/pii/S0950584923001738,Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development repositories,December 2023,"Reproducible research, Mining software repositories, Reproducibility, Validation studies, Empirical software engineering",Jesus M.=Gonzalez-Barahona: jesus.gonzalez.barahona@urjc.es; Gregorio=Robles: Not Found,"Abstract
Context:
In 2012, our paper “On the reproducibility of empirical 
software engineering
 studies based on data retrieved from development repositories” was published. It proposed a method for assessing the reproducibility of studies based on 
mining software repositories
 (MSR studies). Since then, several approaches have happened with respect to the study of the reproducibility of this kind of studies.
Objective:
To revisit the proposals of that paper, analyzing to which extent they remain valid, and how they relate to current initiatives and studies on reproducibility and validation of 
research results
 in empirical software engineering.
Method:
We analyze the most relevant studies affecting assumptions or consequences of the approach of the original paper, and other initiatives related to the evaluation of replicability aspects of empirical software engineering studies. We compare the results of that analysis with the results of the original study, finding similarities and differences. We also run a reproducibility assessment study on current MSR papers. Based on the comparison, and the applicability of the method to current papers, we draw conclusions on the validity of the approach of the original paper.
Main lessons learned:
The method proposed in the original paper is still valid, and compares well with other more recent methods. It matches the results of relevant studies on reproducibility, and a systematic comparison with them shows that our approach is aligned with their proposals. Our method has practical use, and complements well the current major initiatives on the review of reproducibility artifacts. As a side result, we learn that the reproducibility of MSR studies has improved during the last decade.
Vision:
We propose to use our approach as a fundamental element of a more profound review of the reproducibility of MSR studies, and of the characterization of validation studies in this realm.",Information and Software Technology,18 Mar 2025,7,"This abstract revisits a previous study on MSR reproducibility and assesses the validity of the proposed method. While not directly related to startups, the focus on reproducibility and validation in research can have implications for early-stage ventures looking to build on existing studies."
https://www.sciencedirect.com/science/article/pii/S0950584923001775,Software design analysis and technical debt management based on design rule theory,December 2023,Not Found,Yuanfang=Cai: yuanfang.cai@drexel.edu; Rick=Kazman: Not Found,"Abstract
In this paper we reflect on our decade-long journey of creating, evolving, and evaluating a number of 
software design
 concepts and technical debt management technologies. These include: a novel 
maintainability
 metric, a new model for representing design information, a suite of design anti-patterns, and a formalized model of design debt. All of these concepts are rooted in options theory, and they all share the objective of helping a software project team quantify and visualize major design principles, and address the very real 
maintainability
 challenges faced by their organizations in practice. The evolution of our research has been propelled by our continuous interactions with industrial collaborators. For each concept, technology, and supporting tool, we embarked on an ambitious program of empirical validation—in “the lab”, with industry partners, and with 
open source projects
. We reflect on the successes of this research and on areas where significant challenges remain. In particular, we observe that improved 
software design
 education, both for students and professional developers, is the prerequisite for our research and technology to be widely adopted. During this journey, we also observed a number of gaps: between what we offer in research and what practitioners need, between management and development, and between debt detection and debt reduction. Addressing these challenges motivates our research moving forward.",Information and Software Technology,18 Mar 2025,8,The abstract reflects on a decade-long journey of creating design concepts and technical debt management technologies with a focus on practical impact and challenges faced by organizations. This can provide valuable insights for European early-stage ventures and startups.
https://www.sciencedirect.com/science/article/pii/S0950584923001714,Sustainable software engineering: Reflections on advances in research and practice,December 2023,Not Found,Colin C.=Venters: c.venters@cern.ch; Rafael=Capilla: rafael.capilla@urjc.es; Elisa Yumi=Nakagawa: elisa@icmc.usp.br; Stefanie=Betz: stefanie.betz@hs-furtwangen.de; Birgit=Penzenstadler: birgitp@chalmers.se; Tom=Crick: thomas.crick@swansea.ac.uk; Ian=Brooks: Ian.Brooks@uwe.ac.uk,"Abstract
Context:
Modern societies
 are highly dependent on complex, large-scale, software-intensive systems that increasingly operate within an environment of continuous availability, which are challenging to maintain, and evolve in response to changes in stakeholder requirements of the system. Software architectures are the foundation of any software system and provide a mechanism for reasoning about core software quality requirements. Their 
sustainability
 – the capacity to endure in changing environments – is a critical concern for software architecture research and practice.
Objective:
The objective of the paper is to re-examine our previous assumptions and arguments in light of advances in the field. This reflection paper provides an opportunity to obtain new insights into the trends in software 
sustainability
 in both academia and industry, from a software architecture perspective specifically and 
software engineering
 more broadly. Given advances in research in the field, the increasing introduction of academic courses on different sustainability topics, and the engagement of companies to cope with sustainability goals, we reflect on advances and maturity about the role sustainability in general plays in today’s society. More specifically, we revisit the trends, open issues and research challenges identified five years ago in our previous paper on software sustainability research and practice from a software 
architecture viewpoint
, which aimed to provide a foundation and roadmap of emerging research themes in the area of sustainable software architectures in order to consider how this paper influenced and motivated research in the intervening years.
Method:
The forward snowballing method was used to establish the methodological basis for our reflection on the state of the art. A total of 234 studies were identified between April 2018 and June 2023 and 102 studies were found to be relevant according to the selection criteria. A further subset was mapped to the primary themes of the original paper including definitions and concepts, reference architectures, measures and metrics, and education.
Vision:
The vision of this reflection paper is to provide a new foundation and road map of emerging research themes in the area of sustainable 
software engineering
 highlighting recent trends, and open issues and research challenges.",Information and Software Technology,18 Mar 2025,8,"The paper provides new insights into software sustainability, reflecting on trends and challenges in the field. It contributes to advancing research in software architecture and engineering, which is valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001386,AGL: Incorporating behavioral aspects into domain-driven design,November 2023,Not Found,Duc-Hanh=Dang: hanhdd@vnu.edu.vn; Duc Minh=Le: duclm20@fe.edu.vn; Van-Vinh=Le: 21028005@vnu.edu.vn,"Abstract
Context:
Domain-driven design (DDD) aims to iteratively develop software around a realistic domain model. Recent research in DDD has been focusing on using annotation-based domain-specific languages (aDSLs) to build the domain model. However, within current approaches behavioral aspects, that are often represented using UML Activity and 
State machine diagrams
, are not explicitly captured in the domain model.
Objective:
The focus of this paper is to introduce a new approach for incorporating behavioral aspects into domain models within the Domain-Driven Design (DDD) approach. The proposed approach involves using a new 
activity graph
 language (
AGL
) as an aDSL for representing behavioral aspects within a unified domain model. This integration of 
AGL
 and the previously developed aDSL (
DCSL
 to represent domain models) aims to achieve three important features of DDD: feasibility, productivity, and 
understandability
.
Method:
Our approach involves building a unified class model in 
DCSL
 within a domain-driven architecture, which uses the annotation attachment feature of the host programming language (such as Java) to attach 
AGL
 activity graphs directly to the activity class of the unified class model, resulting in a unified domain model. In this work, we define the abstract and 
concrete syntax
 of 
AGL
. To demonstrate our method, we use a Java framework called 
jDomainApp
 and evaluate 
AGL
 through a 
case study
 to show that it is expressive and practical for real-world software.
Results:
This paper presents two contributions. Firstly, it proposes a mechanism to include behavioral aspects in a unified domain model by introducing a new aDSL called AGL to represent domain behaviors. Secondly, it presents a unified modeling method for domain-driven software development.
Conclusion:
Our method significantly extends the state-of-the-art in DDD in two important fronts: constructing a unified domain model for both structural and behavioral aspects of domain models and bridging the gaps between model and code.",Information and Software Technology,18 Mar 2025,7,Introducing a new approach for incorporating behavioral aspects into domain models within the Domain-Driven Design (DDD) approach is practical and can enhance productivity and feasibility in software development.
https://www.sciencedirect.com/science/article/pii/S0950584923001386,AGL: Incorporating behavioral aspects into domain-driven design,November 2023,Not Found,Duc-Hanh=Dang: hanhdd@vnu.edu.vn; Duc Minh=Le: duclm20@fe.edu.vn; Van-Vinh=Le: 21028005@vnu.edu.vn,"Abstract
Context:
Domain-driven design (DDD) aims to iteratively develop software around a realistic domain model. Recent research in DDD has been focusing on using annotation-based domain-specific languages (aDSLs) to build the domain model. However, within current approaches behavioral aspects, that are often represented using UML Activity and 
State machine diagrams
, are not explicitly captured in the domain model.
Objective:
The focus of this paper is to introduce a new approach for incorporating behavioral aspects into domain models within the Domain-Driven Design (DDD) approach. The proposed approach involves using a new 
activity graph
 language (
AGL
) as an aDSL for representing behavioral aspects within a unified domain model. This integration of 
AGL
 and the previously developed aDSL (
DCSL
 to represent domain models) aims to achieve three important features of DDD: feasibility, productivity, and 
understandability
.
Method:
Our approach involves building a unified class model in 
DCSL
 within a domain-driven architecture, which uses the annotation attachment feature of the host programming language (such as Java) to attach 
AGL
 activity graphs directly to the activity class of the unified class model, resulting in a unified domain model. In this work, we define the abstract and 
concrete syntax
 of 
AGL
. To demonstrate our method, we use a Java framework called 
jDomainApp
 and evaluate 
AGL
 through a 
case study
 to show that it is expressive and practical for real-world software.
Results:
This paper presents two contributions. Firstly, it proposes a mechanism to include behavioral aspects in a unified domain model by introducing a new aDSL called AGL to represent domain behaviors. Secondly, it presents a unified modeling method for domain-driven software development.
Conclusion:
Our method significantly extends the state-of-the-art in DDD in two important fronts: constructing a unified domain model for both structural and behavioral aspects of domain models and bridging the gaps between model and code.",Information and Software Technology,18 Mar 2025,7,"Similar to abstract 227, this paper introduces a new approach for incorporating behavioral aspects into domain models within the Domain-Driven Design (DDD) approach, contributing to software development practices."
https://www.sciencedirect.com/science/article/pii/S0950584923001428,"The impact of stressors on the relationship between personality traits, knowledge collection behaviour and programmer creativity intention in software engineering",November 2023,"Creativity, Stress, Knowledge collection behaviour, Personality traits, Software engineering, Programmer, Componential theory of creativity",Aamir=Amin: Aamir.amin@port.ac.uk; Mobashar=Rehman: Not Found; Shuib=Basri: Not Found; Luiz Fernando=Capretz: Not Found; Muhammad Awais Shakir=Goraya: Not Found; Rehan=Akbar: Not Found,"Abstract
Context
Individual and contextual factors have a profound impact on an individual's creativity. In the first part of this research, we concluded that, for a programmer's creativity intention, individual factors including big 5 
personality traits
 and knowledge collection behaviour play a key role. However, it is important to bring contextual factors into the model to provide a holistic understanding.
Objectives
Hence, the objective of the present research is to expand the earlier work by (i) identifying the software engineering occupational stressors relevant to programmers, and (ii) examining their impact as moderators for the relationship between individual factors (i.e., 
big five personality traits
 and knowledge collection behaviour) and the creativity intention of the programmer.
Methods
To analyse the moderating impact of 6 stressors, the survey questionnaire was used to collect data from 294 programmers working in software companies in Pakistan. The data were analysed using the 
Structural Equation Modelling
 (SEM) – Partial Least Square (PLS) technique.
Results
The findings revealed that in the presence of a moderate level of stress, the relationship between knowledge collection behaviour and creativity intention was strengthened. Furthermore, stressors interacted differently with different 
personality traits
. An overarching statement could be that most of the stressors positively moderated the relationships between different personality traits and creativity intentions. However, contrary to the prior research, the majority of the stressors negatively affected the impact of the openness to experience trait on creativity intention.
Conclusion
The research significantly contributes to the body of knowledge of behavioural software engineering. The findings of this research are novel and intriguing in many aspects and will benefit software organizations to increase innovation, by increasing programmers’ creativity through mitigating stress. The study is also one of the few studies which have attempted to understand the interaction between individual and contextual factors with a programmer's creativity.",Information and Software Technology,18 Mar 2025,6,"The research on individual and contextual factors impacting a programmer's creativity provides valuable insights for software organizations to increase innovation. However, the focus on stressors may have limited direct practical application for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001374,Overlapping community detection in software ecosystem based on pheromone guided personalized PageRank algorithm,November 2023,Not Found,Simin=Wang: Not Found; Xiangjuan=Yao: yaoxj@cumt.edu.cn; Dunwei=Gong: Not Found; Huijie=Tu: Not Found,"Abstract
Context:
Software ecosystem has aroused the interest of numerous researchers and plays an important role in many aspects. According to different participants and their relationships, software ecosystem can be constructed into various types of complex networks. Overlapping community detection in complex networks can help reveal the community structure and find the intersection between communities. However, existing overlapping community 
detection algorithms
 often suffer from reduced applicability or accuracy when applied to networks in software ecosystems.
Objective:
To reveal the overlapping community structure in software ecosystem, we propose an overlapping community detection algorithm by improving the standard personalized PageRank (PPR) algorithm.
Method:
We first construct a developer collaboration network in software ecosystem based on the intensity of cooperation between developers. Then, the similarity between developers is calculated to guide the walking process of the PPR algorithm, making it suitable for weighted networks. Finally, in the proposed algorithm PGPPR, inspired by the idea of using pheromones to guide the walking in 
ant colony
 algorithm, we run the algorithm in multiple rounds and use the results of the previous round to guide the walking process in the current round to reduce redundant diffusion.
Results:
The experimental results on the five real-world networks show that our algorithm is applicable and effective in detecting communities. And in the five developer collaboration networks, PGPPR can effectively detect overlapping community structures with higher stability and accuracy than the four baselines.
Conclusion:
The PGPPR algorithm can find overlapping community structures in weighted networks and effectively reduce the redundant diffusion generated when applying the standard PPR algorithm to community detection. Compared to other algorithms, our algorithm can detect the overlapping communities more accurately and stably when applied to developer collaboration networks in software ecosystem.",Information and Software Technology,18 Mar 2025,8,"The proposed algorithm for overlapping community detection in software ecosystems addresses a specific challenge in software development. It provides a practical solution for enhancing community structure analysis, which can benefit early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001398,An empirical study on secure usage of mobile health apps: The attack simulation approach,November 2023,Not Found,Bakheet=Aljedaani: abhjedaani@uqu.edu.sa; Aakash=Ahmad: Not Found; Mansooreh=Zahedi: Not Found; Muhammad Ali=Babar: Not Found,"Abstract
Context
Mobile applications (apps) have proven their usefulness in enhancing service provisioning across a multitude of domains that range from smart healthcare, to mobile commerce, and areas of context-sensitive computing. In smart healthcare context, mobile health (mHealth) apps - representing a specific genre of mobile apps that manage health information - face some critical challenges relating to security and privacy of device and user data. In recent years, a number of empirically grounded, survey-based studies have been conducted to investigate secure usage of mHealth apps. However, such studies rely on self-reported behaviors documented via interviews or survey questions that lack practical approaches that can simulate attack scenario for monitoring users’ actions and behaviors while using mHealth apps.
Objective
Our objective was to conduct an empirical study - engaging participants with attack simulation scenarios and analyze their actions - for investigating the security awareness of 
mHealth app
 users.
Method
We simulated some common security attack scenarios in mHealth context and engaged a total of 105 app users to monitor their actions and analyze their behavior. We analyzed users' data with statistical analysis including correlations test, descriptive analysis, and qualitative data analysis (i.e., thematic analysis method).
Results
Our results indicate that whilst the minority of our participants perceived access permissions positively, the majority had negative views. Users provide their consent, granting permissions, without a careful review of privacy policies that leads to undesired or malicious access to health data. Findings also indicated that 73.3% of our participants had denied at least one access permission, and 36% of our participants preferred no authentication method.
Conclusion
The study complements existing research on secure usage of mHealth apps, simulates security threats to monitor users’ actions, and provides empirically grounded guidelines for secure development and usage of mobile health systems.",Information and Software Technology,18 Mar 2025,7,"The study addresses critical security challenges in mHealth apps and provides practical guidelines for secure development, which can have a significant impact on the startup ecosystem."
https://www.sciencedirect.com/science/article/pii/S0950584923001362,Verifying contracts among software components: An ontology-based approach,November 2023,Not Found,Francisco-Edgar=Castillo-Barrera: ecastillo@uaslp.mx; Hector A.=Duran-Limon: hduran@cucea.udg.mx,"Abstract
Context:
The goal of Component-Based Software Engineering (CBSE) is the development of software systems in terms of an assembly of pre-fabricated software components. One of the main aims of CBSE is to increase 
software reuse
 whereby a software component becomes part of different software systems. Verification is an important task that ensures contract conformance among components. However, current techniques for verification of component matching are poorly used in industry due to the fact that the use of these techniques is complex since they require specialized expertise. Also, the use of such techniques can be time-consuming.
Objective:
In this paper, we present Moctezuma, a framework for verifying the matching of software components that does not require the user possessing highly specialized skills and is able to check contract conformance of 
functional semantics
 aspects.
Method:
Our approach relies on a core ontology of software components, which captures the concepts, properties, relationships, requirements, and software component functionality. We make use of 
architecture description languages
 (ADLs) to specify configurations of component interconnections. Interface contracts are specified with a customized version of CORBA-IDL. We employ 
ontology reasoning
 engines to check conformance among interface contracts.
Results:
The accuracy evaluation results have shown that our verifier has a high accuracy for detecting 
semantics errors
. The scalability evaluation shows that our framework exhibits almost a linear behavior.
Conclusions:
It is concluded that our framework is suitable for verifying the conformance of interface contracts, involving semantics aspects, along a configuration of component interconnections.",Information and Software Technology,18 Mar 2025,5,"While the framework presented is useful for verifying software components, the impact on early-stage ventures may be limited due to the specialized nature of the skills required."
https://www.sciencedirect.com/science/article/pii/S0950584923001441,DeKeDVer: A deep learning-based multi-type software vulnerability classification framework using vulnerability description and source code,November 2023,Not Found,Yukun=Dong: dongyk@upc.edu.cn; Yeer=Tang: Not Found; Xiaotong=Cheng: Not Found; Yufei=Yang: Not Found,"Abstract
Context:
Software vulnerabilities have confused software developers for a long time. Vulnerability classification is thus crucial, through which we can know the specific type of vulnerability and then conduct targeted repair. Stack of papers have looked into deep learning-based multi-type vulnerability classification, among which most are based on vulnerability descriptions and some are based on source code. While vulnerability descriptions can sometimes mislead vulnerability classification and source code-based approaches have been rarely explored in multi-type vulnerability classification.
Objective:
We design DeKeDVer (Vulnerability Descriptions and Key Domain based Vulnerability Classifier) with two objectives: (i) to extract more useful information from vulnerability descriptions; (ii) to better utilize the 
information source
 code can reflect.
Method:
In this work, we propose a multi-type vulnerability classifier which combine vulnerability descriptions and source code together. We process vulnerability descriptions and source code of each project separately. For the vulnerability description of a sample, we preprocess it using a specified way we design based on our observations on numerous descriptions and then select text features. After that, Text Recurrent 
Convolutional Neural Network
 (TextRCNN) is applied to learn text information. For source code, we leverage its Code Property Graph (CPG) and extract key domain from it which are then embedded. Acquired feature vectors are then fed into 
Relational Graph
 
Attention Network
 (RGAT). Result vectors gained from TextRCNN and RGAT are combined together as the feature vector of the current sample. A Multi-Layer 
Perceptron
 (MLP) layer is further added to undertake classification.
Results:
We conduct our experiments on C/C++ projects from NVD. Experimental results show that our work achieves 84.49% in weighted F1-measure which proves our work to be more effective.
Conclusion:
Our work utilizes information reflected both from vulnerability descriptions and source code to facilitate vulnerability classification and achieves higher weighted F1-measure than existing vulnerability classification tools.",Information and Software Technology,18 Mar 2025,8,The integration of vulnerability descriptions and source code for classification can greatly benefit startups in identifying and addressing software vulnerabilities effectively.
https://www.sciencedirect.com/science/article/pii/S095058492300143X,Job satisfaction in agile information systems development: A stakeholder perspective,November 2023,Not Found,Veronika=Huck-Fries: veronika.huck-fries@tum.de; Francisca=Nothaft: Not Found; Manuel=Wiesche: Not Found; Helmut=Krcmar: Not Found,"Abstract
Context
Agile 
information systems development
 (ISD) claims to increase employees’ job satisfaction. While previous research acknowledged increased job satisfaction among team members such as software developers, less attention has been paid to stakeholders in agile ISD. Furthermore, we lack evidence about the role of review meetings between team members and stakeholders.
Objective
With the aim to tackle those current shortcomings, we set out to gain a 
deeper understanding
 on how agile ISD practices affect internal stakeholders’ job satisfaction (SJS).
Method
Using a 
mixed methods
 approach, we identify predictors of SJS in an exploratory 
case study
 first. Second, we develop our theoretical model that was evaluated with a survey of agile ISD stakeholders.
Results
Findings of conditional process analysis show that agile practices positively affect SJS via perceived meaningfulness and interaction frequency. Our results provide evidence that collaboration between team members and stakeholders is crucial for enhancing SJS.
Conclusions
We conclude that agile ISD practices have a positive effect on SJS. These findings have several implications for theory and offer a foundation for future research on stakeholders in agile ISD. Practical implications refer to the establishment of agile practices, meaningfulness at work and awareness for agile transformations.",Information and Software Technology,18 Mar 2025,6,"The exploration of how agile ISD practices affect stakeholders' job satisfaction is insightful, but the direct impact on early-stage ventures may be indirect."
https://www.sciencedirect.com/science/article/pii/S0950584923001520,Formal synthesis of neural Craig interpolant via counterexample guided deep learning,November 2023,Not Found,Wang=Lin: Not Found; Mi=Ding: Not Found; Kaipeng=Lin: Not Found; Zuohua=Ding: zouhuading@hotmail.com,"Abstract
Context:
Craig interpolation is a significant and efficient application to formal verification and synthesis. However, there still remains a challenge in the synthesis of Craig interpolation for 
nonlinear theory
.
Objective:
For quantifier-free theories of nonlinear arithmetic, this paper proposes a new approach to generate nonlinear Craig 
interpolants
 represented as 
deep neural networks
.
Method:
The approach exploits a CEGIS framework where a 
learner
 yields a neural candidate interpolant satisfying the interpolant conditions against training data sets, and a 
verifier
 adopts computer algebra methods to confirm the correctness of the candidate or to generate counterexamples for further refining the candidate.
Results:
We implement the tool 
SyntheNI
 based on our CEGIS procedure, and assess the performance against a collection of benchmark examples. The tool 
SyntheNI
 performs better than existing methods in the aspect of the 
iteration number
 and the computational time. As an application, the tool 
SyntheNI
 is used to synthesize loop invariants.
Conclusion:
The 
SyntheNI
 can generate nonlinear Craig 
interpolants
 for quantifier free nonlinear real arithmetic. The experimental evaluation confirms the high performance of our 
synthesis method
.",Information and Software Technology,18 Mar 2025,7,"The development of a tool to generate nonlinear Craig interpolants can streamline formal verification processes, benefiting startups working on complex software systems."
https://www.sciencedirect.com/science/article/pii/S0950584923001520,Formal synthesis of neural Craig interpolant via counterexample guided deep learning,November 2023,Not Found,Wang=Lin: Not Found; Mi=Ding: Not Found; Kaipeng=Lin: Not Found; Zuohua=Ding: zouhuading@hotmail.com,"Abstract
Context:
Craig interpolation is a significant and efficient application to formal verification and synthesis. However, there still remains a challenge in the synthesis of Craig interpolation for 
nonlinear theory
.
Objective:
For quantifier-free theories of nonlinear arithmetic, this paper proposes a new approach to generate nonlinear Craig 
interpolants
 represented as 
deep neural networks
.
Method:
The approach exploits a CEGIS framework where a 
learner
 yields a neural candidate interpolant satisfying the interpolant conditions against training data sets, and a 
verifier
 adopts computer algebra methods to confirm the correctness of the candidate or to generate counterexamples for further refining the candidate.
Results:
We implement the tool 
SyntheNI
 based on our CEGIS procedure, and assess the performance against a collection of benchmark examples. The tool 
SyntheNI
 performs better than existing methods in the aspect of the 
iteration number
 and the computational time. As an application, the tool 
SyntheNI
 is used to synthesize loop invariants.
Conclusion:
The 
SyntheNI
 can generate nonlinear Craig 
interpolants
 for quantifier free nonlinear real arithmetic. The experimental evaluation confirms the high performance of our 
synthesis method
.",Information and Software Technology,18 Mar 2025,8,"The development of a new approach for generating nonlinear Craig interpolants using deep neural networks has practical value for early-stage ventures working in formal verification and synthesis, potentially improving performance in loop invariant synthesis."
https://www.sciencedirect.com/science/article/pii/S0950584923001404,Reflections on Surrogate-Assisted Search-Based Testing: A Taxonomy and Two Replication Studies based on Industrial ADAS and Simulink Models,November 2023,Not Found,Shiva=Nejati: snejati@uottawa.ca; Lev=Sorokin: sorokin@fortiss.org; Damir=Safin: safin@fortiss.org; Federico=Formica: formicaf@mcmaster.ca; Mohammad Mahdi=Mahboob: mahbom2@mcmaster.ca; Claudio=Menghi: claudio.menghi@unibg.it,"Abstract
Context:
Surrogate-assisted search-based testing (SA-SBT) aims to reduce the 
computational time
 for testing compute-intensive systems. Surrogates enhance testing techniques by improving test case generation focusing the testing budget on the most critical portions of the input domain. In addition, they can serve as 
approximations
 of the system under test (SUT) to predict 
test results
 instead of executing the tests on compute-intensive SUTs.
Objective:
This article reflects on the existing SA-SBT techniques, particularly those applied to system-level testing and often facilitated using simulators or complex test beds. Recognizing the diversity of 
heuristic algorithms
 and evaluation methods employed in existing SA-SBT techniques, our objective is to synthesize these differences and present a comprehensive view of SA-SBT solutions. In addition, by critically reviewing our previous work on SA-SBT, we aim to identify the limitations in our proposed algorithms and evaluation methods and to propose potential improvements.
Method:
We present a taxonomy that categorizes and contrasts existing SA-SBT solutions and highlights key research gaps. To identify the evaluation challenges, we conduct two replication studies of our past SA-SBT solutions: One study uses industrial 
advanced driver assistance system
 (ADAS) and the other relies on a 
Simulink
 
model benchmark
. We compare our results with those of the original studies and identify the difficulties in evaluating SA-SBT techniques, including the impact of different contextual factors on results generalization and the validity of our 
evaluation metrics
.
Results:
Based on our taxonomy and replication studies, we propose future research directions, including re-considerations in the current 
evaluation metrics
 used for SA-SBT solutions, utilizing surrogates for 
fault localization
 and repair in addition to testing, and creating frameworks for large-scale experiments by applying SA-SBT to multiple SUTs and simulators.",Information and Software Technology,18 Mar 2025,7,The reflection on existing SA-SBT techniques and the proposal of future research directions can provide valuable insights for startups looking to optimize testing for compute-intensive systems and improve evaluation methods.
https://www.sciencedirect.com/science/article/pii/S0950584923001271,Towards a better understanding of the mechanics of refactoring detection tools,October 2023,Not Found,Jonhnanthan=Oliveira: jonhnanthan@copin.ufcg.edu.br; Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Leopoldo=Teixeira: lmt@cin.ufpe.br; Márcio=Ribeiro: marcio@ic.ufal.br; Osmar=Leandro: osmar@copin.ufcg.edu.br; Baldoino=Fonseca: baldoino@ic.ufal.br,"Abstract
Context:
Refactoring is a crucial practice used by many developers, available in popular IDEs, like 
Eclipse
. Moreover, refactoring detection tools, such as 
RefDiff
 and 
RefactoringMiner
, help improve the comprehension of refactoring application changes.
Objective:
In this article, we better understand to what extent refactoring detection tools (
RefDiff
 and 
RefactoringMiner
) identify 
refactoring operations
 that developers apply in practice.
Methods:
We survey with 53 developers of popular Java projects on GitHub. We asked them to identify six refactoring transformations applied to small programs.
Results:
There is no unanimity in all questions of our survey. Refactoring detection tools do not detect many 
refactoring operations
 expected by developers. In 4 out of 6 questions, most developers prefer the 
Eclipse
 refactoring mechanics.
Conclusion:
The results highlight the importance of diving deep into the refactoring mechanics and defining a baseline. Empirical studies focused on mining refactoring operations may be limited by an incomplete or unrepresentative sample of such operations, thus posing a challenge for researchers in this field.",Information and Software Technology,18 Mar 2025,5,"While the study on refactoring detection tools provides some insights, the practical impact on European early-stage ventures may be limited as it focuses more on the discrepancy between tool detection and developer expectations."
https://www.sciencedirect.com/science/article/pii/S0950584923001271,Towards a better understanding of the mechanics of refactoring detection tools,October 2023,Not Found,Jonhnanthan=Oliveira: jonhnanthan@copin.ufcg.edu.br; Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Leopoldo=Teixeira: lmt@cin.ufpe.br; Márcio=Ribeiro: marcio@ic.ufal.br; Osmar=Leandro: osmar@copin.ufcg.edu.br; Baldoino=Fonseca: baldoino@ic.ufal.br,"Abstract
Context:
Refactoring is a crucial practice used by many developers, available in popular IDEs, like 
Eclipse
. Moreover, refactoring detection tools, such as 
RefDiff
 and 
RefactoringMiner
, help improve the comprehension of refactoring application changes.
Objective:
In this article, we better understand to what extent refactoring detection tools (
RefDiff
 and 
RefactoringMiner
) identify 
refactoring operations
 that developers apply in practice.
Methods:
We survey with 53 developers of popular Java projects on GitHub. We asked them to identify six refactoring transformations applied to small programs.
Results:
There is no unanimity in all questions of our survey. Refactoring detection tools do not detect many 
refactoring operations
 expected by developers. In 4 out of 6 questions, most developers prefer the 
Eclipse
 refactoring mechanics.
Conclusion:
The results highlight the importance of diving deep into the refactoring mechanics and defining a baseline. Empirical studies focused on mining refactoring operations may be limited by an incomplete or unrepresentative sample of such operations, thus posing a challenge for researchers in this field.",Information and Software Technology,18 Mar 2025,5,"Similar to abstract 238, the study on refactoring detection tools lacks direct practical implications for European early-stage ventures, as it mainly discusses the challenges in mining refactoring operations."
https://www.sciencedirect.com/science/article/pii/S0950584923001180,Use of personas in Requirements Engineering: A systematic mapping study,October 2023,Not Found,Devi=Karolita: devi.karolita@monash.edu; Jennifer=McIntosh: jennifer.mcintosh@unimelb.edu.au; Tanjila=Kanij: tanjila.kanij@monash.edu; John=Grundy: john.grundy@monash.edu; Humphrey O.=Obie: humphrey.obie@monash.edu,"Abstract
Context:
Requirements Engineering
 (RE) is one of the crucial activities in software development that requires a high involvement of humans (i.e., stakeholders). The aim of RE-related tasks is to develop the scope of the target software products to ensure they will fulfil its stakeholder needs. In RE, the requirements engineers have to deeply understand the software stakeholders including their needs, motivations, and goals. Attaining this information directly from stakeholders requires regular interaction which needs considerable effort. The persona, as a user representation, is a useful tool that can reduce effort amount by modelling the software users and being the primary source of information.
Objective:
The aim of this work is to systematically review relevant studies that have investigated the use of personas in RE, the benefits of personas, and challenges during the implementation of personas in RE.
Method:
We conduct a 
systematic mapping study
 (SMS) using a formal protocol based on an established guideline. The systematic search result in a total of 904 publications from six databases. After filtering, we select 78 relevant studies for critical appraisal, analysis, synthesis, and reporting.
Results:
We identify methods to create and validate personas (mostly qualitative), map the benefits of using personas in RE (to ensure stakeholders’ satisfaction, support a human-centric RE, and support requirements engineers’ tasks and roles in RE), identify methods used with personas, discover challenges during persona incorporation in RE and their respective 
mitigation strategies
, and recommend potential strategies for unaddressed challenges. We also make recommendations for future research directions.
Conclusion:
The findings of this SMS will help RE researchers and practitioners better understand the use of personas in RE and highlights key research gaps for future research.",Information and Software Technology,18 Mar 2025,7,"The systematic mapping study on the use of personas in RE can offer valuable insights for startups involved in software development, particularly in understanding stakeholders' needs and motivations through personas."
https://www.sciencedirect.com/science/article/pii/S0950584923001210,Antecedents of psychological safety in agile software development teams,October 2023,"Agile software development, Psychological safety, Mixed-methods",Adam=Alami: adal@cs.aau.dk; Mansooreh=Zahedi: Not Found; Oliver=Krancher: Not Found,"Abstract
Context:
Psychological safety continues to inspire researchers’ curiosity in various fields of study. It has been shown to enhance teams’ performance, efficiency, and learning, among other corollaries. Researchers are stretching the boundaries of these early findings to identify further effects of psychological safety. Recent work shows that psychological safety promotes 
knowledge sharing
, norm clarity, and complements agile values.
Objective:
Studies show that psychological safety enhance agile values and practices, and some practitioners went as far as to claim “agile doesn’t work without psychological safety.” Yet, researchers have not explored its antecedents. In this study, we sought to understand how psychological safety materializes in 
agile software development
 teams.
Method:
We opted for a two-phase mixed-methods study; an exploratory qualitative phase (18 interviews) followed by a quantitative phase (survey study, N = 365) to broaden the empirical coverage and test phase one’s findings.
Results:
Our findings show that psychological safety is established in agile software teams when individuals, the team, and the leadership adopt and promote strategies conducive to promoting a psychologically safe workplace. While openness and no blame towards team members are the “butter and bread” of psychological safety, collective-decision making within the team and the leadership ownership remain the pillars of a psychologically safe workplace. Conversely, team 
autonomy
, technical practices providing a safety net and slack time were not found to promote psychological safety.
Conclusion:
To institutionalize psychological safety in agile software teams, individuals, teams, and the leadership should consolidate their effort to adopt no blame, openness, collective decision-making in the team, and assuming the ownership of promoting a psychologically safe workplace.",Information and Software Technology,18 Mar 2025,8,"The study provides valuable insights on how to promote psychological safety in agile software teams, which can greatly impact the performance and efficiency of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001210,Antecedents of psychological safety in agile software development teams,October 2023,"Agile software development, Psychological safety, Mixed-methods",Adam=Alami: adal@cs.aau.dk; Mansooreh=Zahedi: Not Found; Oliver=Krancher: Not Found,"Abstract
Context:
Psychological safety continues to inspire researchers’ curiosity in various fields of study. It has been shown to enhance teams’ performance, efficiency, and learning, among other corollaries. Researchers are stretching the boundaries of these early findings to identify further effects of psychological safety. Recent work shows that psychological safety promotes 
knowledge sharing
, norm clarity, and complements agile values.
Objective:
Studies show that psychological safety enhance agile values and practices, and some practitioners went as far as to claim “agile doesn’t work without psychological safety.” Yet, researchers have not explored its antecedents. In this study, we sought to understand how psychological safety materializes in 
agile software development
 teams.
Method:
We opted for a two-phase mixed-methods study; an exploratory qualitative phase (18 interviews) followed by a quantitative phase (survey study, N = 365) to broaden the empirical coverage and test phase one’s findings.
Results:
Our findings show that psychological safety is established in agile software teams when individuals, the team, and the leadership adopt and promote strategies conducive to promoting a psychologically safe workplace. While openness and no blame towards team members are the “butter and bread” of psychological safety, collective-decision making within the team and the leadership ownership remain the pillars of a psychologically safe workplace. Conversely, team 
autonomy
, technical practices providing a safety net and slack time were not found to promote psychological safety.
Conclusion:
To institutionalize psychological safety in agile software teams, individuals, teams, and the leadership should consolidate their effort to adopt no blame, openness, collective decision-making in the team, and assuming the ownership of promoting a psychologically safe workplace.",Information and Software Technology,18 Mar 2025,8,The research on psychological safety in agile software teams can have a significant impact on the success of early-stage ventures by highlighting the importance of team dynamics.
https://www.sciencedirect.com/science/article/pii/S0950584923001209,DLRegion: Coverage-guided fuzz testing of deep neural networks with region-based neuron selection strategies,October 2023,Not Found,Chuanqi=Tao: taochuanqi@nuaa.edu.cn; Yali=Tao: Not Found; Hongjing=Guo: Not Found; Zhiqiu=Huang: Not Found; Xiaobing=Sun: Not Found,"Abstract
Context:
Deep Learning
 (DL) systems have been increasingly applied to safety-critical scenarios, such as autonomous driving or medical diagnoses. However, it often exhibits erroneous behaviors that can cause serious consequences. Therefore, it is important to ensure the quality of the DL systems through systematically testing. Recent 
research works
 have proposed many testing techniques for deep 
neural networks
 (DNNs), among which the coverage-guided fuzz testing has achieved remarkable results. The neuron selection strategy is the key ingredient of the technique. It can affect the technique’s effectiveness. However, current neuron selection strategies did not utilize the output distribution of each neuron on the training data, which can characterize the behavior of the neuron.
Objective:
This paper introduces 
DLRegion
, a coverage-guided fuzz testing technique of DNNs with region-based neuron selection strategies. 
DLRegion
 can expose erroneous behaviors of DNNs while maximizing coverage.
Methods:
DLRegion
 first incorporates a seed selection strategy based on the level of confidence in the classification of the inputs to select seed inputs to mutate. 
DLRegion
 also proposes region-based neuron selection strategies, which utilize the region where the output value of each neuron is in its output distribution to select valuable neurons to activate for covering more internal states.
Results:
Empirical studies on three well-known datasets, guided by five existing criteria demonstrate that: (1) the proposed seed selection strategy very effectively improves coverage and 
defect detection
; (2) selecting neurons in different regions has obvious differences in achieving model coverage and detecting defects; (3) 
DLRegion
 outperforms other existing techniques by coverage under different criteria and effectively identifies the quantity and diversity of defects; (4) the effectiveness of 
DLRegion
 in improving the model robustness.
Conclusion:
DLRegion
 not only can cover more internal logic of the DNNs but also can effectively detect DNNs’ erroneous behaviors. Moreover, 
DLRegion
 can improve the robustness of the model.",Information and Software Technology,18 Mar 2025,10,"The DLRegion technique addresses a critical need in the field of deep learning by improving testing techniques for DNNs, which is highly relevant for European early-stage ventures utilizing AI technologies."
https://www.sciencedirect.com/science/article/pii/S0950584923001258,METHODS: A meta-path-based method for heterogeneous community detection in the open source software ecosystem,October 2023,Not Found,Qing=Qi: qi_ng616@sjtu.edu.cn; Jian=Cao: cao-jian@sjtu.edu.cn,"Abstract
Detecting communities in the 
open source software
 (OSS) ecosystem can help understand the collaborations in the 
open source software
 ecosystem and promote an understanding of the dynamics of the ecosystem. However, most existing community detection methods are designed for homogeneous networks, whereas the OSS ecosystem is a 
heterogeneous network
. Therefore, we propose a meta-path-based method for heterogeneous community detection in the OSS ecosystem (METHODS). METHODS comprises four steps. Firstly, a heterogeneous information network is constructed based on meta-paths. Secondly, the Canopy algorithm is used to obtain the number of initial communities. Thirdly, the skip-gram model is used to identify seed nodes for community detection. Finally, METHODS detects heterogeneous communities around the seed nodes. By defining a series of 
evaluation metrics
 and verifying these on GitHub datasets, METHODS achieves the best performance of all the other methods. Moreover, the 
case studies
 on GitHub also shows METHODS can discover latent communities whose members are potentially relevant.",Information and Software Technology,18 Mar 2025,6,"The community detection method for OSS ecosystem, while valuable, may have less immediate impact on European early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001246,Detection and filling of functional holes in microservice systems: Method and infrastructure support,October 2023,Not Found,Zihang=Su: zhsu@stu.hit.edu.cn; Xiang=He: hexiang@hit.edu.cn; Teng=Wang: 21S003068@stu.hit.edu.cn; Lei=Liu: liulei@stu.hit.edu.cn; Zhiying=Tu: tzy_hit@hit.edu.cn; Zhongjie=Wang: rainy@hit.edu.cn,"Abstract
With the widespread use of 
microservices
 technology, a growing number of Microservice Systems (MSS) have emerged. 
Monolithic applications
 are divided into several small and independent 
microservices
 that provide functions through APIs. These microservices can be orchestrated by service composition to satisfy various user requirements: microservice functionalities are aggregated into coarse-grained solutions to provide 
composite functions
 to users. However, although various service composition approaches have been presented in many works of literature, they failed to solve the situation that no 
feasible solutions
 can be found because of missing functions in MSS, named 
Functional Hole (FH)
. As a result, the system cannot satisfy user requirements and faces 
Quality of Service
 (QoS) declines. In this paper, to reduce the impact of missing functions, we defined the FH to describe the absence of functions for MSS when it cannot satisfy user requirements. Moreover, for the first time, we proposed the 
Detection and Filling Problem of FH (DFPFH)
 in a running MSS. A three-phase algorithm with 
supporting infrastructure
 was developed to solve DFPFH. It detects FHs based on hypergraphs, fills FHs with services from an external service system at runtime, and generates suggestions for thoroughly filling FH to developers. Plenty of experiments were conducted, and the results validate our approaches’ usability, effectiveness, and performance.",Information and Software Technology,18 Mar 2025,7,"The proposed solution for detecting and filling Functional Holes in Microservice Systems can benefit early-stage ventures using microservices, although the impact may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001234,PAREI: A progressive approach for Web API recommendation by combining explicit and implicit information,October 2023,Not Found,Ye=Wang: Not Found; Aohui=Zhou: Not Found; Qiao=Huang: qiaohuang@zjgsu.edu.cn; Xiaoyang=Wang: Not Found; Bo=Jiang: Not Found,"Abstract
Context:
Mashup is an application with specific functions by combining Web APIs that can provide services or data on the Internet, thus avoiding the behavior of repeatedly building wheels. Recommending suitable Web APIs in the vast number of Web APIs on the Internet for Mashup developers has become a challenging problem. Previous studies often fail to fully exploit and effectively synthesize various types of information between Web APIs and Mashups.
Objective:
This work proposes a Web API recommendation approach - PAREI by combining both explicit and implicit information to progressively optimize the recommendation results.
Methods:
First, PAREI uses the explicit structural information between Mashups and Web APIs to construct the Call Relationship Network (CRN). Second, PAREI calculates explicit semantic similarities between developer’s requirement and Mashups to obtain candidate Mashup nodes in CRN. Then PAREI further mines the implicit structural information between Mashups. A combined similarity score for each Mashup node is calculated. Finally, PAREI uses CRN to obtain candidate Web APIs related to candidate Mashup nodes, and integrates implicit semantic information of Web APIs with combined scores of corresponding Mashups, so as to obtain Top-K Web APIs.
Results:
Comparison experiments show that PAREI has significantly improved the Recall, Precision, and MAP metrics compared with other approaches. Ablation experiments show that different types of information play various roles in Web API recommendation, and different combination modes have different effects on the recommendation results.
Conclusion:
This work constructs the PAREI model, which combines explicit and implicit information to obtain Web API recommendation results through a progressive strategy. According to the experiment results, we believe that the PAREI approach can help Mashup developers to find demanded Web APIs rapidly and accurately.",Information and Software Technology,18 Mar 2025,8,"The PAREI model proposes a novel approach to Web API recommendation, showing significant improvements in metrics. This can greatly benefit early-stage ventures by facilitating the rapid and accurate discovery of demanded Web APIs, enhancing efficiency and productivity."
https://www.sciencedirect.com/science/article/pii/S095058492300126X,Learning test-mutant relationship for accurate fault localisation,October 2023,Not Found,Jinhan=Kim: Not Found; Gabin=An: Not Found; Robert=Feldt: Not Found; Shin=Yoo: shin.yoo@kaist.ac.kr,"Abstract
Context:
Automated fault localisation aims to assist developers in the task of identifying the root cause of the fault by narrowing down the space of likely fault locations. Simulating variants of the faulty program called mutants, several Mutation Based Fault Localisation (MBFL) techniques have been proposed to automatically locate faults. Despite their success, existing MBFL techniques suffer from the cost of performing mutation analysis after the fault is observed.
Method:
To overcome this shortcoming, we propose a new MBFL technique named SIMFL (Statistical Inference for Mutation-based Fault Localisation). SIMFL localises faults based on the past results of mutation analysis that has been done on the earlier version in the project history, allowing developers to make predictions on the location of incoming faults in a just-in-time manner. Using several statistical inference methods, SIMFL models the relationship between test results of the mutants and their locations, and subsequently infers the location of the current faults.
Results:
The empirical study on 
Defects4J
 dataset shows that SIMFL can localise 113 faults on the first rank out of 224 faults, outperforming other MBFL techniques. Even when SIMFL is trained on the predicted kill matrix, SIMFL can still localise 95 faults on the first rank out of 194 faults. Moreover, removing redundant mutants significantly improves the 
localisation accuracy
 of SIMFL by the number of faults localised at the first rank up to 51.
Conclusion:
This paper proposes a new MBFL technique called SIMFL, which exploits ahead-of-time mutation analysis to localise current faults. SIMFL is not only cost-effective, as it does not need a mutation analysis after the fault is observed, but also capable of localising faults accurately.",Information and Software Technology,18 Mar 2025,9,"SIMFL introduces a cost-effective and accurate MBFL technique, outperforming existing methods in fault localization. This can have a substantial impact on startups by saving time and resources in identifying faults, leading to more efficient development processes."
https://www.sciencedirect.com/science/article/pii/S0950584923001283,BugRadar: Bug localization by knowledge graph link prediction,October 2023,Not Found,Xi=Xiao: xiaox@sz.tsinghua.edu.cn; Renjie=Xiao: xrj20@mails.tsinghua.edu.cn; Qing=Li: liq@pcl.ac.cn; Jianhui=Lv: lvjh@pcl.ac.cn; Shunyan=Cui: cuisy@gditsec.org.cn; Qixu=Liu: liuqixu@iie.ac.cn,"Abstract
Context
: Information Retrieval-based Bug Localization (IRBL) aims to design automatic systems that find buggy files according to 
bug reports
, which can reduce the time consumption to fix bugs for programmers. There has been extensive research on IRBL techniques in recent years. However, these methods cannot make full use of the structure information in 
bug reports
 and source files.
Objective
: In this paper, we propose a novel scheme BugRadar. It combines text features and structure features from bug reports and source files for bug localization. Especially, BugRadar leverages a 
knowledge graph
 to make use of structure features.
Method
: We originally propose a 
knowledge graph
 named TriGraph based on structure features and apply hyperbolic attention embedding to get the link prediction scores. For text features, we propose Partial Text Similarity which improves traditional Text Similarity and Method Level Text Similarity. We also propose Word Collaborative Filtering Score which leverages historical bug reports with more attention on important terms. Finally, we calculate the final suspicious scores based on the structure features, text features, and fixing time information from bug fixing history with a 
neural network
.
Results
: We apply our scheme to four projects (Tomcat, SWT, JDT, and Birt) in a popular dataset and get approving results. BugRadar gets better results than other state-of-the-art methods on three projects out of the four. It achieves a relative improvement of 8.8% in SWT and 9.8% in JDT for 
Mean Average Precision
 compared to the previous best scheme KGBugLocator and 11.4% in Birt compared to Adaptive Regression.
Conclusions
: BugRadar can achieve approving performance on large-scale projects with enough historical bug reports. It verifies that knowledge graphs are capable of representing the structure features for bug localization. The novel Partial Text Similarity and Word Collaborative Filtering Score are both effective improvements for using text features.",Information and Software Technology,18 Mar 2025,10,"BugRadar presents a comprehensive scheme for bug localization, achieving superior results compared to state-of-the-art methods. Its effectiveness in improving Mean Average Precision and leveraging knowledge graphs can provide significant value to startups in enhancing bug-fixing efficiency and code quality."
https://www.sciencedirect.com/science/article/pii/S0950584923001350,Robustness assessment of hyperspectral image CNNs using metamorphic testing,October 2023,Not Found,Rached=Bouchoucha: rached.bouchoucha@polymtl.ca; Houssem Ben=Braiek: houssem.ben-braiek@polymtl.ca; Foutse=Khomh: foutse.khomh@polymtl.ca; Sonia=Bouzidi: sonia.bouzidi@insat.rnu.tn; Rania=Zaatour: rania.zaatour@fst.utm.tn,"Abstract
Remote sensing
 has proven its utility in many critical domains, such as medicine, military, and ecology. Recently, we have been witnessing a surge in the adoption of deep learning (DL) techniques by the remote sensing community. DL-based classifiers, such as convolutional neural networks (CNNs), have been reported to achieve impressive predictive performances reaching 99% of accuracy when applied to 
hyperspectral images
 (HSIs), a high-dimensional type of remote sensing data. However, these deep learners are known to be highly sensitive to even slight perturbations of their high-dimensional raw inputs. In real-world contexts, concerns can be raised about how robust they really are against corner-case scenarios. When HSI classifiers are applied in safety–critical applications, ensuring an adequate level of robustness is crucial to prevent unexpected system behaviors. Yet, there are few studies dealing with their robustness, nor are RGB-testing methods able to cover the HSI-specific challenges. This led us to propose a systematic testing method to assess the robustness of the CNNs trained to classify HSIs. First, we elaborate domain-specific metamorphic transformations that simulate naturally-occurring distortions of remote sensing HSIs. Then, we leverage metaheuristic search algorithms to optimize the fitness of synthetically-distorted inputs to stress the weaknesses of the on-testing CNN, while remaining in compliance with domain expert requirements, in order to preserve the semantic of the generated inputs. Relying on our metamorphic testing method, we assess the robustness of established and novel CNNs for 
HSI classification
, and demonstrate their failure, on average, in 25% of the produced test cases. Furthermore, we fine-tuned the tested CNNs on training data augmented with these failure-revealing metamorphic transformations. Results show that the fined-tuning successfully fixed at least 90% of the CNN weaknesses, with less than 1% of degradation in the original predictive performance, outperforming the common iterative gradient-based adversarial attack, namely, Projected Gradient Descent (PGD).",Information and Software Technology,18 Mar 2025,9,The systematic testing method proposed for assessing the robustness of CNNs trained for HSI classification is innovative and crucial for ensuring reliable performance in safety-critical applications. The findings can benefit startups by enhancing the trustworthiness and resilience of their remote sensing solutions.
https://www.sciencedirect.com/science/article/pii/S0950584923001222,A survey on dataset quality in machine learning,October 2023,"Dataset, Dataset quality, Machine Learning",Youdi=Gong: Not Found; Guangzhen=Liu: Not Found; Yunzhi=Xue: Not Found; Rui=Li: Not Found; Lingzhong=Meng: lingzhong@iscas.ac.cn,"Abstract
With the rise of big data, the quality of datasets has become a crucial factor affecting the performance of 
machine learning
 models. High-quality datasets are essential for the realization of data value. This survey article summarizes the research direction of dataset quality in machine learning, including the definition of related concepts, analysis of quality issues and risks, and a review of dataset quality dimensions and metrics throughout the dataset lifecycle and a review of dataset quality metrics analyzed from a dataset lifecycle perspective and summarized in literatures. Furthermore, this article introduces a comprehensive quality evaluation process, which includes a framework for dataset quality evaluation with dimensions and metrics, computation methods for quality metrics, and assessment models. These studies provide valuable guidance for evaluating dataset quality in the field of machine learning, which can help improve the accuracy, efficiency, and 
generalization ability
 of machine learning models, and promote the development and application of 
artificial intelligence
 technology.",Information and Software Technology,18 Mar 2025,7,"While the survey article provides valuable insights into dataset quality evaluation in machine learning, its practical impact on startups may be more indirect. The guidance offered can help startups improve the performance and generalization ability of their models, but the direct application may vary."
https://www.sciencedirect.com/science/article/pii/S0950584923001179,Metamorphic testing of chess engines,October 2023,"Software testing, Metamorphic testing, Chess engines",Manuel=Méndez: manumend@ucm.es; Miguel=Benito-Parejo: mibeni01@ucm.es; Alfredo=Ibias: aibias@ucm.es; Manuel=Núñez: manuelnu@ucm.es,"Abstract
Context:
Chess engines are computer programs that analyse chess positions. The goal of this analysis is to decide which player has an advantage and evaluate how big the advantage is. Using this analysis, chess engines are really powerful players who can consistently beat the best (human) players. Even though these programs are fantastic players, we cannot be sure that the code is fault free because it is very difficult to test them. In particular, we 
face
 the oracle problem: if the chess engine plays better than any potential tester, how can a tester claim that a certain evaluation is wrong or that a suggested move is not the best one?
Objective:
The main goal of our work is to provide a metamorphic testing tool to evaluate chess engines. In particular, we are interested in looking for inconsistent behaviours in the best publicly available chess engine, 
Stockfish
, but we would also like to consider other chess engines.
Methods:
We developed a metamorphic testing solution to validate chess engines. First, we defined metamorphic relations that might reveal inconsistent behaviours. The underlying idea was that the evaluation of 
related
 positions should be the same. For example, if we consider a position and rotate all the pieces with respect to the central axis, then both positions should have the same evaluation. One of our main priorities was to have a fully automatised tool. 
Source inputs
 are obtained from available datasets while 
follow-up inputs
 are automatically computed by applying sound transformations to the source inputs with respect to the corresponding metamorphic rule. In order to assess the usefulness of our work, we applied it to analyse a dataset with more than 40,000 positions.
Results:
Empirical evidence validates the usefulness of our work to analyse the best available chess engine, Stockfish. Our tool revealed non-negligible deviations from the expected behaviour in Stockfish for all the MRs. Additional experiments showed that our tool can be easily used to analyse other chess engines such as Komodo, Houdini and Gull.
Conclusion:
The experiments demonstrate the usefulness of our approach to identify issues in the latest version of the widely recognised to be the best chess engine: Stockfish (version 15, released in April 2022). Our tool is flexible and can be easily extended with metamorphic relations that can be defined in the future by either us or other users. Since all our metamorphic relations are implemented and the code is freely available, users can use them as a pattern to implement new relations.",Information and Software Technology,18 Mar 2025,7,The abstract presents a practical tool to evaluate chess engines which can be beneficial for startups or ventures in developing AI-powered applications.
https://www.sciencedirect.com/science/article/pii/S0950584923001052,"Retrieving arXiv, SocArXiv, and SSRN metadata for initial review screening",September 2023,Not Found,Rubia=Fatima: rubiafatima91@hotmail.com; Affan=Yasin: affan.yasin@tsinghua.edu.cn; Lin=Liu: linliu@tsinghua.edu.cn; Jianmin=Wang: jimwang@tsinghua.edu.cn; Wasif=Afzal: wasif.afzal@mdh.se,"Abstract
Context:
Researchers around the globe invest a lot of time searching the literature for performing reviews (Systematic Literature Review (SLR), Multivocal Literature Review (MLR)). The steps to performing the review includes inclusion of the grey literature, preprints, and quality assessed non-peer reviewed literature (the purpose is to minimize the publication bias). The 
initial screening
 of the papers takes time and 
bibliographic information
 is only available online for the researcher(s).
Objective:
Objective of our study is to propose, design, and develop a method that will help the research community to download the basic information of the papers (title, abstract, author) for the searched query from arxiv, SSRN, and SocArxiv (Social Science ArXiv).
Method:
We used Web scraping to extract data from the servers and save it in excel file. To retrieve the desired query from the databases, a Python code is used. Two methods have been discussed in the study to download the metadata of the searched query.
Results:
We have used different queries (such as “grey literature”, “testing software”, and “python” etc.) to see the results of our proposed method. Furthermore, we cross-verified the results with the online search results of the databases.
Conclusion:
Initial results from the preliminary pilot evaluations show that it is a viable method to search, download, and shortlist the research articles information (title, abstract etc.) from arXiv,
1
 SSRN,
2
 and SocArXiv.
3
 For external validity more evaluations are needed.",Information and Software Technology,18 Mar 2025,5,"The abstract discusses a method for downloading research papers, which may have limited impact on early-stage ventures or startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001064,Case study identification: A trivial indicator outperforms human classifiers,September 2023,"Case study, Evaluation, Systematic review, Primary study, Smell indicator",Austen=Rainer: Not Found; Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Context:
The definition and term “case study” are not being applied consistently by 
software engineering
 researchers. We previously developed a trivial “smell indicator” to help detect the misclassification of primary studies as case studies.
Objective:
To evaluate the performance of the indicator.
Methods:
We compare the performance of the indicator against human classifiers for three datasets, two datasets comprising classifications by both authors of systematic literature studies and primary studies, and one dataset comprising only primary-study author classifications.
Results:
The indicator outperforms the human classifiers for all datasets.
Conclusions:
The indicator is successful because human classifiers “fail” to properly classify their own, and others’, primary studies. Consequently, reviewers of primary studies and authors of systematic literature studies could use the classifier as a “sanity” check for primary studies. Moreover, authors might use the indicator to double-check how they classified a study, as part of their analysis, and prior to submitting their manuscript for publication. We challenge the research community to both beat the indicator, and to improve its ability to identify true case studies.",Information and Software Technology,18 Mar 2025,6,"The abstract introduces an indicator to evaluate case studies, which could be useful for startups in conducting research or analysis."
https://www.sciencedirect.com/science/article/pii/S0950584923001143,Counter-terrorism in cyber–physical spaces: Best practices and technologies from the state of the art,September 2023,"Internet of Things, Cyber physical spaces, Public spaces, Cyber threat intelligence, Topic modeling, Topological data analysis, Protection public spaces, Smart city",Giuseppe=Cascavilla: g.cascavilla@tue.nl; Damian A.=Tamburri: d.a.tamburri@tue.nl; Francesco=Leotta: leotta@diag.uniroma1.it; Massimo=Mecella: mecella@diag.uniroma1.it; WillemJan=Van Den Heuvel: w.j.a.m.v.d.heuvel@jads.nl,"Abstract
Context:
The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years. We envision with the proposed cyber–physical systems and spaces, a city that would indeed become a smarter 
urbanistic
 object, proactively providing alerts and being protective against any threat.
Objectives:
This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber–physical systems, hence aimed at the protection of cyber–physical spaces. Hence, provide guidelines to 
law enforcement agencies
 and practitioners providing a description of technologies and best practices for the protection of public spaces.
Methods:
We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022.
Results:
(a) There is no one single bullet-proof solution available for the protection of public spaces. (b) From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field. (c) While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and 
industry
.
Conclusion:
The overall analysis has led us to state that there is no one 
single solution
 available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces. The techniques range from 
architectural design
 to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as 
AI
 and predictive surveillance.",Information and Software Technology,18 Mar 2025,6,"The abstract addresses the protection of urban areas using cyber-physical systems, offering insights that could be valuable for startups focusing on security technologies."
https://www.sciencedirect.com/science/article/pii/S0950584923001143,Counter-terrorism in cyber–physical spaces: Best practices and technologies from the state of the art,September 2023,"Internet of Things, Cyber physical spaces, Public spaces, Cyber threat intelligence, Topic modeling, Topological data analysis, Protection public spaces, Smart city",Giuseppe=Cascavilla: g.cascavilla@tue.nl; Damian A.=Tamburri: d.a.tamburri@tue.nl; Francesco=Leotta: leotta@diag.uniroma1.it; Massimo=Mecella: mecella@diag.uniroma1.it; WillemJan=Van Den Heuvel: w.j.a.m.v.d.heuvel@jads.nl,"Abstract
Context:
The demand for protection and security of physical spaces and urban areas increased with the escalation of terroristic attacks in recent years. We envision with the proposed cyber–physical systems and spaces, a city that would indeed become a smarter 
urbanistic
 object, proactively providing alerts and being protective against any threat.
Objectives:
This survey intend to provide a systematic multivocal literature survey comprised of an updated, comprehensive and timely overview of state of the art in counter-terrorism cyber–physical systems, hence aimed at the protection of cyber–physical spaces. Hence, provide guidelines to 
law enforcement agencies
 and practitioners providing a description of technologies and best practices for the protection of public spaces.
Methods:
We analyzed 112 papers collected from different online sources, both from the academic field and from websites and blogs ranging from 2004 till mid-2022.
Results:
(a) There is no one single bullet-proof solution available for the protection of public spaces. (b) From our analysis we found three major active fields for the protection of public spaces: Information Technologies, Architectural approaches, Organizational field. (c) While the academic suggest best practices and methodologies for the protection of urban areas, the market did not provide any type of implementation of such suggested approaches, which shows a lack of fertilization between academia and 
industry
.
Conclusion:
The overall analysis has led us to state that there is no one 
single solution
 available, conversely, multiple methods and techniques can be put in place to guarantee safety and security in public spaces. The techniques range from 
architectural design
 to rethink the design of public spaces keeping security into account in continuity, to emerging technologies such as 
AI
 and predictive surveillance.",Information and Software Technology,18 Mar 2025,6,"Similar to abstract 254, this abstract also discusses the protection of public spaces using cyber-physical systems, providing relevant information for startups in the security industry."
https://www.sciencedirect.com/science/article/pii/S0950584923000964,An approach for modeling the operational requirements of FaaS applications for optimal deployment,September 2023,Not Found,Benedikt=Sigurleifsson: Not Found; Nafisa=Ahmed: nafisa.abdelmutalab-ali-ahmed@polymtl.ca; Alexandre=Verdet: Not Found; Mohammad=Hamdaqa: Not Found; Mohamed=Sabri: Not Found; Isael=Pelletier: Not Found,"Abstract
FaaS can extend cloud capabilities to local edge devices. They enable composing applications into workflows and distributing their processes between the edge and the cloud. When deploying a FaaS application, the key issue is to define the desired 
operational requirements
 and find a configuration that satisfies them. Both problems are complex and subjective as they differ from one application to another depending on the application structure, run-time model, and optimization requirements. We address these issues using a general multi-criteria optimization approach based on a fuzzy 
analytical hierarchy process
 (AHP). Firstly, the specialists intuitively specify their fuzzy privacy in terms of 
data locality
, cost, and performance requirements. Then, a Fuzzy AHP model is constructed to compare and select the optimal workflow configuration that satisfies the requirements. The work is evaluated using a real FaaS application, where 
AWS
 Cloud and 
AWS
 Greengrass present the cloud and the edge. We assessed the ability of the proposed approach to retrieve the optimal configurations for various scenarios and compared the results to one of the state-of-the-art approaches. Unlike existing hard-coded approaches, our approach is intuitive, modular, extensible, and addresses more 
DevOps
 requirements.",Information and Software Technology,18 Mar 2025,8,"The abstract addresses practical issues related to FaaS applications, providing a new approach based on fuzzy analytical hierarchy process. The real-world evaluation and comparison with existing approaches add value to European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000964,An approach for modeling the operational requirements of FaaS applications for optimal deployment,September 2023,Not Found,Benedikt=Sigurleifsson: Not Found; Nafisa=Ahmed: nafisa.abdelmutalab-ali-ahmed@polymtl.ca; Alexandre=Verdet: Not Found; Mohammad=Hamdaqa: Not Found; Mohamed=Sabri: Not Found; Isael=Pelletier: Not Found,"Abstract
FaaS can extend cloud capabilities to local edge devices. They enable composing applications into workflows and distributing their processes between the edge and the cloud. When deploying a FaaS application, the key issue is to define the desired 
operational requirements
 and find a configuration that satisfies them. Both problems are complex and subjective as they differ from one application to another depending on the application structure, run-time model, and optimization requirements. We address these issues using a general multi-criteria optimization approach based on a fuzzy 
analytical hierarchy process
 (AHP). Firstly, the specialists intuitively specify their fuzzy privacy in terms of 
data locality
, cost, and performance requirements. Then, a Fuzzy AHP model is constructed to compare and select the optimal workflow configuration that satisfies the requirements. The work is evaluated using a real FaaS application, where 
AWS
 Cloud and 
AWS
 Greengrass present the cloud and the edge. We assessed the ability of the proposed approach to retrieve the optimal configurations for various scenarios and compared the results to one of the state-of-the-art approaches. Unlike existing hard-coded approaches, our approach is intuitive, modular, extensible, and addresses more 
DevOps
 requirements.",Information and Software Technology,18 Mar 2025,8,"Similar to abstract 256, this abstract also tackles important issues concerning FaaS applications and offers a new approach with real-world evaluation. The emphasis on addressing more DevOps requirements is beneficial for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923001076,The yea-paradox: Cognitive bias in technology acceptance surveys,September 2023,Not Found,Raffaele F=Ciriello: raffaele.ciriello@sydney.edu.au; Sebastian=Loss: Not Found,"Abstract
Context
Technology acceptance is widely regarded as one of the most established, mature, and continuously relevant streams of 
information systems
 research. Its fundamental assumption is that technology acceptance scores accurately reflect users’ intentions, needs, and emotions toward the technology of interest. However, many studies (including this one) present evidence that challenges this assumption, suggesting that the link between reported scores and actual perception is more complex than often assumed.
Objective
We aim to find out how cognitive biases influence self-reported technology acceptance scores.
Method
We draw on dialogical iterations between an exploratory 
case study
 at Cloud Corp, a 
multinational
 Software-as-a-Service provider and related literature on technology acceptance and cognitive bias.
Findings
Our study reveals a paradoxical relationship between reported acceptance scores and actual perceptions: Despite providing high acceptance scores, users did not actually use the technology. Conversely, users who complained a lot by filing many support tickets were more engaged with the technology.
Conclusion
When yea-saying users meet yea-hearing designers, technology acceptance scores may be spuriously inflated, providing an exaggerated picture of users’ attitudes toward a technology. We call this the ‘yea-paradox’ because of its roots in people's tendency to agree half-heartedly (‘yea’), even when they privately disagree, to avoid more complex and potentially difficult conversations.",Information and Software Technology,18 Mar 2025,5,"While the exploration of cognitive biases in technology acceptance scores is interesting, the relevance to practical applications for European early-stage ventures, especially startups, is not as direct compared to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001015,A novel vulnerability severity assessment method for source code based on a graph neural network,September 2023,Not Found,Jingwei=Hao: hjwbit@163.com; Senlin=Luo: Not Found; Limin=Pan: Not Found,"Abstract
Context
Vulnerability severity assessment is an important part of 
vulnerability management
 that can help security personnel determine the priority of vulnerability repair work.
Objective
Aiming at the problems of low evaluation efficiency and poor timeliness in the existing method, a vulnerability severity evaluation method combining a function call graph and vulnerability attribute graph is proposed.
Method
This method constructs a function call graph centered on vulnerable functions and uses the call relationship between vulnerable functions and sensitive API functions to reflect the severity of the damage of the vulnerable functions. The graph attention neural network algorithm is used to mine the key vulnerability characteristics in the function call graph and the vulnerability attribute graph to realize the assessment of vulnerability severity.
Results
The ablation experiment results showed that the combined vulnerability attribute graph and function call graph had higher evaluation accuracy than the vulnerability attribute graph or function call graph alone, which increased by 6.85% and 32.90%, respectively. Compared with other existing methods, our method has achieved a better evaluation effect, and the evaluation accuracy has increased by 10%.
Conclusion
The vulnerability severity assessment method incorporating function call graphs and vulnerability property graphs demonstrates an enhancement in the ability to represent the severity of vulnerabilities and increases the efficiency of vulnerability severity evaluation through elimination of the requirement for manual analysis.",Information and Software Technology,18 Mar 2025,7,The proposed vulnerability severity assessment method is valuable for security personnel and could benefit European early-stage ventures dealing with cybersecurity. The improvement in evaluation accuracy and efficiency is relevant for startups.
https://www.sciencedirect.com/science/article/pii/S0950584923001039,Making existing software quantum safe: A case study on IBM Db2,September 2023,Not Found,Lei=Zhang: leizhang@umbc.edu; Andriy=Miranskyy: avm@torontomu.ca; Walid=Rjaibi: Not Found; Greg=Stager: Not Found; Michael=Gray: Not Found; John=Peck: Not Found,"Abstract
Context:
The 
software engineering
 community is facing challenges from 
quantum computers
 (QCs). In the era of 
quantum computing
, Shor’s algorithm running on QCs can break 
asymmetric encryption
 algorithms that classical computers practically cannot. Though the exact date when QCs will become “dangerous” for practical problems is unknown, the consensus is that this future is near. Thus, the 
software engineering
 community needs to start making software ready for quantum attacks and ensure quantum safety proactively.
Objective:
We argue that the problem of evolving existing software to quantum-safe software is very similar to the Y2K bug. Thus, we leverage some best practices from the Y2K bug and propose our roadmap, called 7E, which gives developers a structured way to prepare for quantum attacks. It is intended to help developers start planning for the creation of new software and the evolution of cryptography in existing software.
Method:
In this paper, we use a 
case study
 to validate the viability of 7E. Our software under study is the IBM Db2 database system. We upgrade the current cryptographic schemes to post-quantum cryptographic ones (using Kyber and Dilithium schemes) and report our findings and lessons learned.
Results:
We show that the 7E roadmap effectively plans the evolution of existing software security features towards quantum safety, but it does require minor revisions. We incorporate our experience with IBM Db2 into the revised 7E roadmap.
Conclusion:
The U.S. Department of Commerce’s National Institute of Standards and Technology is finalizing the post-quantum cryptographic standard. The software engineering community needs to start getting prepared for the quantum advantage era. We hope that our experiential study with IBM Db2 and the 7E roadmap will help the community prepare existing software for quantum attacks in a structured manner.",Information and Software Technology,18 Mar 2025,6,"The focus on evolving software to be quantum-safe is an important issue, but the direct impact on European early-stage ventures, especially startups, might be more long-term. The roadmap proposed could have practical value in the future."
https://www.sciencedirect.com/science/article/pii/S0950584923001155,RoseMatcher: Identifying the impact of user reviews on app updates,September 2023,Not Found,Tianyang=Liu: til040@ucsd.edu; Chong=Wang: cwang@whu.edu.cn; Kun=Huang: hunk_fe@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Beiqi=Zhang: zhangbeiqi@whu.edu.cn; Maya=Daneva: m.daneva@utwente.nl; Marten=van Sinderen: m.j.vansinderen@utwente.nl,"Abstract
Context:
The release planning of mobile apps has become an area of active research, with most studies centering on app analysis through release notes in the Apple App Store and tracking user reviews via issue trackers. However, the correlation between these release notes and user reviews in App Store remains understudied.
Objective:
In this paper, we introduce 
RoseMatcher
, a novel automatic approach to match relevant user reviews with app release notes, and identify matched pairs with high confidence.
Methods:
We collected 944 release notes and 1,046,862 user reviews from 5 mobile apps in the Apple App Store as research data to evaluate the effectiveness and accuracy of 
RoseMatcher
, and conducted deep content analysis on matched pairs.
Results:
Our evaluation shows that 
RoseMatcher
 can reach a hit ratio of 0.718 for identifying relevant matched pairs, and with the manual labeling and content analysis of 984 relevant pairs, we identify 8 roles that user reviews play in app updates according to the relationship between release notes and user reviews in the relevant matched pairs.
Conclusions:
Our findings indicate that both app development teams and users pay close attention to release notes and user reviews, with release notes typically addressing feature requests, 
bug reports
, and complaints, and user reviews offering positive, negative, and constructive feedback. Overall, the study highlights the importance of the communication between app development teams and users in the release planning of mobile apps, with relevant reviews tending to be posed within a short period before and after the release of release notes, with the average time interval between the post time of release notes and user reviews being approximately one year.",Information and Software Technology,18 Mar 2025,7,"The study introduces a novel approach that can help app development teams and users improve communication and understanding in the release planning process, highlighting the importance of user feedback."
https://www.sciencedirect.com/science/article/pii/S0950584923001027,A new combination method based on Pearson coefficient and information entropy for multi-sensor data fusion,September 2023,Not Found,Yang=Zhang: zhang.yang@bjtu.edu.cn; Ao=Xiong: Not Found; Yu=Xiao: Not Found; Ziyang=Chen: Not Found,"Abstract
Context
When confronted with greatly contradictory evidence, the Dempster-Shafer evidence theory may exhibit certain constraints that lead to fused results which are inconsistent with common understanding. Within the existing 
Internet of Things
 landscape, there are occasions when a small number of sensors may malfunction and contradict each other.
Objective
This study addresses contradictory information by processing the bodies of evidence beforehand. Additionally, an enhanced fusion technique for conflicting evidence is introduced, which employs 
Pearson correlation coefficient
 and information entropy.
Methods
We propose a novel combination approach for multi-sensor 
data fusion
 based on evidence theory. Firstly, the credibility for each piece of evidence is computed through amalgamating correlation measurements with evidence distance between two pieces of evidence. Next, based on the information volume, the credibility is adjusted, resulting in the final weighting factor for the evidence. The reasonable weighted average evidence is then created using the weighting factor of each piece of evidence. Finally, the combined result is obtained by applying Dempster's combination rule, which combines the weighted average evidence 
N
−
1
 times.
Results
Upon comparing the fusion results, it has been observed that the performance of the proposed method surpasses that of other methods. Our method can effectively minimize the ramifications of profoundly conflicting evidence in the fusion process, resulting in more logical fusion results than other methods.
Conclusions
The outcomes of numerical examples expose that the technique put forward in this manuscript can manage highly conflicting evidence, thereby yielding fusion results that are more precise and conducive to making sound decisions.",Information and Software Technology,18 Mar 2025,6,"The study proposes a new fusion technique for conflicting evidence in IoT scenarios, which can lead to more logical results and aid in decision-making."
https://www.sciencedirect.com/science/article/pii/S0950584923000836,To group or not to group? Group sizes for requirements elicitation,August 2023,"Group work, Individual work, Requirements elicitation, Requirements idea generation, Requirements engineering, Software engineering",Luisa=Mich: luisa.mich@unitn.it; Victoria=Sakhnini: Not Found; Daniel=Berry: Not Found,"Abstract
Context
Requirement elicitation
 can be done by individuals or by groups. Computer-based system development life-cycle models suggest having people working together for many steps. Also, recommendations about analysis and design methods indicate that some processes could take advantage of group work. In 
requirements engineering
, groups are suggested for requirements elicitation.
Objectives
From the software and the requirements engineering viewpoints, and in turn for companies, a relevant overall research question is “What is a suitable size for a requirements elicitation group?” Our goal was to answer this question, first by looking for available guidelines in textbooks and secondly by investigating requirements elicitation in companies.
Method
To address the research question, we conducted two studies. The first was a review of most widely adopted software and requirements engineering textbooks. The second was a study aimed at identifying factors affecting group size for requirements elicitation, based on an online questionnaire submitted to professional analysts.
Results
The review of the textbooks showed that very few give advice on the number of analysts to involve in requirements elicitation sessions. When they do, guidelines are quite general and not supported by empirical data. According to data gathered from the questionnaire, most companies use and suggest using small groups. Data also allowed identifying four categories of factors useful to make decisions about requirements elicitation group sizes: people, relation, project, and output.
Conclusion
Both the textbook review and the data from the questionnaire say that it is better to aim for small groups than to have individual analysts working separately. The ideal number of analysts for a requirements elicitation session appears to be 2, but large groups are necessary in some cases. Factors in all the four categories have to be considered in deciding the size of groups.",Information and Software Technology,18 Mar 2025,8,"The research addresses a practical question relevant to software companies, providing insights on the suitable size for requirements elicitation groups, which can help improve the software development process."
https://www.sciencedirect.com/science/article/pii/S0950584923000836,To group or not to group? Group sizes for requirements elicitation,August 2023,"Group work, Individual work, Requirements elicitation, Requirements idea generation, Requirements engineering, Software engineering",Luisa=Mich: luisa.mich@unitn.it; Victoria=Sakhnini: Not Found; Daniel=Berry: Not Found,"Abstract
Context
Requirement elicitation
 can be done by individuals or by groups. Computer-based system development life-cycle models suggest having people working together for many steps. Also, recommendations about analysis and design methods indicate that some processes could take advantage of group work. In 
requirements engineering
, groups are suggested for requirements elicitation.
Objectives
From the software and the requirements engineering viewpoints, and in turn for companies, a relevant overall research question is “What is a suitable size for a requirements elicitation group?” Our goal was to answer this question, first by looking for available guidelines in textbooks and secondly by investigating requirements elicitation in companies.
Method
To address the research question, we conducted two studies. The first was a review of most widely adopted software and requirements engineering textbooks. The second was a study aimed at identifying factors affecting group size for requirements elicitation, based on an online questionnaire submitted to professional analysts.
Results
The review of the textbooks showed that very few give advice on the number of analysts to involve in requirements elicitation sessions. When they do, guidelines are quite general and not supported by empirical data. According to data gathered from the questionnaire, most companies use and suggest using small groups. Data also allowed identifying four categories of factors useful to make decisions about requirements elicitation group sizes: people, relation, project, and output.
Conclusion
Both the textbook review and the data from the questionnaire say that it is better to aim for small groups than to have individual analysts working separately. The ideal number of analysts for a requirements elicitation session appears to be 2, but large groups are necessary in some cases. Factors in all the four categories have to be considered in deciding the size of groups.",Information and Software Technology,18 Mar 2025,8,"The research addresses a practical question relevant to software companies, providing insights on the suitable size for requirements elicitation groups, which can help improve the software development process."
https://www.sciencedirect.com/science/article/pii/S0950584923000769,A mapping study of language features improving object-oriented design patterns,August 2023,Not Found,William=Flageol: w_flage@encs.concordia.ca; Éloi=Menaud: Not Found; Yann-Gaël=Guéhéneuc: Not Found; Mourad=Badri: Not Found; Stefan=Monnier: Not Found,"Abstract
Context:
Object-Oriented Programming 
design patterns
 are well-known in the industry and taught in universities as part of 
software engineering
 curricula. Many primary studies exist on the impact of 
design patterns
 on software, in addition to secondary studies summarizing these publications. Some primary studies have proposed new 
language features
 and used them to re-implement design patterns as a way to show improvements. While secondary studies exist, they mainly focus on measuring the impact of design patterns on software.
Objectives:
We performed a 
systematic mapping study
 to catalogue 
language features
 in the literature claiming to improve object-oriented design patterns implementations, as well as how primary studies measure these improvements.
Methods:
We performed a search in three databases, yielding a total of 874 papers, from which we obtained 34 relevant papers. We extracted and studied data about the language features claiming to improve design patterns implementations, the most often cited design patterns, the measures used to assess the improvements, and the 
case studies
 and experiments with which these improvements were studied.
Results:
Using the results, we catalogue 18 language features claimed in the literature to improve design patterns and categorize them into paradigms. We find that some design patterns are more prevalent than others, such as Observer and Visitor. Measures related to code size, code scattering and 
understandability
 are preferred. Case studies are done in-vitro, and experiments are rare.
Conclusion:
This catalogue is useful to identify trends and create a road map for research on language features to improve object-oriented design patterns. Considering the prevalence of design patterns, improving their implementation and adding language features to better solve their underlying concerns is an efficient way to improve object-oriented programming. We intend in the future to use this as a basis to research specific language features that may help in improving object-oriented programming.",Information and Software Technology,18 Mar 2025,7,"The systematic mapping study provides valuable insights into language features to improve object-oriented design pattern implementations, offering a roadmap for future research in object-oriented programming."
https://www.sciencedirect.com/science/article/pii/S0950584923000782,StartCards — A method for early-stage software startups,August 2023,"Software startups, Requirements engineering, Validation, Software engineering method, Action research",Kai-Kristian=Kemell: kai-kristian.kemell@helsinki.fi; Anh=Nguyen-Duc: Not Found; Mari=Suoranta: Not Found,"Abstract
Context:
Software startups are important drivers of economy on a global scale, and have become associated with innovation and high growth. However, the overwhelming majority of startups ends in failure. Many of these startup failures ultimately stem from 
software engineering
 issues, and 
requirements engineering
 (RE) ones in particular. Despite the emphasis placed on the importance of RE activities in the startup context, many startups continue to develop software without a clear market or customer, having never had meaningful contact with their would-be customer.
Objective:
We develop a method aimed at early-stage startups that is intended to help startups through the initial stages of the startup process: StartCards. The method emphasizes the importance of idea and product validation activities in particular in order to tackle anti-patterns related to (a lack of) RE in startups. This method is based on existing literature, both grey and academic literature.
Method:
StartCards was developed using the Canonical 
Action Research
 (CAR) approach, over the course of 4 AR cycles. During the AR process, the method was used by 44 student startup teams in a practical course setting. Data from the use of the method was collected through self-reporting in the form of modified learning diaries, mentoring meetings with the startup teams, and a qualitative survey.
Results:
We consider the current version of StartCards useful for early-stage startups based on the data we have collected. The method can also be used as a pedagogical tool in startup education.
Conclusions:
The paper presents the first published version of the method. While work on the method continues, the method is deemed ready for use.",Information and Software Technology,18 Mar 2025,7,"The development of StartCards offers a practical method to help early-stage startups improve their validation activities, addressing a common issue in the startup ecosystem."
https://www.sciencedirect.com/science/article/pii/S0950584923000733,HGIVul: Detecting inter-procedural vulnerabilities based on hypergraph convolution,August 2023,Not Found,Zihua=Song: Not Found; Junfeng=Wang: wangjf@scu.edu.cn; Kaiyuan=Yang: Not Found; Jigang=Wang: Not Found,"Abstract
Context:
Detecting source code vulnerabilities is one way to block 
cyber attacks
 from an early stage. Vulnerability-triggered code typically involves one or more function procedures, while current research pays more attention to the code on a single procedure. Due to lacking a comprehensive analysis of multiple vulnerability-related procedures, current methods suffer disorder false-positive and false-negative rates, especially in detecting inter-procedural vulnerability.
Objective:
This paper proposes HGIVul, an inter-procedural 
vulnerability detection
 method for source code based on hypergraph convolution. The key of HGIVul is to derive the syntax-semantic characteristic from multiple procedures in a suitable code information space, which brings more balanced detection.
Methods:
Firstly, the potential vulnerability-related code trace across multiple procedures is located via static analyzer Infer. Then, HGIVul reconstructs the soft inter-procedural 
control flow graph
 (ICFG) from the trace to restore the complex relationship between multiple-procedural codes. Next, HGIVul performs multi-level 
graph convolution
 on the soft ICFG to grasp holistic code characteristics within multiple procedures. Finally, a classifier is applied to the extracted code features for 
vulnerability detection
.
Results:
The experimental results show that HGIVul outperforms in detecting vulnerabilities and identifying vulnerability types, with the F1-measure of 66.33% and 79.58% for detection and identification, respectively. Moreover, the experiment on cross-projects indicates HGIVul has a better detection ability.
Conclusion:
The proposed HGIVul achieves a balanced detection performance than the related state-of-the-art methods, which proves that fusing syntactic–semantic information from multiple procedures benefits inter-procedural vulnerability detection. In addition, the results applied to five actual projects indicate that HGIVul has the feasibility of detection in practical.",Information and Software Technology,18 Mar 2025,9,"HGIVul presents a novel method for vulnerability detection in source code, with strong experimental results showing its effectiveness, which can have a significant impact on improving cybersecurity for startups."
https://www.sciencedirect.com/science/article/pii/S0950584923000770,"Human error management in requirements engineering: Should we fix the people, the processes, or the environment?",August 2023,Not Found,Sweta=Mahaju: Sweta.Mahaju@mtsu.edu; Jeffrey C.=Carver: carver@cs.ua.edu; Gary L.=Bradshaw: glb2@psychology.msstate.edu,"Abstract
Context:
Software development is a human-centric activity and hence vulnerable to human error. Human errors are errors in the human thought process. To ensure software quality, it is important for practitioners to understand how to manage these human errors. Organizations often introduce changes into the 
requirements engineering
 process to either 
prevent
 human errors from occurring or to 
mitigate
 the harm caused when those errors do occur. While there are studies on human error management in other disciplines, research on the prevention and mitigation of human errors in 
software engineering
, and 
requirements engineering
 specifically, are limited. The current studies in 
software engineering
 do not provide strong results about the types of changes that are most effective in requirements engineering.
Objective:
The goal of this paper is to develop a taxonomy of human error prevention and 
mitigation strategies
 based on data gathered from requirements engineering professionals.
Methods:
We performed a qualitative 
analysis of data
 from two practitioner surveys on requirements engineering practices to identify and classify strategies for the prevention and mitigation of human errors.
Results:
We organized the human error management strategies into a taxonomy based on whether the changes primarily affect People, Processes, or the Environment. Inside each of these high-level categories, we further organized the strategies into low-level classes. The results show more than 50% of the reported strategies require a change in Process, 23% require a change in Environment, 21% require a change in People, with the remaining 5% too ambiguous to classify. In addition, more than 50% of the strategies focus on Management activities of requirements engineering.
Conclusion:
The Human Error Management Taxonomy provides a systematic classification and organization of strategies for prevention and mitigation of human errors in requirements engineering. This systematic organization provides a foundation upon which research can build.",Information and Software Technology,18 Mar 2025,8,"The Human Error Management Taxonomy provides a systematic classification of strategies for preventing and mitigating human errors in requirements engineering, offering valuable insights for startups to enhance software quality."
https://www.sciencedirect.com/science/article/pii/S0950584923000745,Usefulness and usability of heuristic walkthroughs for evaluating domain-specific developer tools in industry: Evidence from four field simulations,August 2023,"Domain-specific language, Integrated development environment, User experience, Programmer experience, Review, Heuristic walkthrough",Olaf=Leßenich: olaf.lessenich@wu.ac.at; Stefan=Sobernig: stefan.sobernig@wu.ac.at,"Abstract
Context:
The usage of domain-specific languages (DSLs) is an approach to reduce complexity in software development by expert developers for selected application domains. To support expert developers, a DSL is often combined with a tailored, domain-specific developer tool, offering similar functionality as general-purpose programming environments (IDEs like Eclipse for Java) while integrating with a domain-specific toolchain (e.g., code or documentation generators, simulators). General-purpose development environments have been successfully evaluated for the programmer experience (PX) and for anomalies using heuristic walkthroughs as a mixed review technique combining cognitive walkthroughs and a 
heuristic evaluation
.
Objective:
In this paper, we report on the usefulness and acceptance of heuristic walkthroughs as an PX evaluation technique applied to domain-specific languages and 
IDEs
 in an industry context.
Methods:
Heuristic walkthroughs are used in four interventions (field simulations) to assess the programming experience and usability of domain-specific, Eclipse-based IDEs in a concrete industry setting. Data on the usefulness and acceptance (perceived satisfaction) of the walkthroughs themselves are collected and analysed.
Results:
Our studies show that, in practice, the instrument of walkthroughs is useful for revealing practically relevant PX anomalies, while maintaining acceptance by practitioners participating in the walkthroughs.
Conclusion:
The documented variant of heuristic walkthroughs is eligible to become adopted for future field studies in academic research and for evaluation projects in industry, in support of developing domain-specific developer tooling in an evidence-driven manner.",Information and Software Technology,18 Mar 2025,6,"The evaluation of heuristic walkthroughs for assessing domain-specific languages and IDEs contributes to improving developer tooling, but may have a slightly lower impact compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492300085X,Salience-based stakeholder selection to maintain stakeholder coverage in solving the next release problem,August 2023,"Stakeholders theory, Stakeholders selection and quantification",I.M.=del Águila: imaguila@ual.es; J.=del Sagrado: Not Found,"Abstract
Context:
The quantification of stakeholders plays a fundamental role in the selection of appropriate requirements, as their judgement is a significant criterion, as not all stakeholders are equally important. The original proposals modelled stakeholder importance using a weighting approach that may not capture all the dimensions of stakeholder importance. Furthermore, actual projects involve a multitude of stakeholders, making it difficult to consider and compute all their weights. These facts lead us to search for strategies to adequately assess the importance concept, reducing the 
elicitation
 effort.
Objective:
We propose grouping strategies as a means of reducing the number of stakeholders to manage in requirement selection while maintaining adequate stakeholder coverage (how selection meets stakeholder demands).
Methods:
Our approach is based on the salience of stakeholders, defined in terms of their power, legitimacy, and urgency. Diverse strategies are applied to select important 
stakeholder groups
. We use 
k-means
, 
k-medoids
, and 
hierarchical
 clustering, after deciding the number of clusters based on validation indices.
Results:
Each technique found a different group of important stakeholders. The number of stakeholder groups suggested experimentally (3 or 4) coincides with those indicated by the literature as definitive, dominant, dependent, and dangerous for 4 groups; or critical, major, and minor for 3 groups. Either for all the stakeholders and for each important group, several requirements selection 
optimisation problems
 are solved. The tests do not find significant differences in coverage when important stakeholders are filtered using clustering, regardless of the technique and number of groups, with a reduction between 66.32% and 87.75% in the number of stakeholders considered.
Conclusions:
Applying 
clustering methods
 to data obtained from a project is useful in identifying the group of important stakeholders. The number of suggested groups matches the stakeholders’ theory, and the stakeholder coverage values are kept in the requirement selection.",Information and Software Technology,18 Mar 2025,5,"The application of grouping strategies to reduce the number of stakeholders in requirement selection is beneficial, but may have a slightly lower impact compared to other abstracts in terms of practical value."
https://www.sciencedirect.com/science/article/pii/S0950584923001003,A multitype software buffer overflow vulnerability prediction method based on a software graph structure and a self-attentive graph neural network,August 2023,Not Found,Zhangqi=Zheng: Not Found; Yongshan=Liu: 451499304@qq.com; Bing=Zhang: Not Found; Xinqian=Liu: Not Found; Hongyan=He: Not Found; Xiang=Gong: Not Found,"Abstract
Context
Buffer overflow vulnerabilities are one of the most common and dangerous software vulnerabilities; however, the complexity of software code makes predicting 
buffer overflow
 vulnerabilities in software challenging.
Objective
To accurately predict multiple types of software buffer overflow vulnerabilities, this paper proposes a multitype software buffer overflow vulnerability prediction method called MSVAGraph that is based on the graph structure of software and a self-attentive 
graph neural network
.
Method
First
, by analyzing software buffer overflow type vulnerabilities, a vulnerability feature set GSVFset extraction method based on graph structure is proposed to act as the software's basic unit. 
Second,
 a self-attentive pooling mechanism is used to design a vulnerability feature update mechanism based on a self-attentive graph neural network to transform the graph structure of the vulnerability feature set GSVFset into a feature vector representation. 
Finally
, based on the updated GSVFset feature vector, a time-recursive-based neural network is designed to construct a prediction method for multitype software buffer overflow vulnerabilities.
Results
The method proposed in this paper validates executable programs of four types of buffer overflow vulnerabilities in the Juliet dataset using precision, accuracy, recall and F1 value as evaluation metrics. The prediction results have higher values after introducing the self-attentive pooling mechanism.
Conclusion
The proposed MSVAGraph achieves high precision, accuracy, recall and F1 value, and can better preserve the network topology and node content information of graphs in the software's graph structure.",Information and Software Technology,18 Mar 2025,8,"The proposed MSVAGraph method addresses a critical issue in software development - buffer overflow vulnerabilities - and achieves high precision, accuracy, recall, and F1 value, which can greatly impact the security of European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000861,Soft skills required from software professionals in New Zealand,August 2023,"Soft skills, Software engineering practice, Job adverts, Exploratory study, Flexiterm",Matthias=Galster: matthias.galster@canterbury.ac.nz; Antonija=Mitrovic: tanja.mitrovic@canterbury.ac.nz; Sanna=Malinen: sanna.malinen@canterbury.ac.nz; Jay=Holland: jay.holland@canterbury.ac.nz; Pasan=Peiris: pitigalagepasan.peiris@pg.canterbury.ac.nz,"Abstract
Context:
Soft skills (e.g., communication) significantly contribute to software project success.
Objective:
We aim to understand (a) what are relevant soft skills in 
software engineering
, (b) how soft skills relate to characteristics of hiring organizations, and (c) how reliably we can 
automatically
 identify soft skills in job adverts to support their continuous analysis. We focus on soft skills required by organizations in New Zealand, a country with a small but growing software sector characterized by a skills shortage, reliance on offshoring, and embedded in a bi-cultural context.
Method:
We manually analyzed 530 job adverts from New Zealand’s largest portal for technology-related positions. We identified soft skills following an inductive approach, i.e., without pre-defined soft skills. We complemented the manual analysis with an automated analysis using Flexiterm (an approach for term recognition).
Results:
We found explicit references to soft skills in 82% of adverts. Adverts from 
recruitment agencies
 (compared to hiring companies) included fewer soft skills. We identified 17 soft skills and proposed a contextualized software engineering description. Communication-related skills are most in demand. Soft skills related to broader human or societal values (e.g., empathy, cultural awareness) or distributed development are not common. Soft skills do not depend on company size or core business and domain of companies, or whether a company operates globally. Automatically identifying soft skills in adverts is error-prone.
Conclusions:
Employers explicitly ask for soft skills. Our findings support previous studies that highlight the importance of communication. On the other hand, identified soft skills only partially overlap with those reported in other skills classifications. Characteristics specific to New Zealand do not impact the demand for soft skills. Our findings benefit researchers in human aspects of software engineering and to those responsible for staff, curricula and professional development.",Information and Software Technology,18 Mar 2025,6,"The study on soft skills in software engineering provides valuable insights for hiring organizations in New Zealand, but the impact on European early-stage ventures may not be as direct compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492300099X,A fault localization approach based on fault propagation context,August 2023,Not Found,Yue=Yan: Not Found; Shujuan=Jiang: shjjiang@cumt.edu.cn; Yanmei=Zhang: Not Found; Shenggang=Zhang: Not Found; Cheng=Zhang: Not Found,"Abstract
Context:
Spectrum-based 
fault localization
 (SBFL) performs statistical analysis on the coverage information of failed or passed test cases. It provides the programmer with a guide for 
fault localization
 by generating a sorted list of suspicious elements. Unfortunately, the SBFL technology has a tie problem. Many elements in the list have the same suspicious values, which seriously affects the developer finding faults.
Objective:
Therefore, we aim to solve the tie problem of SBFL techniques by applying the 
fault propagation
 context. In this paper, we propose an approach to boost the performance of fault localization by applying inter-class and intra-class 
fault propagation
 context to weight the suspicious elements.
Methods:
We first apply the SBFL techniques to calculate the initial suspicious values of the statements. Then, we analyze the fault propagation context of the suspicious statements. Finally, we weight the initial suspicious values of the statements to solve the tie problem of SBFL techniques. The weighted suspicious values are sorted in 
descending order
 to generate a list, which is provided to developers for fault localization.
Results:
To evaluate our approach, we experiment on Defects4J, a real-world 
software fault
 benchmark. Experimental results show that the proposed approach outperforms existing fault context-based methods that only use intra-class context and traditional 
SBFL methods
. For example, our approach can locate 129 faults in Top-1, which is 94 more than Ochiai method. Moreover, we provide programmers with the fault propagation context for accurate fault localization.
Conclusion:
Applying fault propagation context can effectively solve the tie problem in SBFL. It is valuable to study how to efficiently utilize the fault propagation context for fault localization.",Information and Software Technology,18 Mar 2025,9,"Solving the tie problem in Spectrum-based fault localization techniques has a significant practical value for software developers, as shown by the experimental results on real-world software fault benchmark, making it highly impactful for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000848,Migrating monoliths to cloud-native microservices for customizable SaaS,August 2023,"Microservices, Architecture, Cloud native, Migration, Multi-tenancy, Event-based, Customization",Espen Tønnessen=Nordli: espen.nordli@tietoevry.com; Sindre Grønstøl=Haugeland: sindre.haugeland@tietoevry.com; Phu H.=Nguyen: phu.nguyen@sintef.no; Hui=Song: hui.song@sintef.no; Franck=Chauvel: franck.chauvel@axbit.com,"Abstract
Context:
It was common that software vendors sell licenses to their clients to use software products, such as 
Enterprise Resource Planning
, which are deployed as a monolithic entity on clients’ premises. Moreover, many clients, especially big organizations, often require software products to be customized for their specific needs before deployment on premises.
Objective:
However, as software vendors are migrating their monolithic software products to Cloud-native Software-as-a-Service (SaaS), they face two big challenges that this paper aims at addressing: (1) How to migrate their exclusive monoliths to multi-tenant Cloud-native SaaS; and (2) How to enable tenant-specific 
customizations
 for multi-tenant Cloud-native SaaS.
Method:
This paper suggests an approach for migrating monoliths to microservice-based Cloud-native SaaS, providing customers with a flexible customization opportunity, while taking advantage of the economies of scale that the Cloud and multi-tenancy provide. We develop two proofs-of-concept to demonstrate our approach on migrating a reference application of Microsoft called SportStore to a customizable SaaS as well as customizing another Microsoft’s microservices reference application called eShopOnContainers.
Results:
We have shown not only the migration to microservices but also how to introduce the necessary infrastructure to support the new services and enable tenant-specific customization.
Conclusions:
Our customization-driven migration approach can guide a monolith to become SaaS having (synchronous and asynchronous) customization power for multi-tenant SaaS. Furthermore, our event-based customization approach can reduce the number of API calls to the main product while enabling different tenant-specific customization services for real-world scenarios.",Information and Software Technology,18 Mar 2025,7,"The migration of monolithic software products to Cloud-native SaaS and enabling tenant-specific customizations present challenges that are relevant to software vendors, but the direct impact on European early-stage ventures may require further adaptation and implementation considerations."
https://www.sciencedirect.com/science/article/pii/S0950584923000794,An empirical comparison of combinatorial testing and search-based testing in the context of automated and autonomous driving systems,August 2023,"Test generation, Ontologies, Combinatorial testing, Search-based testing, Genetic algorithm",Florian=Klück: fklueck@ist.tugraz.at; Yihao=Li: yihao.li@ldu.edu.cn; Jianbo=Tao: Jianbo.Tao@avl.com; Franz=Wotawa: wotawa@ist.tugraz.at,"Abstract
Context:
More automated and 
autonomous systems
 are becoming daily use that implements safety–critical functions, e.g., 
autonomous driving
 or mobile robots. Testing such systems people depend on is challenging because some environmental interactions may not be expected during development but occur when those systems are in operation. Deciding when to stop testing or answering how to ensure sufficient testing is challenging and very expensive.
Objectives:
For generating critical environmental interactions, i.e., critical scenarios, we present and compare two testing solutions focusing on generating critical scenarios utilizing combinatorial and search-based testing, respectively.
Methods:
For combinatorial testing, we suggest using ontologies that describe the environment of an autonomous or highly automated system. For search-based testing, we rely on 
genetic algorithms
. We experimentally compared the two testing approaches using two implementations of an industrial emergency braking function and random testing as the baseline. Furthermore, we compared the approaches qualitatively using several categories.
Results:
From the experiments, we see that the combinatorial testing approach can find all different types of faults listed in Table 5 considering a combinatorial 
strength
 of 3. This is not the case for search-based and random testing in all experiments. Combinatorial testing comes with the highest combinatorial coverage. However, all approaches can reveal faulty behavior utilizing appropriate environmental models.
Conclusion:
We present the results of an in-depth comparison of combinatorial and search-based testing. The be as fair as possible, the comparison relied on the same environmental model and other parameters like the number of 
generated test cases
. The results show that combinatorial testing comes with the highest coverage and can find all different kinds of failures summarized in Table 5 providing a certain strength. Meanwhile, search-based testing is also capable of finding different failures depending on the coverage it can reach. Both approaches seem complementary and of use for the application domain of autonomous and automated driving functions.",Information and Software Technology,18 Mar 2025,8,"The comparison of combinatorial and search-based testing for critical scenario generation in automated systems addresses a crucial aspect of ensuring sufficient testing, which can have a significant impact on the reliability and safety of European early-stage ventures utilizing autonomous systems."
https://www.sciencedirect.com/science/article/pii/S0950584923000794,An empirical comparison of combinatorial testing and search-based testing in the context of automated and autonomous driving systems,August 2023,"Test generation, Ontologies, Combinatorial testing, Search-based testing, Genetic algorithm",Florian=Klück: fklueck@ist.tugraz.at; Yihao=Li: yihao.li@ldu.edu.cn; Jianbo=Tao: Jianbo.Tao@avl.com; Franz=Wotawa: wotawa@ist.tugraz.at,"Abstract
Context:
More automated and 
autonomous systems
 are becoming daily use that implements safety–critical functions, e.g., 
autonomous driving
 or mobile robots. Testing such systems people depend on is challenging because some environmental interactions may not be expected during development but occur when those systems are in operation. Deciding when to stop testing or answering how to ensure sufficient testing is challenging and very expensive.
Objectives:
For generating critical environmental interactions, i.e., critical scenarios, we present and compare two testing solutions focusing on generating critical scenarios utilizing combinatorial and search-based testing, respectively.
Methods:
For combinatorial testing, we suggest using ontologies that describe the environment of an autonomous or highly automated system. For search-based testing, we rely on 
genetic algorithms
. We experimentally compared the two testing approaches using two implementations of an industrial emergency braking function and random testing as the baseline. Furthermore, we compared the approaches qualitatively using several categories.
Results:
From the experiments, we see that the combinatorial testing approach can find all different types of faults listed in Table 5 considering a combinatorial 
strength
 of 3. This is not the case for search-based and random testing in all experiments. Combinatorial testing comes with the highest combinatorial coverage. However, all approaches can reveal faulty behavior utilizing appropriate environmental models.
Conclusion:
We present the results of an in-depth comparison of combinatorial and search-based testing. The be as fair as possible, the comparison relied on the same environmental model and other parameters like the number of 
generated test cases
. The results show that combinatorial testing comes with the highest coverage and can find all different kinds of failures summarized in Table 5 providing a certain strength. Meanwhile, search-based testing is also capable of finding different failures depending on the coverage it can reach. Both approaches seem complementary and of use for the application domain of autonomous and automated driving functions.",Information and Software Technology,18 Mar 2025,8,"The abstract provides valuable insights into testing critical environmental interactions for autonomous systems, which is crucial for early-stage ventures in the automotive industry."
https://www.sciencedirect.com/science/article/pii/S0950584923000824,Test case classification via few-shot learning,August 2023,Not Found,Yuan=Zhao: zhaoyuan@smail.nju.edu.cn; Sining=Liu: 181250093@smail.nju.edu.cn; Quanjun=Zhang: quanjun.zhang@smail.nju.edu.cn; Xiuting=Ge: dg20320002@smail.nju.edu.cn; Jia=Liu: jialiu@nju.edu.cn,"Abstract
Context:
Crowdsourced testing can reduce testing costs and improve testing efficiency. However, crowdsourced testing generates massive test cases, requiring testers to select high-quality test cases for execution. Consequently, crowdsourced test cases require much effort to perform labeling due to the costly manual labor and domain knowledge.
Objective:
Existing methods usually fail to consider the crowdsourced testing scenario’s inadequate and 
imbalanced data
 issues. We aim to effectively and efficiently classify many crowdsourced test cases for developers to alleviate manual efforts.
Method:
In this paper, we propose a test case 
classification approach
 based on few-shot learning and test case augmentation to address the limitations mentioned above. The proposed approach generates new test cases by the large pre-trained masked 
language model
 and extracts embedding representation by training 
word embedding
 models. Then a Bidirectional Long Short-Term Memory (BiLSTM)-based classifier is designed to perform test case classification by extracting the in-depth features. Besides, we also apply the 
attention mechanism
 to assign high weights to words that represent the test case category by lexicon matching.
Results:
To verify the effectiveness of the classification framework, we select 1659 test cases from three crowdsourced testing projects to conduct in-usability evaluation experiments. The experimental results show that the proposed approach has a higher accuracy and precision rate than existing classification methods.
Conclusion:
It can be concluded that (1) the proposed approach is an effective test case 
classification technique
 for crowdsourced testing; (2) the proposed approach is practical to help developers select high-quality test cases quickly and effectively.",Information and Software Technology,18 Mar 2025,7,"The abstract offers a practical approach to classifying crowdsourced test cases, which can be beneficial for startups looking to improve testing efficiency and reduce manual efforts."
https://www.sciencedirect.com/science/article/pii/S0950584923000800,On the effectiveness of automated tracing from model changes to project issues,August 2023,"Requirement Traceability, Trace link recovery, Model-driven development, Low-code development, Machine learning",Wouter=van Oosten: Not Found; Randell=Rasiman: Not Found; Fabiano=Dalpiaz: f.dalpiaz@uu.nl; Toine=Hurkmans: Not Found,"Abstract
Context:
Requirements Traceability (RT) is concerned with monitoring and documenting the lifecycle of requirements. Although researchers have proposed several automated tracing tools, trace 
link establishment
 and maintenance are still prevalently manual activities.
Objective:
In order to foster the adoption of automated tracing tools, we study their empirical effectiveness in the context of model-driven development (MDD). We focus on trace link recovery (TLR) from (i) SVN revisions of MDD models to (ii) JIRA issues that represent requirements and bugs.
Method:
Based on the state-of-the-art in automated TLR, we propose the 
LCDTrace
 tool that uses 131 features to train a 
machine learning
 classifier. Some of these features use specific information for MDD contexts. We conduct three experiments on ten datasets from seven MDD projects. First, we evaluate the effectiveness of three 
ML algorithms
 and four rebalancing strategies using all 131 features, and we derive two optimal combinations for trace link recommendation and for trace maintenance. Second, we investigate whether the MDD-specific features convey higher performance than a version of 
LCDTrace
 that excludes those features. Third, we employ automated feature selection and study whether we can reduce the number of features while keeping similar performance, thereby boosting time and energy efficiency.
Results:
In our experiments, the 
gradient boosting
 models outperform those based on 
random forests
. The best combinations for trace recommendation and maintenance achieve an 
F
2
-score of 61% and F
0.5
-score of 67%, respectively. While MDD-specific features do not provide additional value, automated feature selection succeeds at reducing feature numerosity without compromising performance.
Conclusion:
We provide insights on the effectiveness of state-of-the-art TLR techniques in MDD. Our findings are a baseline for devising and experimenting with alternative TLR approaches.",Information and Software Technology,18 Mar 2025,9,"The abstract presents an empirical study on automated tracing tools in model-driven development, offering practical implications for startups to adopt automated tracing tools effectively."
https://www.sciencedirect.com/science/article/pii/S0950584923000800,On the effectiveness of automated tracing from model changes to project issues,August 2023,"Requirement Traceability, Trace link recovery, Model-driven development, Low-code development, Machine learning",Wouter=van Oosten: Not Found; Randell=Rasiman: Not Found; Fabiano=Dalpiaz: f.dalpiaz@uu.nl; Toine=Hurkmans: Not Found,"Abstract
Context:
Requirements Traceability (RT) is concerned with monitoring and documenting the lifecycle of requirements. Although researchers have proposed several automated tracing tools, trace 
link establishment
 and maintenance are still prevalently manual activities.
Objective:
In order to foster the adoption of automated tracing tools, we study their empirical effectiveness in the context of model-driven development (MDD). We focus on trace link recovery (TLR) from (i) SVN revisions of MDD models to (ii) JIRA issues that represent requirements and bugs.
Method:
Based on the state-of-the-art in automated TLR, we propose the 
LCDTrace
 tool that uses 131 features to train a 
machine learning
 classifier. Some of these features use specific information for MDD contexts. We conduct three experiments on ten datasets from seven MDD projects. First, we evaluate the effectiveness of three 
ML algorithms
 and four rebalancing strategies using all 131 features, and we derive two optimal combinations for trace link recommendation and for trace maintenance. Second, we investigate whether the MDD-specific features convey higher performance than a version of 
LCDTrace
 that excludes those features. Third, we employ automated feature selection and study whether we can reduce the number of features while keeping similar performance, thereby boosting time and energy efficiency.
Results:
In our experiments, the 
gradient boosting
 models outperform those based on 
random forests
. The best combinations for trace recommendation and maintenance achieve an 
F
2
-score of 61% and F
0.5
-score of 67%, respectively. While MDD-specific features do not provide additional value, automated feature selection succeeds at reducing feature numerosity without compromising performance.
Conclusion:
We provide insights on the effectiveness of state-of-the-art TLR techniques in MDD. Our findings are a baseline for devising and experimenting with alternative TLR approaches.",Information and Software Technology,18 Mar 2025,9,"The abstract provides valuable insights on automated tracing tools in model-driven development, which can have a significant impact on improving traceability for early-stage ventures."
