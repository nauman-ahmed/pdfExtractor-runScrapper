link,title,published_year,publication_title,keywords,author_email,abstract,created_on,score,justification
https://www.sciencedirect.com/science/article/pii/S0950584925000400,"Metaverse Applications: Challenges, Limitations and Opportunities - A Systematic Literature Review",June 2025,Information and Software Technology,Not Found,Elena=Enamorado-Díaz: eenamorado@us.es; Julián A.=García-García: juliangg@us.es; María José=Escalona-Cuaresma: mjescalona@us.es; David=Lizcano-Casas: david.lizcano@udima.es,"Abstract
Context:
The metaverse, an emerging concept at the intersection of digital technology and society, is gaining relevance in multiple domains, including education, entertainment and healthcare. Shared virtual spaces allow users to interact in innovative ways, but the design and development of these environments pose significant challenges for software engineering teams as well as users.
Objective:
The objective of this study is to provide a comprehensive systematic literature review of metaverse applications over the past decade. The review aims to identify key areas of application, technologies employed, virtualized elements, and economic aspects, as well as to explore the objectives, motivations, scope, challenges, and limitations faced in Software Engineering when conceptualizing metaverse environments. Additionally, the study examines the nature, knowledge area, type, and validation of the studies included in the review.
Methods:
This study was conducted using the Kitchenham methodology for systematic literature reviews (SLR). A total of 35 primary studies were selected from major scientific databases, including IEEE, ACM Digital Library, PubMed, ScienceDirect, and Scopus. These studies were evaluated to extract relevant data.
Results:
We have identified application areas, technologies used, virtualized elements and economic aspects used, as well as the objectives, motivations, scope, challenges and limitations in Software Engineering related to the conceptualization of environments and non-functional characteristics of the metaverse. The nature, area of knowledge, type and validation of the studies chosen in this review are also analyzed.
Conclusion:
The study concludes that while the metaverse presents huge opportunities across multiple domains, its development faces significant challenges, particularly in software engineering related to the non-functional aspects of these environments. To address these challenges, future research should focus on the application of the Model Driven Engineering (MDE) paradigm, which could optimize development processes and better manage the complexities of the metaverse.",21 Mar 2025,8,"The study on metaverse applications provides valuable insights into challenges faced in software engineering, offering potential solutions for the development of metaverse environments with a focus on non-functional aspects, which can be beneficial for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584925000448,Cascading failure prediction and recovery in large-scale critical infrastructure networks: A survey,June 2025,Information and Software Technology,Not Found,Beibei=Li: Not Found; Wei=Hu: weihu@nwpu.edu.cn; Chaoxuan=Yuan: Not Found; Xinxin=Wang: Not Found; Yiwei=Li: Not Found; Yibing=Wu: Not Found,"Abstract
Context:
Large-scale critical infrastructure (CI) networks are crucial to society but prone to cascading failures due to their dynamic and interconnected characteristics. Recent research focuses on their reliability, using network theories and real-world data to develop recovery functions and crash warning indicators.
Objective:
This review evaluates cascading failure prediction and recovery trends, examines verification methods, and addresses challenges in enhancing network reliability and topology recovery within CI systems.
Methods:
A comprehensive survey explores cascading failure prediction and recovery from two perspectives: inter-network and inter-module structures. It summarizes recent research trends, common verification platforms, and datasets for predicting and recovering from cascading failures.
Results:
The review focuses on low-dimensional static networks, revealing significant challenges in dynamic environments. It underscores the necessity for improved recovery techniques and enhanced network reliability.
Conclusion:
This article identifies future research directions and unresolved issues by analyzing existing work in cascading failure prediction and recovery. Understanding cascading failure mechanisms aims to inspire the design of more resilient and reliable network systems, contributing to developing cohesive and low-coupling CI systems.",21 Mar 2025,6,"The review on cascading failures in critical infrastructure networks offers valuable insights into recovery trends and challenges, which could provide useful knowledge for European startups operating in sectors dependent on critical infrastructure networks."
https://www.sciencedirect.com/science/article/pii/S0950584925000461,A review of backdoor attacks and defenses in code large language models: Implications for security measures,June 2025,Information and Software Technology,Not Found,Yubin=Qu: quyubin@hotmail.com; Song=Huang: Not Found; Peng=Nie: Not Found,"Abstract
Context:
Large Language Models (LLMS) have revolutionized software engineering by bridging human language understanding and complex problem solving. However, resource constraints often lead users to rely on open-source models or third-party platforms for training and prompt engineering, introducing significant security vulnerabilities.
Objective:
This study provides a comprehensive analysis of backdoor attacks targeting LLMS in software engineering, with a particular focus on fine-tuning methods. Our work addresses a critical gap in existing literature by proposing a novel three-category framework for backdoor attacks: full-parameter fine-tuning, parameter-efficient fine-tuning, and no-tuning attacks.
Methods:
We systematically reviewed existing studies and analyzed attack success rates across different methods. Full-parameter fine-tuning generally achieves high success rates but requires significant computational resources. Parameter-efficient fine-tuning offers comparable success rates with lower resource demands, while no-tuning attacks exhibit variable success rates depending on prompt design, posing unique challenges due to their minimal resource requirements.
Results:
Our findings underscore the evolving landscape of backdoor attacks, highlighting the shift towards more resource-efficient and stealthy methods. These trends emphasize the need for advanced detection mechanisms and robust defense strategies.
Conclusion:
By focusing on code-specific threats, this study provides unique insights into securing LLMS in software engineering. Our work lays the foundation for future research on developing sophisticated defense mechanisms and understanding stealthy backdoor attacks.",21 Mar 2025,7,"The analysis of backdoor attacks targeting Large Language Models in software engineering provides crucial insights into security vulnerabilities and defense strategies, which can be relevant for European early-stage ventures dealing with sensitive data and intellectual property."
https://www.sciencedirect.com/science/article/pii/S0950584925000588,Dynamic information utilization for securing Ethereum smart contracts: A literature review,June 2025,Information and Software Technology,Not Found,Tianyuan=Hu: tianyuan.hu@njtech.edu.cn; Bixin=Li: bx.li@seu.edu.cn,"Abstract
Smart contracts, self-executing programs that govern digital assets on blockchain platforms, have gained widespread adoption due to their automation and transparency. However, vulnerabilities in smart contracts can lead to financial losses and reputational damage, making their security a critical concern. Static code auditing methods are prone to false positives and false negatives, as they fail to account for real-time execution conditions. The integration of dynamic information offers a promising avenue for addressing these limitations and enhancing smart contract security. Ethereum, the most widely used blockchain platform, provides a wealth of publicly available data and has attracted significant attention from researchers due to its security problems. This paper presents a systematic mapping study focused on Ethereum, reviewing the existing literature on the use of dynamic information for enhancing the security of smart contracts. It offers a comprehensive overview of security problems, dynamic information types, technical approaches, and validation methods. Furthermore, we examine the implications and limitations of current research and propose future directions for further exploration in the field of Ethereum smart contract protection.",21 Mar 2025,6,"The systematic mapping study on Ethereum smart contracts offers a comprehensive overview of security issues and dynamic information utilization, which could be valuable for startups in the blockchain space looking to enhance the security of their smart contracts."
https://www.sciencedirect.com/science/article/pii/S0950584925000667,A systematic literature review of agile software development projects,June 2025,Information and Software Technology,Not Found,Soumya Prakash=Rath: ephd22soumya@iimnagpur.ac.in; Nikunj Kumar=Jain: nikunj@iimnagpur.ac.in; Gunjan=Tomer: Not Found; Alok Kumar=Singh: Not Found,"Abstract
Context
Agile software development (ASD) is gaining prominence as the leading methodology for modern software development organizations because it enables a fast, effective, and customer-centric approach in the current disruptive and dynamic work environment.
Objective
Despite increasing interest in ASD as a research area, the extant literature remains scattered and lacks convergence. This study provides a detailed account of all aspects of ASD, including emerging agile concepts, such as agile governance and large-scale agile implementations.
Method
A systematic literature review (SLR) technique identifies 208 relevant articles. The study included papers published between 1999 and 2024.
Results
This SLR provides a concise overview of the various theories applied in the context of ASD. The study classifies previous literature into numerous different facets of ASD. In addition, the paper has prepared an extensive list of relevant research questions for future investigations in each domain of ASD.
Conclusion
This study offers scholars insights into the status of ASD research as well as the current trends in ASD. Furthermore, the proposed future research questions provide researchers with precise direction for delving deeper into different facets of ASD.",21 Mar 2025,7,"The systematic literature review on Agile software development provides a detailed account of emerging agile concepts, offering scholars and researchers insights into current trends and research questions, which can benefit European startups adopting agile methodologies for software development."
https://www.sciencedirect.com/science/article/pii/S095058492500045X,JIT-CF: Integrating contrastive learning with feature fusion for enhanced just-in-time defect prediction,June 2025,Information and Software Technology,Not Found,Xiaolin=Ju: ju.xl@ntu.edu.cn; Yi=Cao: ntucaoyi@outlook.com; Xiang=Chen: xchencs@ntu.edu.cn; Lina=Gong: gonglina@nuaa.edu.cn; Vaskar=Chakma: vaskarchakma7@gmail.com; Xin=Zhou: xinzhountu@hotmail.com,"Abstract
Context:
Just-in-time defect prediction (JIT-DP) is a crucial process in software development that focuses on identifying potential defects during code changes, facilitating early mitigation and quality assurance. Pre-trained language models like CodeBERT have shown promise in various applications but often struggle to distinguish between defective and non-defective code, especially when dealing with noisy labels.
Objective:
The primary aim of this study is to enhance the robustness of pre-trained language models in identifying software defects by developing an innovative framework that leverages contrastive learning and feature fusion.
Method:
We introduce JIT-CF, a framework that improves model robustness by employing contrastive learning to maximize similarity within positive pairs and minimize it between negative pairs, thereby enhancing the model’s ability to detect subtle differences in code changes. Additionally, feature fusion is used to combine semantic and expert features, enabling the model to capture richer contextual information. This integrated approach aims to improve the identification and resolution of code defects.
Results:
JIT-CF was evaluated using the JIT-Defects4J dataset, which includes 23,379 code commits from 21 projects. The results indicate substantial performance improvements over seven state-of-the-art baselines, with enhancements of up to 13.9% in F1-score, 8% in AUC, and 11% in Recall@20%E. The study also explores the impact of specific customization enhancements, demonstrating the potential for improved just-in-time defect localization.
Conclusion:
The proposed JIT-CF framework significantly advances the field of just-in-time defect prediction by effectively addressing the challenges encountered by pre-trained models in distinguishing code defects. The integration of contrastive learning and feature fusion not only enhances the model’s robustness but also leads to notable improvements in prediction accuracy, offering valuable insights for future applications in software development.",21 Mar 2025,8,"The study addresses an important aspect of software development and proposes an innovative framework that shows substantial performance improvements over existing baselines, offering valuable insights for future applications."
https://www.sciencedirect.com/science/article/pii/S0950584925000497,Fairness-aware practices from developers’ perspective: A survey,June 2025,Information and Software Technology,"Software engineering for artificial intelligence, Machine learning fairness engineering, Survey studies, Empirical software engineering",Gianmario=Voria: gvoria@unisa.it; Giulia=Sellitto: gisellitto@unisa.it; Carmine=Ferrara: cferrara@unisa.it; Francesco=Abate: f.abate20@studenti.unisa.it; Andrea=De Lucia: adelucia@unisa.it; Filomena=Ferrucci: fferrucci@unisa.it; Gemma=Catolino: gcatolino@unisa.it; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
Machine Learning (ML) technologies have shown great promise in many areas, but when used without proper oversight, they can produce biased results that discriminate against historically underrepresented groups. In recent years, the software engineering research community has contributed to addressing the need for ethical machine learning by proposing a number of fairness-aware practices, e.g., fair data balancing or testing approaches, that may support the management of fairness requirements throughout the software lifecycle. Nonetheless, the actual validity of these practices, in terms of practical application, impact, and effort, from the developers’ perspective has not been investigated yet.
Objective:
This paper addresses this limitation, assessing the developers’ perspective of a set of 28 fairness practices collected from the literature.
Methods:
We perform a survey study involving 155 practitioners who have been working on the development and maintenance of ML-enabled systems, analyzing the answers via statistical and clustering analysis to group fairness-aware practices based on their application frequency, impact on bias mitigation, and effort required for their application.
Results:
While all the practices are deemed relevant by developers, those applied at the early stages of development appear to be the most impactful. More importantly, the effort required to implement the practices is average and sometimes high, with a subsequent average application.
Conclusion:
The findings highlight the need for effort-aware automated approaches that ease the application of the available practices, as well as recommendation systems that may suggest when and how to apply fairness-aware practices throughout the software lifecycle.",21 Mar 2025,6,"The paper explores fairness practices in ML development, providing insights from developers' perspective. While the findings are relevant, the impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584925000485,Unveiling security weaknesses in autonomous driving systems: An in-depth empirical study,June 2025,Information and Software Technology,Not Found,Wenyuan=Cheng: closerecover@mails.ccnu.edu.cn; Zengyang=Li: zengyangli@ccnu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Ran=Mo: moran@ccnu.edu.cn; Hui=Liu: hliu@hust.edu.cn,"Abstract
Context:
The advent of Autonomous Driving Systems (ADS) has marked a significant shift towards intelligent transportation, with implications for public safety and traffic efficiency. While these systems integrate a variety of technologies and offer numerous benefits, their security is paramount, as vulnerabilities can have severe consequences for safety and trust.
Objective:
This study aims to systematically investigate potential security weaknesses in the codebases of prominent open-source ADS projects using CodeQL, a static code analysis tool. The goal is to identify common vulnerabilities, their distribution and persistence across versions to enhance the security of ADS.
Methods:
We selected three representative open-source ADS projects, Autoware, AirSim, and Apollo, based on their high GitHub star counts and Level 4 autonomous driving capabilities. Using CodeQL, we analyzed multiple versions of these projects to identify vulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow or Wraparound) and CWE-20 (Improper Input Validation). We also tracked the lifecycle of these vulnerabilities across software versions. This approach allows us to systematically analyze vulnerabilities in projects, which has not been extensively explored in previous ADS research.
Results:
Our analysis revealed that specific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were prevalent across the selected ADS projects. These vulnerabilities often persisted for over six months, spanning multiple version iterations. The empirical assessment showed a direct link between the severity of these vulnerabilities and their tangible effects on ADS performance.
Conclusions:
These security issues among ADS still remain to be resolved. Our findings highlight the need for integrating static code analysis into ADS development to detect and mitigate common vulnerabilities. Meanwhile, proactive protection strategies, such as regular update of third-party libraries, are essential to improve ADS security. And regulatory bodies can play a crucial role in promoting the use of static code analysis tools and setting industry security standards.",21 Mar 2025,9,The investigation of security vulnerabilities in open-source ADS projects using CodeQL is crucial for ensuring public safety and trust in autonomous driving systems. The findings highlight the importance of integrating static code analysis and proactive protection strategies.
https://www.sciencedirect.com/science/article/pii/S0950584925000473,Investigating the relationship between coordination strategy and coordination effectiveness in agile software development projects,June 2025,Information and Software Technology,Not Found,Geetha=Kanaparan: geetha.kanaparan@xmu.edu.my; Diane E.=Strode: diane.strode@alumni.unimelb.edu.au,"Abstract
Context
Agile software development (ASD) provides a way to coordinate teams and projects. Coordination is achieved by adopting a set of agile practices; however, these agile practices may differ for each project. The chosen assemblage of practices can be considered an agile project coordination strategy. The current body of knowledge about coordinative practices and theories of coordination in ASD is almost exclusively based on case studies. A validated model is currently lacking.
Objective
The objective is to validate a theoretical model to explain coordination in ASD, particularly the relationship between coordination strategy and coordination effectiveness.
Method
We validate this relationship based on an international survey of 340 agile practitioners and use PLS-SEM to estimate the relationships.
Results
The results show that an agile coordination strategy, that includes synchronisation, structure, and boundary-spanning, has a positive relationship with coordination effectiveness (implicit and explicit). Customer involvement moderates the relationship between coordination strategy and coordination effectiveness. These results are primarily supported by evidence from virtual work arrangements.
Conclusion
This research provides a validated coordination theory and information on what agile practices are related to effective coordination in agile software development. This coordination theory can be used to investigate coordination in future agile method variants used in system and software development projects.",21 Mar 2025,7,"The research validates a theoretical model for coordination in agile software development, offering insights into coordination strategies and effectiveness. While the results are valuable, the practical impact on European early-stage ventures may be moderate."
https://www.sciencedirect.com/science/article/pii/S0950584925000424,A software vulnerability detection method based on multi-modality with unified processing,June 2025,Information and Software Technology,Not Found,Wenjing=Cai: Not Found; Junlin=Chen: Not Found; Jiaping=Yu: Not Found; Wei=Hu: Not Found; Lipeng=Gao: gaolipeng@nwpu.edu.cn,"Abstract
With the development of the Internet and the Internet of Things, software has become an indispensable part, making software vulnerabilities one of the main threats to computer security. In recent years, a multitude of deep learning-based software vulnerability detection methods have been proposed, especially those based on multimodal approaches. Although these multimodal methods have proven to be effective, they often treat each modality separately. We propose a novel multimodal deep learning method for software vulnerability detection that achieves unified processing of various modalities. This method uses complex network analysis to convert the Code Property Graph into an image-like matrix, obtains key fragments from the source code using code slicing, and then uses a Transformer for function-level vulnerability detection. This enables deeper integration of information from multiple modalities, enhancing detection accuracy. Additionally, it significantly simplifies the model architecture. The result shows that compared to the state-of-the-art methods, our method has improved accuracy by 3%. Furthermore, our approach is capable of detecting some of the vulnerabilities recently released by CVE.",21 Mar 2025,9,"The proposed multimodal deep learning method for software vulnerability detection presents a novel approach with improved accuracy compared to existing methods. The integration of information from multiple modalities enhances detection accuracy, making it impactful for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584925000230,Practical assessment of the e-commerce multivariant user interface,May 2025,Information and Software Technology,Not Found,Adam=Wasilewski: adam.wasilewski@pwr.edu.pl; Elżbieta=Pawełek-Lubera: Not Found,"Abstract
Context:
Personalization is recognized as one of the key trends in e-commerce development, often including the personalization of offers and prices. However, a rarely used and underestimated personalization opportunity is the customization of the user interface provided to customers. Customers of e-shops differ in their behaviors and usage patterns, yet there is no clear evidence verifying the potential of the user interface to influence the performance indicators of e-shops.
Objective:
The research discussed in this paper aims to verify the impact of a dedicated interface on the most common indicators describing e-commerce performance and to identify limitations to the use of user interface personalization in e-commerce.
Method:
To achieve this, a solution was developed to collect information about e-commerce customer behavior, segment customers using clustering methods, and provide a dedicated user interface. During the pilot implementation, data was collected to verify the impact of the dedicated interface on the purchasing behavior of customer groups.
Results:
The results showed that a dedicated interface can significantly improve the conversion rate(by 46% in the analyzed group) and average order value (11%).
Conclusion:
These findings confirm that tailored UI variants can positively influence customer behavior in e-shops by increasing key performance indicators.",21 Mar 2025,8,The research on the impact of customized user interfaces in e-commerce can provide valuable insights for startups looking to improve performance indicators and customer behavior.
https://www.sciencedirect.com/science/article/pii/S0950584925000205,Why and how do organizations create user-led open source consortia? A systematic literature review,May 2025,Information and Software Technology,"Open source foundations, User-led open source consortia, Collaborative software development, Open-source software projects, User-driven open-source software development, Community-source software development, Coopetition, SLR, Systematic literature review",Elçin=Yenişen Yavuz: elcin.yenisen@fau.de; Dirk=Riehle: dirk@riehle.org,"Abstract
Context
User-led open source (OS) consortia (foundations) consist of organizations from industries beyond the software industry collaborating to create open-source software solutions for their internal processes. Initially pioneered by higher education organizations in the 2000s, this concept has gained traction in recent years across various industries.
Objective
This study has two research objectives. The first objective is to provide an overview of the current state of the art in this field by identifying previously studied topics and gathering examples from different industries. The second objective is to understand the structure of user-led OS consortia and the motivations of organizations for participating in such consortia.
Method
To gain a comprehensive understanding of this phenomenon, we conducted a systematic literature review, covering the years 2000 to 2023. Furthermore, we performed thematic analysis on 43 selected studies to identify and examine the key characteristics, ecosystems, and the benefits organizations gain from involvement in user-led OS consortia.
Results
We identified 43 unique papers on user-led OS consortia and provided details on 14 sample user-led OS consortia projects. We defined 19 characteristics of user-led OS consortia and 16 benefits for organizations’ involvement. Additionally, we outlined the key actors and their roles in user-led OS consortia.
Conclusion
We provided an overview of the current state of the art in this field. We identified the structure of user-led OS consortia and the organizations’ motivations for participating in such consortia.",21 Mar 2025,6,"The study on user-led open source consortia can offer insights for startups interested in collaborating with other industries to create open-source solutions, but the practical implications may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584925000175,Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review,May 2025,Information and Software Technology,"Software engineering for artificial intelligence, Non-functional requirements, Systematic literature reviews",Vincenzo=De Martino: vdemartino@unisa.it; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
Machine learning (ML) is nowadays so pervasive and diffused that virtually no application can avoid its use. Nonetheless, its enormous potential is often tempered by the need to manage non-functional requirements (NFRs) and navigate pressing, contrasting trade-offs.
Objective:
In this respect, we notice a lack of systematic synthesis of challenges explicitly tied to achieving and managing NFRs in ML-enabled systems. Such a synthesis may not only provide a comprehensive summary of the state of the art but also drive further research on the analysis, management, and optimization of NFRS of ML-enabled systems.
Method:
In this paper, we propose a systematic literature review targeting two key aspects such as (1) the classification of the NFRs investigated so far, and (2) the challenges associated with achieving and managing NFRs in ML-enabled systems during model development Through the combination of well-established guidelines for conducting systematic literature reviews and additional search criteria, we survey a total amount of 130 research articles.
Results:
Our findings report that current research identified 31 different NFRs, which can be grouped into six main classes. We also compiled a catalog of 26 software engineering challenges, emphasizing the need for further research to systematically address, prioritize, and balance NFRs in ML-enabled systems.
Conclusion:
We conclude our work by distilling implications and a future outlook on the topic.",21 Mar 2025,7,"The systematic review on managing non-functional requirements in ML-enabled systems can provide valuable guidance for startups incorporating machine learning in their products, enhancing their understanding of challenges and trade-offs."
https://www.sciencedirect.com/science/article/pii/S0950584925000163,Mining software repositories for software architecture — A systematic mapping study,May 2025,Information and Software Technology,"Mining software repositories, Software architecture, Empirical research",Mohamed=Soliman: Not Found; Michel=Albonico: Not Found; Ivano=Malavolta: i.malavolta@vu.nl; Andreas=Wortmann: Not Found,"Abstract
Context:
A growing number of researchers are investigating how Mining Software Repositories (MSR) approaches can support software architecture activities, such as architecture recovery, tactics identification, architectural smell detection, and others. However, as of today, it is difficult to have a clear view of existing research on MSR for software architecture.
Objectives:
The objective of this study is to identify, classify, and summarize the state-of-the-art MSR approaches applied to software architecture (MSR4SA).
Methods:
This study is designed according to the 
systematic mapping study
 research method. Specifically, out of 2442 potentially relevant studies, we systematically identify 151 primary studies where MSR approaches are applied to perform software architecture activities. Then, we rigorously extract relevant data from each primary study and synthesize the obtained results to produce a clear map of reasons for adopting MSR approaches to support architecting activities, used data sources, applied MSR techniques, and captured architectural information.
Results:
The major reasons to adopt MSR4SA techniques are about addressing industrial concerns like 
achieving quality attributes
 and 
minimizing practitioners’ efforts
. Most MSR4SA studies support architectural analysis, while architectural synthesis and evaluation are not commonly supported in MSR4SA studies. The most frequently mined data sources are 
source code repositories
 and 
issue trackers
, which are also commonly mined together. Most of the MSR4SA studies apply more than one mining technique, where the most common MSR techniques are: (
source code analysis
, 
model analysis
, 
statistical analysis
), (
machine learning
, 
NLP
). 
Architectural quality issues
 and 
components
 are the mostly mined type of information.
Conclusion:
Our results give a solid foundation for researchers and practitioners towards future research and applications of MSR approaches for software architecture.",21 Mar 2025,8,"The mapping study on mining software repositories for software architecture can offer startups valuable insights into leveraging MSR approaches to support software architecture activities, aiding in improving software quality and minimizing efforts."
https://www.sciencedirect.com/science/article/pii/S0950584925000151,Beyond the lab: An in-depth analysis of real-world practices in government-to-citizen software user documentation,May 2025,Information and Software Technology,"User documentation, Digital transformation, Digital switzerland strategy, Data analysis",Francesco=Sovrano: francesco.sovrano@uzh.ch; Sandro=Vonlanthen: sandro.vonlanthen@uzh.ch; Alberto=Bacchelli: alberto.bacchelli@uzh.ch,"Abstract
Context:
Governments, including Switzerland through its 
Digital Switzerland Strategy
, are using new technologies to improve public services. However, unclear user guides often lead people to prefer expensive help desk services. Current research on software documentation is limited by small-scale surveys that do not reflect real-world challenges. This paper addresses these gaps by examining the limitations of user guides in a more practical context.
Objective:
Building on the identified need for a more comprehensive understanding of user documentation in real-world applications, this study aims to critically analyse user documentation in government-to-citizen (G2C) interactions within Switzerland. We intend to identify both common and critical issues in existing documentation to direct future research towards substantial improvements. By doing so, this research will contribute to the development of more effective user guides, ultimately improving the digital experience for citizens and reducing reliance on costly help desk support.
Methods:
Our research methodology involved a thorough analysis of user documentation in German-speaking Swiss cantons. We began with around 5’000 links from official cantonal websites and narrowed it down to nearly 600 user guides relevant to G2C applications. The study progressed in phases: we first assessed the content to identify real-world documentation characteristics, then compared these with common issues from academic research to pinpoint frequent problems. Finally, we analysed the data to identify overarching trends in the documentation characteristics and issues.
Results:
Our analyses, which linked guide features to documentation issues, uncovered prevalent real-world issue trends, characterized by significant statistical correlations (
p
<
.
05
) with the socioeconomic status of the cantons, such as their wealth and population size.
Conclusions:
Identifying these trends will help researchers and practitioners concentrate on the most common and critical issues encountered in practice. This, in turn, holds the potential to drive the development of more effective technology for documenting software. 
Data and Materials:
 
https://doi.org/10.5281/zenodo.10592871",21 Mar 2025,9,"The critical analysis of user documentation in government-to-citizen interactions within Switzerland can be highly beneficial for startups aiming to enhance user guides and digital experiences, potentially reducing reliance on costly help desk support."
https://www.sciencedirect.com/science/article/pii/S095058492500014X,A more accurate bug localization technique for bugs with multiple buggy code files,May 2025,Information and Software Technology,Not Found,Hui=Xu: lyraxv@nuaa.edu.cn; Zhaodan=Wang: wangzhaodan@nuaa.edu.cn; Weiqin=Zou: weiqin@nuaa.edu.cn,"Abstract
Context:
Bug localization is a key step in bug fixing. Despite considerable progress, existing bug localization techniques still perform unsatisfactorily in situations where the complete fix to a bug involves touching multiple buggy code files. That is, for such bugs, those techniques tend to locate correctly only one or at least not all buggy code files, leaving other buggy code files undetected.
Objective:
This study aims to improve bug localization in cases where resolving a bug requires modifications to multiple buggy code files by proposing HitMore to rank more truly buggy files higher in the recommendation list.
Method:
The basic idea of HitMore is to attempt to retrieve a subset of truly buggy code files first, then use these files to retrieve other buggy code files based on code relation analysis. For the first part, we designed three kinds of domain-specific features to build a machine-learning model to identify the truly buggy code file subset. For the second part, we make use of three types of code relations between the code base and the buggy file subset to better retrieve the remaining truly buggy code files.
Results:
The experiments on six widely open-source projects show that: Our technique is effective in identifying the subset of truly buggy code files, with a weighted prediction F1-Score of 86.1%–92.1%. By leveraging the code relations to the retrieved subset and the code base, our HitMore could retrieve all truly buggy code files for 29.31%–69.56% of bugs across six projects. For multiple-buggy-code-file bugs, HitMore could completely localize such bugs by up to 15.38%, 19.36%, and 11.86% more than three representative IRBL baselines across six projects.
Conclusion:
The experimental results demonstrate the potential of HitMore in reducing developers’ burden of locating and further fixing relatively complex bugs such as those with multiple buggy code files in practice.",21 Mar 2025,9,"The study addresses a practical and significant issue in bug localization for software development, providing a novel HitMore technique that shows promising results in improving bug localization for multiple-buggy-code-file bugs, which can reduce developers' burden and enhance bug-fixing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584925000102,Test smell: A parasitic energy consumer in software testing,May 2025,Information and Software Technology,"Test smell, Energy efficiency, Test smell refactoring, Sustainable software engineering, Green software engineering",Md Rakib Hossain=Misu: mdrh@uci.edu; Jiawei=Li: jiawl28@uci.edu; Adithya=Bhattiprolu: abhattip@uci.edu; Yang=Liu: yangl73@uci.edu; Eduardo Santana=de Almeida: eduardo.almeida@ufba.br; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
Traditionally, energy efficiency research has focused on reducing energy consumption at the hardware level and, more recently, in the design and coding phases of the software development life cycle. However, software testing’s impact on energy consumption did not receive attention from the research community. Specifically, how test code design quality and test smell (e.g., sub-optimal design and bad practices in test code) impact energy consumption has not been investigated yet.
Objective:
This study aims to examine open-source software projects to analyze the association between test smell and its effects on energy consumption in software testing.
Methods:
We conducted a mixed-method empirical analysis from two perspectives; software (data mining in 12 Apache projects) and developers’ views (a survey of 62 software practitioners).
Results:
Our findings show that: (1) test smell is associated with energy consumption in software testing. Specifically, the smelly part of a test case consumes more energy compared to the non-smelly part. (2) certain test smells are more energy-hungry than others, (3) refactored test cases tend to consume less energy than their smelly counterparts, and (4) most developers (45
%
 of the survey respondents) lack knowledge about test smells’ impact on energy consumption.
Conclusion:
Based on the results, we emphasize raising developers awareness regarding the impact of test smells on energy consumption. Additionally we present several observations that can direct future research and developments.",21 Mar 2025,7,"The research sheds light on the impact of test smells on energy consumption in software testing, providing valuable insights for developers to optimize test code design quality. The findings can potentially lead to more energy-efficient software testing practices and raise awareness among developers."
https://www.sciencedirect.com/science/article/pii/S0950584925000242,Production and test bug report classification based on transfer learning,May 2025,Information and Software Technology,Not Found,Misoo=Kim: misoo.kim@jnu.ac.kr; Youngkyoung=Kim: agness66@skku.edu; Eunseok=Lee: leees@skku.edu,"Abstract
Context:
Recent studies indicate that the classification of production and test bug reports can substantially enhance the accuracy of performance evaluation and the effectiveness of information retrieval–based bug localization (IRBL) for software reliability.
Objective:
However, manually classifying these bug reports is time-consuming for developers. This study introduces a production and test bug report classification (ProTeC) framework for automatically classifying these reports.
Methods:
The framework’s novelty lies in leveraging a set of production- and test-source files and employing transfer learning to address the issue of insufficient and sparse bug reports in machine-learning applications. The ProTeC framework trains and fine-tunes a source file classifier to develop a bug report classifier by transferring production-test distinguishing knowledge.
Results:
To validate the effectiveness and general practicality of ProTeC, we conducted large-scale experiments using 2,522 bug reports across 12 machine/deep learning model variations to train an automatic classifier. Our results, on average, demonstrate that ProTeC’s macro F1-score is 28.6% higher than that of a bug report-based classifier, and it can improve the mean average precision of IRBL by 17.6%.
Conclusion:
These positive trends were observed in most model variations, indicating that ProTeC consistently performs well in classifying bug reports regardless of the model used, thereby improving IRBL performance.",21 Mar 2025,8,"The ProTeC framework introduces an innovative approach to automatically classify production and test bug reports, which can significantly enhance bug localization performance. The results demonstrate the practicality and effectiveness of ProTeC in improving bug report classification for software reliability."
https://www.sciencedirect.com/science/article/pii/S0950584925000199,Process mining for agile software process assessment and improvement,May 2025,Information and Software Technology,Not Found,Katiane Oliveira Alpes=da Silva: koas@cin.ufpe.br; Ricardo Massa Ferreira=Lima: rmfl@cin.ufpe.br; Vanderson Botelho=da Silva: vanderson.silva@serpro.gov.br,"Abstract
Context:
Agile software processes, designed for flexibility and continuous improvement, pose challenges in extracting actionable insights from event logs due to their inherent unstructured nature.
Objective:
The study evaluates whether existing process mining techniques can effectively uncover reliable and insightful information on software development processes adopting agile methodologies.
Method:
The work uses various algorithms to analyze procedural flows and business rules within an event log containing data from 3,418 agile software development projects at a company with over 1,500 employees. By categorizing processes according to project size, our analysis aimed to determine the kind of insights these algorithms could reveal. We specifically focused on algorithms that produced high-quality insights for a deeper examination of aspects like effort rate, frequency of activities, and relationships between activities. Subsequently, technical and managerial staff reviewed the results to assess the quality and relevance of the insights generated. Validation involved a semi-structured interview with managers and technicians to ensure the relevance and applicability of the findings.
Results:
The analysis demonstrates the efficacy of declarative business process techniques in extracting actionable insights from agile development teams’ data. Such techniques accurately capture the daily routines and documented processes of the teams. High-performing teams typically followed fewer rules, had less job rotation, involved fewer individuals, and engaged in a more limited range of activities. Domain experts and team managers found these insights to be coherent and potentially valuable for enhancing the performance of software development processes.
Conclusions:
Declarative modeling is particularly adept at revealing the patterns of flexible software development workflows, presenting initial support for teams, managers, and decision-makers through both descriptive and prescriptive analysis.",21 Mar 2025,6,"The study evaluates process mining techniques in agile software development processes, uncovering valuable insights for improving software development workflows. The findings contribute to enhancing performance and flexibility in agile projects, although the impact may vary based on project size and team dynamics."
https://www.sciencedirect.com/science/article/pii/S0950584925000187,Re-evaluating metamorphic testing of chess engines: A replication study,May 2025,Information and Software Technology,Not Found,Axel=Martin: Not Found; Djamel Eddine=Khelladi: Not Found; Théo=Matricon: Not Found; Mathieu=Acher: mathieu.acher@irisa.fr,"Abstract
Context:
This study aims to confirm, replicate and extend the findings of a previous article entitled 
”Metamorphic Testing of Chess Engines”
 that reported inconsistencies in the analyses provided by 
Stockfish
, the most widely used chess engine, for transformed chess positions that are fundamentally identical. Initial findings, under conditions strictly identical to those of the original study, corroborate the reported inconsistencies.
Objective:
However, the original article considers a specific dataset (including randomly generated chess positions, end-games, or checkmate problems) and very low analysis depth (10 plies,
1
 corresponding to 5 moves). These decisions pose threats that limit generalizability of the results, but also their practical usefulness both for chess players and maintainers of Stockfish. Thus, we replicate the original study.
Methods:
We consider this time (1) positions derived from actual chess games, (2) analyses at appropriate and larger depths, and (3) different versions of Stockfish. We conduct novel experiments on thousands of positions, employing significantly deeper searches.
Results:
The replication results show that the Stockfish chess engines demonstrate significantly greater consistency in its evaluations. The metamorphic relations are not as effective as in the original article, especially on realistic chess positions. We also demonstrate that, for any given position, there exists a depth threshold beyond which further increases in depth do not result in any evaluation differences for the studied metamorphic relations. We perform an in-depth analysis to identify and clarify the implementation reasons behind Stockfish’s inconsistencies when dealing with transformed positions.
Conclusion:
A first concrete result is thus that metamorphic testing of chess engines is not yet an effective technique for finding faults of Stockfish. Another result is the lessons learned through this replication effort: metamorphic relations must be verified in the context of the domain’s specificities; without such contextual validation, they may lead to misleading or irrelevant conclusions; changes in parameters and input dataset can drastically alter the effectiveness of a testing method.",21 Mar 2025,5,"The replication study on metamorphic testing of chess engines provides insights on the limitations and effectiveness of the technique. While the findings contribute to understanding the evaluation consistency of Stockfish, the practical implications for chess players and engine maintainers may be limited due to specific experimental conditions and results."
https://www.sciencedirect.com/science/article/pii/S0950584925000254,Vulnerability detection with feature fusion and learnable edge-type embedding graph neural network,May 2025,Information and Software Technology,Not Found,Ge=Cheng: chengge@xtu.edu.cn; Qifan=Luo: 202221632987@smail.xtu.edu.cn; Yun=Zhang: yunzhangcn@outlook.com,"Abstract
Deep learning methods are widely employed in vulnerability detection, and graph neural networks have shown effectiveness in learning source code representation. However, current methods overlook non-relevant noise information in the code property graph and lack specific graph neural networks designed for code property graph. To address these issues, this paper introduces Leev, an automated vulnerability detection method. We developed a graph neural network tailored to the code property graph, assigning iterative vectors to diverse edge types and integrating them into the message passing between nodes to enable the model to extract hidden vulnerability information. In addition, virtual nodes are incorporated into the graph for feature fusion, mitigating the impact of irrelevant features on vulnerability information within the code. Specifically, for the FFMPeg+Qemu, Reveal, and Fan et al. datasets, the F1 metrics exhibited improvements of 7.02%, 21.69%, and 27.74% over the best baseline, correspondingly.",21 Mar 2025,7,The development of an automated vulnerability detection method using graph neural networks tailored for code property graph with significant improvements in F1 metrics for various datasets could have a practical impact on improving security for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584925000229,BinOpLeR: Optimization level recovery from binaries based on rich semantic instruction image and weighted voting,May 2025,Information and Software Technology,Not Found,Xiaonan=Li: Not Found; Qingbao=Li: Not Found; Guimin=Zhang: zh.guimin@163.com; Jinjin=Liu: Not Found; Shudan=Yue: Not Found; Weihua=Jiao: Not Found,"Abstract
Context:
Compiler toolchain differences result in binary code diversity, wherein the impacts of different optimization levels on binary code severely constrains the performance improvement of software security detection tasks such as malware detection, software copyright protection, and vulnerability homology detection. However, binaries compiled with different optimization levels often contain numerous identical or similar code fragments, posing severe challenges to recovering the optimization levels from binaries.
Objective:
The existing optimization level detection methods based on statistical features have poor generalization capabilities, and those based on automated learning have low detection accuracy due to using coarse-grained instruction normalization. To improve accuracy and generalization capabilities, this paper proposes BinOpLeR, a binary optimization level recovery method based on rich semantic instruction images and weighted voting.
Method:
In this paper, we perform fine-grained normalization on disassembly instructions to retain the elements that reflect instruction semantics and code execution characteristics, and utilize the mappings from the ASCII code values of assembly codes to pixel grayscale values to convert functions into grayscale images. Then, a balanced dataset is constructed using the grayscale images of functions to train a convolutional neural network model with adaptive pooling to capture optimization level-related features. Finally, a weighted voting scheme that incorporates prediction probabilities and function lengths is innovatively introduced to infer the optimization levels of binaries.
Results:
We evaluate the performance of BinOpLeR on the public dataset of ARM and MIPS binaries using precision, accuracy, recall and F1 score. The results show that BinOpLeR outperforms the comparison methods in prediction performance.
Conclusion:
The findings indicate that: BinOpLeR effectively improves the accuracy of the optimization levels recovery from binaries. It exhibits stable performance across different compiler versions. The granularity and normalization significantly influence feature extraction, and function lengths along with prediction probabilities are crucial factors in inferring the optimization level of binaries.",21 Mar 2025,9,"The proposal of BinOpLeR, a method for binary optimization level recovery using rich semantic instruction images and weighted voting, shows promising results in outperforming comparison methods. This could have a significant impact on software security tasks for startups."
https://www.sciencedirect.com/science/article/pii/S0950584925000370,Different and similar perceptions of communication among software developers,May 2025,Information and Software Technology,"Perception, Software developer, Software project, Development team, Social aspects, Personality, Human values, Exploratory data analysis, Cluster analysis",Marc=Herrmann: marc.herrmann@inf.uni-hannover.de; Martin=Obaidi: martin.obaidi@inf.uni-hannover.de; Jil=Klünder: jil.kluender@inf.uni-hannover.de,"Abstract
Context:
Software development is a collaborative task involving different persons. Development team members are often diverse in regard to several aspects, including experience, (soft) skills, and communication habits. Different preferences in what adequate communication looks like influence how communication is perceived and interpreted by team members.
Objective:
In this paper, we investigate differences and similarities in how software developers with varying levels of experience and skills perceive statements from exemplary software project communication.
Methods:
By applying hierarchical cluster analysis on the perception data of 94 software developers, we aim to find groups of developers sharing similar perceptions towards statements from software project communication, and to identify factors that influence this perception.
Results:
We contribute the following key findings: (1) We statistically identify two groups of software developers whose perceptions differ significantly for about 65% of statements from software project communication; (2) For a logistic regression model, five polarizing statements suffice to assign each participant to their group; (3) Although there is a significant difference in the communication perception, there are no demographic characteristics that differ notably across the two groups.
Conclusion:
From our results, we conclude that different perceptions of software project communication during collaboration within development teams are a potential risk for the teams’ mood and the project success. We outline how our results can serve use cases like the application of sentiment analysis in software engineering and mindful communication in software teams in general.",21 Mar 2025,5,"Investigating how software developers perceive communication in development teams, while interesting, may not have a direct practical impact on early-stage ventures or startups in terms of improving their operations or outcomes."
https://www.sciencedirect.com/science/article/pii/S0950584925000382,Assessing and improving syntactic adversarial robustness of pre-trained models for code translation,May 2025,Information and Software Technology,Not Found,Guang=Yang: yang.guang@nuaa.edu.cn; Yu=Zhou: zhouyu@nuaa.edu.cn; Xiangyu=Zhang: zhangx1angyu@nuaa.edu.cn; Xiang=Chen: xchencs@ntu.edu.cn; Tingting=Han: t.han@bbk.ac.uk; Taolue=Chen: t.chen@bbk.ac.uk,"Abstract
Context:
Pre-trained models (PTMs) have demonstrated significant potential in automatic code translation. However, the vulnerability of these models in translation tasks, particularly in terms of syntax, has not been extensively investigated.
Objective:
To fill this gap, our study aims to propose a novel approach 
CoTR
 to assess and improve the syntactic adversarial robustness of PTMs in code translation.
Methods:
CoTR
 consists of two components: 
CoTR-A
 and 
CoTR-D
. 
CoTR-A
 generates adversarial examples by transforming programs, while 
CoTR-D
 proposes a semantic distance-based sampling data augmentation method and adversarial training method to improve the model’s robustness and generalization capabilities. The Pass@1 metric is used by 
CoTR
 to assess the performance of PTMs, which is more suitable for code translation tasks and offers a more precise evaluation in real-world scenarios.
Results:
The effectiveness of 
CoTR
 is evaluated through experiments on real-world Java
↔
Python datasets. The results demonstrate that 
CoTR-A
 can significantly reduce the performance of existing PTMs, while 
CoTR-D
 effectively improves the robustness of PTMs.
Conclusion:
Our study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of 
CoTR
 as an effective solution to enhance the robustness of PTMs for code translation tasks.",21 Mar 2025,8,The study focusing on improving the syntactic adversarial robustness of pre-trained models in code translation tasks through CoTR offers a potential solution to enhance the performance of PTMs. This could be beneficial for startups dealing with code translation tasks.
https://www.sciencedirect.com/science/article/pii/S0950584925000217,"Exploring the means to measure explainability: Metrics, heuristics and questionnaires",May 2025,Information and Software Technology,"Explainability, Requirements engineering, Quality models, Metrics, Heuristics, Literature studies",Hannah=Deters: hannah.deters@inf.uni-hannover.de; Jakob=Droste: jakob.droste@inf.uni-hannover.de; Martin=Obaidi: martin.obaidi@inf.uni-hannover.de; Kurt=Schneider: kurt.schneider@inf.uni-hannover.de,"Abstract
Context:
As the complexity of modern software is steadily growing, these systems become increasingly difficult to understand for their stakeholders. At the same time, opaque and artificially intelligent systems permeate a growing number of safety-critical areas, such as medicine and finance. As a result, explainability is becoming more important as a software quality aspect and non-functional requirement.
Objective:
Contemporary research has mainly focused on making artificial intelligence and its decision-making processes more understandable. However, explainability has also gained traction in recent requirements engineering research. This work aims to contribute to that body of research by providing a quality model for explainability as a software quality aspect. Quality models provide means and measures to specify and evaluate quality requirements.
Method:
In order to design a user-centered quality model for explainability, we conducted a literature review.
Results:
We identified ten fundamental aspects of explainability. Furthermore, we aggregated criteria and metrics to measure them as well as alternative means of evaluation in the form of heuristics and questionnaires.
Conclusion:
Our quality model and the related means of evaluation enable software engineers to develop and validate explainable systems in accordance with their explainability goals and intentions. This is achieved by offering a view from different angles at fundamental aspects of explainability and the related development goals. Thus, we provide a foundation that improves the management and verification of explainability requirements.",21 Mar 2025,6,"The quality model for explainability in software systems, while important, may have a more indirect impact on early-stage ventures compared to other abstracts that directly address software security tasks or code optimization."
https://www.sciencedirect.com/science/article/pii/S0950584925000369,Formal requirements engineering and large language models: A two-way roadmap,May 2025,Information and Software Technology,"Requirements engineering, Formal methods, Large language models, LLMs, Natural language processing, NLP, NLP4RE, Prompt engineering, Prompt requirements engineering",Alessio=Ferrari: alessio.ferrari@isti.cnr.it; Paola=Spoletini: pspoleti@kennesaw.edu,"Abstract
Context:
Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness.
Objective:
This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible.
Methods:
We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies.
Results:
The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community.
Conclusion:
The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs.",21 Mar 2025,8,"This abstract addresses important concerns in the field of requirements engineering and software development, providing a roadmap for leveraging formal methods to ensure correctness, fairness, and trustworthiness in LLM-generated solutions. The practical implications and impact on early-stage ventures are significant."
https://www.sciencedirect.com/science/article/pii/S0950584924002532,Concept definition review: A method for studying terminology in software engineering,April 2025,Information and Software Technology,"Literature review, Research method, Concept definition, Software engineering, Requirements engineering",Sabine=Molenaar: s.molenaar@uu.nl; Nikita=van den Berg: Not Found; Fabiano=Dalpiaz: Not Found; Sjaak=Brinkkemper: Not Found,"Abstract
Context:
In scientific domains, definitions provide a precise description of fundamental concepts. Although the debate within the philosophy of computer science regarding the scientific nature of software engineering (SE) is inconclusive, SE researchers have laid down important steps toward treating SE as a scientific paradigm.
Objective:
We aim to support precise and effective communication among SE researchers and practitioners by providing a systematic process for the identification and analysis of definitions, in order to support the selection of a suitable definition for a certain use case.
Method:
Inspired by methods for the planning and execution of systematic literature reviews, we construct a method that is specific for concept definition reviews (CDRs). These reviews are performed whenever a research team wishes to obtain a detailed understanding of an SE concept that may have been characterized by dozens, if not hundreds, definitions.
Results:
We built our method via two 
design science
 iterations. The first one focused on the concept 
feature
 and resulted in the definitive version of the CDR method presented in this paper. We then applied the revised method to two, related concepts: 
quality requirement
 and 
non-functional requirement
. Besides showing the applicability of the CDR method, our results include findings regarding the characteristics and evolution of the terms.
Conclusions:
The two applications of the CDR method highlight the existence and citation of hundreds of definitions, many of which are nearly (but not exactly) identical. We put forward our method for other researchers to shed light on the key terminology in other sub-fields of SE.",21 Mar 2025,4,"While the systematic process for identifying and analyzing definitions in software engineering is valuable for SE researchers and practitioners, the practical impact on early-stage ventures and startups may not be as immediate or significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002647,MT-Nod: Metamorphic testing for detecting non-optimal decisions of autonomous driving systems in interactive scenarios,April 2025,Information and Software Technology,Not Found,Zhen=Yang: yangzhen@aeu.edu.cn; Song=Huang: huangsong@aeu.edu.cn; Xingya=Wang: xingyawang@outlook.com; Tongtong=Bai: btt070619@163.com; Yang=Wang: wangy621@aeu.edu.cn,"Abstract
Context:
Autonomous driving technology advances into daily life, with expectations for autonomous driving systems (ADSs) to make optimal, human-like decisions. However, ADSs often exhibit “unintelligent” behaviors like inefficient path choices, significantly impacting travel efficiency and potentially causing delays. Therefore, testing the decision optimality of ADSs is critically urgent. However, the testing process faces a significant “testing oracle” problem, and current methods overlook behavior interactions, which do not reflect real-world traffic scenarios.
Objective:
To assess the performance and reliability of ADSs in optimal decision-making, mitigate test oracle problems, and detect non-optimal decisions without calculating the optimal path.
Method:
This paper proposes a metamorphic testing method for optimal decision-making in autonomous driving under interactive scenarios, MT-Nod. Our method introduces a novel metamorphic relation to evaluate the optimality of path directions, along with a follow-up scenario generation method. The scenario generation method includes mutation points selection based on key behaviors, scenario mutation based on behavioral interactions, and road accessibility assessment, to generate scenarios with dynamic interactions. Additionally, a scenario scheduling strategy is designed to prioritize and schedule scenarios based on the priority of mutation points.
Results:
We evaluate MT-Nod extensively on the advanced Apollo ADS. Across four source scenarios, it generates 69.3 non-optimal decision scenarios (NoDSs), classified into eight types. Compared to baselines, MT-Nod efficiently produces and detects a greater variety and quantity of NoDSs.
Conclusion:
The proposed method for optimal decision testing under interactive scenarios, MT-Nod, effectively detects non-optimal decisions of ADSs. These “unintelligent” behaviors are crucial for enhancing the performance and reliability of ADSs.",21 Mar 2025,9,"The proposed metamorphic testing method for optimal decision-making in autonomous driving systems addresses a critical issue in the field, showing promising results in enhancing the performance and reliability of ADSs. The impact on European early-stage ventures, especially in the tech and automotive sectors, is substantial."
https://www.sciencedirect.com/science/article/pii/S0950584924002659,Boosting mutation-based fault localization by effectively generating Higher-Order Mutants,April 2025,Information and Software Technology,Not Found,Shumei=Wu: wsm@mail.buct.edu.cn; Binbin=Yang: Not Found; Zexing=Chang: Not Found; Zheng=Li: lizheng@mail.buct.edu.cn; Xiang=Chen: Not Found; Yong=Liu: lyong@mail.buct.edu.cn,"Abstract
Context:
Fault Localization (FL) is an important and tedious phase of software debugging. Among various FL techniques, Mutation-Based Fault Localization (MBFL) demonstrates promising FL accuracy utilizing impact information of statements provided by First-Order-Mutants (FOMs). Despite its success in Single-Fault Scenarios (SFSs), it fails to achieve satisfactory performance in Multiple-Fault Scenarios (MFSs). Higher-Order-Mutants (HOMs) provide a solution for MFSs. However, existing work on HOM generation is inadequate and ignores the correlation among faults in MFSs.
Objective:
In this article, we systematically analyze three relationships among single-faults in MFSs, and further propose three HOM generation methods (i.e., SFClu, SFDis, and SFDen) to simulate different multiple-faults and improve the effectiveness of MBFL in MFSs.
Method:
We investigate the multiple-fault composition on real-world 393 faulty programs from Defects4J, and then apply our methods to generate HOMs for FL. Specifically, SFClu focuses on generating appropriate HOMs for Multi-Single-Source Fault (MSSF) scenarios, where each single-fault is responsible for different observed failures. SFDis is well-suited for Multi-Coupled-Source Fault (MCSF) scenarios where at least two single-faults can interact with each other, leading to certain failures either being observable or masked. SFDen aims to generate suitable HOMs for Single-Coupled-Source Fault (SCSF) scenarios with multiple single-faults that occur within a statement.
Results:
(1) The proportion of MFSs is as high as 63.10% in real-world programs, with MSSF, MCSF , and SCSF scenarios accounting for 35.08%, 53.23%, and 11.69%, respectively. (2) Compared to the best-performing mutant generation method Neural-MBFL, SFClu, SFDis, and SFDen can improve the FL performance for MBFL by 36.78%, 49.80%, and 16.36% in 
T
o
p
-1, respectively, outperforming eight established SBFL and MBFL techniques. (3) SFClu, SFDis, and SFDen are more suitable for MSSF, MCSF, and SCSF scenarios, respectively, which aligns with their design intend. (4) Their combination further enhances FL accuracy, achieving up to 85 faults successfully localized and an average improvement of 29.54% in 
T
o
p
-1. Finally, extensive evaluations on SIR with artificial faults and Codeflaws containing student programs demonstrate the generalization of SFClu, the applicability of SFDis on real faults, and the suitability of SFDen for student programs.
Conclusion:
Empirical studies have confirmed the prevalence of MFSs, highlighting the significance of multi-fault localization. Moreover, our proposed three HOM generation methods can further enhance the performance of MBFL with HOMs in MFSs, showing their effectiveness and applicability.",21 Mar 2025,7,"The systematic analysis of fault localization techniques and the proposal of new methods to improve the effectiveness of MBFL in multiple-fault scenarios have practical value for software debugging and development. While the impact on European early-stage ventures is significant, it may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584924002660,An input-denoising-based defense against stealthy backdoor attacks in large language models for code,April 2025,Information and Software Technology,Not Found,Yubin=Qu: Not Found; Song=Huang: huangsong@aeu.edu.cn; Xiang=Chen: Not Found; Tongtong=Bai: Not Found; Yongming=Yao: Not Found,"Abstract
Context:
Large Language Models are becoming integral to software development. They are trained on open data from platforms like GitHub, making them vulnerable to poisoning attacks. Research shows that backdoor attacks with traditional static triggers using fixed code patterns are relatively easy to detect. The novel attack approach uses specific Syntax Tree structures as triggers, offering greater stealthiness while maintaining explicit code structures. This method poses new challenges for backdoor detection.
Objective:
We propose an 
I
nput-
D
 enoising-based defense against stealthy 
B
ackdoor 
A
ttacks with dynamic triggers (
IDBA
) in Large Language Models for Code.
Method:
We overlay a set of malicious code segments onto the code segment with dynamic triggers, convert the output state of the input code into a random walk graph neural network, calculate the expected value of the final state through particle filtering, and thus detect the existence of a backdoor attack.
Results:
Empirical studies are conducted on Codebert, GraphCodebert, and CodeT5 for vulnerability and code clone detection tasks. Our results show that 
IDBA
 achieves an average detection rate of 73.75% and 68.12% for vulnerability and code clone detection tasks, respectively.
Conclusion:
Detecting backdoor attacks using 
IDBA
 on code models allows for the early identification of potential backdoor threats after model deployment, enhancing the security of code models.",21 Mar 2025,6,"The defense against stealthy backdoor attacks in Large Language Models for code using IDBA is an important contribution to the field. While the practical implications for software security are noteworthy, the direct impact on European early-stage ventures may be more indirect."
https://www.sciencedirect.com/science/article/pii/S0950584925000096,MPCA: Constructing the APTs provenance graphs through multi-perspective confidence and association,April 2025,Information and Software Technology,Not Found,Zhao=Zhang: Not Found; Senlin=Luo: Not Found; Yingdan=Guan: Not Found; Limin=Pan: panlimin2016@gmail.com,"Abstract
The forensic analysis of Advanced Persistent Threats (APTs) attacks is crucial for maintaining cybersecurity. To address the challenges posed by the high complexity and strong concealment of APT attacks, provenance graph based on inter entity dependencies are used for forensic investigation. However, under long-term persistent attacks, entities with semantically consistent behavior patterns become excessively redundant, leading to an explosion of inter entity dependencies and a decrease in forensic efficiency. In addition, the implicit relationships within and between events are not fully represented, and alarm information spreads to neighboring benign events, making it difficult to accurately reconstruct attack scenario. In this paper, we propose an APT attack attribution method MPCA that combines multi-perspective confidence and association. Firstly, by merging parallel branches with semantically consistent behavior patterns in the process connected subgraph, redundant entities and their dependencies are reduced. Secondly, event confidence is estimated to exclude benign events, the association between events and alarms is analyzed to highlight attack events. Experimental results demonstrate that MPCA achieves state-of-the-art performance. MPCA can improve the efficiency of constructing attack scenario graphs, reduce false positive and false negative rates, and demonstrate greater adaptability in attack attribution tasks.",21 Mar 2025,8,"The proposed APT attack attribution method MPCA demonstrates state-of-the-art performance and can improve efficiency in constructing attack scenario graphs, reducing false positives and negatives. This can have a significant impact on early-stage ventures dealing with cybersecurity issues."
https://www.sciencedirect.com/science/article/pii/S0950584925000126,Alleviating class imbalance in Feature Envy prediction: An oversampling technique based on code entity attributes,April 2025,Information and Software Technology,Not Found,Jiamin=Guo: jiaminguo822@163.com; Yangyang=Zhao: yangyangzhao@zstu.edu.cn; Tao=Zheng: z337997332@163.com; Zhifei=Chen: chenzhifei@njust.edu.cn; Mingyue=Jiang: mjiang@zstu.edu.cn; Zuohua=Ding: zouhuading@hotmail.com,"Abstract
Context:
Feature Envy is a common code smell that occurs when a method heavily relies on data or functionality from other classes. Detecting Feature Envy is essential for improving software modularity and reducing technical debt. However, real-world datasets often exhibit severe class imbalance, with far fewer Feature Envy instances than non-smelly ones, posing challenges for prediction models. Traditional oversampling techniques attempt to address this issue by relying solely on numerical vectors but often fail to capture the complex relationships between code entities, potentially deviating from the nature of Feature Envy.
Objective:
This study introduces STANDER, a novel oversampling technique based on code entity similarity, designed to handle class imbalance in Feature Envy prediction by generating synthetic samples that better reflect the characteristics of Feature Envy.
Method:
STANDER creates synthetic samples by leveraging multidimensional code entity similarity, incorporating attributes such as dependency relationships, historical changes and code text. It was evaluated on five datasets using five classifiers: Naive Bayes, Logistic Regression, Support Vector Machine, Random Forest, and Decision Tree. Its performance was compared to baseline over-sampling techniques based on precision, recall, F1-score, and Matthews Correlation Coefficient.
Results:
STANDER enhances dataset diversity while maintaining clear boundaries between minority and majority classes, as reflected by higher Nearest Neighbor Diversity and Silhouette Score values. Models balanced with STANDER exhibited significant improvements in predictive performance, particularly in recall, F1-score, and Matthews Correlation Coefficient. Compared to the other oversampling techniques, STANDER demonstrated advantages in handling imbalanced datasets, especially in the Logistic Regression and Decision Tree classifiers. Statistical results confirm significant performance improvements across most models, highlighting its effectiveness and applicability.
Conclusion:
STANDER is an effective solution to alleviate class imbalance problem in Feature Envy detection by generating more representative synthetic samples that improve prediction performance.",21 Mar 2025,9,"STANDER, a novel oversampling technique for Feature Envy prediction, shows significant improvements in predictive performance, especially in recall, F1-score, and Matthews Correlation Coefficient. This can greatly benefit startups aiming to improve software modularity and reduce technical debt."
https://www.sciencedirect.com/science/article/pii/S0950584925000138,XL-HQL: A HQL query generation method via XLNet and column attention,April 2025,Information and Software Technology,Not Found,Rongcun=Wang: rcwang@cumt.edu.cn; Yiqian=Hou: Not Found; Yuan=Tian: Not Found; Zhanqi=Cui: Not Found; Shujuan=Jiang: Not Found,"Abstract
Context:
Object-relational mapping (ORM) tools, like Hibernate, are widely used to facilitate the development of database applications by bridging the gap between object-oriented programming (OOP) and relational database management systems (DBMS). These ORM tools simplify the process of mapping OOP objects to relational tables, addressing issues of data inconsistency and performance. However, they also introduce the need to write queries in specific languages, such as Hibernate Query Language (HQL), to manage data interactions within the database.
Objective:
These query languages can be difficult to write and error-prone due to the complexities of accurately mapping object models to relational schema with intricate relationships and inheritance hierarchies. To mitigate this issue, a recent study introduced the task of automated HQL query generation, i.e., automatically generating HQL from program context (target method’s signature, properties, and optional method comments and call context). However, the existing solution, HQLgen, has shown limited performance, with an accuracy of 34.52%.
Method:
In this paper, we propose a novel HQL query generation approach named XL-HQL. XL-HQL aims to address two main challenges in HQL query generation: limited context information and large search space. Specifically, XL-HQL contains a pre-trained model-based encoder, rules defined to reduce search space, and a column-attention-enabled decoder, which is shown to be effective in SQL generation approaches.
Result:
To evaluate the effectiveness of XL-HQL, we designed and conducted experiments on an existing HQL query generation benchmark, which contains 24,118 HQL queries extracted from 3,481 open-source projects. The experimental results show that our approach achieves 66.93% and 64.47% accuracy on mixed and cross-project datasets, respectively, nearly doubling the performance of the state-of-the-art (SOTA) baseline.
Conclusions:
The application of pre-trained models that are suitable for handling long sequences for the HQL query generation task shows great potential. Moreover, the defined rules based on OOP knowledge are effective for reducing search space and improving the performance of the task.",21 Mar 2025,7,"XL-HQL, a novel approach for HQL query generation, achieves a significant increase in accuracy compared to the state-of-the-art baseline. This can be valuable for startups dealing with database applications and ORM tools, improving query writing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584925000114,Towards an understanding of requirements management in software ecosystems,April 2025,Information and Software Technology,Not Found,Paulo=Malcher: malcher@edu.unirio.br; Davi=Viana: Not Found; Pablo Oliveira=Antonino: Not Found; Rodrigo Pereira dos=Santos: Not Found,"Abstract
Context:
Software ecosystems (SECO) have introduced complexity in requirements management due to multiple actors’ collaboration through several organizational boundaries.
Objective:
The main contribution of this article is to improve the understanding of requirements management in SECO. We propose a conceptual model whose concepts, definitions, and relationships are grounded in the literature and the modern software industry’s practices.
Methods:
We applied Design Science to build the conceptual model and conducted a Delphi study with 22 experts to assess it. We performed two rounds and adjusted our model according to the experts’ judgment.
Results:
We reached a conceptual model comprising 43 concepts and their relationships that help to understand requirements management in SECO. Moreover, we provided a glossary with a definition of each concept. This conceptual model can help abstract the complexity of the requirements management in SECO.
Conclusions:
By organizing concepts and relationships in requirements management in SECO, this conceptual model makes it possible to expand the body of knowledge in the area and serves as a basis for new solutions to support requirements management in SECO.",21 Mar 2025,6,"The conceptual model proposed for requirements management in software ecosystems can help abstract complexity, expand knowledge in the area, and provide a basis for new solutions. While valuable, the practical impact on early-stage ventures may be less direct compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492400257X,Data analytics in software startups: Understanding key concepts and critical challenges,April 2025,Information and Software Technology,Not Found,Usman=Rafiq: Usman.Rafiq@unibz.it; Xiaofeng=Wang: Not Found; Eduardo=Guerra: Not Found,"Abstract
Context:
The continuous proliferation of data nowadays has inspired companies to make data-informed decisions. Despite the acknowledged benefits of analytics, there is a persistent question about how companies, especially software startup companies with distinguishing characteristics, can effectively create value from it. In the startup context, analytics refers to the use of startup data and insights to inform strategies and tactics across startup business, product, team, sales, and marketing dimensions.
Objective:
In this study, we aim to bridge the knowledge gap by eliciting an understanding of the analytics that software startup companies hold and identifying critical challenges they face in the realm of analytics.
Method:
We conducted a multiple-case study with eight software startups at different startup stages. In addition to the data collected through semi-structured interviews, we considered other data sources such as analytics dashboards and online data about the startups, including websites and social media platforms. We analyzed the data using thematic analysis.
Results:
Our results firstly revealed a divergent understanding of analytics by software startups, based on which we reported essential characteristics of analytics perceived by them. Then we identified 22 analytics challenges classified into six main themes. The themes encompass data capture and access challenges, data interpretation and bias, communication challenges, cultural challenges, external influences and constraints, and analytics implementation challenges.
Conclusions:
Our findings contribute to a conceptual understanding of analytics in software startups and the identification of critical challenges faced by these startups across different stages. The conceptual understanding lays the foundation for comprehending what constitutes analytics for software startups, while the identification of challenges anticipates critical barriers to the adoption and implementation of analytics. We also provide practical implications to both researchers and practitioners.",21 Mar 2025,8,The study on analytics challenges faced by software startups provides valuable insights into critical barriers to adoption and implementation of analytics. This can offer practical implications for startups looking to effectively create value from analytics.
https://www.sciencedirect.com/science/article/pii/S0950584924002544,Exploring the impact of feedback on remote SW development teams,March 2025,Information and Software Technology,Not Found,Ana Beatriz=Cavalcanti: abcr@cin.ufpe.br; Carina=Alves: Not Found; João=Araújo: Not Found,"Abstract
Context
Feedback is essential in the routine of software development teams. It provides information on professionals’ performance, align goals, and manage conflicts. The growing adoption of the remote work model has created new challenges for the effective use of feedback by organizations. Despite the recognized importance of feedback, few studies focused on understanding how feedback practices are currently conducted in remote software development teams.
Objective
This work aims to explore the impact of feedback on software development teams working remotely. In addition, we aim to provide valuable insights on how teams can optimize the outcomes of feedback practices.
Method
We adopted a mixed-method approach to investigate how feedback practices are conducted in remote software development teams. We performed a multivocal literature review to map the benefits, challenges, and good practices mentioned in the literature. Then, we conducted semi-structured interviews with 10 leaders and managers to understand their perceptions about feedback practices. Finally, we surveyed 83 team members to understand their perceptions and feelings about receiving feedback.
Results
We found out that the key benefits of feedback include a boost in individual engagement and team performance. In contrast, common challenges involved in remote feedback are communication gaps due to the adoption of digital channels and difficulty in providing and receiving negative feedback. Finally, our study proposes good practices to improve the feedback outcomes, such as: using multidimensional indicators to evaluate team members, providing a tangible goal-oriented development plan, and adopting continuous feedback follow-up.
Conclusion
We synthesized evidence from multiple sources by adopting three research methods to understand the effects of feedback on remote software development teams. Finally, we provided a set of actionable insights on how to optimize the feedback on remote software development teams.",21 Mar 2025,7,The study on feedback practices in remote software development teams provides actionable insights that can benefit early-stage ventures and startups by improving team performance and engagement.
https://www.sciencedirect.com/science/article/pii/S0950584924002465,Performance regression testing initiatives: a systematic mapping,March 2025,Information and Software Technology,Not Found,Luciana Brasil Rebelo=dos Santos: luciana.rebelo@gssi.it; Érica Ferreira=de Souza: ericasouza@utfpr.edu.br; André Takeshi=Endo: andreendo@ufscar.br; Catia=Trubiani: catia.trubiani@gssi.it; Riccardo=Pinciroli: riccardo.pinciroli@gssi.it; Nandamudi Lankalapalli=Vijaykumar: vijay.nl@inpe.br,"Abstract
Context:
Issues related to the performance of software systems are crucial, as they have the potential to impede the effective utilization of products, compromise user satisfaction, escalate costs, and lead to failures. Performance regression testing has been identified as a prominent research domain, since it aims to prevent anomalies and substantial slowdowns.
Objective:
The objective of this paper is to examine recent approaches proposed in the literature concerning performance regression testing. Our interest lies in contributing insights that offer a forward-looking perspective on what is essential in this promising research domain.
Methods:
We carried out a systematic mapping study with the objective of gathering information on various initiatives related to performance regression testing. Our methodology follows the state-of-the-art guidelines for systematic mappings comprising planning, conducting, and reporting activities, thus obtaining a comprehensive set of selected studies.
Results:
Our selection includes 68 papers, and our analysis focuses on four key research questions, delving into (i) publication trends, (ii) developed approaches, (iii) conducted evaluations, and (iv) challenges. As a result of this investigation, we present a roadmap highlighting research opportunities.
Conclusion:
This flourishing research field entails a broad set of challenges, such as deciding the granularity of tests and the frequency of launching the performance regression process. Consequently, there is still much work to be undertaken to trade-off between the accuracy and the efficiency of capturing complex performance issues across diverse application domains and/or execution environments.",21 Mar 2025,6,"The research on performance regression testing offers valuable insights for software systems, which can be beneficial for startups looking to enhance their product performance and prevent failures."
https://www.sciencedirect.com/science/article/pii/S0950584924002556,Industry 4.0/IIoT Platforms for manufacturing systems — A systematic review contrasting the scientific and the industrial side,March 2025,Information and Software Technology,"0000, 1111",Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Christian=Sauer: Not Found; Amir Shayan=Ahmadian: Not Found; Christian=Kröher: Not Found,"Abstract
Context:
IIoT, Industry 4.0 or CPPS software platforms are cornerstones of smart manufacturing production systems. Such platforms integrate machines, IIoT and edge devices, realize distributed (management) functionality and provide the basis for user-defined IIoT applications. Individual instances in research and industrial practice do share commonalities while they also differ significantly.
Objective:
A detailed overview of the platform landscape is fundamental for innovative research. However, actual surveys and literature reviews concentrate on specific aspects and usually focus only on the research works, neglecting specific aspects of the industrial use of IIoT platforms. We aim at a systematic overview of the functionalities and approaches of scientific and industrial IIoT platforms along 16 analysis dimensions and thereby exposing gaps between the focuses of research on IIoT platforms and actual industrial IIoT platforms in use. By doing so we are able to highlight future areas of interest to research as well as indicating potentially over-researched areas which are of less interest in actual industrial IIoT platforms.
Method:
We combine a systematic literature review of scientific IIoT platform research with a systematic analysis of industrial IIoT platforms.
Results:
We start off with 1620 research papers plus 70 from snowballing that we systematically filter down to 36 papers (plus 11 added by a SLR update) providing sufficient information for a data extraction, which we analyze along 16 topics to extract actual capabilities and differences of relevant platform approaches. In a second step, we contrast these results with an analysis of 21 industrial platforms.
Conclusion:
Similar approaches, differences and topics for future are exhibited. In comparison with 21 industrial platforms along the same analysis topics, we distill various commonalities, differences, trends and gaps.",21 Mar 2025,5,"The overview of IIoT platforms in research and industrial use exposes potential gaps and future areas of interest, providing moderate value for early-stage ventures in the manufacturing sector."
https://www.sciencedirect.com/science/article/pii/S0950584924002441,Solutions toCybersecurity Challenges in Secure Vehicle-to-Vehicle Communications: A Multivocal Literature Review.,March 2025,Information and Software Technology,Not Found,Naeem=Ullah: Naeemullah72@gmail.com; Siffat Ullah=Khan: siffatullah@uom.edu.pk; Mahmood=Niazi: Not Found; Matteo=Esposito: Not Found; Arif Ali=Khan: Not Found; Jamal Abdul=Nasir: Not Found,"Abstract
Context
Vehicle-to-Vehicle (V2V) technology is evolving rapidly, meeting modern transportation needs and driving economic and technological progress. V2V brings numerous benefits, enabling vehicles to communicate with each other and with infrastructure like Roadside Units (RSUs), which helps minimize collisions, reduce fatalities, and boost road safety for passengers, drivers, and pedestrians alike. Beyond safety, V2V improves traffic management and optimizes routes. However, these advancements also introduce new challenges. Greater reliance on IT makes vehicles more vulnerable to cyber-attacks and increases costs related to system installation and maintenance. This highlights a pressing need to advance V2V technology to enhance overall safety.
Objective
This research focuses on identifying the primary challenges and effective practices within Vehicle-to-Vehicle (V2V) communication.
Method
We conducted a Multivocal Literature Review (MLR) using tailored search strings derived from our research questions. This process adhered to all standard MLR steps, including protocol development, initial and final selection, quality assessment, data extraction, and synthesis.
Results
We have identified a list of 18 challenges in the context of V2V communication. 10 of these challenges were marked as critical challenges based on the criterion of ≥20 % occurrences in both formal and grey literature. We also identified related practices for the identified critical challenges. The identified challenges were further analyzed based on different variables such as publication periods and study strategies.
Conclusion
We recommend that automotive industries should prioritize addressing these challenges to enhance their readiness for secure V2V communication. Our overarching goal is to develop a Cybersecurity Challenges Mitigation Model (CCMM) based on MLR findings and industrial survey outcomes, assisting companies in the automobile sector to assess their readiness for secure V2V communication development.",21 Mar 2025,8,"The study on V2V communication challenges and practices is highly relevant for startups in the automotive industry, offering insights on improving road safety and cybersecurity in their products."
https://www.sciencedirect.com/science/article/pii/S0950584924002453,DeepCNP: An efficient white-box testing of deep neural networks by aligning critical neuron paths,March 2025,Information and Software Technology,Not Found,Weiguang=Liu: 3220235389@bit.edu.cn; Senlin=Luo: luosenlin2019@126.com; Limin=Pan: panlimin2016@gmail.com; Zhao=Zhang: zzhao8735@163.com,"Abstract
Context
Erroneous decisions of Deep Neural Networks may pose a significant threat to Deep Learning systems deployed in security-critical domains. The key to testing DNNs is to propose a testing technique to generate test cases that can detect more defects of the models. It has been demonstrated that coverage-guided fuzz testing methods are difficult to detect the correctness defects of model's decision logic. Meanwhile, the neuron activation threshold is set based on experience, which increases the uncertainty of the test even more. In addition, the randomly selected seed mutations are prone to generate a large number of invalid test cases, which has a great impact on the testing efficiency.
Objective
This paper introduces DeepCNP, a method that combines Critical Neuron Paths alignment and dynamic seeds selection strategy, which can comprehensively and efficiently test all the decision paths of DNN and generate as many different classes of test cases as possible to expose misbehaviors of the model and thus finding defects.
Method
DeepCNP utilizes training data to construct decision paths determined by the neuron output distribution, and aligns different decision paths in order to generate test cases. Seeds that are easy to align are dynamically selected based on the decision paths to be tested, and the labeling of seed mutations is specified during the path alignment process, thus improving the efficiency of fuzz testing.
Results
Experimental results show that DeepCNP achieves new state-of-the-art results, pioneering the testing of all decision logics of the model through critical neuron path alignment, which greatly enhances the number of defects found, the efficiency and number of generated test cases.
Conclusion
DeepCNP comprehensively tests the decision logic of DNNs, efficiently generating a large number of test cases of different categories to expose model's misbehaviors and thus finding additional defects.",21 Mar 2025,9,The DeepCNP method for testing DNNs introduces innovative techniques that can significantly benefit startups in security-critical domains by enhancing testing efficiency and detecting defects.
https://www.sciencedirect.com/science/article/pii/S0950584924002477,Improving bug triage with the bug personalized tossing relationship,March 2025,Information and Software Technology,Not Found,Wei=Wei: weiwei961112@163.com; Haojie=Li: lihaojie@qust.edu.cn; Xinshuang=Ren: 1317467738@qq.com; Feng=Jiang: jiangfeng@qust.edu.cn; Xu=Yu: yuxu0532@upc.edu.cn; Xingyu=Gao: gxy9910@gmail.com; Junwei=Du: djwqd@163.com,"Abstract
Background:
In open-source software projects, the main task of bug triage is accurately assigning bugs to appropriate developers. Statistics indicate that about 50% of bugs are reassigned (also called “tossed”) at least once, greatly extending the time for bug fixing. Research studies have shown that combining historical tossing relationships can significantly improve bug triage performance.
Objective:
The current research on utilizing bug tossing relationships can be mainly divided into two categories: (1) During the reassignment phase, only developers with the highest probability of tossing relationships are selected. (2) Use attribute filtering mechanism to filter and match developers. However, these approaches fail to fully consider the matching degree between developers’ abilities and the knowledge required to fix current bugs. We are attempting to propose an approach to address the above problem.
Approach:
We propose an approach to improve bug triage with the Bug Personalized Tossing Relationship (BPTRM). It uses a tossing transition probability matrix derived from historical tossing paths to help recommend suitable developers for solving bug reports.
Result:
Experimental results from various data sets within Eclipse and Mozilla indicate that BPTRM improves average recommendation performance by at least 14.38% compared to different initial assignment approaches. In addition, compared to baselines, BPTRM improves the average accuracy by 14.66% and shortens the average bug tossing length by 16.19%.
Conclusion:
1. The BPTRM approach, combined with personalized bug tossing relationships, precisely matches developers’ abilities and the knowledge required to fix current bugs. 2. This effectively improves the bug triage’s accuracy and shortens the bug tossing’s length.",21 Mar 2025,9,The research on bug triage performance improvement through personalized tossing relationships can greatly benefit early-stage ventures by enhancing bug fixing efficiency and accuracy.
https://www.sciencedirect.com/science/article/pii/S0950584924002520,Why do software developers like working from the office?,March 2025,Information and Software Technology,Not Found,Nurit=Zaidman: zeidman@bgu.ac.il; Dina=Van Dijk: dinav@bgu.ac.il,"Abstract
Context
The inquiry of the optimal number of working days per week for home-based (versus office-based) work, poses a challenge for many organizations within the high-tech sector. Studies in this area tend to overlook the responses and preferences of specific populations, and there is a lack of contextualization in the discussion. Given that software developers' needs have an impact on their performance, turnover, and well-being, it is important to understand their needs in relation to where work should be accomplished. Research that illuminates this topic can lead to different presumptions regarding developers’ preferences for home and/or office work.
Objective
To analyze preferences for home- versus office-based work among Scrum team software developers employed in a multinational organization.
Method
To achieve a broad, global scope and an in-depth understanding of developers’ preferences for work that was office-based or home-based, we used a combination of two data collection methods: a survey administered to 651 employees and in-depth interviews conducted with 35 employees from the same organization.
Results
The results show that the employees preferred to work from home for the majority of weekdays, yet, about 70 % of them preferred to come to the office at least once a week. The main reasons for home-based preference were “no commuting,” and “more productive and concentrated work”, more time for myself, and a relaxed comfortable environment. The reasons for office-based preference were to socialize with colleagues, to engage in work interactions, and to enjoy the ambiance and facilities available at the workplace.
Conclusions
The study illuminates developers' social preferences and their work motivations as related to their need for peer interaction, which contrasts the dominant argument in existing research that portrays developers as having a low need for social interaction. Second, the study depicts a contextual factor, working in Scrum teams, as an explaining variable to the developers’ preferences for office social interaction.",21 Mar 2025,6,"Understanding software developers' preferences for home vs. office work can provide insights for startup culture, but the practical impact may vary for different ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924002568,Improving seed quality with historical fuzzing results,March 2025,Information and Software Technology,Not Found,Yang=Li: chrisly@hdu.edu.cn; Yingpei=Zeng: yzeng@hdu.edu.cn; Xiangpu=Song: songxiangpu@sdu.edu.cn; Shanqing=Guo: guoshanqing@sdu.edu.cn,"Abstract
Context:
Coverage-guided fuzzing (CGF) has achieved great success in discovering software vulnerabilities. The efficiency of CGF highly relies on the quality of the initial seed corpus. Although there have been some works in recent years investigating the initial seed selection, usually only the corpus given by developers or downloaded from the Internet is used to get the initial seed corpus.
Objective:
We assess several existing corpus minimization tools and find that none of them effectively leverage information contained in historical fuzzing results. The historical fuzzing results may come from previous fuzz testing or the emerging continuous fuzzing integration in the software development cycle. Therefore, we want to utilize history fuzzing results to generate a high-quality initial corpus to enhance the fuzzing performance. Besides, the size of the initial corpus will affect the fuzzing efficiency, so using a minimization tool to extract valuable seeds from historical results is essential.
Method:
We propose to use historical fuzzing results to help construct the initial seed corpus and further develop a corpus minimization tool named MCM (multiple corpora minimization), which can analyze multiple fuzzing results and use information including edge appearance frequency to help seed selection.
Results:
We implement a prototype of MCM and evaluate it on 10 open-source programs. Our experiments show that by using historical fuzzing results to expand the size of the initial seed corpus even a small number, e.g., from 20 to only 100, the branch coverage improves up to 14%. Meanwhile, MCM can achieve higher code coverage than existing corpus minimization tools, including AFL-CMIN and OPTIMIN.
Conclusion:
Our study shows using historical results to generate a high-quality initial corpus is practical and can effectively improve the fuzzing performance.",21 Mar 2025,8,Utilizing historical fuzzing results to enhance fuzzing performance can be valuable for cybersecurity startups focused on developing secure software products.
https://www.sciencedirect.com/science/article/pii/S0950584924002581,Energy attack method for adaptive multi-exit neural networks,March 2025,Information and Software Technology,Not Found,Dongfang=Du: 21212010012@m.fudan.edu.cn; Chaofeng=Sha: cfsha@fudan.edu.cn; Xin=Peng: pengxin@fudan.edu.cn,"Abstract
Context:
Adaptive Multi-Exit Neural Networks (AMENNs) have emerged as a promising solution for energy-efficient and faster inference in resource-constrained environments. To ensure that these networks meet performance requirements, evaluating their energy robustness is essential. Recent works have focused on energy attacks against models in both white-box and black-box scenarios. However, existing approaches in a black-box scenarios require a significant amount of additional training data to train auxiliary models, resulting in prohibitively high costs for the attacks.
Objectives:
In this work, we leverage genetic algorithm (GA) to search for high-energy samples to conduct attacks and evaluate the energy robustness of the AMENN models directly in black-box scenario, named 
E
nergy 
A
ttack using 
G
enetic 
A
lgorithm (EAGA).
Methods:
In the context of black-box scenarios, we propose an energy attack method based on genetic algorithm for AMENNs used in image classification tasks. By enhancing the fitness function to target high-energy consumption samples and improving population initialization and crossover mutation operations, we ensure a diverse and rich sample space for robust evaluation.
Results:
The results show that EAGA outperforms current baseline methods, demonstrating an average improvement of over 17% in the mean percentage increase in energy consumption of AMENNs. Furthermore, we guarantee the high quality of the generated attack inputs by ensuring sufficient similarity between the original image and the attack image.
Conclusion:
EAGA introduces a novel and efficient method for assessing the energy robustness of AMENNs in a black-box setting, devoid of the need for local gradient information. Through the utilization of genetic algorithms, this approach allows for a direct evaluation of model performance in resource-constrained environments. The study emphasizes the importance of EAGA in enhancing the evaluation process of AMENN models and underscores its potential to advance energy-efficient neural network deployments.",21 Mar 2025,7,The evaluation of energy robustness in AMENNs through EAGA can be beneficial for startups working on resource-constrained neural network deployments.
https://www.sciencedirect.com/science/article/pii/S0950584924002519,DHG-BiGRU: Dual-attention based hierarchical gated BiGRU for software defect prediction,March 2025,Information and Software Technology,Not Found,Ruchika=Malhotra: ruchikamalhotra2004@yahoo.com; Priya=Singh: priya.singh.academia@gmail.com,"Abstract
Context:
Software defect prediction (SDP) is a prominent research area focussed on anticipating defects early in the software lifecycle. Traditional machine learning models are based on static features, which are not enough to capture contextual information in the source code. In recent years, researchers have also developed deep learning models that extract semantic information from source code using the abstract syntax tree (AST). These approaches often combine static and semantic features by a simple merger operation.
Objective:
The article aims to address the limitations of the existing models by utilizing advanced feature extraction and integration techniques. It develops a deep learning model that can effectively prioritize the crucial features and intelligently combine the static and semantic features to provide robust predictions
Method:
The article proposes a novel model namely, dual-attention-based hierarchical gated BiGRU (DHG-BiGRU). The model first employs a static feature extractor (StatFE) and a semantic feature extractor (SemFE) to capture static and semantic features, respectively. Next, the outputs from StatFE and SemFE are passed to individual BiGRUs. The BiGRU output associated with the semantic features is subsequently processed by a dual attention mechanism (DAM), that captures the complex semantic information with emphasis on the most crucial features. Afterward, the hierarchical gated fusion (HGF) meticulously merges the static and semantic features. Finally, these integrated features are passed through a sigmoid function to predict defects.
Results:
The extensive experiments on extensively utilized datasets from the PROMISE repository reveal that DHG-BiGRU performs significantly better than the most advanced models and consistently achieves higher precision, recall and f-measure, demonstrating a reliable prediction capability.
Conclusion:
The results of the study underscore the potential advanced feature extraction and integration techniques for SDP. By achieving considerable improvements over state-of-the-art techniques, the proposed approach paves the way for sophisticated defect prediction models to improve software quality and reliability.",21 Mar 2025,8,"The advanced feature extraction and integration techniques proposed for software defect prediction can offer startups improved software quality and reliability, enhancing their competitiveness."
https://www.sciencedirect.com/science/article/pii/S0950584924002611,Automated detection of inter-language design smells in multi-language deep learning frameworks,March 2025,Information and Software Technology,Not Found,Zengyang=Li: zengyangli@ccnu.edu.cn; Xiaoyong=Zhang: charles@mails.ccnu.edu.cn; Wenshuo=Wang: wenshuowang@mails.ccnu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Ran=Mo: moran@ccnu.edu.cn; Jie=Tan: j.tanjie@outlook.com; Hui=Liu: hliu@hust.edu.cn,"Abstract
Context:
Nowadays, most deep learning frameworks (DLFs) use multilingual programming of Python and C/C++, facilitating the flexibility and performance of the DLF. However, inappropriate inter-language interaction may introduce design smells involving multiple programming languages (PLs), i.e., Inter-Language Design Smells (ILDS). Despite the negative impact of ILDS on multi-language DLFs, there is a lack of an automated approach for detecting ILDS in multi-language DLFs and a comprehensive understanding on ILDS in such DLFs.
Objective:
This work aims to automatically detect ILDS in multi-language DLFs written in the combination of Python and C/C++, and to obtain a comprehensive understanding on such ILDS in DLFs.
Methods:
We first developed an approach to automatically detecting ILDS in the multi-language DLFs written in the combination of Python and C/C++, including a number of ILDS and their detection rules defined based on inter-language communication mechanisms and code analysis. Then, we developed the 
CPsmell
 tool that implements detection rules for automatically detecting such ILDS, and manually validated the accuracy of the tool. Finally, we performed an empirical study to evaluate the ILDS in multi-language DLFs.
Results:
We proposed seven ILDS and achieved an accuracy of 98.17% in the manual validation of 
CPsmell
 in 5 popular multi-language DLFs. The study results revealed that among the 5 DLFs, TensorFlow, PyTorch, and PaddlePaddle exhibit relatively high prevalence of ILDS; each smelly file contains around 5 ILDS instances on average, with ILDS 
Long Lambda Function For Inter-language Binding
 and 
Unused Native Entity
 being relatively prominent; throughout the evolution process of the 5 DLFs, some ILDS were resolved to a certain extent, but the overall count of ILDS instances shows an upward trend.
Conclusions:
The automated detection of the proposed ILDS achieved a high accuracy, and the empirical study provides a comprehensive understanding on ILDS in the multi-language DLFs.",21 Mar 2025,4,The work addresses an important issue of design smells in deep learning frameworks but may have limited practical impact on European startups.
https://www.sciencedirect.com/science/article/pii/S0950584924002507,Robustness evaluation of code generation systems via concretizing instructions,March 2025,Information and Software Technology,Not Found,Ming=Yan: yanming@tju.edu.cn; Junjie=Chen: junjiechen@tju.edu.cn; Jie M.=Zhang: jie.zhang@kcl.ac.uk; Xuejie=Cao: caoxuejie@tju.edu.cn; Chen=Yang: yangchenyc@tju.edu.cn; Mark=Harman: mark.harman@ucl.ac.uk,"Abstract
Context:
Code generation systems have been extensively developed in recent years to generate source code based on natural language instructions. However, despite their advancements, these systems still face robustness issues where even slightly different instructions can result in significantly different code semantics. Robustness is critical for code generation systems, as it can have significant impacts on software development, software quality, and trust in the generated code. Although existing testing techniques for general text-to-text software can detect some robustness issues, they can produce many false positives and are limited in effectiveness due to ignoring the characteristics of this kind of systems.
Objective:
To better evaluate (and further enhance) the robustness of code generation systems, in this work, we conducted the first exploration by carefully considering the characteristics of code generation systems. Specifically, we propose such a novel technique (called COCO) and perform an extensive study to evaluate the robustness of code generation systems with COCO.
Method:
COCO exploits the usage scenario of code generation systems to make the original programming instruction more concrete by incorporating features known to be present in the original code. A robust system should maintain code semantics for the concretized instruction, and COCO detects robustness inconsistencies when it does not. In the extensive study, we evaluated the robustness of eight advanced code generation systems (including commercial tools Copilot and ChatGPT) with COCO, using two widely-used datasets.
Results:
Our results demonstrate the effectiveness of COCO. It does not produce any false positive, ensuring the accuracy of robustness evaluation. Additionally, it outperforms the two baselines adopted from general text-to-text software testing, detecting 440.31% and 95.81% more inconsistencies, respectively. Concretized instructions generated by COCO can further help reduce robustness inconsistencies by 21.90% to 60.18% via fine-tuning.
Conclusions:
COCO is effective in detecting robust inconsistencies in code generation systems and significantly outperforms baselines. Additionally, fine-tuning code generation systems with the concretized instructions provided by COCO can largely enhance their robustness.",21 Mar 2025,9,"The proposed COCO technique has a significant impact on enhancing the robustness of code generation systems, which is highly relevant for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492400260X,What are the emotions of developers towards deep learning documentation? — An exploratory study on Stack Overflow posts,March 2025,Information and Software Technology,Not Found,Akhila Sri Manasa=Venigalla: cs19d504@iittp.ac.in; Sridhar=Chimalakonda: ch@iittp.ac.in,"Abstract
Context:
Non native machine learning and deep learning (DL) developers face several challenges in using DL frameworks owing to the issues persistent in DL documentation. However, there are no studies that explore the reasons for issues in documentation.
Objective:
Investigating the underlying emotions in developer discussions on documentation could help in identifying reasons for issues in documentation. Hence, in this study, we analyse emotions of Stack Overflow posts corresponding to documentation of DL frameworks.
Methodology:
We identify relevant deep-learning related tags using integrated snowballing approach and extract 159.2K posts related to DL. We then identify documentation related posts among these using keyword matching approach, which resulted in 13,572 DL documentation related posts. We use Random Forest Classifier to build six emotion classifier models based on Gold Label Dataset for emotions. We then classify the extracted posts into each of the six emotions — 
Anger
, 
Fear
, 
Love
, 
Joy
, 
Sadness
 and 
Surprise
 using the classifier models, and curate the results.
Results:
We observe a large expression of anger and sadness, with more than half of posts having ‘yolo’ and ‘activation-function’ tags exhibiting these emotions, while 
Love
 emotion is predominantly present in posts with ‘theano’ tag. During our analysis, we observed that 40% of ‘Body’ and ‘Answer’ posts exhibited anger and sadness emotions.
Conclusion:
Our study reveals the large presence of Anger, Fear and Sadness emphasizing the need to improve DL framework documentation. Specifically, maintainers of the ‘yolo’ and ‘matcaffe’ libraries could improve their documentation, as the corresponding posts exhibit more of 
Anger
 and 
Sadness
.",21 Mar 2025,3,"While the study on emotions in developer discussions is interesting, the direct practical application to startups may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584924002623,Instance gravity oversampling method for software defect prediction,March 2025,Information and Software Technology,Not Found,Yu=Tang: yutang@bjtu.edu.cn; Yang=Zhou: 23120496@bjtu.edu.cn; Cheng=Yang: 23115081@bjtu.edu.cn; Ye=Du: ydu@bjtu.edu.cn; Ming-song=Yang: 21281204@bjtu.edu.cn,"Abstract
Context
In the software defect datasets, the number of defective instances is significantly lower than that of non-defective instances. This imbalance adversely impacts the predictive performance of the model. Oversampling methods can effectively balance datasets. However, traditional oversampling methods often struggle to capture the underlying relationships between features and are prone to introducing noise during instance synthesis.
Objective
Inspired by the law of gravity, we propose a novel oversampling method based on instance gravity (MOSIG).
Method
This method begins by introducing a new metric, instance gravity, to measure the similarity between instances. Subsequently, feature models are constructed, and instance groups are generated. Instances that meet specific conditions based on instance gravity are then identified within different instance groups. Finally, we propose a novel method for synthesizing defective instances by assigning weights to instances according to their gravity.
Results
Experimental results demonstrate that MOSIG significantly enhances the predictive performance of both the CART decision tree and Naive Bayes models across 21 publicly available software defect datasets. The experimental results are further validated using the Friedman ranking and Nemenyi post-hoc test, confirming that MOSIG is statistically significant.
Conclusion
MOSIG represents a more promising oversampling method.",21 Mar 2025,7,"The MOSIG oversampling method shows promising results in enhancing predictive performance, which could benefit European startups in software defect prediction."
https://www.sciencedirect.com/science/article/pii/S0950584924002593,Reinforcement learning for test case prioritization based on LLEed K-means clustering and dynamic priority factor,March 2025,Information and Software Technology,Not Found,Zhongsheng=Qian: Not Found; Qingyuan=Yu: 2202210067@stu.jxufe.edu.cn; Hui=Zhu: Not Found; Jinping=Liu: Not Found; Tingfeng=Fu: Not Found,"Abstract
Integrating reinforcement learning (RL) into test case prioritization (TCP) aims to cope with the dynamic nature and time constraints of continuous integration (CI) testing. However, achieving optimal ranking across CI cycles is challenging if the RL agent starts from an unfavorable initial environment and deals with a dynamic environment characterized by continuous errors during learning. To mitigate the influence of adverse environments, this work proposes an approach to 
T
est 
C
ase 
P
rioritization which incorporates Locally Linear Embedding-based 
K
-means Clustering and 
D
ynamic Priority Factor into 
R
einforcement 
L
earning (
TCP-KDRL
). Firstly, we exploit the K-means clustering method with Locally Linear Embedding (LLE) to mine the relationships between test cases, followed by assigning initial priority factors to the test cases. These test cases are ranked based on their initial factors, providing an improved initial learning environment for the agent in RL. Secondly, with the agent learning the ranking strategy in various cycles, we design a comprehensive reward indicator by considering running discrepancy and the position between test cases. Additionally, based on the reward values, the dynamic priority factors for the ranked test cases in each learning round of RL are adaptively updated and the sequence is locally fine-tuned. The fine-tuning strategy provides ample feedback to the agent and enables real-time correction of the erroneous ranking environment, enhancing the generalization of RL across various cycles. Finally, the experimental results demonstrate that TCP-KDRL, as an enhanced RL-based TCP method, outperforms other competitive TCP approaches. Specifically, incorporating the reward indicator and the fine-tuning strategy components, the results are significantly better than that of combining any other two components. For instance, in 12 projects, the average improvements are 0.1548 in APFD and 0.0793 in NRPA. Compared to other TCP methods, the proposed method achieves notable enhancement, with an increase of 0.6902 in APFD and 0.3816 in NRPA.",21 Mar 2025,8,"The TCP-KDRL approach presents a comprehensive method for test case prioritization, which can greatly benefit European startups involved in continuous integration testing."
https://www.sciencedirect.com/science/article/pii/S0950584924002489,Service engineering for quantum computing: Ensuring high-quality quantum services,March 2025,Information and Software Technology,Not Found,Ana=Díaz: Not Found; Jaime=Alvarado-Valiente: jaimeav@unex.es; Javier=Romero-Álvarez: Not Found; Enrique=Moguel: Not Found; Jose=Garcia-Alonso: Not Found; Moisés=Rodríguez: Not Found; Ignacio=García-Rodríguez: Not Found; Juan M.=Murillo: Not Found,"Abstract
Context:
Quantum computing is transforming the world and driving advanced applications in fields such as healthcare and economics. However, ensuring high-quality quantum software remains critical to its adoption across the industry. As quantum technology moves closer to practical applications, it faces significant challenges. Developers face platform-dependent complexities that make the creation of quantum applications a time-consuming process. In addition, the lack of mature tools further hampers progress and can compromise the quality of service.
Objective:
The objective of this paper is to address the pressing need for quantum software quality assurance, presenting a solution for defining and using quantum services, by employing classical service engineering techniques and methods.
Methods:
A process is presented for improving the generation, deployment, and quality assessment of quantum services using an extended OpenAPI Specification and the SonarQube tool. This process also integrates the automatic generation of code for the IBM Quantum provider and its deployment in containers ready for user consumption.
Results:
After a detailed and individualized evaluation of the 40 implementations of quantum algorithms using the developed environment, the results reveal significant variability in the analyzability of the algorithms. This will serve in the future as a reference and guide for the continuous improvement of quantum algorithms in terms of their performance and efficiency in solving complex problems in various quantum application areas.
Conclusions:
This research offers a fundamental contribution to the evolution of quantum computing by introducing a comprehensive framework for quantum software quality assurance. The proposed approach not only addresses some of the existing problems in quantum software, but also paves the way for the development of quantum algorithms and their servitization.",21 Mar 2025,7,"The research addresses a pressing need for quantum software quality assurance, offering a comprehensive framework that can pave the way for the development of quantum algorithms and their servitization."
https://www.sciencedirect.com/science/article/pii/S0950584924002490,Locating requirements in backlog items: Content analysis and experiments with large language models,March 2025,Information and Software Technology,"Agile requirements Engineering, User stories, Backlog items, Issue tracking systems, Content analysis, Large language models",Ashley T.=van Can: a.t.vancan@uu.nl; Fabiano=Dalpiaz: f.dalpiaz@uu.nl,"Abstract
Context:
As agile development has become mainstream, requirements are increasingly managed via issue tracking systems (ITSs). These systems provide a single point of access to the product and sprint backlogs, bugs, ideas, and tasks for the development team. ITSs do not clearly separate requirements from work items.
Objective:
We first tackle a 
knowledge problem
 concerning how requirements are formulated in ITSs, including their categorization and granularity, the presence of multiple requirements, and the existence of a motivation. Second, to assist practitioners in finding requirements in poorly organized ITSs without changing their way of working, we investigate the potential of automated techniques for identifying and classifying requirements in backlog items.
Method:
Through quantitative content analysis, we analyze 1,636 product backlog items sampled from fourteen projects. To explore automated techniques for identifying requirements, we experiment with large language models (LLMs) due to their recent significance in NLP.
Results:
The labeling of backlog items is largely inconsistent, and user-oriented functional requirements are the prevalent category. A backlog item often contains multiple requirements with different levels of granularity. The experiments with LLMs reveal that encoder-only models (BERT and RoBERTa) are most suitable for extracting and classifying requirements in backlog items compared to decoder-only models (Llama 3, Mistral 7B and ChatGPT with GPT 4).
Conclusion:
We reveal knowledge and patterns about requirements documentation in ITSs, leading to a better empirical understanding of Agile RE. The experimental results with LLMs provide the foundation for developing automated, unobtrusive tools that identify and classify requirements in ITSs.",21 Mar 2025,5,"The study provides insights into requirements documentation in Agile RE and experiments with automated techniques for identifying requirements, but the practical impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584924002131,A multivocal literature review on the benefits and limitations of industry-leading AutoML tools,February 2025,Information and Software Technology,Not Found,Luigi=Quaranta: Not Found; Kelly=Azevedo: Not Found; Fabio=Calefato: Not Found; Marcos=Kalinowski: kalinowski@inf.puc-rio.br,"Abstract
Context:
Rapid advancements in Artificial Intelligence (AI) and Machine Learning (ML) are revolutionizing software engineering in every application domain, driving unprecedented transformations and fostering innovation. However, despite these advances, several organizations are experiencing friction in the adoption of ML-based technologies, mainly due to the current shortage of ML professionals. In this context, Automated Machine Learning (AutoML) techniques have been presented as a promising solution to democratize ML adoption, even in the absence of specialized people.
Objective:
Our research aims to provide an overview of the evidence on the benefits and limitations of AutoML tools being adopted in industry.
Methods:
We conducted a Multivocal Literature Review, which allowed us to identify 54 sources from the academic literature and 108 sources from the grey literature reporting on AutoML benefits and limitations. We extracted explicitly reported benefits and limitations from the papers and applied the thematic analysis method for synthesis.
Results:
In general, we identified 18 reported benefits and 25 limitations. Concerning the benefits, we highlight that AutoML tools can help streamline the core steps of ML workflows, namely data preparation, feature engineering, model construction, and hyperparameter tuning—with concrete benefits on model performance, efficiency, and scalability. In addition, AutoML empowers both novice and experienced data scientists, promoting ML accessibility. However, we highlight several limitations that may represent obstacles to the widespread adoption of AutoML. For instance, AutoML tools may introduce barriers to transparency and interoperability, exhibit limited flexibility for complex scenarios, and offer inconsistent coverage of the ML workflow.
Conclusion:
The effectiveness of AutoML in facilitating the adoption of machine learning by users may vary depending on the specific tool and the context in which it is used. Today, AutoML tools are used to increase human expertise rather than replace it and, as such, require skilled users.",21 Mar 2025,8,"The research reviews the benefits and limitations of AutoML tools, highlighting their potential to democratize ML adoption. This can have a significant impact on early-stage ventures facing a shortage of ML professionals."
https://www.sciencedirect.com/science/article/pii/S0950584924002106,FOBICS: Assessing project security level through a metrics framework that evaluates DevSecOps performance,February 2025,Information and Software Technology,"DevSecOps, Metrics framework, Project security, Software engineering, Evaluating security, DevOps, Security assessment, Software metrics, Secure software development, Business metrics",Alessandro=Caniglia: alessandro.caniglia95@gmail.com; Vincenzo=Dentamaro: vincenzo.dentamaro@uniba.it; Stefano=Galantucci: stefano.galantucci@uniba.it; Donato=Impedovo: donato.impedovo@uniba.it,"Abstract
Context:
In today’s software development landscape, the DevSecOps approach has gained traction due to its focus on the software development process and bolstering security measures in projects, a task in light of the ever-evolving cybersecurity threats.
Objective:
This study aims to address the lack of metrics for quantitatively assessing its efficacy from both security and business logic perspectives.
Methods:
To tackle this issue, the research introduces the Framework of Business Index Concerning Security (FOBICS), a set of metrics designed to enable transparent evaluations of project security. FOBICS considers various perspectives relevant to DevSecOps practices. It includes factors such as project duration and financial outcomes, making it appealing for implementation in business settings.
Results:
The effectiveness of FOBICS is validated theoretically and empirically via its application in two real-world projects: the results from these implementations show a correlation between FOBICS metrics and the security strategies employed as the development methodologies adopted by diverse teams throughout the projects.
Conclusion:
Hence, FOBICS emerges as a tool for assessing and continuously monitoring project security, offering insights into areas of strength and areas that may require enhancement. FOBICS is shown to be effective in assessing the level of DevSecOps implementation. The ease of calculating FOBICS metrics makes them easily interpretable and continuously verifiable. Moreover, FOBICS summarizes most of the other quantitative and qualitative metrics in the literature.",21 Mar 2025,6,"The study introduces a framework for assessing DevSecOps efficacy, but the practical application and impact on early-stage ventures may be limited due to its focus on security and business logic perspectives."
https://www.sciencedirect.com/science/article/pii/S0950584924002179,Impact of minimum viable product on software ecosystem failure,February 2025,Information and Software Technology,Not Found,Kati=Saarni: katimarika.saarni@gmail.com; Marjo=Kauppinen: Not Found; Tomi=Männistö: Not Found,"Abstract
Context
Companies are interested in building successful value-producing ecosystems together to offer end users a broader digital service offering and better meet customer needs. However, most ecosystems fail in the early years.
Objective
We investigated one small software ecosystem from the planning phase to the operative phase, where the participating companies left one by one because the software ecosystem was unsuccessful, and the software ecosystem ended after four operative years. The software ecosystem provided a digital service offering based on the defined MVP (Minimum Viable Product). That is why we were interested in understanding the MVP's impact on the ecosystem's failure.
Method
We conducted a case study, the results of which are based on the semi-structured interviews of eight representatives of the software ecosystem.
Results
This study showed that the actors prioritized out functionalities from the MVP, and the MVP was no longer based on the defined value proposition, target customer groups, and customer paths. It was then difficult for the actors to achieve their objectives. The companies’ commitment depended on the set objectives, and when the objectives were not achieved, the actors left the ecosystem, and the software ecosystem failed.
Conclusion
The results show that the MVP can significantly affect the failure of the small software ecosystem, where all actors have a keystone role. The MVP largely defines what kind of digital service offering the software ecosystem provides and whether the actors can achieve the objectives, especially their sales goals. Thus, prioritizing the functionalities of the MVP is a critical activity.",21 Mar 2025,4,"The case study explores the impact of Minimum Viable Product (MVP) on the failure of a software ecosystem, which may provide insights but lacks a direct practical application and impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924002143,Who uses personas in requirements engineering: The practitioners’ perspective,February 2025,Information and Software Technology,"Requirements engineering, Personas, Human aspects, Survey, Interviews",Yi=Wang: xve@deakin.edu.au; Chetan=Arora: chetan.arora@monash.edu; Xiao=Liu: xiao.liu@deakin.edu.au; Thuong=Hoang: thuong.hoang@deakin.edu.au; Vasudha=Malhotra: vasu.malhotra@monash.edu; Ben=Cheng: chengye@deakin.edu.au; John=Grundy: john.grundy@monash.edu,"Abstract
Context:
Personas are commonly employed in software projects to better understand end-users needs. Despite their frequent usage, there is a limited understanding of their practical application and effectiveness.
Objective:
This paper aims to investigate the current practices, methods, and challenges associated with using personas in software development.
Methods:
A two-step investigation was conducted, comprising interviews with 26 software developers, UI/UX designers, business analysts, and product managers, along with a survey of 203 practitioners.
Results:
The findings reveal variations in the frequency and effectiveness of personas across different software projects and IT companies. Additionally, the study highlights the challenges practitioners face when using personas and the reasons for not using them. Notably, the research shows that some human aspects (e.g., the needs of users with disabilities), often assumed to be a key feature of personas, are frequently not considered for various reasons in requirements engineering.
Conclusions:
The study provides actionable insights for practitioners to overcome challenges in using personas during the requirements engineering stages. Furthermore, it identifies areas for future research to enhance the effectiveness of personas in software development.",21 Mar 2025,5,"The study provides insights into the challenges faced by practitioners in using personas, offering actionable steps for improvement, but the impact on early-stage ventures is not clearly specified."
https://www.sciencedirect.com/science/article/pii/S0950584924001745,"A systematic literature review on Agile, Cloud, and DevOps integration: Challenges, benefits",January 2025,Information and Software Technology,"DevOps, Cloud, Agile, Integration, Synergy",Fatiha El=Aouni: Not Found; Karima=Moumane: karima.moumane@ensias.um5.ac.ma; Ali=Idri: Not Found; Mehdi=Najib: Not Found; Saeed Ullah=Jan: Not Found,"Abstract
Context:
In today’s fast-paced digital landscape, integrating DevOps, cloud, and agile methodologies is crucial for meeting software demands. However, this integration remains under-researched.
Objective:
This study explores the integration of Agile, Cloud, and DevOps in today’s software development landscape. It aims to analyze the challenges and benefits associated with merging these three approaches, focusing on their impact on software testing and the role of mindset in successful implementation and identifying the most suitable Agile methodologies.
Methods:
This investigation utilizes a Systematic Literature Review(SLR) to enrich comprehension of this integration in current software development practices.
Results:
The analysis of 31 articles highlights benefits such as improved collaboration and accelerated development, despite challenges with tool proliferation. Platforms like Jenkins, GitLab, Kubernetes, and Docker show promise in addressing these complexities. Our study examines the advantages and challenges of this integration, focusing on its impact on software testing and the role of mindset in successful implementation and identifying the most suitable Agile methodologies.
Conclusion:
The integration of Agile, DevOps, and Cloud signifies a vital move towards collaborative, scalable, and automated methods, crucial for swift delivery, enhanced quality, and ongoing competitiveness. This unified approach is fundamental for organizational advancement and innovation in the ever-evolving software development realm. Further research should tackle challenges in merging these methods and delve into their interactions with emerging technologies to refine practices for increased efficiency.",21 Mar 2025,7,"The integration of Agile, DevOps, and Cloud is crucial for software development, with a focus on collaborative and scalable methods. This unified approach can benefit early-stage ventures by enhancing quality and competitiveness."
https://www.sciencedirect.com/science/article/pii/S0950584924001484,Automated description generation for software patches,January 2025,Information and Software Technology,"Software patch, Patch description, Neural machine translation, Code-to-text",Thanh Trong=Vu: thanhvu@vnu.edu.vn; Tuan-Dung=Bui: 21020006@vnu.edu.vn; Thanh-Dat=Do: 20020045@vnu.edu.vn; Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn; Son=Nguyen: sonnguyen@vnu.edu.vn,"Abstract
Software patches are pivotal in refining and evolving codebases, addressing bugs, vulnerabilities, and optimizations. Patch descriptions provide detailed accounts of changes, aiding comprehension and collaboration among developers. However, manual description creation poses challenges in terms of time consumption and variations in quality and detail. In this paper, we propose 
PatchExplainer
, an approach that addresses these challenges by framing patch description generation as a machine translation task. In 
PatchExplainer
, we leverage explicit representations of critical elements, historical context, and syntactic conventions. Moreover, the translation model in 
PatchExplainer
 is designed with an awareness of description similarity. Particularly, the model is 
explicitly
 trained to recognize and incorporate similarities present in patch descriptions clustered into groups, improving its ability to generate accurate and consistent descriptions across similar patches. The dual objectives maximize similarity and accurately predict affiliating groups. Our experimental results on a large dataset of real-world software patches show that 
PatchExplainer
 consistently outperforms existing methods, with improvements up to 189% in 
BLEU
, 5.7X in 
Exact Match
 rate, and 154% in 
Semantic Similarity
, affirming its effectiveness in generating software patch descriptions.",21 Mar 2025,8,"PatchExplainer offers a innovative approach to generating software patch descriptions, improving accuracy and consistency significantly. This can have a direct positive impact on software development speed and quality for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001939,DeepMig: A transformer-based approach to support coupled library and code migrations,January 2025,Information and Software Technology,"Recommender system, Library migration, Transformers",Juri=Di Rocco: juri.dirocco@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Claudio=Di Sipio: claudio.disipio@univaq.it; Riccardo=Rubei: riccardo.rubei@univaq.it; Davide=Di Ruscio: davide.diruscio@univaq.it; Massimiliano=Di Penta: dipenta@unisannio.it,"Abstract
Context:
While working on software projects, developers often replace third-party libraries (TPLs) with different ones offering similar functionalities. However, choosing a suitable TPL to migrate to is a complex task. As TPLs provide developers with Application Programming Interfaces (APIs) to allow for the invocation of their functionalities after adopting a new TPL, projects need to be migrated by the methods containing the affected API calls. Altogether, the coupled migration of TPLs and code is a strenuous process, requiring massive development effort. Most of the existing approaches either deal with library or API call migration but usually fail to solve both problems coherently simultaneously.
Objective:
This paper presents DeepMig, a novel approach to the coupled migration of TPLs and API calls. We aim to support developers in managing their projects, at the library and API level, allowing them to increase their productivity.
Methods:
DeepMig is based on a transformer architecture, accepts a set of libraries to predict a new set of libraries. Then, it looks for the changed API calls and recommends a migration plan for the affected methods. We evaluate DeepMig using datasets of Java projects collected from the Maven Central Repository, ensuring an assessment based on real-world dependency configurations.
Results:
Our evaluation reveals promising outcomes: DeepMig recommends both libraries and code; by several projects, it retrieves a perfect match for the recommended items, obtaining an accuracy of 1.0. Moreover, being fed with proper training data, DeepMig provides comparable code migration steps of a static API migrator, a baseline for the code migration task.
Conclusion:
We conclude that DeepMig is capable of recommending both TPL and API migration, providing developers with a practical tool to migrate the entire project.",21 Mar 2025,7,"DeepMig presents a novel approach to coupled migration of TPLs and API calls, potentially increasing developers' productivity. This tool could provide valuable assistance to early-stage ventures in managing their projects efficiently."
https://www.sciencedirect.com/science/article/pii/S0950584924001551,Bringing architecture-based adaption to the mainstream,December 2024,Information and Software Technology,Not Found,Negar=Ghorbani: negargh@uci.edu; Joshua=Garcia: joshug4@uci.edu; Sam=Malek: malek@uci.edu,"Abstract
Software architecture has been shown to provide an appropriate level of granularity for representation of a managed software system and reasoning about the impact of adaptation choices on its properties. Software architecture-based adaptability is the ability to adapt a software system in terms of its architectural elements, such as its components and their interfaces. Despite its promise, architecture-based adaptation has remained largely elusive, mainly because it involves heavy engineering effort of making non-trivial changes to the manner in which a software system is implemented. In this paper, we present 
Acadia
—a framework that automatically enables architecture-based adaptation of practically any Java 9+ application without requiring any changes to the implementation of the application itself. 
Acadia
 builds on the 
Java Platform Module System (JPMS)
, which has brought extensive support for architecture-based development to Java 9 and subsequent versions. 
Acadia
 extends JPMS with the ability to provide and maintain a representation of an application’s architecture and make changes to it at runtime. The results of our experimental evaluation, conducted on three large open-source Java applications, indicate that 
Acadia
 is able to efficiently apply dynamic changes to the architecture of these applications without requiring any changes to their implementation.",21 Mar 2025,6,"Acadia presents a framework for architecture-based adaptation without changes to application implementation, potentially reducing engineering effort. While promising, the direct impact on early-stage ventures is not explicitly discussed."
https://www.sciencedirect.com/science/article/pii/S0950584924001708,A3Test: Assertion-Augmented Automated Test case generation,December 2024,Information and Software Technology,"Test case generation, Deep learning",Saranya=Alagarsamy: Not Found; Chakkrit=Tantithamthavorn: chakkrit@monash.edu; Aldeida=Aleti: Not Found,"Abstract
Context:
Test case generation is a critical yet challenging task in software development. Recently, AthenaTest – a Deep Learning (DL) approach for generating unit test cases has been proposed. However, our revisiting study reveals that AthenaTest can generate less than one-fifth of the test cases correctly, due to a lack of assertion knowledge and test signature verification.
Objective:
This paper introduces A3Test, a novel DL-based approach to the generation of test cases, enhanced with assertion knowledge and a mechanism to verify consistency of the name and signatures of the tests. A3Test aims to adapt domain knowledge from assertion generation to test case generation.
Method:
A3Test employs domain adaptation principles and introduces a verification approach to name consistency and test signatures. We evaluate its effectiveness using 5,278 focal methods from the Defects4j dataset.
Results:
Our findings indicate that A3Test outperforms AthenaTest and ChatUniTest. A3Test generates 2.16% to 395.43% more correct test cases, achieves 2.17% to 34.29% higher method coverage, and 25.64% higher line coverage. A3Test achieves 2.13% to 12.20% higher branch coverage, 2.22% to 12.20% higher mutation scores, and 2.44% to 55.56% more correct assertions compared to both ChatUniTest and AthenaTest respectively for one iteration. When generating multiple test cases per method A3Test still shows improvements and comparable efficacy to ChatUnitTest. A survey of developers reveals that the majority of the participants 70.51% agree that test cases generated by A3Test are more readable than those generated by EvoSuite.
Conclusions:
A3Test significantly enhances test case generation through its incorporation of assertion knowledge and test signature verification, contributing to the generation of correct test cases.",21 Mar 2025,8,"The development of A3Test significantly improves test case generation, leading to more correct test cases and higher coverage metrics. This can have a positive impact on European early-stage ventures by enhancing software quality and reducing defects."
https://www.sciencedirect.com/science/article/pii/S0950584924001575,Perceived impact of agile principles: Insights from a survey-based study on agile software development project success,December 2024,Information and Software Technology,Not Found,Yulianus=Palopak: ypalopak@unai.edu; Sun-Jen=Huang: huangsj@mail.ntust.edu.tw,"Abstract
Context
Agile methodology has emerged as a fundamental framework guiding software development projects, emphasizing values and principles for achieving successful project outcomes. Despite the widespread recognition of the importance of agile principles, there remains a gap in empirical research investigating their actual impact on agile project success.
Objective
This research aims to examine the relationship between agile principles and project outcomes and provide empirical evidence supporting the importance of agile principles in achieving success in agile software development (ASD) projects.
Method
A total of 298 Agile project practitioners participated in an online survey between August and September 2023 to test this study's research model using the partial least square structural equation modeling (PLS-SEM) method.
Results
We find a significant relationship between adopting agile principles and project success, with regular delivery, technical excellence, team member proactivity, and customer collaboration showing the highest impact on Agile project success. However, process simplicity was found not to be significant in the study.
Conclusions
Our analysis verifies the importance of Agile principles and suggests areas for further study to successfully understand their impact on Agile projects. The findings contribute to the ongoing discourse on agile principles and their impact on software development project success, opening avenues for future research and the refinement of agile methodologies. These insights could assist organizations in optimizing Agile practices and decision-making, leading to more successful and efficient software development projects.",21 Mar 2025,7,"The study on agile principles and project success provides valuable insights for Agile projects. Understanding the importance of agile principles can contribute to the success of software development projects, which is relevant for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924001307,The quantum frontier of software engineering: A systematic mapping study,November 2024,Information and Software Technology,"Quantum computing, Quantum software engineering, Software engineering for quantum programming, Empirical software engineering, Systematic mapping study",Manuel=De Stefano: madestefano@unisa.it; Fabiano=Pecorelli: fpecorelli@unisa.it; Dario=Di Nucci: ddinucci@unisa.it; Fabio=Palomba: fpalomba@unisa.it; Andrea=De Lucia: adelucia@unisa.it,"Abstract
Context:
Quantum computing is becoming a reality, and quantum software engineering (QSE) is emerging as a new discipline to enable developers to design and develop quantum programs.
Objective:
This paper presents a systematic mapping study of the current state of QSE research, aiming to identify the most investigated topics, the types and number of studies, the main reported results, and the most studied quantum computing tools/frameworks. Additionally, the study aims to explore the research community’s interest in QSE, how it has evolved, and any prior contributions to the discipline before its formal introduction through the Talavera Manifesto.
Method:
We searched for relevant articles in several databases and applied inclusion and exclusion criteria to select the most relevant studies. After evaluating the quality of the selected resources, we extracted relevant data from the primary studies and analyzed them.
Results:
We found that QSE research has primarily focused on software testing, with little attention given to other topics, such as software engineering management. The most commonly studied technology for techniques and tools is Qiskit, although, in most studies, either multiple or none specific technologies were employed. The researchers most interested in QSE are interconnected through direct collaborations, and several strong collaboration clusters have been identified. Most articles in QSE have been published in non-thematic venues, with a preference for conferences.
Conclusions:
The study’s implications are providing a centralized source of information for researchers and practitioners in the field, facilitating knowledge transfer, and contributing to the advancement and growth of QSE.",21 Mar 2025,5,"The systematic mapping study on Quantum software engineering provides valuable information for researchers and practitioners in the field. While it may not have direct practical impact on European early-stage ventures, it contributes to the growth of the discipline."
https://www.sciencedirect.com/science/article/pii/S0950584924001496,Hidden code vulnerability detection: A study of the Graph-BiLSTM algorithm,November 2024,Information and Software Technology,Not Found,Kao=Ge: Not Found; Qing-Bang=Han: 20111841@hhu.edu.cn,"Abstract
Context:
The accelerated growth of the Internet and the advent of artificial intelligence have led to a heightened interdependence of open source products, which has in turn resulted in a rise in the frequency of security incidents. Consequently, the cost-effective, fast and efficient detection of hidden code vulnerabilities in open source software products has become an urgent challenge for both academic and engineering communities.
Objectives:
In response to this pressing need, a novel and efficient code vulnerability detection model has been proposed: the Graph-Bi-Directional Long Short-Term Memory Network Algorithm (Graph-BiLSTM). The algorithm is designed to enable the detection of vulnerabilities in Github’s code commit records on a large scale, at low cost and in an efficient manner.
Methods:
In order to extract the most effective code vulnerability features, state-of-the-art vulnerability datasets were compared in order to identify the optimal training dataset. Initially, the Joern tool was employed to transform function-level code blocks into Code Property Graphs (CPGs). Thereafter, structural features (degree centrality, Katz centrality, and closeness centrality) of these CPGs were computed and combined with the embedding features of the node sequences to form a two-dimensional feature vector space for the function-level code blocks. Subsequently, the BiLSTM network algorithm was employed for the automated extraction and iterative model training of a substantial number of vulnerability code samples. Finally, the trained algorithmic model was applied to code commit records of open-source software products on GitHub, achieving effective detection of hidden code vulnerabilities.
Conclusion:
Experimental results indicate that the PrimeVul dataset represents the most optimal resource for vulnerability detection. Moreover, the Graph-BiLSTM model demonstrated superior performance in terms of accuracy, training cost, and inference time when compared to state-of-the-art algorithms for the detection of vulnerabilities in open-source software code on GitHub. This highlights the significant value of the model for engineering applications.",21 Mar 2025,9,The Graph-BiLSTM model for code vulnerability detection addresses a pressing need in the software development community. The efficient detection of vulnerabilities in open-source software can significantly benefit European early-stage ventures by improving security and reducing risks.
https://www.sciencedirect.com/science/article/pii/S0950584924001241,Towards antifragility of cloud systems: An adaptive chaos driven framework,October 2024,Information and Software Technology,"Antifragility, Resilience, Chaos engineering, Self-adaptive software, Resilience testing, Cloud computing",Joseph S.=Botros: Not Found; Lamis F.=Al-Qora'n: Not Found; Amro=Al-Said Ahmad: a.m.al-said.ahmad@keele.ac.uk,"Abstract
Context
Unlike resilience, antifragility describes systems that get stronger rather than weaker under stress and chaos. Antifragile systems have the capacity to overcome stressors and come out stronger, whereas resilient systems are focused on their capacity to return to their previous state following a failure. As technology environments become increasingly complex, there is a great need for developing software systems that can benefit from failures while continuously improving. Most applications nowadays operate in cloud environments. Thus, with this increasing adoption of Cloud-Native Systems they require antifragility due to their distributed nature.
Objective
The paper proposes UNFRAGILE framework, which facilitates the transformation of existing systems into antifragile systems. The framework employs chaos engineering to introduce failures incrementally and assess the 
system's response
 under such perturbation and improves the quality of system response by removing fragilities and introducing adaptive 
fault tolerance
 strategies.
Method
The UNFRAGILE framework's feasibility has been validated by applying it to a cloud-native using a real-world architecture to enhance its antifragility towards long outbound service latencies. The empirical investigation of fragility is undertaken, and the results show how chaos affects application performance metrics and causes disturbances in them. To deal with chaotic 
network latency
, an adaptation phase is put into effect.
Results
The findings indicate that the steady stage's behaviour is like the antifragile stage's behaviour. This suggests that the system could self-stabilise during the chaos without the need to define a 
static configuration
 after determining from the context of the environment that the dependent system was experiencing difficulties.
Conclusion
Overall, this paper contributes to ongoing efforts to develop antifragile software capable of adapting to the rapidly changing complex environment. Overall, the research provides an operational framework for engineering software systems that learn and improve through exposure to failures rather than just surviving them.",21 Mar 2025,6,"The UNFRAGILE framework for developing antifragile software systems presents an innovative approach to software engineering. While the concept is valuable, its direct impact on European early-stage ventures may be limited at the current stage of adoption."
https://www.sciencedirect.com/science/article/pii/S0950584924001265,Towards assessing the quality of knowledge graphs via differential testing,October 2024,Information and Software Technology,Not Found,Jiajun=Tan: jjtan@smail.nju.edu.cn; Dong=Wang: juliawdd@henu.edu.cn; Jingyu=Sun: MF21320132@smail.nju.edu.cn; Zixi=Liu: zxliu@smail.nju.edu.cn; Xiaoruo=Li: lixiaoruo@henu.edu.cn; Yang=Feng: fengyang@nju.edu.cn,"Abstract
Knowledge graphs
 (KG) can aggregate data and make information resources easier to calculate and understand. With tremendous advancements in knowledge graphs, they have been incorporated into plenty of software systems to assist various tasks. However, while KGs determine the performance of downstream software systems, their quality is often measured by the accuracy of test data. Considering the limitation of accessible high-quality test data, an automated quality assessment technique could fundamentally improve the testing efficiency of KG-driven software systems and save plenty of manual labeling resources.
In this paper, we propose an automated approach to quantify the quality of KGs via differential testing. It first constructs multiple 
Knowledge Graph Embedding
 Models (KGEM) and conducts head prediction tasks on models. Then, it can produce a differential score that reflects the quality of KGs by comparing the proximity of output results. To validate the effectiveness of this approach, we experiment with four open-sourced knowledge graphs. The experiment results show that our approach is capable of accurately evaluating the quality of KGs and producing reliable results on different datasets. Moreover, we compared our method with existing methods and achieved certain advantages. The potential usefulness of our approach sheds light on the development of various KG-driven software systems.",21 Mar 2025,8,"The automated approach proposed to quantify the quality of KGs can significantly improve the testing efficiency of KG-driven software systems, saving manual labeling resources and shedding light on the development of various software systems."
https://www.sciencedirect.com/science/article/pii/S0950584924000934,Coverage-enhanced fault diagnosis for Deep Learning programs: A learning-based approach with hybrid metrics,September 2024,Information and Software Technology,Not Found,Xiaofang=Qi: Not Found; Tiangang=Zhu: Not Found; Yanhui=Li: yanhuili@nju.edu.cn,"Abstract
Context:
Given the data-driven paradigm inherent to 
Deep Learning
 (DL), it is inevitable that DL software will exhibit incorrect behavior in real-world applications. DL programs have been identified as a primary source of DL faults. To tackle this, researchers have devised a unique framework that approaches fault diagnosis as a learning task, which leverages runtime data as metrics to construct predictive models, enabling effective fault diagnosis.
Object:
In this paper, we aim to propose new metrics, especially from the coverage view, to enhance the performance of fault diagnosis models.
Method:
We combine coverage criteria and statistical operators to propose 80 coverage metrics, which summarize the trend of coverage values in the model training procedure. We construct hybrid prediction models by combining our new coverage metrics and existing runtime metrics under four widely used classifiers.
Results:
To examine whether adding our new coverage metrics performs well in DL program fault diagnosis, we conduct our experiments on six widely used datasets under four indicators (i.e., accuracy, F1 score, AUC, and MCC). Through the experiments, we observe that (a) coverage metrics are 
not redundant
 with respect to the original runtime metrics, and (b) adding extra coverage metrics can 
significantly enhance
 the performance of fault diagnosis models.
Conclusions:
Our study shows that our proposed coverage metrics are helpful in constructing effective fault diagnosis models for DL programs.",21 Mar 2025,7,"The new coverage metrics proposed to enhance the performance of fault diagnosis models for DL programs show promising results, highlighting the potential for effective fault diagnosis in real-world applications."
https://www.sciencedirect.com/science/article/pii/S095058492400082X,Machine learning for requirements engineering (ML4RE): A systematic literature review complemented by practitioners’ voices from Stack Overflow,August 2024,Information and Software Technology,"Requirements engineering, Machine learning, Systematic literature review",Tong=Li: litong@bjut.edu.cn; Xinran=Zhang: zhangxinran028@gmail.com; Yunduo=Wang: wangyunduo@buaa.edu.cn; Qixiang=Zhou: qixiang.zho@gmail.com; Yiting=Wang: wangyiting.official@gmail.com; Fangqi=Dong: dfq902@gmail.com,"Abstract
Context:
The research of 
machine learning
 for 
requirements engineering
 (ML4RE) has attracted more and more attention from researchers and practitioners. Although pioneering research has shown the potential of using 
ML techniques
 to improve RE practices, there lacks a systematic and comprehensive literature review in academia that integrates an industrial perspective. Specifically, none of the reviews available in ML4RE have considered the grey literature, which is primarily from practitioner origin and is more reflective of the real issues and challenges faced in practice.
Objective:
In this paper, we conduct a systematic survey of academic publications in ML4RE and complement it with the practitioners’ voices from Stack Overflow to complete a comprehensive literature review. Our research objective is to provide a comprehensive view of the current research progress in ML4RE, present the main questions and challenges faced in RE practice, understand the gap between research and practice, and provide our insights into how the RE academic domain can pragmatically develop in the future.
Method:
We systematically investigated 207 academic papers on ML4RE from 2010 to 2022, along with 375 questions related to RE practices on Stack Overflow and their corresponding answers. Our analysis encompassed their trends, focused RE activities and tasks, employed solutions, and associated data. Finally, we conducted a 
joint
 analysis, contrasting the outcomes of both parts.
Results:
Based on the statistical results from collected literature, we summarize an academic roadmap and analyse the disparities, offering research recommendations. Our suggestions include the development of intelligent question-answering assistants employing 
large language models
, the integration of machine learning into industrial tools, and the promotion of collaboration between academia and industry.
Conclusion:
This study contributes by providing a holistic view of ML4RE, delineating disparities between research and practice, and proposing pragmatic suggestions to bridge the academia-industry gap.",21 Mar 2025,5,"The systematic survey of academic publications in ML4RE and the inclusion of practitioners' voices provide valuable insights into the gap between research and practice, offering suggestions for future development."
https://www.sciencedirect.com/science/article/pii/S0950584924000831,Studying and recommending information highlighting in Stack Overflow answers,August 2024,Information and Software Technology,Not Found,Shahla Shaan=Ahmed: ahmeds27@myumanitoba.ca; Shaowei=Wang: shaowei.wang@umanitoba.ca; Yuan=Tian: y.tian@queensu.ca; Tse-Hsun (Peter)=Chen: peterc@encs.concordia.ca; Haoxiang=Zhang: haoxiang.zhang@acm.org,"Abstract
Context:
Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or 
HTML
 so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information.
Objective:
We carried out the first large-scale 
exploratory study
 on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using 
neural network
 architectures initially designed for the 
Named Entity Recognition
 task.
Method:
In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN-based and BERT-based models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
Results:
Our models achieve a precision ranging from 0.50 to 0.72 for different formatting types. It is easier to build a model to recommend Code than other types. Models for text formatting types (i.e., Heading, Bold, and Italic) suffer low recall. Our analysis of failure cases indicates that the majority of the failure cases are due to missing identification. One explanation is that the models are easy to learn the frequent highlighted words while struggling to learn less frequent words (i.g., long-tail knowledge).
Conclusion:
Our findings suggest that it is possible to develop recommendation models for highlighting information for answers with different formatting styles on Stack Overflow.",21 Mar 2025,6,The large-scale exploratory study on the information highlighted in Stack Overflow answers and the development of recommendation models for formatting styles show potential for improving user experience and accessibility of the platform.
https://www.sciencedirect.com/science/article/pii/S0950584924000648,Architecting for sustainability of and in the cloud: A systematic literature review,July 2024,Information and Software Technology,"Cloud computing, Sustainability, Survey, Software architecture",Sahar=Ahmadisakha: s.ahmadisakha@rug.nl; Vasilios=Andrikopoulos: v.andrikopoulos@rug.nl,"Abstract
Context:
The interest in the intersection between 
cloud computing
 and 
sustainability
 is naturally growing as the popularity of the former makes it in many cases the 
default model
 for delivering software functionalities to end users. Furthermore, software architecture offers a fundamental route to address sustainability, with the recent shift towards recognizing sustainability as a software quality.
Objective:
Approaching the intersection between sustainability and cloud computing from the perspective of the study of software architectures, in this work we aim to collect the data necessary for us to understand the relation between these two areas as reflected in the literature. Given the lack of suitable surveys for this purpose in this paper we report on our review of the relevant literature, designed to address the question of how 
architectural solutions
 specifically for the cloud are addressing sustainability.
Methods:
Following the steps prescribed by a well-known method for this purpose on running a systematic literature review, we answer our defined research questions using both (qualitative) analysis and synthesis of data.
Results:
As a result of the review process, we are able to identify 10 solution types, 8 recurring design decisions, 11 involved architectural entities, and 17 distinct perceptions of the cloud with respect to sustainability. The adoption of reusable 
architectural tactics
 and patterns has been observed to be less prevalent than anticipated. Furthermore, certain fundamental characteristics of cloud computing, including multi-tenancy, on-demand 
resource provisioning
, and the pay-as-you-go model, have been identified across diverse cloud perceptions.
Conclusion:
Our findings point to the need for more systematic work required on developing architectural solutions specifically for cloud computing incorporating sustainability goals. We also suggest that achieving sustainability through cloud in software architecture may be feasible. Furthermore, we identify a persistent threat to further secondary studies on the same topic due to improper use of terms.",21 Mar 2025,4,"The study on the intersection between sustainability and cloud computing from a software architecture perspective provides insights into how architectural solutions are addressing sustainability, but lacks concrete practical implications for startups."
https://www.sciencedirect.com/science/article/pii/S0950584924000764,Not yet another BPM lifecycle: A synthesis of existing approaches using BPMN,July 2024,Information and Software Technology,Not Found,Nikolaos=Nousias: Not Found; George=Tsakalidis: Not Found; Kostas=Vergidis: kvergidis@uom.edu.gr,"Abstract
Context
Business Process Management (BPM) is considered an important management approach that encompasses a set of methods for managing the business processes of an organization. To maximize the benefits of BPM, scholars have conceptualized its steps in schematic diagrams with interrelated phases called BPM lifecycles. As this approach has been established, the phenomenon of perpetual proposition of BPM lifecycles has been observed in relevant literature. This practice obscures what should be relatively straightforward: a consensus among researchers and practitioners regarding the steps that a business process should flow through during its lifecycle.
Objective
The aim of this work is to investigate the existing BPM 
lifecycle models
 proposed in literature, identify convergences and variations in these models, analyze their core components and locate common patterns that will enable the synthesis of a BPM lifecycle that is conceptualized with 
Business Process Model and Notation
 (BPMN), the community de-facto business process modeling notation.
Method
To formalize the research problem and develop the design of a solution, the Design Science Research Process (DSRP) model was adopted. To investigate the perpetual proposition of BPM lifecycles in literature, the authors conducted a Systematic Literature Review (SLR). On whether recurring patterns can emerge from the BPM lifecycles, a normalization process was introduced to homogenize the data and four metrics were used to evaluate the results. The emerging patterns were assembled into a graph that formed the basis for proposing a synthetic BPM lifecycle.
Results
The outcome of the paper is three-fold: First, the identification of four major inefficiencies of existing BPM lifecycles, namely varying 
granularity
, inconsistent nomenclature, subjective polysemy, and lack of formal conceptualization approaches. Also, a standardized definition of the inclusive steps that exist in the lifecycles by clustering the existing ones in a conceptually systematic manner. Finally, a synthetic BPM lifecycle is conceptualized that systematizes the existing concepts and their interrelations based on a formalized BPMN model in two levels of 
granularity
: a basic version that illustrates the functional and control-flow aspects of the BPM lifecycle and an enhanced version incorporating additionally the resource and data perspectives.
Conclusion
This paper proposes a BPMN-based conceptualization of the BPM lifecycle that can facilitate the management of business processes by providing enhanced clarity, improved resource management, and predefined error handling in a BPM initiative. By systematizing the control-flow, data, and resource perspective of the BPM lifecycle, stakeholders can gain a clear understanding of the sequence of steps, the interrelated data flows, and the distribution of work.",21 Mar 2025,6,"The research on synthesizing BPM lifecycles using BPMN notation can provide a structured approach for managing business processes, which can be useful for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492400051X,Search-based approaches to optimizing software product line architectures: A systematic literature review,June 2024,Information and Software Technology,Not Found,Sedigheh=Khoshnevis: Sedigheh.Khoshnevis@iau.ac.ir; Omid=Ardestani: Not Found,"Abstract
Context
Software product line architecture (PLA) plays an important role in developing software product lines (SPLs) and other configurable systems. Search-based (SB) approaches can optimize the design of PLAs according to a given set of metrics as fitness functions. Although this area has been explored by researchers, there is a lack of synthesis of search-based PLA (SBPLA) research. A comprehensive review would offer valuable insights into previous contributions and identify areas for further research.
Objective
The objective of this work is to identify and summarize quality-assessed peer-reviewed studies on search-based PLA design from the aspects of the research scope, problems, contributions, evaluation, and open issues.
Methods
We conducted a systematic literature review based on Kitchenham's methodology. Based on a predefined search protocol we identified related studies limited to the ones published between 2000 and 2022 in journals and conference proceedings.
Results
Out of 686 initial search results, 34 papers were finally selected after a set of deep search, and criteria application activities. We provided a taxonomy of optimization problems in SBPLA and found that PLA remodularization and refactoring were the two categories most emphasized by the researchers. We also provided several other categorizations regarding contributions, research design, open issues, and other subjects of interest.
Conclusions
The interest in SBPLA design has been growing since 2014. PLA cloning and re-engineering problems have never been addressed in the literature. Performing subjective evaluation with the participation of experts from the industry will be profitable, as a complementary method to objective experimental evaluation, and therefore carrying out quanti-qualitative research.",21 Mar 2025,5,"The review on search-based PLA design offers insights into optimizing software product lines, which can benefit startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584924000405,Behaviour-driven development and metrics framework for enhanced agile practices in scrum teams,June 2024,Information and Software Technology,Not Found,Thamizhiniyan=Natarajan: Not Found; Shanmugavadivu=Pichai: p.shanmugavadivu@ruraluniv.ac.in,"Abstract
Context
Agile methodologies
 highlight collaborative efforts among 
software engineering
 groups for iterative, high-quality product delivery within short timeframes. However, Scrum teams face persistent challenges in achieving these objectives, stemming from difficulties in seamless collaboration and effective communication among various roles, such as developers and testers. To address these issues, Scrum teams are increasingly adopting Behaviour-Driven Development (BDD), a testing technique fostering collaboration and shared understanding through test scenarios.
Objectives
This research investigates the adoption of BDD practices in Scrum teams and the formulation of a metrics framework tailored for optimizing Scrum practices and 
product quality
.
Methods
Employing action research, this study extends over two and half years, actively engaging Scrum team members and stakeholders to encompass their collaborative contributions, insights, and perspectives. It commences with defining a metrics framework through exploration within agile teams to measure and evaluate Scrum team performance. Subsequently, the focus shifts to implementing BDD practices systematically, employing training sessions, workshops, and iterative refinements.
Results
The results of the study emphasize the substantial role of Behaviour-Driven Development (BDD) in improving collaboration, communication, and the comprehension of requirements within the Scrum team. Concurrently, the tailored metrics framework bolsters quality assurance practices, enhancing software quality and customer satisfaction. BDD adoption expedites automation and product delivery, while the metrics framework enables informed decision-making.
Conclusions
Combining BDD practices with a custom metrics framework offers a holistic strategy for addressing Scrum challenges. Enhanced collaboration, communication, and requirements comprehension, resulting from BDD, synergize with the metrics framework to elevate Scrum teams' performance, software quality, and customer value. This research underlines the importance of adopting BDD as a testing methodology to achieve these improvements in Scrum teams.",21 Mar 2025,7,"The investigation of BDD practices in Scrum teams, along with a metrics framework, can enhance collaboration and product quality, valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584924000119,Context-based statement-level vulnerability localization,May 2024,Information and Software Technology,Not Found,Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn,"Abstract
Context:
The number of attacks exploring software vulnerabilities has dramatically increased, which has caused various severe damages. Thus, early and accurately detecting vulnerabilities becomes essential to guarantee software quality and prevent the systems from 
malicious attacks
. Multiple automated 
vulnerability detection
 approaches have been proposed and obtained promising results. However, most studies detect vulnerabilities at a coarse-grained, i.e., file or method level. Thus, developers still have to spend significant investigation efforts on localizing vulnerable statements.
Objective:
In this paper, we introduce 
COSTA
, a novel context-based approach to localize vulnerable statements.
Method:
In particular, given a vulnerable function, 
COSTA
 identifies vulnerable statements based on their suspiciousness scores. Specifically, the suspiciousness of each statement is measured according to its semantics captured by four contexts, including 
operation context, dependence context, surrounding context
, and 
vulnerability type
.
Results:
Our experimental results on a large vulnerability dataset show that 
COSTA
 outperforms the state-of-the-art approaches up to 
96%
 in F1-score and 
167%
 in Accuracy. 
COSTA
 also surpasses these approaches up to 
two times
 in Top-1 Accuracy. Especially, 
COSTA
 obtains about 
80% at Top-3 Recall
. In other words, developers can find about 80% of the vulnerable statements by investigating only three first-ranked statements in each function.
Conclusion:
COSTA
 effectively addresses the challenge of statement-level vulnerability localization by leveraging multiple contextual features. Our experimental results show that 
COSTA
 outperforms existing state-of-the-art approaches. With the ability to accurately and efficiently identify vulnerable statements, developers can better allocate their investigation efforts, reduce the risk of potential security threats, and ensure software quality and security in real-world applications.",21 Mar 2025,8,"The context-based vulnerability detection approach presented can significantly improve software security, which is crucial for startups looking to protect their products."
https://www.sciencedirect.com/science/article/pii/S0950584924000296,"Stakeholders collaborations, challenges and emerging concepts in digital twin ecosystems",May 2024,Information and Software Technology,"Digital twin, Digital twin ecosystem, Stakeholders, Systematic literature review, Empirical study, Definition, Software development",Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Heidi=Hietala: Not Found; Yueqiang=Xu: Not Found; Reshani=Liyanage: Not Found,"Abstract
Context:
Digital twin
 (DT) ecosystems are rapidly evolving, connecting many stakeholders, such as manufacturers, customers, and application platform providers. These ecosystems require collaboration and interaction between diverse actors to create value. This study delves into the collaboration of such stakeholders within DT-focused ecosystems.
Objective:
This research aims to understand stakeholder collaboration within DT ecosystems, identify potential challenges, and provide insights for managing these stakeholders. It also seeks to define the DT ecosystem and its implications for both research and practice.
Method:
A systematic literature review was conducted, supplemented by empirical evidence gathered from interviews with DT experts who were knowledgeable about the DT ecosystem. The study also analyzed DT systems, stakeholder roles, and the challenges with ecosystem-focused DT development.
Results:
The study identified various stakeholders and their roles in adding value to a DT ecosystem. It highlighted the benefits of stakeholder collaboration, such as knowledge gain during DT system development. The research also revealed the technical and non-technical challenges encountered in ecosystem-focused DTs, emphasizing the importance of standardization as a solution. A new definition of the DT ecosystem was proposed, emphasizing its data-driven nature, interconnected DTs, stakeholder value creation, and technology enablement.
Conclusion:
Stakeholder collaboration is pivotal in DT ecosystems, with each actor playing a distinct role. Addressing challenges, especially through standardization (OPC UA and ISO 23247), can lead to more efficient and coherent DT ecosystems. The insights provided by this study can guide industries in designing, developing, and maintaining their DT ecosystems, ensuring value creation and stakeholder satisfaction. Future research avenues that emphasize the importance of understanding the challenges involved and deploy appropriate solutions were suggested.",21 Mar 2025,6,"The study on stakeholder collaboration within DT ecosystems offers insights and challenges for managing diverse actors, relevant for startups engaging in digital twin technologies."
https://www.sciencedirect.com/science/article/pii/S0950584924000132,UXH-GEDAPP: A set of user experience heuristics for evaluating generative design applications,April 2024,Information and Software Technology,Not Found,Daniela=Quiñones: daniela.quinones@pucv.cl; Claudia=Ojeda: Not Found; Rodrigo F.=Herrera: Not Found; Luis Felipe=Rojas: Not Found,"Abstract
Context
Traditional building and infrastructure design methodologies are inflexible and inefficient, leading to high costs and environmental damage. Generative design, with an algorithm that provides multiple options, could be a potential solution. The challenge is creating an intuitive, user-friendly application that optimizes engineers’ time, reducing manual iterations and lead to a good 
user experience
 (UX). A method for evaluating the UX is 
heuristic evaluation
, in which heuristics are used to inspect a software product.
Objective
Since generative design applications have specific features, generic heuristics may not detect all problems related to UX. This article presents a novel set of 9 heuristics to evaluate UX in generative design applications: UXH-GEDAPP. This set is focused on evaluating both UX attributes and specific features of generative design applications.
Method
A formal methodology was used to develop the heuristics, through 7 stages: exploratory, descriptive, correlational, selection, specification, validation, and refinement. We performed 3 iterations and validated UXH-GEDAPP in 2 iterations through: 
heuristic evaluation
, 
expert judgment
, and user test. Since the methodology can be applied iteratively, we validated and refined the set to improve the proposal.
Results
The results obtained in the validation stage indicate that UXH-GEDAPP is useful and more effective than generic heuristics when evaluating generative design applications. UXH-GEDAPP allows to detect specific usability/UX problems as well as more severe problems related to generative design applications. Furthermore, 
evaluators
 made fewer errors associating the detected problems with the proposed heuristics, compared to generic sets.
Conclusion
UXH-GEDAPP is a new set of heuristics that encourages the creation and use of generative design applications with good UX. It can detect usability/UX problems and help correct them, as well as guide the development of new generative design applications for a pleasant and intuitive 
user experience
.",21 Mar 2025,7,"The development of a new set of heuristics for evaluating generative design applications can significantly impact user experience and guide the development of new applications, providing practical value to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923002306,Technical debt management automation: State of the art and future perspectives,March 2024,Information and Software Technology,"Systematic mapping study, Technical debt, Technical debt management, Tools, Automation",João Paulo=Biazotto: j.p.biazotto@rug.nl; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl; Elisa Yumi=Nakagawa: elisa@icmc.usp.br,"Abstract
Context:
Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system’s maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most.
Objectives:
The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM.
Methods:
We conducted a 
systematic mapping study
 (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation.
Results:
We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges.
Conclusion:
The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts.",21 Mar 2025,9,"The overview of the state of the art in technical debt management automation can greatly benefit startups by helping them select optimal automation strategies for efficient TD management, addressing a crucial need in the industry."
https://www.sciencedirect.com/science/article/pii/S0950584923002197,Experiences from conducting rapid reviews in collaboration with practitioners — Two industrial cases,March 2024,Information and Software Technology,"Literature reviews, Systematic review, Rapid reviews, Research relevance, Industry-academia collaboration",Sergio=Rico: Sergio.Rico@cs.lth.se; Nauman Bin=Ali: nauman.ali@bth.se; Emelie=Engström: emelie.engstrom@cs.lth.se; Martin=Höst: martin.host@cs.lth.se,"Abstract
Context:
Evidence-based 
software engineering
 (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice.
Objective:
This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and 
industry
. Through the lens of two 
case studies
, we delve into the practical application and experience of conducting RRs.
Methods:
We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We 
collected data
 through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis.
Results:
We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners.
Conclusion:
The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange.",21 Mar 2025,8,"The study on Rapid Reviews in fostering knowledge exchange between academia and industry can provide valuable insights for startups looking to bridge the gap between research and practice, enhancing the impact of EBSE in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923002318,Towards automating self-admitted technical debt repayment,March 2024,Information and Software Technology,"Self-admitted technical debt, Software quality, Software maintenance, Software analytics, Deep learning, Technical debt repayment",Abdulaziz=Alhefdhi: aa043@uowmail.edu.au; Hoa Khanh=Dam: hoa@uow.edu.au; Aditya=Ghose: aditya@uow.edu.au,"Abstract
Context:
Self-Admitted Technical Debt (SATD) refers to the technical debt in software that is explicitly flagged, typically by the 
source code
 comment. The SATD literature has mainly focused on comprehending, describing, detecting, and recommending SATD. Most recently, there have been efforts to study the state of the code before and after removing the SATD comment. While these efforts serve as a preliminary step towards the repayment of SATD, actual attempts towards automating SATD repayment, to the best of our knowledge, are yet to be made.
Objective:
In this paper, we propose the first attempt towards direct, complete, and automated SATD repayment by providing two main contributions. The first contribution is an empirical study of how the SATD comment relates to repaying the debt. The second contribution is 
DLRepay
, our deep 
learning approach
 for SATD repayment.
Method:
We developed a SATD Repayment dataset, namely SATD-R, and established a taxonomy based on the relationship and helpfulness of the SATD comment to/in repaying the debt. In addition, we developed 
DLRepay
 which takes as an input a pair of SATD comment and code, and generates a new, TD-free code.
Results:
We found that there are five different categories in which the SATD comment relates to Technical Debt repayment. We also identify when the SATD comment has a positive and logical connection to repaying the debt, both generally and in every category. Furthermore, we illustrate the results of our SATD repayment approach across two datasets, three input types, two output types, and two 
neural networks
.
Conclusion:
The resulting taxonomy of our empirical study paves the way for research to tackle further in-depth questions concerning SATD repayment comprehension, identification, and automation. In addition, the various experimental setups we conduct provide multiple insights regarding the applicability of our SATD repayment approach.",21 Mar 2025,6,"The attempt towards automating SATD repayment presents a novel approach, but its impact on early-stage ventures may be limited compared to other abstracts due to the early stage of research in this area."
https://www.sciencedirect.com/science/article/pii/S0950584923002082,Developer and End-User Perspectives on Addressing Human Aspects in Mobile eHealth Apps,February 2024,Information and Software Technology,"eHealth App, Human Aspect, User Study, App Development, Stakeholders Perspectives",Md.=Shamsujjoha: md.shamsujjoha@monash.edu; John=Grundy: john.grundy@monash.edu; Hourieh=Khalajzadeh: hkhalajzadeh@deakin.edu.au; Qinghua=Lu: qinghua.lu@data61.csiro.au; Li=Li: lilicoding@ieee.org,"Abstract
Context:
eHealth apps are mobile apps that help in self-management of critical illnesses, provide home-based disease management, and help with personalized care. Users of eHealth apps are naturally very diverse in terms of their 
human aspects
, e.g., their age, gender, emotional reactions to the apps, cognitive style, physical and mental challenges. Unfortunately, many eHealth apps do not take these user differences sufficiently into account, making them ineffective or even unusable.
Objective:
This paper reports a study from eHealth app stakeholders’ – developers and end-users – perspectives on critical challenges and benefits of better incorporating 
human aspects
 into eHealth app development and usage. We also investigate how different 
human aspects
 are being addressed by developers, which ones are the most important for different user groups, and which ones are currently missing/poorly handled.
Method:
A mixed-method approach that integrates qualitative and quantitative research was used for this study. We gathered and analyzed data from 240 online survey responses and 25 detailed interviews within the same study and validated the results.
Results:
We report key issues encountered in eHealth app design, difficulty in addressing different 
human aspects
, areas requiring further research and practical assistance, and recommend our findings to best address these challenges. We found addressing 
human aspects
 throughout the app development life-cycle is beneficial for more effective eHealth apps. Our findings also suggest the need for improved standards and guidelines, better developer-user collaborative culture, and better 
human aspects
 education to produce more effective eHealth apps.
Conclusion:
This paper investigates current approaches used in the eHealth app domain that take into account the 
human aspects
 of app users. The paper guides eHealth app stakeholders, future researchers, academia and industry partners be aware of 
human aspects
 related challenges and improve produce apps.",21 Mar 2025,8,"The study on incorporating human aspects into eHealth app development addresses critical challenges and benefits, providing practical guidance for startups to create more effective eHealth apps based on user diversity, enhancing their impact in the industry."
https://www.sciencedirect.com/science/article/pii/S0950584923002239,To change or not to change? Modeling software system interactions using Temporal Graphs and Graph Neural Networks: A focus on change propagation,February 2024,Information and Software Technology,Not Found,Manuella=Germanos: Not Found; Danielle=Azar: danielle.azar@lau.edu.lb; Eileen Marie=Hanna: Not Found,"Abstract
Context:
The world is quickly adopting new technologies and evolving to rely on software systems for the simplest tasks. This prompts developers to expand their software systems by adding new product features. However, this expansion should be cautiously tackled to prevent the degradation of the quality of the software product.
Objective:
One challenge when modifying code – whether to patch a bug or add a feature – is knowing which components will be affected by the change and amending possible misbehavior. In this context, the study of change propagation or the impact of introducing a change is needed. By investigating how changing one component may impact the functionality of a dependency (another component), developers can prevent unexpected behavior and maintain the quality of their system.
Methods:
In this work, we tackle the change propagation problem by modeling a software system as a 
temporal graph
 where nodes represent system files and edges co-changeability, i.e., the tendency of two files to change together. The 
graph representation
 is temporal so that nodes and edges can change with time, reflecting the addition of files in the system and changes in dependencies. We then employ a Temporal Graph Network and a Long Short-Term Memory model to predict which other files will be impacted by a modification performed on a file.
Results:
We test our model on software systems of different functionality, size, and nature. We compare our results to other published work, and our model shows a significantly higher ability to predict files impacted by a change.
Conclusion:
The proposed approach effectively predicts change propagation in software systems and can guide developers and software engineers in planning the change and estimating the cost in terms of time and money.",21 Mar 2025,8,"The study addresses a crucial aspect of software development, predicting change propagation, which can significantly impact the quality of software systems for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923001933,A systematic mapping study of bug reproduction and localization,January 2024,Information and Software Technology,"Bug reproduction, Bug localization, Bug fixing, Mapping study",Di=Wang: di.wang@pg.canterbury.ac.nz; Matthias=Galster: matthias.galster@canterbury.ac.nz; Miguel=Morales-Trujillo: miguel.morales@canterbury.ac.nz,"Abstract
Context:
Identifying the root cause of a software bug and fixing it is challenging. One reason for this is that many bugs are not reproducible during bug fixing.
Objective:
We aim to provide an overview of existing works on bug reproduction and localization. We ask four research questions: RQ1: What types of problems have been studied in the area of bug reproduction and localization? RQ2: How are problems studied in previous research? RQ3: What are the main findings and outcomes of previous studies? RQ4: What are the gaps and challenges identified in previous studies?
Method:
We conducted a 
systematic mapping study
 analyzing research literature published between 2011 and 2021. The search for primary studies involved four major computer science digital libraries and resulted in 134 studies for analysis.
Results:
Regarding RQ1 we found that many studies focus on information retrieval-based approaches to support bug reproduction and localization. Regarding RQ2 we found that 
bug reports
 and 
source code
 are the typical data sources of bug reproduction and localization. Also, most studies include experiments with historical data but do not investigate ongoing projects. Regarding RQ3 we found that many studies adapt or combine existing approaches for bug reproduction and localization to improve their accuracy or applicability (e.g., combine requirements-related information and bug reports to increase information-retrieval-based techniques). Regarding RQ4 we found that existing solutions for bug reproduction and localization have rarely been integrated into the workflow of developers.
Conclusion:
Although bug reproduction and localization have been studied in quite some detail, new challenges and gaps emerge due to the evolution of software technologies and practices and the practical needs of software developers. For example, bug reproduction approaches for 
traditional web applications
 do not work well with modern “Single Page Web Applications” (SPA) and related technologies, e.g., Angular or React.",21 Mar 2025,7,"Bug reproduction and localization are common challenges in software development, and the study provides insights that can benefit early-stage ventures in improving software quality."
https://www.sciencedirect.com/science/article/pii/S0950584923001891,Microservice-based projects in agile world: A structured interview,January 2024,Information and Software Technology,Not Found,Hüseyin=Ünlü: huseyinunlu@iyte.edu.tr; Dhia Eddine=Kennouche: Not Found; Görkem Kılınç=Soylu: Not Found; Onur=Demirörs: Not Found,"Abstract
Context
During the last decade
,
 Microservice-based software architecture (MSSA) has been a preferred design paradigm for a growing number of companies. MSSA, specifically in the form of reactive systems, has substantial differences from the more conventional design paradigms, such as object-oriented analysis and design. Therefore, adaptation demands software organizations to transform their culture. However, there is a lack of research studies that explore 
common practices
 utilized by software companies that implement MSSAs.
Objective
In this study
,
 our goal is to get an insight into how practices such as an 
agile methodology
, software analysis, design, test, size measurement, and effort estimation are performed in software projects which embrace the Microservice-based software architecture paradigm. Together with the identification of practices utilized for the MSSA paradigm, we aim to determine the challenges organizations face to adopt microservice-based software architectures.
Method
We performed a structured interview with participants coming from 20 different organizations over different roles, domains, and countries to collect information on their views, experience, and the challenges faced.
Results
Our results reveal that organizations find 
agile development
 compatible with microservices. In general, they continue to use traditional object-oriented 
modeling notations
 for analysis and design in an abstract way. They continue to use the same subjective size measurement and effort estimation approaches that they were using previously in traditional architectures. However, they face unique challenges in developing microservices.
Conclusion
Although organizations face challenges, practitioners continue to use familiar techniques that they have been using for traditional architectures. The results provide a snapshot of the software industry that utilizes microservices.",21 Mar 2025,6,"The study explores the practices of organizations implementing Microservice-based software architecture, which can be valuable for startups looking to adopt this paradigm."
https://www.sciencedirect.com/science/article/pii/S0950584923001805,The role of Reinforcement Learning in software testing,December 2023,Information and Software Technology,"Software testing, Machine learning, Reinforcement Learning, Artificial intelligence",Amr=Abo-eleneen: aa1405465@student.qu.edu.qa; Ahammed=Palliyali: ap1304567@student.qu.edu.qa; Cagatay=Catal: ccatal@qu.edu.qa,"Abstract
Context:
Software testing is applied to validate the behavior of the software system and identify flaws and bugs. Different 
machine learning technique
 types such as supervised and 
unsupervised learning
 were utilized in software testing. However, for some complex software testing scenarios, neither supervised nor unsupervised machine learning techniques were adequate. As such, researchers applied 
Reinforcement Learning
 (RL) techniques in some cases. However, a 
systematic overview
 of the state-of-the-art on the role of reinforcement learning in software testing is lacking.
Objective:
The objective of this study is to determine how and to what extent RL was used in software testing.
Methods:
In this study, a Systematic Literature Review (SLR) was conducted on the use of RL in software testing, and 40 primary studies were investigated.
Results:
This study highlights different software testing types to which RL has been applied, commonly used RL algorithms and architecture for learning, challenges faced, advantages and disadvantages of using RL, and the performance comparison of RL-based models against other techniques.
Conclusions:
RL has been widely used in software testing but has almost narrowed to two applications. There is a shortage of papers using advanced RL techniques in addition to multi-agent RL. Several challenges were presented in this study.",21 Mar 2025,7,The use of Reinforcement Learning in software testing is an innovative approach that can benefit early-stage ventures in improving testing efficiency and accuracy.
https://www.sciencedirect.com/science/article/pii/S0950584923001702,Towards a successful secure software acquisition,December 2023,Information and Software Technology,Not Found,Faisal=Alnaseef: Not Found; Mahmood=Niazi: mkniazi@kfupm.edu.sa; Sajjad=Mahmood: Not Found; Mohammad=Alshayeb: Not Found; Irfan=Ahmad: Not Found,"Abstract
Context
Security is a critical attribute 
of software quality
. Organizations invest considerable sums of money in protecting their assets. Despite investing in secure infrastructure, organizations remain prone to security risks and cyberattacks that exploit security flaws. Many factors contribute to the challenges related to software security, e.g., the exponential increase in Internet-enabled applications, threats from hackers, and the susceptibility of inexperienced Internet users. Moreover, organizations tend to procure off-the-shelf software from third-party suppliers. However, gaining a complete understanding of ways to assess suppliers’ readiness to provide secure software before selecting a supplier is imperative.
Objective
We have developed a readiness model for secure software acquisition (RMSSA) to help software organizations select suppliers who can provide secure software.
Method
We employed state-of-the-art techniques based on systematic literature review to determine the best practices undertaken by organizations in terms of acquiring secure software, which depends on six core security knowledge areas: confidentiality, integrity, availability, authorization, 
authentication
, and accountability.
Results
We evaluated the RMSSA theoretically and in a practical environment based on three 
case studies
 with software organizations. Our findings can guide software organizations in selecting the supplier who can develop secure software.
Conclusion
The proposed RMSSA can be used to evaluate suppliers’ readiness to provide secure software.",21 Mar 2025,5,"The readiness model for secure software acquisition (RMSSA) offers valuable insights for organizations looking to enhance their software security measures, which are crucial for startups dealing with sensitive data."
https://www.sciencedirect.com/science/article/pii/S0950584923001635,Auto-COP: Adaptation generation in Context-oriented Programming using Reinforcement Learning options,December 2023,Information and Software Technology,"Context-oriented programming, Reinforcement learning, Macro actions, Option learning, Self-adaptive systems",Nicolás=Cardozo: n.cardozo@uniandes.edu.co; Ivana=Dusparic: ivana.dusparic@scss.tcd.ie,"Abstract
Context:
Self-adaptive software systems continuously adapt in response to internal and external changes in their execution environment, captured as contexts. The Context-oriented Programming (COP) paradigm posits a technique for the development of self-adaptive systems, capturing their main characteristics with specialized programming language constructs. In COP, adaptations are specified as independent modules that are composed in and out of the base system as contexts are activated and deactivated in response to sensed circumstances from the surrounding environment. However, the definition of adaptations, their contexts and associated specialized behavior, need to be specified at design time. In complex cyber–physical systems this is intractable, if not impossible, due to new unpredicted operating conditions arising.
Objective:
In this paper, we propose Auto-COP, a new technique to enable generation of adaptations at run time. Auto-COP uses 
Reinforcement Learning
 (RL) options to build action sequences, based on the previous instances of the system execution (for example, atomic system actions enacted by human operators). Options are further explored in interaction with the environment, and the most suitable options for each context are used to generate the adaptations, exploiting COP abstractions.
Method:
To validate Auto-COP, we present two 
case studies
 exhibiting different system characteristics and application domains: a driving assistant and a robot delivery system. We present examples of Auto-COP to illustrate the types of circumstances (contexts) requiring adaptation at run time, and the corresponding generated adaptations for each context.
Results:
We confirm that the generated adaptations exhibit correct 
system behavior
 measured by domain-specific performance metrics (
e.g.,
 conformance to specified speed limit), while reducing the number of required execution/actuation steps by a factor of two showing that the adaptations are regularly selected by the running system as adaptive behavior is more appropriate than the execution of 
atomic actions
.
Conclusion:
Therefore, we demonstrate that Auto-COP is able to increase system adaptivity by enabling run-time generation of new adaptations for conditions detected at run time, while retaining the modularity offered by COP languages, and reducing the upfront specification required by system developers.",21 Mar 2025,7,"Auto-COP introduces a new technique for run-time generation of adaptations, increasing system adaptivity and reducing upfront specifications, which can benefit early-stage ventures in complex systems."
https://www.sciencedirect.com/science/article/pii/S0950584923001878,Towards accurate recommendations of merge conflicts resolution strategies,December 2023,Information and Software Technology,Not Found,Paulo=Elias: pauloe@id.uff.br; Heleno de S.=Campos: helenocampos@id.uff.br; Eduardo=Ogasawara: eogasawara@ieee.org; Leonardo Gresta Paulino=Murta: leomurta@ic.uff.br,"Abstract
Context:
in 
software engineering
, developers working concurrently on a project frequently need to merge changes in the source code. The manual resolution of merge conflicts is a laborious and time-consuming task. Some studies have investigated the nature of merge conflicts and proposed methods to predict, mitigate, and resolve conflicts. However, the automatic resolution of conflicts is still an open problem.
Objective:
in this paper, we design and evaluate MESTRE (MErge STrategy REcommender), a conflict resolution strategy recommender that predicts the merge resolution strategy among version 1, version 2, concatenation of version 1 and 2, concatenation of version 2 and 1, combination of lines from version 1 and 2, and manually writing new code. For the first four strategies, MESTRE is able to not only recommend the strategy but also automatically resolve the conflict.
Methods:
we 
collected data
 from 20 open-source projects with more than 1000 merge conflicts each. Using this data, we trained and evaluated a separate classifier for each project to predict the conflict resolution strategy for a conflicting chunk.
Results:
MESTRE achieved an overall average accuracy of 80.8% among all projects. It represents a normalized improvement of 54.8% over the 
majority class
 baseline. Furthermore, since MESTRE can provide the exact conflict resolution for the most frequent conflict resolution strategies, it could automatically resolve 70.5% of the conflicts. We also found that 
attributes related
 to the conflicting chunk notably impact the 
classification accuracy
 more than those related to the merge and the file.
Conclusion:
This paper makes the following contributions: (1) MESTRE, a tool to predict merge conflict resolutions based on attributes of a Git repository; (2) an analysis of the relevance of attributes to the prediction of resolution strategies; and (3) an ablation study to find the contribution of each group of attributes to MESTRE’s performance.",21 Mar 2025,6,"MESTRE provides a conflict resolution strategy recommender with high accuracy and automatic resolution capabilities, which can streamline development processes for startups working on collaborative projects."
https://www.sciencedirect.com/science/article/pii/S0950584923001416,MDSSED: A safety and security enhanced model-driven development approach for smart home apps,November 2023,Information and Software Technology,Not Found,Tong=Ye: yetong@nuaa.edu.cn; Yi=Zhuang: zy16@nuaa.edu.cn; Gongzhe=Qiao: qgz@nuaa.edu.cn,"Abstract
Context:
With the popularization of 
smart home devices
, people rely more on automation functions provided by 
smart home
 apps. This increases the attack surface for safety and security threats. Many of these threats are at the interaction level, caused by unintended or malicious interactions between apps.
Objective:
Most of the current studies focus on identifying unsafe interactions between 
smart home
 apps by code analysis. To the best of our knowledge, none of the existing studies focuses on enhancing the safety and security of smart home apps under interaction threats in the design phase. To fill this gap, this paper presents MDSSED, a safety and security enhanced model-driven development approach for smart home apps.
Method:
First, this paper identifies eleven types of interaction threats faced by smart home apps. Second, the MDSSED profile is proposed to support modeling smart home apps using 
UML
. Third, the MDSSED prototype tool is developed to generate threat models and corresponding safety and security properties automatically. Then, the safety and security properties are automatically verified by model checking. Finally, the MDSSED tool automatically converts the 
UML models
 to the Samsung SmartThings apps.
Results:
To evaluate the accuracy and effectiveness of MDSSED, this paper uses the benchmarks in existing state-of-the-art studies. The results show that MDSSED not only identified the safety and security problems in the existing benchmarks but also pointed out vulnerabilities of apps under other interaction threats identified in this paper.
Conclusion:
To the best of our knowledge, MDSSED is the first model-driven development approach that supports the automatic verification of the safety and security properties of smart home apps under interaction threats. The accuracy, practicality, and efficiency of MDSSED are corroborated by experiments. The 
source code
 of the MDSSED tool and the experimental data are available online.
1",21 Mar 2025,8,"MDSSED presents a model-driven development approach for enhancing safety and security in smart home apps, addressing interaction threats at the design phase, which is crucial for startups focusing on smart home device development."
https://www.sciencedirect.com/science/article/pii/S0950584923001532,Code review guidelines for GUI-based testing artifacts,November 2023,Information and Software Technology,"GUI testing, GUI-based testing, Software testing, Code review, Modern code review, Guidelines, Practices",Andreas=Bauer: andreas.bauer@bth.se; Riccardo=Coppola: riccardo.coppola@polito.it; Emil=Alégroth: emil.alegroth@bth.se; Tony=Gorschek: tony.gorschek@bth.se,"Abstract
Context:
Review of software artifacts, such as source or test code, is a 
common practice
 in industrial practice. However, although review guidelines are available for source and low-level test code, for GUI-based testing artifacts, such guidelines are missing.
Objective:
The goal of this work is to define a set of guidelines from literature about production and test code, that can be mapped to GUI-based testing artifacts.
Method:
A systematic literature review is conducted, using white and gray literature to identify guidelines for source and test code. These synthesized guidelines are then mapped, through examples, to create actionable, and applicable, guidelines for GUI-based testing artifacts.
Results:
The results of the study are 33 guidelines, summarized in nine guideline categories, that are successfully mapped as applicable to GUI-based testing artifacts. Of the collected literature, only 10 sources contained test-specific code review guidelines. These guideline categories are: 
perform automated checks, use checklists, provide context information, utilize metrics, ensure readability, visualize changes, reduce complexity, check conformity with the requirements
 and 
follow design principles and patterns
.
Conclusion:
This pivotal set of guidelines provides an industrial contribution in filling the gap of general guidelines for review of GUI-based testing artifacts. Additionally, this work highlights, from an academic perspective, the need for future research in this area to also develop guidelines for other specific aspects of GUI-based testing practice, and to take into account other facets of the review process not covered by this work, such as reviewer selection.",21 Mar 2025,5,"The guidelines for review of GUI-based testing artifacts can provide valuable insights for software companies, but may have limited direct impact on early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923001192,Information needs and presentation in agile software development,October 2023,Information and Software Technology,"Software engineering, Agile software development, DevOps, Information needs, Visualization",Henri=Bomström: henri.bomstrom@oulu.fi; Markus=Kelanti: markus.kelanti@oulu.fi; Elina=Annanperä: elina.annanpera@oulu.fi; Kari=Liukkunen: kari.liukkunen@oulu.fi; Terhi=Kilamo: terhi.kilamo@tuni.fi; Outi=Sievi-Korte: outi.sievi-korte@tuni.fi; Kari=Systä: kari.systa@tuni.fi,"Abstract
Context:
Agile software companies applying the 
DevOps
 approach require collaboration and information sharing between practitioners in various roles to produce value. Adopting new development practices affects how practitioners collaborate, requiring companies to form a closer connection between business strategy and software development. However, the types of information management, sales, and development needed to plan, evaluate features, and reconcile their expectations with each other need to be clarified.
Objective:
To support practitioners in collaborating and realizing changes to their practices, we investigated what information is needed and how it should be represented to support different stakeholders in their tasks. Compared to earlier research, we adopted a holistic approach – by including practitioners throughout the 
development process
 – to better understand the information needs from a broader viewpoint.
Method:
We conducted six workshops and 12 semi-structured interviews at three Finnish small and medium-sized enterprises from different software domains. Thematic analysis was used to identify information-related issues and information and visualization needs for daily tasks. Three themes were constructed as the result of our analysis.
Results:
Visual information representation catalyzes stakeholder discussion, and supporting information exchange between 
stakeholder groups
 is vital for efficient collaboration in software product development. Additionally, user-centric data collection practices are needed to understand how software products are used and to support practitioners’ daily information needs. We also found that a passive way of representing information, such as a dashboard that would disturb practitioners only when attention is needed, was preferred for daily information needs.
Conclusion:
The 
software engineering
 community should consider reviewing the information needs of practitioners from a more holistic view to better understand how tooling support can benefit information exchange between stakeholder groups when making product development decisions and how those tools should be built to accommodate different stakeholder views.",21 Mar 2025,4,"The investigation on information needs for collaboration in software development, while important, may have a more indirect impact on startups compared to other more specific and actionable abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584923000988,A mixed method study of DevOps challenges,September 2023,Information and Software Technology,Not Found,Minaoar Hossain=Tanzil: minaoar.tanzil@ucalgary.ca; Masud=Sarker: Not Found; Gias=Uddin: Not Found; Anindya=Iqbal: Not Found,"Abstract
Context:
DevOps
 practices combine software development and IT (Information Technology) operations. The continuous needs for rapid but quality software development requires the adoption of high-quality 
DevOps
 tools. There is a growing number of DevOps related posts in popular online developer forum Stack Overflow (SO). While previous research analyzed SO posts related to build/release engineering, we are aware of no research that specifically focused on DevOps related discussions.
Objective:
This paper aims to learn the challenges developers face while using the currently available DevOps tools and techniques along with the organizational challenges in DevOps practices.
Method:
We conduct an empirical study by applying 
topic modeling
 on 174K SO posts that contain DevOps discussions. We then validate and extend the empirical study findings with a survey of 21 professional DevOps practitioners.
Results:
We find that: (1) There are 23 DevOps topics grouped into four categories: Cloud & CI/CD Tools, Infrastructure as Code, Container & Orchestration, and Quality Assurance. (2) The topic category ‘Cloud & CI/CD Tools’ contains the highest number of topics (10) which cover 48.6% of all questions in our dataset, followed by the category Infrastructure as Code (28.9%). (3) The file management is the most popular topic followed by Jenkins Pipeline, while infrastructural Exception Handling and Jenkins Distributed Architecture are the most difficult topics (with least accepted answers). (4) In the survey, developers mention that it requires hands-on experience before current DevOps tools can be considered easy. They raised the needs for better documentation and learning resources to learn the rapidly changing DevOps tools and techniques. Practitioners also emphasized on the formal training approach by the organizations for DevOps skill development.
Conclusion:
Architects and managers can use the findings of this research to adopt appropriate DevOps technologies, and organizations can design tool or process specific DevOps training programs.",21 Mar 2025,7,The research on DevOps challenges and practices can provide valuable insights for early-stage ventures in improving software development processes and tool adoption.
https://www.sciencedirect.com/science/article/pii/S095058492300071X,BERT- and TF-IDF-based feature extraction for long-lived bug prediction in FLOSS: A comparative study,August 2023,Information and Software Technology,Not Found,Luiz=Gomes: luizgomes@pucpcaldas.br; Ricardo=da Silva Torres: ricardo.torres@ntnu.no; Mario Lúcio=Côrtes: cortes@ic.unicamp.br,"Abstract
Context:
The correct prediction of long-lived bugs could help 
maintenance teams
 to build their plan and to fix more bugs that often adversely affect software quality and disturb the 
user experience
 across versions in Free/Libre Open-Source Software (FLOSS). 
Machine Learning
 and Text Mining methods have been applied to solve many real-world prediction problems, including 
bug report
 handling.
Objective:
Our research aims to compare the accuracy of ML classifiers on long-lived bug prediction in FLOSS using 
Bidirectional Encoder Representations from Transformers
 (BERT)- and Term Frequency - 
Inverse Document Frequency
 (TF-IDF)-based feature extraction. Besides that, we aim to investigate BERT variants on the same task.
Method:
We collected bug reports from six popular FLOSS and used the 
Machine Learning
 classifiers to predict long-lived bugs. Furthermore, we compare different feature extractors, based on BERT and TF-IDF methods, in long-lived bug prediction.
Results:
We found that long-lived bug prediction using BERT-based feature extraction systematically outperformed the TF-IDF. The 
SVM
 and 
Random Forest
 outperformed other classifiers in almost all datasets using BERT. Furthermore, smaller BERT architectures show themselves as competitive.
Conclusion:
Our results demonstrated a promising avenue to predict long-lived bugs based on BERT contextual embedding features and fine-tuning procedures.",21 Mar 2025,8,"The use of Machine Learning techniques and BERT for bug prediction in FLOSS can significantly impact the quality and maintenance of software, which can benefit early-stage ventures in improving their software products."
https://www.sciencedirect.com/science/article/pii/S0950584923000708,The lifecycle of Technical Debt that manifests in both source code and issue trackers,July 2023,Information and Software Technology,"Technical Debt, Source code, Issue tracker",Jie=Tan: j.tanjie@outlook.com; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
Although Technical Debt (TD) has increasingly gained attention in recent years, most studies exploring TD are based on a single source (e.g., source code, code comments or issue trackers).
Objective:
Investigating information combined from different sources may yield insight that is more than the sum of its parts. In particular, we argue that exploring how TD items are managed in both issue trackers and software repositories (including source code and commit messages) can shed some light on what happens between the commits that incur TD and those that pay it back.
Method:
To this end, we randomly selected 3,000 issues from the trackers of five projects, manually analyzed 300 issues that contained TD information, and identified and investigated the lifecycle of 312 TD items.
Results:
The results indicate that most of the TD items marked as resolved in issue trackers are also paid back in source code, although many are not discussed after being identified in the issue tracker. Test Debt items are the least likely to be paid back in source code. We also learned that although TD items may be resolved a few days after being identified, it often takes a long time to be identified (around one year). In general, time is reduced if the same developer is involved in consecutive moments (i.e., introduction, identification, repayment decision-making and remediation), but whether the developer who paid back the item is involved in discussing the TD item does not seem to affect how quickly it is resolved.
Conclusions:
Investigating how developers manage TD across both 
source code repositories
 and issue trackers can lead to a more comprehensive oversight of this activity and support efforts to shorten the lifecycle of undesirable debt.",21 Mar 2025,6,"The study on Technical Debt management can provide valuable insights for startups on how to effectively manage technical debt in their software development process, thereby improving product quality and reducing maintenance costs."
https://www.sciencedirect.com/science/article/pii/S0950584923000289,Double-counting in software engineering tertiary studies — An overlooked threat to validity,June 2023,Information and Software Technology,"Bias, Double-counting, Empirical, Guidelines, Meta-review, Overview of reviews, Recommendations, Research methods, Review of reviews, Tertiary review, Tertiary study, Umbrella review",Jürgen=Börstler: jurgen.borstler@bth.se; Nauman=bin Ali: nauman.ali@bth.se; Kai=Petersen: kai.petersen@bth.se,"Abstract
Context:
Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in 
software engineering
.
Objective:
We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue.
Method:
We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them.
Results:
In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies.
Conclusions:
Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies.",21 Mar 2025,5,"The investigation on double-counting in tertiary studies in software engineering may have limited direct practical impact on early-stage ventures, as the issue is more academic in nature."
https://www.sciencedirect.com/science/article/pii/S0950584923000228,SedSVD: Statement-level software vulnerability detection based on Relational Graph Convolutional Network with subgraph embedding,June 2023,Information and Software Technology,Not Found,Yukun=Dong: dongyk@upc.edu.cn; Yeer=Tang: Not Found; Xiaotong=Cheng: Not Found; Yufei=Yang: Not Found; Shuqi=Wang: Not Found,"Abstract
Context:
Current deep-learning based 
vulnerability detection
 methods have been proven more automatic and correct to a certain extent, nonetheless, they are limited to detect at function-level or file-level, which can hinder software developers from acquiring more detailed information and conducting more targeted repairs. Graph-based detection methods have shown dominant performance over others. Unfortunately, the information they reveal has not been fully utilized.
Objective:
We design SedSVD (Subgraph embedding driven Statement-level Vulnerability Detection) with two objectives: (i) to better utilize the information the code-related graphs can reflect; (ii) to detect vulnerabilities at a finer-grained level.
Method:
In our work, we propose a novel graph-based detection framework that embeds graphs at subgraph-level to realize statement-level detection. It first leverages Code Property Graph (CPG) to learn both semantic and 
syntactic information
 from source code, and then selects several center nodes (code elements) in CPG to build their subgraphs. After embedding each subgraph with its nodes and edges, we apply Relational 
Graph Convolutional Network
 (RGCN) to process different edges differently. A Multi-Layer 
Perceptron
 (MLP) layer is further added to ensure its prediction performance.
Results:
We conduct our experiments on C/C++ projects from NVD and SARD. Experimental results show that SedSVD achieves 95.15% in F1-measure which proves our work to be more effective.
Conclusion:
Our work detects at a finer-grained level and achieves higher F1-measure than existing state-of-art 
vulnerability detection
 techniques. Besides, we provide a more detailed detection report pointing the specific error code elements within statements.",21 Mar 2025,9,The development of SedSVD for statement-level vulnerability detection with high F1-measure can greatly benefit startups by providing more accurate and detailed vulnerability detection in their software products.
https://www.sciencedirect.com/science/article/pii/S0950584923000265,Scripted and scriptless GUI testing for web applications: An industrial case,June 2023,Information and Software Technology,"Case study, Complementarity, Scriptless testing, Scripted testing",Axel=Bons: Not Found; Beatriz=Marín: Not Found; Pekka=Aho: Not Found; Tanja E.J.=Vos: Not Found,"Abstract
Context:
Automation is required in the software development to reduce the high costs of producing software and to address the short release cycles of modern 
development processes
. Lot of effort has been performed to automate testing, which is one of the most resource-consuming development phases. Automation of testing through the Graphical User Interface (GUI) has been researched to improve the system testing.
Objective:
We aim to evaluate the complementarity of automated GUI testing tools in a real industrial context, which refers to the capability of the tools to work usefully together.
Methods:
To address the objective, we conduct an exploratory 
case study
 in an IT development company from The Netherlands. We select two representative tools for automated GUI testing, one for scripted and another for scriptless testing. We measure the complementarity by measuring the effectiveness, the efficiency, and 
subjective satisfaction
 of the tools.
Results:
It can be observed that the scripted tool performs better in detecting process failures, and the scriptless tool performs better in detecting visible failures and also reaching higher coverage. Both tools perform in a similar way in terms of efficiency. Additionally, both tools were perceived to be useful in the survey performed for the subjective satisfaction.
Conclusion:
We conclude that scriptless and scripted testing approaches are complementary, and they can improve the effectiveness compared to manual testing processes performed in an industrial context by detecting different failures and reducing the effort and time to find these failures and to reproduce them.",21 Mar 2025,6,The evaluation of automated GUI testing tools in an industrial context can provide valuable insights for startups looking to streamline their testing processes.
https://www.sciencedirect.com/science/article/pii/S0950584923000435,Application of Project-Based Learning to a Software Engineering course in a hybrid class environment,June 2023,Information and Software Technology,Not Found,Edgar=Ceh-Varela: eduardo.ceh@enmu.edu; Carlos=Canto-Bonilla: carlos.canto@utmetropolitana.edu.mx; Dhimitraq=Duni: dhimitraq.duni@enmu.edu,"Abstract
Context:
This paper centers on Project-Based Learning (PBL). In PBL, the student is now the center of the whole 
teaching and learning
 process, while the instructor‘s role is now of a facilitator presenting to the students the resources and guidance to solve the given problem. Most existing studies, apply PBL to courses having in-person students.
Objective:
The paper presents the application of a 
PBL approach
 to a Software Engineering (SE) course having a hybrid class environment (i.e., online and in-person students). The main objective of this paper is to analyze the students’ attitudes after experiencing working on a real-life problem as part of our 
PBL approach
 in a hybrid class environment.
Methods:
We propose a 
relaxed plan-based
 software development model as basis for guiding the project execution. At the end of the course, we applied a survey to the students to evaluate their experience in the course.
Results:
We obtained the answers of 70.8% of students taking a 
SE
 course. With these answers, we could measure the students’ perception of using PBL in a 
SE
 course and how this strategy helped them to gain soft and hard skills in software development. We divided the answers for their analysis into different categories: soft skills, technical skills, 
learning experience
, and other results. Moreover, we compare the performance of the teams and students based on their type (i.e., online and in-person).
Conclusion:
We found qualitative differences in the experience of online and in-person students. Based on our experience with this study, we provide guidelines for applying PBL in a hybrid environment. Overall, our study has demonstrated a positive contribution in supporting teaching SE using a PBL in a hybrid class environment.",21 Mar 2025,7,The application of PBL in a hybrid class environment for teaching SE can offer useful guidelines for startups focusing on innovative educational approaches.
https://www.sciencedirect.com/science/article/pii/S0950584922002415,An expressive and modular layer activation mechanism for Context-Oriented Programming,April 2023,Information and Software Technology,Not Found,Paul=Leger: pleger@ucn.cl; Nicolás=Cardozo: Not Found; Hidehiko=Masuhara: Not Found,"Abstract
Context.
There is a trend in the software industry towards 
building systems
 that dynamically adapt their behavior in response to their surrounding environment, given the proliferation of various technological devices, such as notebooks, smartphones, and wearables, capable of capturing their execution context. Context-oriented Programming (COP) allows developers to use layer abstractions to adapt software behavior to the context. A layer is associated with a context and can be dynamically activated in direct response to gathered information from its surrounding execution environment. However, most existing layer activation mechanisms have been tailored specifically to address a particular concern; implying that developers need to tweak layer definitions in contortive ways or create new specialized activation mechanisms altogether if their specific needs are not supported.
Objective.
Complementing ideas to expressively declare activation mechanism models with interfaces that define conditionals of activation mechanisms modularly, this paper proposes an Expressive and Modular Activation mechanism, named EMA.
Method.
To propose EMA, we analyze existing activation mechanisms in COP regarding activation features and scope strategies. After, we propose the design of EMA and validate it with a 
case study
 discussion.
Results.
Using a concrete JavaScript implementation of EMA, named EMAjs, we can implement two Web applications: a 
smartphone application
 as an example to illustrate EMAjs in action, and an application of home automation to discuss and compare our proposal.
Conclusions.
Our proposed mechanism allows developers to instantiate different activation scope strategies and interfaces to decouple the declaration of activation mechanism conditionals from the base code.",21 Mar 2025,5,"The proposed EMA mechanism can be insightful for startups developing adaptive software systems, but may have limited immediate practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584923000010,How SonarQube-identified technical debt is prioritized: An exploratory case study,April 2023,Information and Software Technology,Not Found,Reem=Alfayez: reealfayez@ksu.edu.sa; Robert=Winn: Not Found; Wesam=Alwehaibi: Not Found; Elaine=Venson: Not Found; Barry=Boehm: Not Found,"Abstract
Context:
Repaying all technical debt (TD) in a system may be unviable, as there is typically a shortage of resources allocated for 
TD repayment
 activities. Therefore, TD prioritization is essential to best allocate such limited resources. Fortunately, one can utilize a 
static code analysis tool
, such as SonarQube, to aid in expediting the TD prioritization process.
Objective:
Given that SonarQube is one of the most utilized tools in the context of TD, this exploratory 
case study
 seeks to explore how SonarQube-identified TD items are perceived and prioritized for repayment.
Methods:
The study was designed, replicated, and conducted in four companies and a master’s level course, with a total of 89 participants. The participants were requested to select TD items to include for repayment under a 
resources constraint
.
Results:
The results revealed that the overwhelming majority of participants prioritized TD by factoring in a TD item’s value and cost, a smaller number prioritized higher value TD items, and only one participant prioritized lower cost TD items. Furthermore, it was revealed that the value of a TD item is subjective and context-dependent, and the majority of participants perceive the 
cost estimations
 provided by SonarQube for repaying TD items to be reliable and trustworthy when prioritizing TD.
Conclusion:
Based on the results, one can conclude that there is no silver bullet TD prioritization approach that addresses all of a developer’s objectives and needs. New TD prioritization approaches should be designed without concentrating on a specific prioritization perspective and should be independent of value estimation methods.",21 Mar 2025,8,The exploration of TD prioritization using SonarQube can directly benefit startups dealing with resource constraints and technical debt management.
https://www.sciencedirect.com/science/article/pii/S095058492200252X,Probabilistic program performance analysis with confidence intervals,April 2023,Information and Software Technology,"Program quality analysis, Software performance, Discrete-time Markov chains, Probabilistic model checking, Formal verification with confidence intervals",Ioannis=Stefanakos: ioannis.stefanakos@york.ac.uk; Radu=Calinescu: Not Found; Simos=Gerasimou: Not Found,"Abstract
Context:
More often than not, the algorithms implemented by software systems continue to operate correctly when executed on different platforms or with different inputs, and can be easily replaced with functionally equivalent ones. However, such changes can have a significant and difficult to predict impact on the software performance, resource use, and other key quality properties.
Objective:
The paper introduces a method for the formal analysis of timing, resource use, cost and other 
quality aspects
 of computer programs, and a tool that automates the application of the method to Java code.
Method:
A tool-supported 
p
robabilistic p
ro
gram 
per
formance analysis (PROPER) method was developed, and was evaluated using Java code from the Apache Commons Math library, the 
Android
 messaging app Telegram, and open-source implementations of the 
knapsack
, binary search, and minimum path sum algorithms. PROPER synthesises a parametric Markov-chain model of the analysed code, uses information from program logs to calculate confidence intervals for the parameters of this model, and employs 
formal verification
 with confidence intervals to obtain confidence intervals for the performance properties of interest. A PROPER variant that operates with point estimates instead of confidence intervals can be used when large program logs are available.
Results:
The PROPER point estimates for the analysed performance properties were accurate within 7.9% and 1.75% of the ground truth when using program logs with 
 and 
 entries, respectively. All PROPER confidence intervals for these properties contained the true property value, and became narrower when larger logs were used in the analysis. The analyses were completed in under 15 ms for point estimates, and in between 6.7 s and 7.8 s for confidence intervals on a regular laptop computer.
Conclusion:
PROPER can synthesise and reuse a parametric Markov model to accurately predict how software performance would change if the code ran on a different hardware platform, used a new function library, or had a different usage profile—supporting practitioners who are interested in these analyses.",21 Mar 2025,4,The formal analysis method for computer program quality aspects may have limited applicability and immediate impact for startups in early stages.
https://www.sciencedirect.com/science/article/pii/S0950584922002208,Detecting code smells in React-based Web apps,March 2023,Information and Software Technology,Not Found,Fabio=Ferreira: fabio.ferreira@ifsudestemg.edu.br; Marco Tulio=Valente: Not Found,"Abstract
Context:
Facebook’s 
React
 is a widely popular 
JavaScript library
 to build rich and 
interactive user interfaces
 (UI). However, due to the complexity of modern Web UIs, 
React
 applications can have hundreds of components and 
source code
 files. Therefore, front-end developers are facing increasing challenges when designing and modularizing 
React
-based applications. As a result, it is natural to expect 
maintainability
 problems in 
React
-based UIs due to suboptimal design decisions.
Objective:
To help developers with these problems, we propose a catalog with twelve 
React
-related code smells and a prototype tool to detect the proposed smells in 
React
-based Web apps.
Method:
The smells were identified by conducting a grey literature review and by interviewing six professional software developers. We also use the tool in the top-10 most popular GitHub projects that use 
React
 and conducted a historical analysis to check how often developers remove the proposed smells.
Results:
We detect 2,565 instances of the proposed code smells. The results show that the removal rates range from 0.9% to 50.5%. The smell with the most significant removal rate is 
Large File
 (50.5%). The smells with the lowest removal rates are 
Inheritance Instead of Composition (IIC)
 (0.9%), and 
Direct DOM Manipulation
 (14.7%).
Conclusion:
The list of 
React
 smells proposed in this paper as well as the tool to detect them can assist developers to improve the 
source code
 quality of 
React
 applications. While the catalog describes common problems with 
React
 applications, our tool helps to detect them. Our historical analysis also shows the importance of each smell from the developers’ perspective, showing how often each smell is removed.",21 Mar 2025,7,"The study provides valuable insights into common Code Smells in React applications and offers a tool to help developers improve source code quality, which can be beneficial for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922002178,Enterprise architecture artifacts as boundary objects: An empirical analysis,March 2023,Information and Software Technology,Not Found,Svyatoslav=Kotusev: kotusev@kotusev.com; Sherah=Kurnia: Not Found; Rod=Dilnutt: Not Found,"Abstract
Context
Enterprise architecture (EA) is a collection of artifacts describing various aspects of an organization from an integrated business and IT perspective. EA artifacts intend to bridge the communication gap between business and IT stakeholders to improve business and IT alignment in organizations and, therefore, can be considered as 
boundary objects
 between diverse business and IT communities. However, an intentional analysis of EA artifacts as boundary objects in the current EA literature has been rather shallow and insufficient.
Objective
This study aims to explore how exactly EA artifacts as boundary objects facilitate communication between different professional communities. Specifically, it intends to identify what types of EA artifacts represent boundary objects, analyze their properties and 
usage scenarios
, as well as the differences between them.
Method
This study is based on an in-depth case study of an organization with an established EA practice. 
Data collection procedures
 include both interviews with various participants of its EA practice and comprehensive scrutiny of its EA documentation.
Results
We identify five specific types of EA artifacts used in the organization as boundary objects and analyze them in detail. In particular, we analyze their 
informational contents
 and usage scenarios, their target audiences and value for cross-community collaboration, as well as their 
syntactic
, semantic and pragmatic boundary-spanning capacity. Moreover, we also introduce the notion of duality as a characteristic of interpretive flexibility of EA artifacts and distinguish two different types of duality leveraging somewhat different boundary-spanning mechanisms: implicit duality and explicit duality.
Conclusions
This paper provides arguably the first inductive qualitative analysis of EA artifacts as boundary objects available in the existing EA literature. It contributes to our understanding of their boundary-spanning properties, distinctive features and general roles in an EA practice. Also, the concepts of implicit and explicit duality that we introduce further advance the theory of boundary objects.",21 Mar 2025,9,"The research on Enterprise Architecture artifacts as boundary objects contributes to improving communication between business and IT stakeholders, which can have a significant impact on the alignment and success of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922001653,"Concerns identified in code review: A fine-grained, faceted classification",January 2023,Information and Software Technology,Not Found,Sanuri=Gunawardena: sanuri.gunawardena@auckland.ac.nz; Ewan=Tempero: e.tempero@auckland.ac.nz; Kelly=Blincoe: k.blincoe@auckland.ac.nz,"Abstract
Context:
Code review is a valuable software process that helps software practitioners to identify a variety of defects in code. Even though many code review tools and 
static analysis
 tools used to improve the efficiency of the process exist, code review is still costly.
Objective:
Understanding the types of defects that code reviews help to identify could reveal other means of cost improvement. Thus, our goal was to identify defect types detected in real-world code reviews, and the extent to which code review can be benefited from defect detection tools.
Method:
To this end, we classified 417 comments from code reviews of 7 OSS Java projects using thematic analysis.
Results:
We identified 116 defect types that we grouped into 15 groups to create a defect classification. Additionally, 38% of these defects could be automatically detected accurately.
Conclusion:
We learnt that even though many capable defect detection tools are available today, a substantial amount of defects that can be detected automatically, reach code review. Also, we identified several code review cost reduction opportunities.",21 Mar 2025,6,"Identifying defect types in real-world code reviews and the potential for automated detection can help reduce costs and improve software quality, which is relevant for startups looking to optimize their development processes."
https://www.sciencedirect.com/science/article/pii/S0950584922001896,Detecting false-passing products and mitigating their impact on variability fault localization in software product lines,January 2023,Information and Software Technology,Not Found,Thu-Trang=Nguyen: trang.nguyen@vnu.edu.vn; Kien-Tuan=Ngo: tuanngokien@vnu.edu.vn; Son=Nguyen: sonnguyen@vnu.edu.vn; Hieu Dinh=Vo: hieuvd@vnu.edu.vn,"Abstract
In a Software Product Line (SPL) system, variability bugs can cause failures in certain products (buggy products), not in the others. In practice, variability bugs are not always exposed, and buggy products can still pass all the tests due to their ineffective test suites (so-called 
false-passing
 products). The misleading indications caused by those 
false-passing
 products’ test results can negatively impact variability fault 
localization performance
. In this paper, we introduce 
Clap
, a novel approach to detect 
false-passing
 products in SPL systems failed by variability bugs. Our key idea is that given a set of tested products of an SPL system, we collect failure indications in failing products based on their implementation and test quality. For a passing product, we evaluate these indications, and the stronger indications, the more likely the product is 
false-passing
. Specifically, the possibility of the product to be false-passing is evaluated based on if it has a large number of the statements which are highly suspicious in the failing products, and if its test suite is in lower quality compared to the failing products’ test suites. We conducted several experiments to evaluate our 
false-passing
 product detection approach on a large benchmark of 14,191 
false-passing
 products and 22,555 
true-passing
 products in 823 buggy versions of the existing SPL systems. The experimental results show that 
Clap
 can effectively detect 
false-passing
 and 
true-passing
 products with the average accuracy of more than 90%. Especially, the precision of 
false-passing
 product detection by 
Clap
 is up to 96%. This means, among 10 products predicted as 
false-passing
 products, more than 9 products are precisely detected. Furthermore, we propose two simple and effective methods to mitigate the 
negative impact
 of 
false-passing
 products on variability 
fault localization
. These methods can improve the performance of the state-of-the-art variability 
fault localization
 techniques by up to 34%.",21 Mar 2025,8,"The detection of false-passing products in SPL systems can greatly enhance the fault localization performance, which is crucial for maintaining product quality and reliability in early-stage startups."
https://www.sciencedirect.com/science/article/pii/S0950584922001501,Variability testing of software product line: A preference-based dimensionality reduction approach,December 2022,Information and Software Technology,Not Found,Thiago=Ferreira: thiagod@umich.edu; Silvia Regina=Vergilio: silvia@inf.ufpr.br; Marouane=Kessentini: marouane@umich.edu,"Abstract
Context:
Multi- and many-evolutionary algorithms have been applied to derive products for the variability testing of Software Product Lines (SPLs). This problem refers to the selection of an adequate product set to test a SPL by optimizing some objectives related to the number of products to be tested, testing criteria to be satisfied, and revealed faults. However, some problems emerge when the number of objectives to be optimized increases, for example: the solutions generated by the 
optimization algorithms
 become incomparable, designing a Pareto-front in this context requires a large number of solutions, and the visualization of such solutions requires special techniques. Several techniques are proposed to tackle this problem, such as decomposition and algorithms based on indicators. Among them, algorithms based on dimensionality reduction and user preferences are widely used, but there are no studies in the literature investigating the usage of both in a combined way.
Objective:
In light of this, we introduce COR-NSGA-II (Confidence-based Objective Reduction NSGA-II). COR-NSGA-II defines for each objective a confidence-level calculated with the user preferences provided interactively. The objectives with higher values of confidence are removed from the next algorithm execution.
Method:
For assessing the feasibility of COR-NSGA-II, experiments were conducted by using six different SPLs, seven objectives, two types of 
reference points
 representing the user preferences, and two scenarios to simulate different user profiles.
Results:
COR-NSGA-II is evaluated against four algorithms explored in the literature for the problem, and outperforms most of them according to R-HV and R-IGD. It takes less time to execute and generates a reduced number of solutions, all of them satisfying the user preferences.
Conclusion:
A qualitative analysis performed with 12 potential users shows that the task of selecting a solution generated by COR-NSGA-II is easier than selecting a solution generated by the other algorithms.",21 Mar 2025,8,"Introducing COR-NSGA-II for optimizing objectives in SPL testing demonstrates advancements in algorithm performance and user preference interaction, which can benefit startups seeking efficient testing and quality assurance methods."
https://www.sciencedirect.com/science/article/pii/S0950584922001513,Help me with this: A categorization of open source software problems,December 2022,Information and Software Technology,Not Found,Georgia M.=Kapitsaki: Not Found; Nikolaos D.=Tselikas: ntsel@uop.gr; Kyriakos-Ioannis D.=Kyriakou: Not Found; Maria=Papoutsoglou: Not Found,"Abstract
Context:
Free and 
Open Source Software
 is widely used in the research community and the software industry. In this context, developers come across various issues they need to handle in order to use and create software responsibly and without causing legal violations. For instance, using open source software that carries a specific license or how contributions to open source software should be handled are among the issues that need to be considered.
Objective:
As practitioners turn primarily to Q&A sites to seek help, it is important to understand which specific open source software issues they face. In this research, our main objective is to provide a categorization of open source software problems present in the user questions of the Open Source Stack Exchange site and perform a meta-analysis on the encountered questions.
Method:
We have performed a qualitative study analyzing manually 1,500 most popular posts in the Open Source Stack Exchange site and have mapped them to categories and more generic clusters. The coding task was performed in iterations with the participation of three of the authors. Agreement was calculated and cases of disagreement were resolved. Meta-analysis on questions and answers was also performed for discussion purposes.
Results:
We have created 26 categories of problems discussed in the Open Source Stack Exchange site, and grouped them into 6 clusters. Our results show that posts on license texts/conditions and license/copyright notices are more common, whereas posts on license differences are the most popular in terms of views by other users.
Conclusion:
The results can assist any participant of the open source software community to understand on which basic issues she should focus on to gain a good understanding of open source software. They are also useful for improving education on open source software and community support using the implications presented for each category.",21 Mar 2025,8,The categorization of open source software problems can provide valuable insights for practitioners in the open source community and contribute to the improvement of education and community support.
https://www.sciencedirect.com/science/article/pii/S0950584922001525,"Different, Really! A comparison of Highly-Configurable Systems and Single Systems",December 2022,Information and Software Technology,Not Found,Raphael Pereira=de Oliveira: raphael.oliveira@academico.ufs.br; Paulo Anselmo da Mota Silveira=Neto: Not Found; Qi Hong=Chen: Not Found; Eduardo Santana=de Almeida: Not Found; Iftekhar=Ahmed: Not Found,"Abstract
Context:
The development of systems that handle configuration options according to a specific environment is considered a hard activity. These kind of systems, Highly-Configurable Systems (HCS) are perceived by researchers and developers as complex and difficult to maintain due to the necessity of handling variation points. Although this perception is reported in the literature, no prior study investigated the differences between HCS and Single Systems (SS).
Objective:
This study investigated similarities and differences between HCS and SS using well known metrics from the literature according to three different perspectives: 
product perspective
 (bug-proneness, complexity, and change size); 
process perspective
 (number of contributors, number of core developers, and accidental contributors); and 
people perspective
 (contributor retention and number of paid contributors).
Method:
To perform this comparison, we 
collected data
 from two surveys and from a mining study (within 15,769 releases of 124 GitHub projects written in C).
Results:
In general, we identified that for the majority of the metrics, the perception of practitioners and researchers about HCS and SS is different from our mining results.
Conclusion:
The identification of similarities and differences of HCS and SS will help to initiate a discussion and further research in this direction.",21 Mar 2025,5,"While the study provides insights into Highly-Configurable Systems and Single Systems, the practical impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922001288,Yet another combination of IR- and neural-based comment generation,December 2022,Information and Software Technology,Not Found,Yuchao=Huang: yuchao2019@iscas.ac.cn; Moshi=Wei: moshiwei@yorku.ca; Song=Wang: wangsong@eecs.yorku.ca; Junjie=Wang: junjie@iscas.ac.cn; Qing=Wang: wq@iscas.ac.cn,"Abstract
Background:
Code comment generation techniques aim to generate natural language descriptions for 
source code
. There are two orthogonal approaches for this task, i.e., information retrieval (IR) based and neural-based methods. Recent studies have focused on combining their 
strengths
 by feeding the input code and its similar code snippets retrieved by the IR-based approach to the neural-based approach, which can enhance the neural-based approach’s ability to output low-frequency words and further improve the performance.
Aim:
However, despite the tremendous progress, our pilot study reveals that the current combination is not generalizable and can lead to 
performance degradation
. In this paper, we propose a straightforward but effective approach to tackle the issue of existing combinations of these two comment generation approaches.
Method:
Instead of binding IR- and neural-based approaches statically, we combine them in a dynamic manner. Specifically, given an input code snippet, we first use an IR-based technique to retrieve a similar code snippet from the corpus. Then we use a Cross-Encoder based classifier to decide the comment generation method to be used dynamically, i.e., if the retrieved similar code snippet is a 
true positive
 (i.e., is semantically similar to the input), we directly use the IR-based technique. Otherwise, we pass the input to the neural-based model to generate the comment.
Results:
We evaluate our approach on a large-scale dataset of Java projects. Experiment results show that our approach can achieve 25.45 BLEU score, which improves the state-of-the-art IR-based approach, neural-based approach, and their combination by 41%, 26%, and 7%, respectively.
Conclusions:
We propose a straightforward but effective dynamic combination of IR-based and neural-based comment generation, which outperforms state-of-the-art approaches by a substantial margin.",21 Mar 2025,7,"The proposed dynamic combination of IR-based and neural-based comment generation approaches shows promising results in improving performance, which could benefit startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584922001537,"An empirical study on ML DevOps adoption trends, efforts, and benefits analysis",December 2022,Information and Software Technology,Not Found,Dhia Elhaq=Rzig: dhiarzig@umich.edu; Foyzul=Hassan: foyzul@umich.edu; Marouane=Kessentini: kessentini@oakland.edu,"Abstract
Context:
Machine Learning
 (ML), including Deep Learning(DL), based systems, have become ubiquitous in today’s solutions to many real-world problems. ML-based approaches are being applied to solve complex problems such as 
autonomous driving
, recommendation systems, etc.
Objective:
To improve the quality and 
deliverability
 of ML-based applications, the software development community is adopting state-of-the-art 
DevOps
 practices within them. However, we currently lack knowledge about the 
DevOps
 adoption trends, maintenance efforts and benefits among ML-based projects, and this work attempts to remedy this knowledge-gap.
Methods:
In this 
research work
, we conducted a large-scale empirical analysis on 4031 ML projects, including 1116 ML Tools and 2915 
ML Applied
 projects to quantify DevOps adoption, maintenance effort and benefits. To characterize the development behaviors, we performed configuration-script-analysis and commit-change-analysis on DevOps 
configuration files
. To compare the characteristics of ML DevOps to those of traditional software projects, we performed the same analysis on 4076 non-ML projects.
Results:
Our analysis identified that ML projects, more specifically ML-Applied projects, have a slower, lower, and less efficient adoption of DevOps tools in general. DevOps 
configuration files
 in ML-Applied projects tended to experience more frequent changes than ML-Tool projects and were less likely to occur in conjunction with build and bug fixes. It’s also evident that adopting DevOps in ML projects correlates with an increase in development productivity, code quality, and a decrease in bug resolution time, especially in ML-Applied projects which have the most to gain by adopting these tools.
Conclusion:
We identified the characteristics and improvement scopes of ML DevOps, such as the slower adoption of DevOps in certain ML projects, and the need for automatic configuration 
synchronization
 tools for these projects. We also identified the improvements the productivity of ML teams and projects associated with DevOps adoption, including better code quality, more frequent code sharing and integration and faster issue resolution.",21 Mar 2025,9,"The analysis on DevOps adoption, maintenance effort, and benefits in ML projects provides valuable information for early-stage ventures looking to integrate ML-based solutions with DevOps practices."
https://www.sciencedirect.com/science/article/pii/S0950584922001562,Crex: Predicting patch correctness in automated repair of C programs through transfer learning of execution semantics,December 2022,Information and Software Technology,Not Found,Dapeng=Yan: dapeng.yan@nuaa.edu.cn; Kui=Liu: brucekuiliu@gmail.com; Yuqing=Niu: 977012358@qq.com; Li=Li: li.li@monash.edu; Zhe=Liu: zhe.liu@nuaa.edu.cn; Zhiming=Liu: zliu@nwpu.edu.cn; Jacques=Klein: jacques.klein@uni.lu; Tegawendé F.=Bissyandé: tegawende.bissyande@uni.lu,"Abstract
A significant body of automated program repair literature relies on test suites to assess the validity of generated patches. Because such oracles are weak, state-of-the-art repair tools can validate some patches that overfit the test cases but are actually incorrect. This situation has become a prime concern in APR, hindering its adoption by the industry. This work investigates execution 
semantic features
 based on micro-traces, a form of under-constrained dynamic traces. We build on 
transfer learning
 to explore function code representations that are amenable to semantic similarity computation and can therefore be leveraged for classifying patch correctness. Our 
Crex
 prototype implementation is based on the 
Trex
 framework. Experimental results on patches generated by the CoCoNut APR tool on CodeFlaws programs indicate that our approach can yield high accuracy in predicting patch correctness. The learned embeddings were proven to capture semantic similarities between functions, which was instrumental in training a classifier that identifies patch correctness by learning to discriminate between correctly patched code and incorrectly patched code based on their semantic similarity with the buggy function.",21 Mar 2025,6,"The investigation into predicting patch correctness using execution semantic features and transfer learning is innovative, but the direct impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922001665,Introduction to the special issue on managing software processes using soft computing techniques,December 2022,Information and Software Technology,Not Found,Arif Ali=Khan: arif.khan@oulu.fi; Mahmood=Niazi: Not Found,"Abstract
The coronavirus outbreak dramatically changed the work culture in the software industry. Most software practitioners began working remotely, which significantly revolutionized the traditional software processes landscape. Software development organizations have begun thinking about automating software processes to cope with the challenges raised by remote work. This special issue presents papers describing soft computing solutions for improving traditional software processes and capabilities. This editorial introduces the accepted papers and reflects on their contributions.",21 Mar 2025,5,"While the topic of automating software processes is relevant, the impact on early-stage ventures is not clearly outlined in the abstract. Therefore, a neutral score is appropriate."
https://www.sciencedirect.com/science/article/pii/S0950584922001549,Undulate: A framework for data-driven software engineering enabling soft computing,December 2022,Information and Software Technology,"Soft computing, Multilevel modelling, Dimensional database, Continuous experimentation, Data-driven software engineering",Timo=Asikainen: timo.o.asikainen@helsinki.fi; Tomi=Männistö: tomi.mannisto@helsinki.fi,"Abstract
Context.
Especially web-facing software systems enable the collection of usage data at a massive scale. At the same time, the scale and scope of software processes have grown substantively. Automated tools are needed to increase the speed and quality of controlling software processes. The usage data has great potential as a driver for software processes. However, research still lacks constructs for collecting, refining and utilising usage data in controlling software processes.
Objective.
The objective of this paper is to introduce a framework for data-driven 
software engineering
. The 
Undulate
 framework covers generating, collecting and utilising usage data from software processes and business processes supported by the software produced. In addition, we define the concepts and process of extreme continuous experimentation as an exemplar of a software engineering process.
Method.
We derive requirements for the framework from the research literature, with a focus on papers inspired by practical problems. In addition, we apply a multilevel 
modelling language
 to describe the concepts related to extreme continuous experimentation.
Results.
We introduce the 
Undulate
 framework and give requirements and provide an overview of the processes of collecting usage data, augmenting it with additional 
dimensional data
, aggregating the data along the dimensions and computing different metrics based on the data and other metrics.
Conclusions.
The paper represents significant steps inspired by previous research and practical insight towards standardised processes for data-driven software engineering, enabling the application of soft computing and other methods based on 
artificial intelligence
.",21 Mar 2025,7,"The framework for data-driven software engineering presented in this abstract could have a positive impact on European early-stage ventures, especially startups, by providing guidance on utilizing usage data. The practical application of this framework could be beneficial."
https://www.sciencedirect.com/science/article/pii/S0950584922001690,Agile software development and UX design: A case study of integration by mutual adjustment,December 2022,Information and Software Technology,Not Found,John Stouby=Persson: Not Found; Anders=Bruun: bruun@cs.aau.dk; Marta Kristín=Lárusdóttir: Not Found; Peter Axel=Nielsen: Not Found,"Abstract
Context
Agility is an overarching ideal for empirically-driven software development processes that embrace change in order to improve quality, economy, and simplicity. While the pursuit of Agility has held prominence in software practice and research for over two decades, 
user experience
 (UX) designers struggle to integrate their work processes with 
agile software development
.
Objective
As empirical processes are constantly evolving, so is this integration struggle for UX designers. We, therefore, present an industrial 
case study
 of how a Danish software company integrates UX design and agile software development.
Method
We conducted a case study involving (a) one iteration of individual interviews with 10 employees (four UX designers, three software developers, two project managers, and one solution architect) and (b) a follow-up iteration consisting of a workshop with 6 employees (three UX designers, two solution architects, and one project manager) two years later. We analyzed how the company's approach to integration with 'upfront design' and 'work in parallel' involve mutual adjustments as opposed to assimilation or separation of UX design and software development.
Results
Our analysis shows how integration through mutual adjustments made distinct contributions to UX designers' and software developers' pursuit of Agility. They experienced notably different work processes that still dealt effectively with change and contributed to quality, economy, or simplicity. Nevertheless, as shown from a follow-up workshop two years after our first interviews, these processes were still susceptible to integration struggles over time.
Conclusion
We conclude that integration based on mutual adjustment potentially makes Agility for UX designers and software developers different and mutually complementary. This integration contrasts with assimilation, which potentially makes their Agility mutually indistinguishably, and with separation, which makes their Agility different and mutually competing.",21 Mar 2025,8,"The integration of UX design and agile software development in software companies is a critical issue, especially for startups. The insights from the case study presented in this abstract could provide valuable guidance for European early-stage ventures looking to improve their processes."
https://www.sciencedirect.com/science/article/pii/S0950584922001550,Influences of UX factors in the Agile UX context of software startups,December 2022,Information and Software Technology,Not Found,Joelma=Choma: jchoma@ufscar.br; Eduardo M.=Guerra: eduardo.guerra@unibz.it; Alexandre=Alvaro: alvaro@ufscar.br; Roberto=Pereira: rpereira@inf.ufpr.br; Luciana=Zaina: lzaina@ufscar.br,"Abstract
Context:
Software startups work under uncertain market conditions, constant time pressures, and extremely limited resources. Startup practitioners commonly adopt agile practices and lean development to build and release software quickly. Within this context, User eXperience (UX) work is critical for generating user value and creating a competitive advantage. However, integrating agile and 
UX
 remains an open question and little explored in software startups.
Objective:
In this study, we investigate how startup practitioners understand the 
UX
 concept and what are the influences of UX factors on the agile context of software startups.
Method:
To achieve this goal, we surveyed software practitioners from software startups in Brazil. We obtained 97 valid responses from professionals working in different areas, in positions of UX experts, software engineers, and managers.
Results:
Our findings show that most software startup practitioners understand UX from a perspective that gives value to the user/customer interaction with the product and company, focusing on achieving a good UX. Regarding the influences of UX factors, we found that most selected factors carried the meaning of delivering value to the business and the user for producing successful products. On the other hand, the lack of resources is a factor that significantly hinders UX work in early-stage startups and with small teams.
Conclusion:
By analyzing our results on four dimensions, covering business & market, product & process, customers & users, and UX work & teams, we provided four takeaways to help practitioners with the adoption of Agile UX in software startups context.",21 Mar 2025,9,The study focusing on the influences of UX factors on agile in software startups is highly relevant for early-stage ventures. Understanding how to integrate UX effectively within agile practices can have a significant impact on product development and user satisfaction.
https://www.sciencedirect.com/science/article/pii/S0950584922001677,Software defect prediction with semantic and structural information of codes based on Graph Neural Networks,December 2022,Information and Software Technology,Not Found,Chunying=Zhou: zcy9838@stu.hubu.edu.cn; Peng=He: penghe@hubu.edu.cn; Cheng=Zeng: zc@hubu.edu.cn; Ju=Ma: dcsxan@nus.edu.sg,"Abstract
Context:
Most 
defect prediction
 methods consider a series of traditional manually designed static 
code metrics
. However, only using these hand-crafted features is impractical. Some researchers use the Convolutional 
Neural Network
 (CNN) to capture the potential semantic information based on the program’s Syntax 
Trees
 (ASTs). In recent years, leveraging the 
dependency relationships
 between software modules to construct a software network and using network embedding models to capture the structural information have been helpful in 
defect prediction
. This paper simultaneously takes the semantic and structural information into account and proposes a method called CGCN.
Objective:
This study aims to validate the feasibility and performance of the proposed method in 
software defect
 prediction.
Method:
Abstract Syntax Trees
 and a Class Dependency Network (CDN) are first generated based on the 
source code
. For ASTs, symbolic tokens are extracted and encoded into vectors. The numerical vectors are then used as input to the CNN to capture the semantic information. For CDN, a 
Graph Convolutional Network
 (GCN) is used to learn the structural information of the network automatically. Afterward, the learned semantic and structural information are combined with different weights. Finally, we concatenate the learned features with traditional hand-crafted features to train a classifier for more accurate defect prediction.
Results:
The proposed method outperforms the state-of-the-art defect prediction models for both within-project prediction (including within-version and cross-version) and cross-project prediction on 21 open-source projects. In general, within-version prediction achieves better performance in the three prediction tasks.
Conclusion:
The proposed method of combining semantic and structural information can improve the performance of 
software defect
 prediction.",21 Mar 2025,7,The proposed method for software defect prediction incorporating semantic and structural information could benefit early-stage ventures by improving the accuracy of defect prediction. This could lead to better software quality and user satisfaction.
https://www.sciencedirect.com/science/article/pii/S0950584922001707,Guidelines for the development of a critical software under emergency,December 2022,Information and Software Technology,"Safety–critical systems development, Software certification, Lessons learned, Guidelines, Healthcare",Andrea=Bombarda: Not Found; Silvia=Bonfanti: silvia.bonfanti@unibg.it; Cristiano=Galbiati: Not Found; Angelo=Gargantini: Not Found; Patrizio=Pelliccione: Not Found; Elvinia=Riccobene: Not Found; Masayuki=Wada: Not Found,"Abstract
Context:
During the first wave of the COVID-19 pandemic, an international and heterogeneous team of scientists collaborated on a social project to produce a mechanical ventilator for intensive care units (MVM). MVM has been conceived to be produced and used also in poor countries: it is open-source, no patents, cheap, and can be produced with materials that are easy to retrieve.
Objective:
The objective of this work is to extract from the experience of the MVM development and software certification a set of lessons learned and then guidelines that can help developers to produce safety–critical devices in similar 
emergency situations
.
Method:
We conducted a 
case study
. We had full access to source code, comments on code, change requests, test reports, every deliverable (60 in total) produced for the software certification (safety concepts, requirements specifications, architecture and design, testing activities, etc.), notes, whiteboard sketches, emails, etc. We validated both lessons learned and guidelines with experts.
Findings:
We contribute a set of validated lessons learned and a set of validated guidelines, together with a discussion of benefits and risks of each guideline.
Conclusion:
In this work we share our experience in certifying software for healthcare devices produced under emergency, i.e. with strict and pressing time constraints and with the difficulty of establishing a heterogeneous development team made of volunteers. We believe that the guidelines will help engineers during the development of critical software under emergency.",21 Mar 2025,8,The development of guidelines for producing safety-critical devices in emergency situations like the COVID-19 pandemic can have a significant impact on early-stage ventures dealing with healthcare technologies.
https://www.sciencedirect.com/science/article/pii/S0950584922001689,"Theories in Agile Software Development: Past, Present, and Future Introduction to the XP 2020 Special Section",December 2022,Information and Software Technology,Not Found,Viktoria=Stray: Not Found; Rashina=Hoda: Not Found; Maria=Paasivaara: Not Found; Valentina=Lenarduzzi: Not Found; Daniel=Mendez: Not Found,"Abstract
Over the last two decades, agile software development has gained popularity among software engineering researchers and practitioners. However, the development and use of theories in agile research remain relatively low. While analyzing publications on agile software development in the Scopus database from the last decade, we found that only 7% of the papers used or developed a theory. This trend seems stable. However, it is promising that most theory-centric studies use or propose theories to address cognitive and behavioral aspects of people working in agile development. We argue that these aspects build fundamental pillars in agile software development. In this special section, we introduce extended versions of four papers selected from the XP2020 Conference. These papers make valuable contributions to aspects of learning and behavior in agile software development. We encourage researchers to be more theory-centric in their future empirical studies of agile methods and practices by familiarizing themselves with existing theories and applying and developing theories. This way, they can contribute to a reliable, evidence-based body of knowledge in our community.",21 Mar 2025,7,Encouraging researchers to be more theory-centric in agile software development studies can provide valuable insights for startups working in this domain.
https://www.sciencedirect.com/science/article/pii/S0950584922001331,Predictive maintenance using digital twins: A systematic literature review,November 2022,Information and Software Technology,"Systematic literature review, Active learning, Digital twin, Predictive maintenance",Raymon=van Dinter: Not Found; Bedir=Tekinerdogan: bedir.tekinerdogan@wur.nl; Cagatay=Catal: Not Found,"Abstract
Context
Predictive maintenance
 is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review.
Objective
This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research.
Method
A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed.
Results
This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry.
Conclusion
This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models.",21 Mar 2025,9,Gathering studies on predictive maintenance using Digital Twins can offer innovative solutions for early-stage ventures in industries focusing on sustainable practices.
https://www.sciencedirect.com/science/article/pii/S0950584922001318,UX professionals’ learning and usage of UX methods in agile,November 2022,Information and Software Technology,"User experience, UX, Agile development, UX professionals, UX methods, Lifelong learning",Åsa=Cajander: asa.cajander@it.uu.se; Marta=Larusdottir: marta@ru.is; Johannes L.=Geiser: jogeiser@icloud.com,"Abstract
Context
The usage of 
User Experience
 (UX) methods has been studied through the years. However, little is known about UX professionals’ 
lifelong learning
 processes related to UX methods in Agile, choosing what UX methods to use, and the enablers and hindrances for using the UX methods.
Objective
The study aims to broaden current knowledge about UX professionals’ lifelong learning practices to understand their work situations better. The paper describes how UX professionals learn about and choose UX methods, their frequency of use, and the enablers and barriers when using the UX methods in Agile.
Method
An interview study was conducted with 13 UX professionals from various industries and two countries working with Agile and UX. We used a qualitative approach, and a thematic analysis was carried out to answer the research questions.
Results
The results show that support from colleagues is an essential component for learning about the methods and how to use UX methods. Time pressure makes UX professionals choose methods they know will deliver their desired results. Prototyping, user testing, user journeys, and workshops are the most frequently used UX methods. Additionally, the results show that UX professionals think that the UX methods are often too complicated and take too long to learn. Additionally, they find it challenging to integrate UX methods into Agile.
Conclusion
These findings indicate that UX methods might work better if designed to be less complicated and deliver results more efficiently. Moreover, collegial and 
peer learning
 is central to UX professionals. The HCI community could be more active in supporting this culture by sharing information and learning. Finally, the usability and UX of the tools affect which UX methods are used.",21 Mar 2025,6,Understanding UX professionals' lifelong learning practices can provide valuable insights for startups developing user-centric products and services.
https://www.sciencedirect.com/science/article/pii/S0950584922001276,Detecting relevant app reviews for software evolution and maintenance through multimodal one-class learning,November 2022,Information and Software Technology,Not Found,Marcos P.S.=Gôlo: marcosgolo@usp.br; Adailton F.=Araújo: adailton.araujo@usp.br; Rafael G.=Rossi: rafael.g.rossi@ufms.br; Ricardo M.=Marcacini: ricardo.marcacini@icmc.usp.br,"Abstract
Context:
Mobile app reviews are a rich source of information for software evolution and maintenance. Several studies have shown the effectiveness of exploring relevant reviews in the 
software development lifecycle
, such as release planning and 
requirements engineering
 tasks. Popular apps receive even millions of reviews, thereby making manual extraction of relevant information an impractical task. The literature presents several 
machine learning approaches
 to detect relevant reviews. However, these approaches use multi-class learning, implying more user effort for data labeling since users must label a significant set of relevant and irrelevant reviews.
Objective:
This article investigates methods for detecting relevant app reviews considering scenarios with small sets of labeled data. We evaluated unimodal and multimodal representations, different labeling levels, as well as different app review domains and languages.
Method:
We present a one-class multimodal learning method for detecting relevant reviews. Our approaches have two main contributions. First, we use one-class learning that requires only the labeling of relevant app reviews, thereby minimizing the labeling effort. Second, to handle the smaller amount of labeled reviews without harming classification performance, we also present methods to improve feature extraction and reviews representation. We propose the Multimodal 
Autoencoder
 and the Multimodal 
Variational Autoencoder
. The methods learn representations which explore both textual data and visual information based on the density of the reviews. Density information can be interpreted as a summary of the main topics or clusters extracted from the reviews.
Results:
Our methods achieved competitive results even using only 25% of labeled reviews compared to models that used the entire training set. Also, our 
multimodal approaches
 obtain the highest 
F
1
-Score and AUC-ROC in twenty-three out of twenty-four scenarios.
Conclusion:
Our one-class multimodal methods proved to be a competitive alternative for detecting relevant reviews and promising for practical scenarios involving data-driven software evolution and maintenance.",21 Mar 2025,8,"Introducing methods for detecting relevant app reviews with minimal labeling effort can benefit startups in the software development lifecycle, especially in data-driven software evolution."
https://www.sciencedirect.com/science/article/pii/S095058492200129X,Recruiting credible participants for field studies in software engineering research,November 2022,Information and Software Technology,"Credibility, Validity, Reliability, Data collection, Sampling, Subjects, Participants, Recruitment",Austen=Rainer: Not Found; Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Context:
Software practitioners are a primary provider of information for 
field studies
 in 
software engineering
. Research typically recruits practitioners through some kind of sampling. But sampling may not in itself recruit the “right” participants.
Objective:
To assess existing guidance on participant recruitment, and to propose and illustrate a framework for recruiting professional practitioners as credible participants in field studies of software engineering.
Methods:
We review 
existing guidelines
, checklists and other advisory sources on recruiting participants for field studies. We develop a framework, partly based on our prior research and on the research of others. We search for and select three exemplar studies (a 
case study
, an interview study and a survey study) and use those to illustrate the framework.
Results:
Whilst existing guidance recognises the importance of recruiting participants, there is limited guidance on how to recruit the “right” participants. The framework suggests the conceptualisation of participants as “research instruments” or, alternatively, as a sampling frame for items of interest. The exemplars suggest that at least some members of the research community are aware of the need to carefully recruit the “right” participants.
Conclusions:
The framework is intended to encourage researchers to 
think differently
 about the involvement of practitioners in field studies of software engineering. Also, the framework identifies a number of characteristics not explicitly addressed by existing guidelines.",21 Mar 2025,6,"The framework proposed in the abstract can potentially help researchers recruit the 'right' participants for field studies, which could have a positive impact on the quality of research outcomes in software engineering."
https://www.sciencedirect.com/science/article/pii/S0950584922001306,A comprehensive empirical study on bug characteristics of deep learning frameworks,November 2022,Information and Software Technology,Not Found,Yilin=Yang: yilin.yang@smail.nju.edu.cn; Tianxing=He: Not Found; Zhilong=Xia: Not Found; Yang=Feng: fengyang@nju.edu.cn,"Abstract
Context:
Deep Learning
 (DL) frameworks enable developers to build 
DNN
 models without learning the underlying algorithms and models. While some of these DL-based software systems have been deployed in safety-critical areas, such as self-driving cars and medical diagnostics, for DL frameworks, characterizing their bugs and thus helping researchers to design specific 
quality assurance techniques
 become desperately needed.
Objective:
Our research aims to characterize bugs typical of DL frameworks at the 
source code
 level for an in-depth analysis of bug symptoms, root causes, and bug fixes. In this way, we hope to provide insights for researchers to design automatic 
quality assurance techniques
, such as automatic repair techniques and 
fault location
 techniques, applicable to DL frameworks and DL-based software systems.
Method:
We started by summarizing the DL framework reference architecture and proposing the DL framework bug taxonomy. Then, we mined 1,127 DL framework 
bug reports
 from eight popular DL frameworks and labeled the bug types, root causes, and symptoms. Finally, we discussed the bug characteristics and explored how developers could possibly deal with these bugs.
Results:
Our main findings are: (i) 
DNN
 model building bugs and general type bugs accounted for one-third of the total defects. (ii) DNN model building bugs are more prone to algorithm logic constraints, internal API errors, and data/numerical errors. (iii) Fifteen bug-fixing patterns are summarized, providing reference for common DL framework bug repair and future research on the development of automatic DL framework bug detection tools.
Conclusion:
By analyzing the bug-fixing changes, we characterize the occurrences, root causes, symptoms, and fixing of these bugs. The study results have provided researchers with insights into how to ensure DL framework quality and presented actionable suggestions for DL framework developers to improve their code quality.",21 Mar 2025,8,"Characterizing bugs and providing insights for designing automatic quality assurance techniques for DL frameworks can greatly benefit developers working on safety-critical systems, potentially improving overall software quality and reliability."
https://www.sciencedirect.com/science/article/pii/S0950584922001264,Improving microservices extraction using evolutionary search,November 2022,Information and Software Technology,Not Found,Khaled=Sellami: khaled.sellami.1@ulaval.ca; Mohamed Aymen=Saied: mohamed-aymen.saied@ift.ulaval.ca; Salah=Bouktif: salahb@uaeu.ac.ae; Mohamed Wiem=Mkaouer: mwmvse@rit.edu,"Abstract
Context:
Microservices
 constitute a modern style of building 
software applications
 as collections of small, cohesive, and loosely coupled services, 
i.e.
, modules, that are developed, deployed, and scaled independently.
Objective:
The migration from legacy systems towards the microservice-based architecture is not a trivial task. It is still manual, time-consuming, error-prone and subsequently costly. The most critical and challenging issue is the cost-effective identification of microservices boundaries that ensure adequate 
granularity
 and cohesiveness.
Method:
To address this problem, we introduce in this paper a novel approach, named 
MSExtractor
 , that formulates microservices identification as a multi-objective 
optimization problem
. The proposed solution aims at decomposing a legacy application into a set of cohesive, loosely-coupled and coarse-grained services. We employ the Indicator-Based 
Evolutionary Algorithm
 (IBEA) to drive a search process towards optimal microservices identification while considering structural and 
semantic dependencies
 in the source code.
Results:
We conduct an empirical evaluation on a benchmark of seven software systems to assess the efficiency of our approach. Results show that 
MSExtractor
 is able to carry out an effective identification of relevant microservice candidates and outperforms three other existing approaches.
Conclusion:
In this paper, we show that MSExtractor is able to extract cohesive and loosely coupled services with higher performance than three other considered methods. However, we advocate that while automated microservices identification approaches are very helpful, the role of the human experts remains crucial to validate and calibrate the extracted microservices.",21 Mar 2025,7,"The novel approach of MSExtractor for identifying microservices can significantly help in the migration from legacy systems to microservice-based architecture, reducing manual efforts, costs, and errors in the process."
https://www.sciencedirect.com/science/article/pii/S0950584922001410,Dealing with imbalanced data for interpretable defect prediction,November 2022,Information and Software Technology,Not Found,Yuxiang=Gao: gaoyx@jsnu.edu.cn; Yi=Zhu: zhuy@jsnu.edu.cn; Yu=Zhao: zhaoyu@jsnu.edu.cn,"Abstract
Context
Interpretation has been considered as a key factor to apply 
defect prediction
 in practice. As interpretation from rule-based interpretable models can provide insights about past defects with high quality, many prior studies attempt to construct interpretable models for both accurate prediction and comprehensible interpretation. However, 
class imbalance
 is usually ignored, which may bring huge 
negative impact
 on interpretation.
Objective
In this paper, we are going to investigate resampling techniques, a popular solution to deal with 
imbalanced data
, on interpretation for interpretable models. We also investigate the feasibility to construct interpretable 
defect prediction
 models directly on original data. Further, we are going to propose a rule-based interpretable model which can deal with 
imbalanced data
 directly.
Method
We conduct an empirical study on 47 publicly available datasets to investigate the impact of resampling techniques on rule-based interpretable models and the feasibility to construct such models directly on original data. We also improve gain function and tolerate lower confidence based on 
rule induction
 algorithms to deal with imbalanced data.
Results
We find that (1) resampling techniques impact on interpretable models heavily from both feature importance and model complexity, (2) it is not feasible to construct meaningful interpretable models on original but imbalanced data due to low coverage of defects and poor performance, and (3) our proposed approach is effective to deal with imbalanced data compared with other rule-based models.
Conclusion
Imbalanced data heavily impacts on the interpretable defect prediction models. Resampling techniques tend to shift the learned concept, while constructing rule-based interpretable models on original data may also be infeasible. Thus, it is necessary to construct rule-based models which can deal with imbalanced data well in further studies.",21 Mar 2025,7,"Investigating the impact of resampling techniques on interpretable defect prediction models for imbalanced data can provide valuable insights for building more effective prediction models in software engineering, contributing to better defect management."
https://www.sciencedirect.com/science/article/pii/S0950584922001434,Using clarification questions to improve software developers’ Web search,November 2022,Information and Software Technology,Not Found,Mia Mohammad=Imran: imranm3@vcu.edu; Kostadin=Damevski: kdamevski@vcu.edu,"Abstract
Context:
Recent research indicates that Web queries written by software developers are not very successful in retrieving relevant results, performing measurably worse compared to general purpose Web queries. Most approaches up to this point have addressed this problem with software engineering-specific automated 
query reformulation
 techniques, which work without 
developer involvement
 but are limited by the content of the original query. In other words, these techniques automatically improve the existing query but cannot contribute new, previously unmentioned, concepts.
Objective:
In this paper, we propose a technique to guide software developers in manually improving their own 
Web search
 queries. We examine a conversational approach that follows unsuccessful queries with a clarification question aimed at eliciting additional query terms, thus providing to the developer a clear dimension along which the query could be improved.
Methods:
We describe a set of clarification questions derived from a corpus of software developer queries and a neural approach to recommending them for a newly issued query.
Results:
Our evaluation indicates that the recommendation technique is accurate, predicting a valid clarification question 80% of the time and outperforms simple baselines, as well as, state-of-the-art Learning To Rank (LTR) baselines.
Conclusion:
As shown in the experimental results, the described approach is capable at recommending appropriate clarification questions to software developers and considered useful by a sample of developers ranging from novices to experienced professionals.",21 Mar 2025,5,"The proposed technique to guide software developers in improving their Web search queries can be useful in enhancing search results, but the practical impact may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922001446,The essential competencies of software professionals: A unified competence framework,November 2022,Information and Software Technology,"Software engineering, Software development, Competence, Competencies, Kano model",Nana=Assyne: nana.m.a.assyne@student.jyu.fi; Hadi=Ghanbari: Not Found; Mirja=Pulkkinen: Not Found,"Abstract
Context
Developing high-quality software requires skilled software professionals equipped with a set of basic and essential software engineering competencies (SEC). These competencies and the satisfaction levels derived from them change over a project's lifecycle, or as software professionals move from one project to another.
Objective
Previous studies suggest a lack of means enabling SEC stakeholders to identify and assess competencies suitable for different projects. Additionally, previous research has mainly portrayed SEC to be static and overlooked their evolution over time and across projects. We investigate how we could effectively identify and match the competencies of software professionals necessary for different projects.
Method
We follow a mixed-method approach to iteratively develop and evaluate a framework for identifying and managing SEC. In so doing, we use the results of an extensive literature review, focus 
group discussions
 with experts from academia and 
industry
, and 
data collected
 through interviews with 138 individuals with a supervisory role in the software industry.
Results
Drawing on the Kano model and Competency Framework for Software Engineers, we propose a Unified Competence Gate for Software Professionals (UComGSP), a framework for identifying and managing SEC. The UComGSP consists of 62 hard competencies, 63 soft competencies, and 25 essential SEC competencies. Additionally, we propose three stakeholders’ satisfaction levels for SEC assessment: basic, performance, and delighter. Furthermore, based on empirical observation, we report 27 competencies not mentioned in the reviewed literature; 11 of them are considered essential competencies.
Conclusion
Competence development involves different stakeholders, including software professionals, educators, and the software industry. The UComGSP framework enables SEC stakeholders to (i) identify SE competencies, (ii) identify the essential SEC, and (iii) assess the satisfaction levels that can be derived from different competencies. Future research is needed to evaluate the effectiveness of the proposed framework across software development projects.",21 Mar 2025,7,The development of a framework for identifying and managing software engineering competencies can provide practical value for early-stage ventures by improving skill matching in projects.
https://www.sciencedirect.com/science/article/pii/S0950584922001458,A multi-objective agile project planning model and a comparative meta-heuristic approach,November 2022,Information and Software Technology,Not Found,Nilay=Ozcelikkan: nilay.ozcelikkan@gmail.com; Gulfem=Tuzkaya: gulfem.tuzkaya@marmara.edu.tr; Cigdem=Alabas-Uslu: cigdem.uslu@marmara.edu.tr; Bahar=Sennaroglu: bsennar@marmara.edu.tr,"Abstract
Agile software development
 methodologies are used to meet the changing needs in the market. The most popular framework among these methodologies is the Scrum framework. In Scrum planning, the assignment of user stories to sprints requires the consideration of multiple objectives to use the limited resources more effectively. In this paper, a multi-objective mixed-integer programming model is developed which considers three objectives: maximizing the sprint capacity usage, maximizing the assignment of user stories with high priority to primary sprints, and maximizing the assignment of affine user stories to the same sprint. The aim is to contribute to both theory and practice of Scrum planning considering multiple objectives. Additionally, different from the existing literature of Scrum planning, alternative user stories are also taken into account. The proposed model is applied to the small, medium, and big-sized instances of the problem taken from a real-life system. Non-dominated Sorting 
Genetic Algorithm
 (NSGA-II) and Strong Pareto 
Evolutionary Algorithm
 (SPEA2) are used as heuristic approaches since big-sized instances of the problem could not be solved using optimization approaches. To analyze the performances of these algorithms, Hypervolume (HV), Epsilon (
ϵ
), Generational Distance (GD), Inverted Generational Distance (IGD), Inverted Generational Distance Plus (IGD+), and Spread (
Δ
) indicators are used. Results showed that NSGA-II performs better than SPEA2 according to 
ϵ
 indicator for big-sized instance. On the other hand, SPEA2 performs better than NSGA-II according to HV, GD, IGD, IGD+, and 
Δ
 indicators. However, the results are very close to each other for HV, 
ϵ
, IGD, and IGD+ indicators. In conclusion, both algorithms can be used to deal with the multi-objective Scrum planning problem.",21 Mar 2025,9,The multi-objective mixed-integer programming model for Scrum planning can greatly impact the effectiveness and efficiency of software development in startups.
https://www.sciencedirect.com/science/article/pii/S0950584922001495,Towards a model and methodology for evaluating data quality in software engineering experiments,November 2022,Information and Software Technology,Not Found,Carolina=Valverde: mvalverde@fing.edu.uy; Adriana=Marotta: Not Found; José Ignacio=Panach: Not Found; Diego=Vallespir: Not Found,"Abstract
Context
Data collected
 during 
software engineering
 experiments might contain quality problems, leading to wrong experimental conclusions.
Objective
We present a 
data quality
 (DQ) model and a methodology specific to 
software engineering
 experiments, which provides a systematic approach in order to analyze and improve 
data quality
 in this domain.
Method
Our proposal considers a multifaceted view of data quality suitable for this context, which enables the discovery of DQ problems that are not generally addressed. We successfully applied the model (DQMoS) and methodology (DQMeS) in four controlled experiments, detecting different quality problems that could impact the experimental results. We present, through a running example, how we applied the DQMoS and DQMeS to one of the four experimental data.
Results
We found that between 55% and 75% of the 
DQ metrics
 applied showed the presence of a DQ problem in all four experiments. In all cases, the experimental results had already been obtained before the DQMeS application. This means that the DQ problems we found, were not discovered by the experimenters during or before making their experiment's analysis. Results yield data quality problems that experimenters did not detect on their own analysis, and that affect the experimental response variables. Our proposal shows a formalized framework that measures and improves the 
quality of software
 engineering experimental data. The results of a survey distributed to the experiments’ responsibles show that they value the improvements introduced by the model and methodology, and that they intend to apply them again in future experiences.
Conclusions
DQMoS and DQMeS are useful to increase the confidence in the quality of data used in software engineering experiments, and improve the trust in experimental results.",21 Mar 2025,8,"The DQ model and methodology for software engineering experiments can help startups improve the quality of their experimental data, leading to more reliable results and decision-making."
https://www.sciencedirect.com/science/article/pii/S095058492200146X,S-DABT: Schedule and Dependency-aware Bug Triage in open-source bug tracking systems,November 2022,Information and Software Technology,Not Found,Hadi=Jahanshahi: hadi.jahanshahi@ryerson.ca; Mucahit=Cevik: Not Found,"Abstract
Context:
In 
software engineering
 practice, fixing bugs in a timely manner lowers various potential costs in software maintenance. However, manual bug fixing scheduling can be time-consuming, cumbersome, and error-prone.
Objective:
In this paper, we propose the Schedule and Dependency-aware Bug Triage (S-DABT), a bug triaging method that utilizes 
integer programming
 and 
machine learning techniques
 to assign bugs to suitable developers.
Methods:
Unlike prior works that largely focus on a single component of the 
bug reports
, our approach takes into account the textual data, bug fixing costs, and bug dependencies. We further incorporate the schedule of developers in our formulation to have a more comprehensive model for this multifaceted problem. As a result, this complete formulation considers developers’ schedules and the blocking effects of the bugs while covering the most significant aspects of the previously proposed methods.
Results:
Our numerical study on four open-source software systems, namely, ECLIPSEJDT, LIBREOFFICE, GCC, and MOZILLA, shows that taking into account the schedules of the developers decreases the average bug fixing times. We find that S-DABT leads to a high level of developer utilization by a fair distribution of the tasks among the developers and efficient use of the free spots in their schedules. Via the simulation of the issue tracking system, we also show how incorporating the schedule in the model formulation reduces the bug fixing time, improves the assignment accuracy, and utilizes the capability of each developer without much comprising in the model run times.
Conclusion:
We find that S-DABT decreases the complexity of the bug 
dependency graph
 by prioritizing blocking bugs and effectively reduces the infeasible assignment ratio due to bug dependencies. Consequently, we recommend considering developers’ schedules while automating bug triage.",21 Mar 2025,9,The S-DABT bug triaging method utilizing integer programming and machine learning techniques can significantly reduce bug fixing times in startups and improve developer utilization.
https://www.sciencedirect.com/science/article/pii/S0950584922001422,Sentiment analysis tools in software engineering: A systematic mapping study,November 2022,Information and Software Technology,Not Found,Martin=Obaidi: martin.obaidi@inf.uni-hannover.de; Lukas=Nagel: lukas.nagel@inf.uni-hannover.de; Alexander=Specht: alexander.specht@inf.uni-hannover.de; Jil=Klünder: jil.kluender@inf.uni-hannover.de,"Abstract
Context:
Software development is a collaborative task. Previous research has shown 
social aspects
 within development teams to be highly relevant for the success of software projects. A team’s mood has been proven to be particularly important. It is paramount for project managers to be aware of negative moods within their teams, as such awareness enables them to intervene. 
Sentiment analysis
 tools offer a way to determine the mood of a team based on textual communication.
Objective:
We aim to help developers or stakeholders in their choice of sentiment analysis tools for their specific purpose. Therefore, we conducted a 
systematic mapping study
 (SMS).
Methods:
We present the results of our SMS of sentiment analysis tools developed for or applied in the context of 
software engineering
 (SE). Our results summarize insights from 106 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools, (5) the usage of already existing tools, and (6) the difficulties researchers face. We analyzed in more detail which tools and approaches perform how in terms of their performance.
Results:
According to our results, sentiment analysis is frequently applied to open-source software projects, and most approaches are 
neural networks
 or support-vector machines. The best performing approach in our analysis is 
neural networks
 and the best tool is 
BERT
. Despite the frequent use of sentiment analysis in SE, there are open issues, e.g. regarding the identification of irony or sarcasm, pointing to future research directions.
Conclusion:
We conducted an SMS to gain an overview of the current state of sentiment analysis in order to help developers or stakeholders in this matter. Our results include interesting findings e.g. on the used tools and their difficulties. We present several suggestions on how to solve these identified problems.",21 Mar 2025,7,"The systematic mapping study on sentiment analysis tools can help developers in choosing the right tools for improving team mood analysis, which is crucial for project success."
https://www.sciencedirect.com/science/article/pii/S095058492200132X,Collaborative program comprehension via software visualization in extended reality,November 2022,Information and Software Technology,"Program comprehension, Software visualization, City metaphor, Extended reality, Virtual reality, Augmented reality",Alexander=Krause-Glau: akr@informatik.uni-kiel.de; Malte=Hansen: Not Found; Wilhelm=Hasselbring: Not Found,"Abstract
Context:
In software visualization research, various approaches strive to create 
immersive environments
 by employing 
extended reality
 devices. In that context, only few research has been conducted on the effect of collaborative, i.e., multi-user, extended reality environments.
Objective:
We present our journey toward a web-based approach to enable (location-independent) collaborative 
program comprehension
 using desktop, virtual reality, and mobile 
augmented reality
 devices.
Method:
We designed and implemented three multi-user modes in our web-based live trace visualization tool ExplorViz. Users can employ desktop, mobile, and virtual reality devices to collaboratively explore software visualizations. We conducted two preliminary user studies in which subjects evaluated our VR and AR modes after solving common program comprehension tasks.
Results:
The VR and AR environments can be suitable for collaborative work in the context of program comprehension. The analyzed feedback revealed problems regarding the usability, e.g., readability of visualized entities and performance issues. Nonetheless, our approach can be seen as a blueprint for other researchers to replicate or build upon these modes and results.
Conclusions:
ExplorViz’s multi-user modes are our approach to enable heterogeneous 
collaborative software
 visualizations. The preliminary results indicate the need for more research regarding effectiveness, usability, and acceptance. Unlike related work, we approach the latter by introducing a multi-user augmented reality environment for software visualizations based on off-the-shelf mobile devices.",21 Mar 2025,7,The research on enabling collaborative software visualizations using extended reality environments has potential practical value for early-stage ventures in the tech sector.
https://www.sciencedirect.com/science/article/pii/S0950584922001471,Automatically repairing tensor shape faults in deep learning programs,November 2022,Information and Software Technology,Not Found,Dangwei=Wu: wudangwei@sjtu.edu.cn; Beijun=Shen: bjshen@sjtu.edu.cn; Yuting=Chen: chenyt@sjtu.edu.cn; He=Jiang: jianghe@dlut.edu.cn; Lei=Qiao: fly2moon@163.com,"Abstract
Context:
Software developers frequently invoke 
deep learning
 (DL) APIs to incorporate 
artificial intelligence
 solutions into software systems. However, misuses of these APIs can cause various DL faults, such as 
tensor shape faults
. Tensor shape faults occur when restriction conditions of operations are not met; they are prevalent in practice, leading to many system crashes. Meanwhile, researchers and engineers still face a strong challenge in detecting tensor shape faults — static techniques incur heavy overheads in defining detection rules, and the only dynamic technique requires human engineers to rewrite APIs for tracking shape changes.
Objective:
This paper introduces a novel technique that leverages 
machine learning
 to detect tensor shape faults, and as well uses patterns to repair faults detected.
Methods:
We first construct SFData, a set of 146 buggy programs with 
crashing tensor shape faults
 (i.e., those causing programs to crash). We also conduct an empirical study on crashing tensor shape faults, categorizing them into four types and revealing twelve repair patterns. Then we propose Tensfa2, an automated approach to detecting and repairing crashing tensor shape faults. Tensfa2 employs a 
machine learning method
 to learn from crash messages and 
decision trees
 to detect tensor shape faults. Next, Tensfa2 tracks shape properties by a customized Python 
debugger
, analyzes their 
data dependences
, and uses the twelve patterns to generate patches. Tensfa2 is an extended version of Tensfa—our previous approach presented at ISSRE’21. Its performance is enhanced by two techniques: a search-based method for repairing shape value faults, and a bundle of three ranking strategies for prioritizing the repair patterns.
Results:
Tensfa2 is evaluated on SFData and IslamData (another dataset of tensor shape faults). The results show the effectiveness of Tensfa2. In particular, Tensfa2 achieves an F1-score of 96.88% in detecting the faults and repairs 82 out of 146 buggy programs in SFData.
Conclusion:
We believe that repair patches generated by our approach will help engineers fix their 
deep learning
 programs much more efficiently, saving their time and efforts.",21 Mar 2025,9,The novel technique for detecting and repairing tensor shape faults in deep learning programs has high practical impact and value for startups dealing with AI solutions.
https://www.sciencedirect.com/science/article/pii/S0950584922001033,Find potential partners: A GitHub user recommendation method based on event data,October 2022,Information and Software Technology,Not Found,Shuotong=Bai: Not Found; Lei=Liu: Not Found; Huaxiao=Liu: liuhuaxiao@jlu.edu.cn; Mengxi=Zhang: Not Found; Chenkun=Meng: Not Found; Peng=Zhang: Not Found,"Abstract
Context:
GitHub has attracted much popularity among a large number of software developers around the world and introduced the social function 
follow
 to strengthen the relationship among developers. Like other social networks, GitHub users usually follow others who are popular in the community, co-workers, or friends in real life. However, according to our investigation, more than half of GitHub users prefer to follow recently like-minded developers other than their traditional networks for communicating with timely feedback, discovering niche repositories, and attracting more active contributors to cooperate, while these users are hard to find.
Objective:
Our objective in this paper is to leverage recent activities-
Event Data
 of GitHub users and conduct a recommendation approach to help them match some recently like-minded developers to follow or reach out.
Methods:
As a first step, we conduct one empirical research—an online survey to investigate and analyze the opinions of GitHub users whether they are willing to follow others with similar recent events and which kind of events they will focus on during the follow process. Regarding the results from our survey, we partition 12 types of events focused by participants into three 
Event
 sets of 
Communication
, 
Exploration
, and 
Cooperation
. As a second step, we collect 
Event Data
 of 12,713 GitHub users who participated in repositories written in python and build a time-based multi-dimensional recommendation approach based on a calculating vector-similarity method, a 
clustering approach
, and a 
deep learning model
.
Results and Conclusion:
The experimental results show that our approach achieves an improvement of 607.64%, 564.59%, and 599.19% on average compared with two baselines in terms of 
P
r
e
c
i
s
i
o
n
@
N
, 
R
e
c
a
l
l
@
N
, and 
F
1
−
S
c
o
r
e
@
N
. Such a series of experiments have proved that our method is effective and feasible.",21 Mar 2025,8,The recommendation approach based on recent activities of GitHub users can benefit early-stage ventures in software development by improving developer collaboration and networking.
https://www.sciencedirect.com/science/article/pii/S0950584922001045,"Like, dislike, or just do it? How developers approach software development tasks",October 2022,Information and Software Technology,Not Found,Zainab=Masood: zmas690@aucklanduni.ac.nz; Rashina=Hoda: Not Found; Kelly=Blincoe: Not Found; Daniela=Damian: Not Found,"Abstract
Context:
Software developers work on various tasks and activities that contribute towards creating and maintaining 
software applications
, frameworks, or other software components. These include technical (e.g., writing code and fixing bugs) and non-technical activities (e.g., communicating within or outside teams to understand, clarify, and resolve issues) as part of their day-to-day responsibilities. Interestingly, there is an aspect of desirability associated with these tasks and activities.
Objective:
However, not all of these tasks are desirable to developers, and yet they still need to be done. This study explores desirability and undesirability of developers for software development tasks.
Method:
Based on semi-structured interviews from 32 software developers and applying a grounded theory research approach, the study investigates what tasks are desirable and undesirable for developers, what makes tasks desirable and undesirable for them, what are the perceived consequences of working on these tasks, and how do they deal with such tasks.
Results:
We identified a set of underlying factors that make tasks (un)desirable for developers, categorised as personal, social, organisational, technical, and operational factors. We also found that working on desirable tasks has positive consequences while working on undesirable tasks has negative consequences. We reported different standard, assisted, and 
mitigation strategies
 that aid software practitioners manage developers’ likes and dislikes.
Conclusion:
Understanding these likes and dislikes, contributing factors, and strategies can help the managers and teams ensure balanced work distribution, developers’ happiness, and productivity, ultimately increasing the value developers add to software products.",21 Mar 2025,6,Understanding the desirability and undesirability of software development tasks can provide insights for startups to better manage developer preferences and productivity.
https://www.sciencedirect.com/science/article/pii/S0950584922000994,How higher order mutant testing performs for deep learning models: A fine-grained evaluation of test effectiveness and efficiency improved from second-order mutant-classification tuples,October 2022,Information and Software Technology,Not Found,Yanhui=Li: Not Found; Weijun=Shen: Not Found; Tengchao=Wu: Not Found; Lin=Chen: lchen@nju.edu.cn; Di=Wu: Not Found; Yuming=Zhou: Not Found; Baowen=Xu: Not Found,"Abstract
Context:
Given the prevalence of 
Deep Learning
 (DL) models in 
daily life
, it is crucial to guarantee their reliability by 
DL
 testing. Recently, researchers have adapted mutation testing into 
DL
 testing to measure the test power of test sets. The bottleneck of DL mutation testing is the expensive costs of generating a large number of mutants.
Objective:
We want to study whether the traditional ideology of “Higher Order” and “Strongly Subsuming” in Higher Order Mutant Testing is still applicable for DL mutation testing, i.e., whether they can be used to optimize DL mutation testing by reducing the number of mutants.
Method:
We propose a new mutation testing framework supporting a fine-grained evaluation of test power, called mutant-classification tuples which consist of mutants and classification categories. Based on mutant-classification tuples, we construct First Order (FOTs) and Higher (Second) Order Tuples (HOTs) by applying 
mutation operators
 twice, and search for “Strongly Subsuming” HOTs (SSHOTs) from HOTs.
Results:
The experimental results conducted on four widely used datasets and five DL model structures tell us that (1) we can find a considerable number of SSHOTs (from 720 to 25,840 in five models) which can greatly reduce the 
original set
 of FOTs (with the reduction ratio from 28.69% to 91.97% in our studied DL models). (2) The reduced tuples by SSHOTs can perform very well in test case selection, since the selected test set is almost the same effective (i.e., with almost the same mutation score) and much more efficient (i.e., with a smaller test size, which is more than 50% reduced) for most studied DL models.
Conclusions:
Our study shows that “Higher Order” and “Strongly Subsuming” are useful to optimize DL mutation testing, i.e., SSHOTs can be introduced to reduce the number of mutants and test cases.",21 Mar 2025,9,The study on optimizing DL mutation testing by reducing the number of mutants has significant practical implications for startups working with deep learning models.
https://www.sciencedirect.com/science/article/pii/S0950584922001082,An empirical study of IoT security aspects at sentence-level in developer textual discussions,October 2022,Information and Software Technology,Not Found,Nibir=Mandal: Not Found; Gias=Uddin: gias.uddin@ucalgary.ca,"Abstract
Context:
IoT
 is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, ensuring the security of 
IoT devices
 is crucial. 
IoT
 devices can differ from traditional computing (e.g., low power, storage, computing), thereby the design and implementation of proper security measures can be challenging in 
IoT devices
. We observed that IoT developers discuss their security-related challenges in developer forums like Stack Overflow (SO). However, we find that IoT security discussions can also be buried inside non-security discussions in SO.
Objective:
In this paper, we aim to understand the challenges IoT developers face while applying security practices and techniques to IoT devices. We have two goals: (1) Develop a model that can automatically find security-related IoT discussions in SO, and (2) Study the model output (i.e., the security discussions) to learn about IoT developer security-related challenges.
Methods:
First, we download all 53K posts from StackOverflow (SO) that contain discussions about various IoT devices, tools, and techniques. Second, we manually labeled 5,919 sentences from 53K posts as 1 or 0 (i.e., whether they contain a 
security aspect
 or not). Third, we then use this benchmark to investigate a suite of 
deep learning
 transformer models. The best performing model is called SecBot. Fourth, we apply SecBot on the entire 53K posts and find around 30K sentences labeled as security. Fifth, we apply 
topic modeling
 to the 30K security-related sentences labeled by SecBot. Then we label and categorize the topics. Sixth, we analyze the evolution of the topics in SO.
Results:
We found that (1) SecBot is based on the retraining of the 
deep learning model
 RoBERTa. SecBot offers the best F1-Score of .935, (2) there are six error categories in misclassified samples by SecBot. SecBot was mostly wrong when the keywords/contexts were ambiguous (e.g., ‘gateway’ can be a 
security gateway
 or a simple gateway), (3) there are 9 security topics grouped into three categories: Software, Hardware, and Network, and (4) the highest number of topics belongs to software security, followed by 
network security
 and hardware security.
Conclusion:
IoT researchers and vendors can use SecBot to collect and analyze security-related discussions from developer discussions in SO. The analysis of nine security-related topics can guide major IoT stakeholders like IoT Security Enthusiasts, Developers, Vendors, Educators, and Researchers in the rapidly emerging IoT ecosystems.",21 Mar 2025,8,"The development of SecBot and the identification of security-related topics can greatly benefit IoT stakeholders in addressing security challenges in IoT devices, providing practical value and impact."
https://www.sciencedirect.com/science/article/pii/S0950584922001069,On the effectiveness of testing sentiment analysis systems with metamorphic testing,October 2022,Information and Software Technology,Not Found,Mingyue=Jiang: mjiang@zstu.edu.cn; Tsong Yueh=Chen: tychen@swin.edu.au; Shuai=Wang: shuaiw@cse.ust.hk,"Abstract
Context:
Metamorphic testing (MT) has been successfully applied to a wide scope of software systems. In these applications, the testing results of MT form the basis for drawing conclusions about the target system’s performance. Therefore, the effectiveness of MT is crucial to the trustfulness of the derived conclusions.
Objective:
However, due to the nature of MT, its effectiveness can be affected by various factors. Despite of MT’s success, it is still important to study its effectiveness under different application contexts.
Method:
To investigate the effectiveness of MT, we focus on an important aspect, namely, false satisfactions (which are satisfactions of metamorphic relations that involve at least one failing execution), and revisit the application of MT to 
sentiment analysis
 (SA) systems. An in-depth analysis of the essence of false satisfactions reveals the situations where they would occur, and how they would affect the effectiveness of MT. Furthermore, 20 metamorphic relations (MRs) are identified for supporting a user-oriented evaluation of SA systems.
Results:
The occurrence rates of false satisfactions are reported with respect to four SA systems. For the majority of MRs, false satisfactions account for about 20% to 50% of all MR satisfactions, suggesting that false satisfactions occur quite frequently in the evaluation of SA systems. It is also demonstrated that such high occurrence rates of false satisfactions adversely affect the users’ selection of SA systems.
Conclusion:
Our analysis reveals that without considering the occurrence of false satisfactions, MT may overestimate the system’s conformance to the relevant MR. Furthermore, our experiments empirically show that conclusions derived from MT can be adversely affected when there are many false satisfactions. Our findings will help the MT community to adopt a more fair and reliable way of using the test outcomes of MT, and can also inspire the development of solid foundations for MT.",21 Mar 2025,7,"The study on false satisfactions in Metamorphic Testing contributes to the understanding of the effectiveness of testing methods, which can be valuable for software systems development, although the impact may be slightly less direct for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492200101X,Approaches to manage the user experience process in Agile software development: A systematic literature review,October 2022,Information and Software Technology,"User experience management, UX process, User experience, UX, Usability, HCI, Agile methods, Agile, Systematic literature review",Andreas=Hinderks: andreas.hinderks@iwt2.org; Francisco José=Domínguez Mayo: fjdominguez@us.es; Jörg=Thomaschewski: joerg.thomaschewski@hs-emden-leer.de; María José=Escalona: mjescalona@us.es,"Abstract
Context:
Software development companies use Agile methods to develop their products or services efficiently and in a goal-oriented way. But this alone is not enough to satisfy user demands today. It is much more important nowadays that a product or service should offer a great 
user experience
 — the user wants to have some positive user experience while interacting with the product or service.
Objective:
An essential requirement is the integration of user experience methods in 
Agile software development
. Based on this, the development of positive user experience must be managed. We understand management in general as a combination of a goal, a strategy, and resources. When applied to UX, user experience management consists of a UX goal, a UX strategy, and UX resources.
Method:
We have conducted a systematic literature review (SLR) to analyse suitable approaches for managing user experience in the context of Agile software development.
Results:
We have identified 49 relevant studies in this regard. After analysing the studies in detail, we have identified different primary approaches that can be deemed suitable for UX management. Additionally, we have identified several UX methods that are used in combination with the primary approaches.
Conclusions:
However, we could not identify any approaches that directly address UX management. There is also no general definition or common understanding of UX management. To successfully implement UX management, it is important to know what UX management actually is and how to measure or determine successful UX management.",21 Mar 2025,6,"The analysis on managing user experience in Agile software development provides insights that can enhance product development, but the practical impact on startups may vary depending on their focus on UX management."
https://www.sciencedirect.com/science/article/pii/S0950584922000921,The role of awareness and gamification on technical debt management,October 2022,Information and Software Technology,Not Found,Yania=Crespo: yania@infor.uva.es; Carlos=López-Nozal: clopezno@ubu.es; Raúl=Marticorena-Sánchez: rmartico@ubu.es; Margarita=Gonzalo-Tasis: marga@infor.uva.es; Mario=Piattini: Mario.Piattini@uclm.es,"Abstract
Context:
Managing technical debt and developing easy-to-maintain software are very important aspects for technological companies. Integrated development environments (IDEs) and static measurement and analysis tools are used for this purpose. Meanwhile, 
gamification
 also is gaining popularity in professional settings, particularly in software development.
Objective:
This paper aims to analyse the improvement in technical debt indicators due to the use of techniques to raise developers’ awareness of technical debt and the introduction of 
gamification
 into technical debt management.
Method:
A quasi-experiment that manipulates a training environment with three different treatments was conducted. The first treatment was based on training in the concept of technical debt, bad smells and refactoring, while using multiple plugins in IDEs to obtain reports on quality indicators of both the code and the tests. The second treatment was based on enriching previous training with the use of 

 to continuously raise awareness of technical debt. The third was based on adding a 
gamification
 component to technical debt management based on a contest with a top ten ranking. The results of the first treatment are compared with the use of 

 for continuously raising developers’ awareness of technical debt; while the possible effect of 
gamification
 is compared with the results of the previous treatment.
Results:
It was observed that continuously raising awareness using a technical debt management tool, such as 

 , significantly improves the technical debt indicators of the code developed by the participants versus using multiple code and test quality checking tools. On the other hand, incorporating some kind of competition between developers by defining a contest and creating a ranking does not bring about any significant differences in the technical debt indicators.
Conclusion:
Investment in staff training through tools to raise developers’ awareness of technical debt and incorporating it into continuous integration pipelines does bring improvements in technical debt management.",21 Mar 2025,9,"The research on improving technical debt indicators through training and gamification offers actionable strategies for software development companies to enhance software quality, making it highly relevant and impactful for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922001173,Towards building a pragmatic cross-project defect prediction model combining non-effort based and effort-based performance measures for a balanced evaluation,October 2022,Information and Software Technology,Not Found,Yogita=Khatri: 19403019@mail.jiit.ac.in; Sandeep Kumar=Singh: sandeepk.singh@jiit.ac.in,"Abstract
Context
Recent years have witnessed the growing trend in cross-project defect prediction (CPDP), where the training and the testing data come from different projects having different data distributions. Several CPDP methods have been presented in the literature to overcome differences in their distributions, but the majority of the existing approaches have been evaluated considering the availability of unlimited inspection effort, which is practically impossible, thus leading to fallacious conclusions. Further, they focused more on improving Recall over Precision leading to a high probability of false alarm (PF), causing significant wastage of developer's efforts and time.
Objective
Addressing these issues, we propose a Two-Phase Transfer Boosting (TPTB) model, which aims at improving the performance not only in terms of non-effort based measures (NEBMs) (making a balance between Recall and PF) but also in terms of effort based measures (EBMs), considering the availability of limited inspection effort.
Method
To mitigate the distribution differences, the first phase assigns initial weights to the training modules based on the feature distribution and feature importance. The second phase applies the Dynamic Transfer AdaBoost algorithm to build an ensemble classifier to lessen the impact of contradictory training modules. In addition, a sorting strategy is designed to prioritize the modules for further inspection.
Results
Statistical results on 62 datasets revealed a better-balanced performance of our TPTB model holistically over NN-filter, ManualDown, EASC, and Cruz model with performance comparable to WPDP (Within-project defect prediction) considering NEBMs. Besides, when considering EBMs together, TPTB showed statistically and practically more balanced performance as compared to ManualUP and Cruz with overall performance comparable to EASC.
Conclusions
Our results demonstrate the efficacy of the TPTB model in a practical setting empowering the quality assurance team to predict and prioritize the defective modules allocating limited inspection effort by optimally focusing on highly defective modules.",21 Mar 2025,7,"The Two-Phase Transfer Boosting model addresses practical challenges in cross-project defect prediction, providing a valuable tool for quality assurance teams, although the direct impact on startups may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922001185,Can test input selection methods for deep neural network guarantee test diversity? A large-scale empirical study,October 2022,Information and Software Technology,Not Found,Chunyu=Zhao: 2010320004@stmail.ntu.edu.cn; Yanzhou=Mu: 2019218009@tju.edu.cn; Xiang=Chen: xchencs@ntu.edu.cn; Jingke=Zhao: ke1206371563@gmail.com; Xiaolin=Ju: ju.xl@ntu.edu.cn; Gan=Wang: wg_98@tju.edu.cn,"Abstract
Context:
Recently, various methods on test input selection for deep 
neural network
 (TIS-DNN) have been proposed. These methods can effectively reduce the labeling cost by selecting a subset from the original test inputs, which can still accurately estimate the performance (such as accuracy) of the target 
DNN models
.
Objective:
Previous studies on TIS-DNN mainly focused on the performance on all the classes. However, the selected subset may miss the coverage of some classes or decrease the performance on some classes, which will reduce the test diversity of the original test inputs.
Methods:
Therefore, we conducted a large-scale empirical study to investigate whether previous TIS-DNN methods can guarantee test diversity in the subset. In our study, we selected five state-of-the-art TIS-DNN methods: SRS, 
CSS
, CES, DeepReduce and PACE. Then we selected 18 pairs of 
DNN models
 and the corresponding test inputs from seven popular DNN datasets.
Results:
Our experimental results can be summarized as follows. (1) Previous TIS-DNN methods can guarantee the performance on all the classes. However, these methods have a 
negative impact
 on the test diversity and the performance on each class is not satisfactory. (2) Reducing the performance 
estimation error
 on each class can help reduce the 
estimation error
 on the test adequacy of the original inputs based on DNN-based coverage criteria (especially for the criterion NC and the criterion TKNC). (3) There still exists great room for 
performance improvement
 (i.e., 7.637% improvement on all the classes and 12.833% improvement on each class) after comparing the TIS-DNN method PACE with approximately optimal solutions.
Conclusion:
The above experimental findings implicate there is still a long way for the TIS-DNN issue to go. Given this, we present observations about the road ahead for this issue.",21 Mar 2025,5,"The study provides insights into the limitations of current TIS-DNN methods, suggesting room for improvement, but the practical impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584922001203,A three-stage transfer learning framework for multi-source cross-project software defect prediction,October 2022,Information and Software Technology,"Transfer learning, Cross-project defect prediction, Source selection, Multi-source utilization, 3SW-MSTL",Jiaojiao=Bai: Not Found; Jingdong=Jia: jiajingdong@buaa.edu.cn; Luiz Fernando=Capretz: Not Found,"Abstract
Context
Transfer learning techniques have been proved to be effective in the field of Cross-project defect prediction (CPDP). However, some questions still remain. First, the conditional distribution difference between source and target projects has not been considered. Second, facing multiple source projects, most studies only rarely consider the issues of source selection and multi-source data utilization; instead, they use all available projects and merge multi-source data together to obtain one final dataset.
Objective
To address these issues, in this paper, we propose a three-stage weighting framework for multi-source 
transfer learning
 (3SW-MSTL) in CPDP. In stage 1, a source selection strategy is needed to select a suitable number of source projects from all available projects. In stage 2, a transfer technique is applied to minimize marginal differences. In stage 3, a multi-source data utilization scheme that uses conditional distribution information is needed to help guide researchers in the use of multi-source transferred data.
Method
First, we have designed five source selection strategies and four multi-source utilization schemes and chosen the best one to be used in stage 1 and 3 in 3SW-MSTL by comparing their influences on prediction performance. Second, to validate the performance of 3SW-MSTL, we compared it with four multi-source and six single-source CPDP methods, a baseline within-project defect prediction (WPDP) method, and two unsupervised methods on the data from 30 widely used open-source projects.
Results
Through experiments, bellwether and weighted vote are separately chosen as a source selection strategy and a multi-source utilization scheme used in 3SW-MSTL. And, our results indicate that 3SW-MSTL outperforms four multi-source, six single-source CPDP methods and two unsupervised methods. And, 3SW-MSTL is comparable to the WPDP method.
Conclusion
The proposed 3SW-MSTL model is more effective for considering the two issues mentioned before.",21 Mar 2025,8,"The 3SW-MSTL model addresses key issues in CPDP, outperforming existing methods, which could have a significant impact on early-stage ventures by improving defect prediction."
https://www.sciencedirect.com/science/article/pii/S0950584922001197,System and software architecting harmonization practices in ultra-large-scale systems of systems: A confirmatory case study,October 2022,Information and Software Technology,"Systems of systems, SoS architecting, Confirmatory case study, Empirical software engineering, Scientific instruments, Qualitative research",Héctor=Cadavid: h.f.cadavid.rengifo@rug.nl; Vasilios=Andrikopoulos: v.andrikopoulos@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl; P. Chris=Broekema: broekema@astron.nl,"Abstract
Context:
The challenges posed by the architecting of System of Systems (SoS) has motivated a significant number of research efforts in the area. However, literature is lacking when it comes to the interplay between the disciplines involved in the 
architecting process
, a key factor in addressing these challenges.
Objective:
This paper aims to contribute to this line of research by confirming and extending previously characterized architecting harmonization practices from Systems and 
Software Engineering
, adopted in an ultra-large-scale SoS.
Methods:
We conducted a confirmatory 
case study
 on the Square-Kilometre Array (SKA) project to evaluate and extend the findings of our exploratory case on the LOFAR/LOFAR2.0 radio-telescope projects. In doing so, a pre-study was conducted to map the findings of the previous study with respect to the SKA context. A survey was then designed, through which the views of 46 SKA engineers were collected and analyzed.
Results:
The study confirmed in various degrees the four practices identified in the exploratory case, and provided further insights about them: (1) the friction between disciplines caused by long-term 
system requirements
, and how they can be ameliorated through intermediate, short-term requirements; (2) the way design choices with a cross-cutting impact on multiple agile teams have an indirect impact on the 
system architecture
; (3) how these design choices are often caused by the criteria that guided early system decomposition; (4) the seemingly 
recurrent
 issue with the lack of details about the dynamic elements of the interfaces; and (5) the use of machine-readable 
interface specifications
 for aligning hardware/software development processes.
Conclusions:
The findings of this study and its predecessor support the importance of a cross-disciplinary view in the Software Engineering 
research agenda
 in SoS as a whole, not to mention their value as a convergence point for research on SoS architecting from the Systems and Software Engineering standpoints.",21 Mar 2025,6,"The study confirms and extends harmonization practices in architecting SoS, which is valuable, but the direct impact on early-stage ventures may not be immediate."
https://www.sciencedirect.com/science/article/pii/S0950584922001215,Keyword-guided abstractive code summarization via incorporating structural and contextual information,October 2022,Information and Software Technology,Not Found,Wuyan=Cheng: wuyanc@mails.ccnu.edu.cn; Po=Hu: phu@mail.ccnu.edu.cn; Shaozhi=Wei: wsz@mails.ccnu.edu.cn; Ran=Mo: moran@mail.ccnu.edu.cn,"Abstract
Context:
Source code
 summarization is a crucial yet far from settled task for describing structured code snippets in natural language. High-quality code summaries could effectively facilitate 
program comprehension
 and software maintenance. A good code summary is supposed to have the following characteristics: complete information, correct meaning, and consistent description. In recent years, numerous approaches have been proposed for code summarization, but it is still very challenging for developers to automatically learn the complex semantics from the source code and generate complete, correct and consistent code summaries.
Objective:
In this paper, we propose 
KGCodeSum
, a novel keyword-guided abstractive code summarization approach that incorporates structural and contextual information.
Methods:
To improve summaries’ quality, we leverage both the structural 
information embedded
 in code itself and the contextual information from related code snippets. Meanwhile, we make use of keywords to guide summaries’ generation to guarantee the code summaries contain key information. Finally, we propose a new dynamic vocabulary strategy which can effectively resolve the UNK problems in code summaries.
Results:
Through our evaluation on the large-scale benchmark datasets with 2.1 million java method-comment pairs and 1.1 million C/C++ function-summary pairs, We have observed that our approach could generate better code summaries than existing state-of-the-art approaches in terms of completeness, correctness and consistency. In addition, we also find that incorporating the dynamic vocabulary strategy into our approach could significantly save time and space in the model training process.
Conclusion:
Our 
KGCodeSum
 approach could effectively generate code summaries.",21 Mar 2025,7,"KGCodeSum introduces a novel approach to code summarization with promising results, potentially benefiting developers and startups by improving program comprehension."
https://www.sciencedirect.com/science/article/pii/S0950584922001070,A deep learning-based automated framework for functional User Interface testing,October 2022,Information and Software Technology,Not Found,Zubair=Khaliq: zikayem@gmail.com; Sheikh Umar=Farooq: suf.cs@uok.edu.in; Dawood Ashraf=Khan: dawood.khan@uok.edu.in,"Abstract
Context:
The use of automation tools in software testing helps keep pace with the timeline of the deliverables. Over time with the inclusion of continuous integration/continuous delivery (CI/CD) pipelines, automation tools are becoming less effective. The testing community is turning to 
AI
 to help keep the pace.
Objective:
We study the use of transformers to automate the process of test case generation directly from the User Interface (UI) element description instead of relying on the test specification document from which test cases are extracted manually. We also demonstrate the capability of the proposed approach in repairing flaky tests.
Method:
We employ object 
detection algorithms
 
EfficientDet
 and 
DEtectionTRansformer
 for detecting the elements from an application UI automatically without requiring a tester to locate complex-scripted UI elements. We also use 
Tesseract
 to automatically identify the text present on the UI elements. We transform the generated UI element description to actual test designer-written test cases using text-generation transformers like 
GPT-2
 and 
T5
. The 
generated test cases
 are then translated into executable test scripts using a simple parser. We carry out our 
cases study
 on 30 e-commerce applications.
Results:
The percentage of correct executable test cases generated by the framework employing EfficientDet is 
93.82%
 and employing DEtectionTRansformer is 
98.08%
. The framework eliminates an average of 
96.05%
 flakiness across the applications selected for the study.
Conclusion:
It is concluded that the proposed approach can be used with current automation tools in the industry to enhance their capability in generating test cases and repairing the flaky tests.",21 Mar 2025,9,"The use of transformers for test case generation and flaky test repair shows high accuracy and effectiveness, which could greatly benefit startups in the software testing domain."
https://www.sciencedirect.com/science/article/pii/S0950584922001252,Test case recommendation based on balanced distance of test targets,October 2022,Information and Software Technology,Not Found,Weisong=Sun: weisongsun@smail.nju.edu.cn; Quanjun=Zhang: quanjun.zhang@smail.nju.edu.cn; Chunrong=Fang: fangchunrong@nju.edu.cn; Yuchen=Chen: yuc.chen@outlook.com; Xingya=Wang: xingyawang@outlook.com; Ziyuan=Wang: wangziyuan@njupt.edu.cn,"Abstract
Context:
Unit testing has been widely regarded as an effective technique to ensure software quality. Writing unit test cases is time-consuming and requires developers to have abundant knowledge and experience. Automated test case generation, a promising technology for liberating developers and improving test efficiency, currently performs not satisfactory in real-world projects. As a complement, test case recommendation (TCR) has been receiving the attention of researchers. TCR can improve the efficiency of test case writing by recommending test case code to developers for their reference and reuse. The overarching idea of TCR techniques is that two similar test targets can reuse each other’s test cases.
Objective:
Existing TCR techniques either fail to recommend relevant test cases for a given test target or are vulnerable to the mismatch of test target signatures. Our objective is to effectively and robustly recommend relevant test cases for test targets given by developers.
Method:
In this paper, we propose a novel TCR technique that measures the similarity of test targets based on a balanced distance. The balanced distance integrates the distances on code snippets and comments, making the measurement of test target similarity more accurate and robust. In particular, we take the distance on control flows into account to compensate for the shortcomings in measuring the similarity only based on the literal text of code snippets. As a proof-of-concept application, we implement a test case recommender named BDTCR.
Results:
We construct a test case corpus containing more than 13,000 test cases collected from GitHub. Based on this corpus, we conduct comprehensive experiments to evaluate the effectiveness and usefulness of BDTCR. The experimental results show that BDTCR can effectively recommend relevant test cases and outperform the state-of-the-art techniques.
Conclusion:
It can be concluded that (1) BDTCR is an effective TCR technique; (2) BDTCR is a robust TCR technique that can effectively resist the interference of the mismatch of test target signatures; (3) BDTCR is practical to help developers write test cases quickly and effectively.",21 Mar 2025,8,"The proposed TCR technique BDTCR addresses a practical issue faced by developers, improving test efficiency and effectiveness."
https://www.sciencedirect.com/science/article/pii/S0950584922001240,Microservice extraction based on knowledge graph from monolithic applications,October 2022,Information and Software Technology,Not Found,Zhiding=Li: Not Found; Chenqi=Shang: Not Found; Jianjie=Wu: wujianjie@hust.edu.cn; Yuan=Li: lyjingmen@sina.com,"Abstract
Context
Re-architecting monolithic systems with microservice architecture is a common trend. However, determining the ""optimal"" size of individual services during microservice extraction has been a challenge in software engineering. Common limitations of the literature include not being reasonable enough to be put into practical application; relying too much on human experience; neglection of the impact of hardware environment on the performance.
Objective
To address these problems, this paper proposes a novel method based on knowledge-graph to support the extraction of microservices during the initial phases of re-architecting existing applications.
Method
According to the microservice extraction method based on the AKF principle which is a widely practiced microservice design principle in the industry, four kinds of entities and four types of entity-entity relationships are designed and automatically extracted from specification and design artifacts of the monolithic application to build the knowledge graph. A constrained Louvain algorithm is proposed to identify microservice candidates.
Results
Our approach is tested based on two open-source projects with the other three typical methods: the domain-driven design-based method, the similarity calculation-based method, and the graph clustering-based method . Conducted experiments show that our method performs well concerning all the evaluation metrics.",21 Mar 2025,6,The method proposed for microservice extraction based on knowledge graph is innovative but may have limited impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584922001239,A comparative study on vectorization methods for non-functional requirements classification,October 2022,Information and Software Technology,Not Found,Pattara=Leelaprute: pattara.l@ku.ac.th; Sousuke=Amasaki: amasaki@cse.oka-pu.ac.jp,"Abstract
Context:
Identifying non-functional requirements (NFRs) and their categories at the early phase is crucial for analysts to design software systems and recognize constraints. Automatic non-functional requirements classification methods have been studied for reducing the costs of that labor-intensive task. Our previous study focused on the differences among 
vectorization
 methods that converted requirements written in natural language into numerical vectors for classification. It had some limitations regarding the number of datasets used, the types of 
vectorization
 methods supporting pre-trained data, and the performance evaluation procedure.
Objective:
To examine whether different vectorization methods lead to differences in the classification performance of NFRs and their categories with extended settings.
Methods:
Comparative experiments
 were conducted with five 
open data
. Nine vectorization methods, including ones with pre-trained data and four 
supervised classification
 methods, were supplied. Performance was evaluated with AUC and Scott-Knott 
ESD
 test.
Results:
Some advanced methods could achieve better performance than traditional ones when combined with some classifiers. The use of pre-trained data was useful for some categories.
Conclusion:
It is beneficial to consider using some combinations of vectorization methods and classifiers for classifying non-functional requirements categories.",21 Mar 2025,7,"The study on classifying non-functional requirements with different vectorization methods is valuable for software design, though the practical impact on startups may vary."
https://www.sciencedirect.com/science/article/pii/S0950584922001021,The journey to technical excellence in agile software development,October 2022,Information and Software Technology,"Agile software development, Software development methods, Technical excellence, Agile principles",Adam=Alami: adaa@itu.dk; Oliver=Krancher: Not Found; Maria=Paasivaara: Not Found,"Abstract
Context:
Technical excellence is a nebulous term in 
agile software development
. This vagueness is risky because it may lead to misunderstandings and to agile implementations that may overlook a key principle of 
agile development
.
Objective:
This study investigates how agile practitioners interpret the concept of technical excellence brought up in Principle 9 of the 
Agile manifesto
. Moreover, we investigate how agile practitioners put the concept into practice and what conditions facilitate putting technical excellence into practice.
Methods:
We conducted semi-structured interviews with twenty agile practitioners, coded the data inductively, and performed two sessions to validate the emerging findings.
Results:
We find that technical excellence is first and foremost a mindset that is underpinned by continuous attention to sustainable code, continuous learning, and teamwork. Fostering technical excellence requires the adoption of design and development practices, such as continuous architecting, and is supported by continuous learning. We also identify three enabling conditions for technical excellence: Leadership support, customer buy-in, and psychological safety. These enablers provide teams with leeway to nurture their pursuit of technical excellence.
Conclusion:
Our findings highlight the key role of people-based strategies in promoting technical excellence in agile software development. They show that the attainment of technical excellence does not only involve technical practices. On the contrary, it relies on social and 
organizational support
 and, most importantly, a mindset.",21 Mar 2025,9,"The investigation on technical excellence in agile development provides actionable insights for startups, emphasizing a mindset and people-based strategies."
https://www.sciencedirect.com/science/article/pii/S0950584922001094,Taxonomy of bug tracking process smells: Perceptions of practitioners and an empirical analysis,October 2022,Information and Software Technology,Not Found,Khushbakht Ali=Qamar: ali.qamar@bilkent.edu.tr; Emre=Sülün: emre.sulun@bilkent.edu.tr; Eray=Tüzün: eraytuzun@cs.bilkent.edu.tr,"Abstract
Context:
While there is no consensus on a formally specified bug tracking process, some certain rules and best practices for an optimal bug tracking process are accepted by many companies and open-source software (OSS) projects. Despite slight variations between different platforms, the primary aim of all these rules and practices is to perform a more efficient bug tracking process. Practitioners’ non-compliance with the best practices not only impedes the benefits of the bug tracking process but also negatively affects the other phases of 
software development life cycle
.
Objective:
The goal of this study is to gain a better knowledge of the bad practices that occur during the bug tracking process (
bug tracking process smells
) and to perform quantitative analysis to show that these process smells exist in 
bug tracking systems
. Moreover, we want to know the perception of software practitioners related to these process smells and also observe the impact of process smells on the bug tracking process.
Methods:
Based on the results of a multivocal literature review, we analyzed 60 sources in academic and gray literature and propose a taxonomy of 12 bad practices in the bug tracking process. To quantitatively analyze these process smells, we inspected 
bug reports
 collected from eight projects which use Jira, Bugzilla, and GitHub Issues. To get an idea about the perception of practitioners about the taxonomy of bug tracking process smells, we conducted a targeted survey with 30 software practitioners. Moreover, we statistically analyzed the impact of bug tracking process smells on the resolution time and reopening count of bugs.
Results:
We observed from our empirical results that a considerable amount of bug tracking process smells exist in all projects and some of the process smell categories have statistically significant impacts on quality and speed. Survey results shows that the majority of software practitioners agree with the proposed taxonomy of BT process smells.
Conclusion:
The statistical analysis reveals that bug tracking process smells have an impact on OSS projects. The proposed taxonomy may serve as a foundation for best practices and tool support for detecting and avoiding bug tracking process smells.",21 Mar 2025,7,"The analysis of bad practices in bug tracking process and their impact is relevant for software quality, but the direct impact on startups may not be as significant."
https://www.sciencedirect.com/science/article/pii/S0950584922001227,Trace visualization within the Software City metaphor: Controlled experiments on program comprehension,October 2022,Information and Software Technology,"Trace visualization, Software city, Program comprehension, Aggregation, Heatmap, Root cause analysis",Veronika=Dashuber: veronika.dashuber@qaware.de; Michael=Philippsen: michael.philippsen@fau.de,"Abstract
Context:
Especially with the rise of 
microservice architectures
, software is hard to understand when just the static dependencies are known. The actual call paths and the dynamic 
behavior
 of the application are hidden behind network communication. To comprehend what is going on in the software the vast amount of runtime data (traces) needs to be reduced and visualized.
Objective:
This work explores more effective visualizations to support 
program comprehension
 based on runtime data. The pure 
DynaCity
 visualization supports understanding normal behavior, while 
DynaCity


rc
 supports the comprehension of faulty behavior.
Method:
DynaCity
 uses the city metaphor for visualization. Its novel trace visualization displays dynamic dependencies as arcs atop the city. To reduce the number of traces, 
DynaCity
 aggregates all requests between the same two components into one arc whose brightness reflects both the number and the total duration of the requests. 
DynaCity
 also encodes dynamic trace data in a heatmap that it uses to light up the building: the brighter a building is, the more active it is, i.e., the more and the longer the requests are that it receives and/or spawns. An additional color scheme reflects any error/status codes among the aggregated traces. In a controlled experiment, we compare our approach with a traditional trace visualization built into the same Software City but showing all dependencies (without aggregation) as individual arcs and also disabling the heatmap. We also report on a second study that evaluates if an error-based coloring of only the arcs is sufficient or if the buildings should also be colored. We call this extension 
DynaCity


rc
 as it is meant to support 
r
oot 
c
ause analyses. The 
source code
 and the raw data of the 
quantitative evaluations
 are available from 
https://github.com/qaware/dynacity
.
Results:
We show quantitatively that a group of professional software developers who participated in a controlled experiment solve typical software comprehension tasks more correctly (11.7%) and also saved 5.83% of the total allotted time with the help of 
DynaCity
 and that they prefer it over the more traditional dynamic trace visualization. The color scheme based on HTTP error codes in 
DynaCity


rc
 supports developers when performing 
root cause analyses
, as the median of them stated that the visualization helped them 
much
 in solving the tasks. The evaluation also shows that subjects using 
DynaCity


rc
 with colored arcs and buildings find the responsible component 26.2% and the underlying root cause 33.3% more correctly than the group with just colored arcs. They also ranked it 40% more helpful to color both.
Conclusion:
The 
DynaCity
 visualization helps professional software engineers to understand the dynamic behavior of a software system better and faster. The color encoding of error codes in 
DynaCity


rc
 also helps them with 
root cause analyses
.",21 Mar 2025,8,"The visualization tool DynaCity helps professional software engineers understand software systems better and faster, improving software comprehension tasks and root cause analyses."
https://www.sciencedirect.com/science/article/pii/S0950584922001057,Consolidating a common perspective on Technical Debt and its Management through a Tertiary Study,September 2022,Information and Software Technology,Not Found,Helvio Jeronimo=Junior: jeronimohjr@cos.ufrj.br; Guilherme Horta=Travassos: ght@cos.ufrj.br,"Abstract
Context
Technical Debt
 (TD) contextualizes the technical decisions on shortcuts and workarounds during software development, positively and negatively influencing software evolution. However, TD still seems to confound with any issue occurring during software development, impacting its proper understanding and management in software projects.
Goal
To synthesize evidence regarding the conceptualization, characteristics, and management of TD in software projects.
Method
To undertake a tertiary study to strengthen the knowledge of TD using the principles of Grounded Theory to support qualitative analysis.
Results
Nineteen secondary studies provide evidence on TD and its management. They provided information regarding the TD's understanding (definitions and characteristics) and management (actions and technologies). Some causes, such as project constraints, technical decisions, and team members, promote different types of TD in software projects. The secondary studies also supported identifying the impacts of TD regarding project management, team members, the 
organization's business
, and internal software quality. Besides helping identify TD challenges, such studies contributed to integrating a conjectured conceptual model of TD that can support future discussions and investigations regarding TD's understanding and management.
Conclusions
The set of evidence regarding TD's understanding, actions, and technologies to manage TD can aid software practitioners in their software projects. However, it is observable an interpretation overload regarding its definition, inducing to classify any issue occurring during the software development as TD. Therefore, further discussions and investigations still represent essential steps towards consolidating a common perspective on TD and its management.",21 Mar 2025,6,"The study on Technical Debt provides evidence on its conceptualization and management in software projects, aiding software practitioners, but faces challenges in interpretation overload."
https://www.sciencedirect.com/science/article/pii/S0950584922000878,Alternatives for testing of context-aware software systems in non-academic settings: results from a Rapid Review,September 2022,Information and Software Technology,"Context-aware software systems, Software testing, Rapid review, Contemporary software systems",Domenico=Amalfitano: Not Found; Andrea=Doreste: Not Found; Anna Rita=Fasolino: Not Found; Guilherme Horta=Travassos: Not Found,"Abstract
Context
Context-awareness challenges the engineering of contemporary software systems and jeopardizes their testing. The variation of context represents a relevant behavior that deepens the limitations of available software testing practices and technologies. However, such software systems are mainstream. Therefore, researchers in non-academic settings also face challenges when developing and testing contemporary software systems.
Objective
To understand how researchers deal with the variation of context when testing context-aware software systems developed in non-academic settings.
Method
To undertake a secondary study (
Rapid Review
) to uncover the necessary evidence from primary sources describing the testing of context-aware software systems outside academia.
Results
The current testing initiatives in non-academic settings aim to generate or improve test suites that can deal with the context variation and the sheer volume of test input possibilities. They mostly rely on modeling the systems' dynamic behavior and increasing computing resources to generate test inputs to achieve this. We found no evidence of test results aiming at managing context variation through the testing lifecycle process.
Conclusions
So far, the identified testing initiatives and strategies are not ready for mainstream adoption. They are all domain-specific, and while the ideas and approaches can be reproduced in distinct settings, the technologies are to be re-engineered and tailored to the context-awareness of contemporary software systems in different problem domains. Further and joint investigations in academia and experiences in non-academic settings can evolve the body of knowledge regarding the testing of contemporary software systems in the field.",21 Mar 2025,4,"The study on testing context-aware software systems in non-academic settings highlights the challenges and limitations, showing that current initiatives are not ready for mainstream adoption."
https://www.sciencedirect.com/science/article/pii/S095058492200088X,ST-TLF: Cross-version defect prediction framework based transfer learning,September 2022,Information and Software Technology,Not Found,Yanyang=Zhao: Not Found; Yawen=Wang: wangyawen@bupt.edu.cn; Yuwei=Zhang: Not Found; Dalin=Zhang: Not Found; Yunzhan=Gong: Not Found; Dahai=Jin: Not Found,"Abstract
Context:
Cross-version 
defect prediction
 (CVDP) is a practical scenario in which 
defect prediction
 models are derived from defect data of historical versions to predict potential defects in the current version. Prior research employed defect data of the latest historical version as the training set using the empirical recommended method, ignoring the concept drift between versions, which undermines the accuracy of CVDP.
Objective:
We customized a 
S
elected 
T
raining set and 
T
ransfer 
L
earning 
F
ramework (ST-TLF) with two objectives: a) to obtain the best training set for the version at hand, proposing an approach to select the training set from the 
historical data
; b) to eliminate the concept drift, designing a transfer strategy for CVDP.
Method:
To evaluate the performance of ST-TLF, we investigated three research problems, covering the generalization of ST-TLF for 
multiple classifiers
, the accuracy of our training set matching methods, and the performance of ST-TLF in CVDP compared against state-of-the-art approaches.
Results:
The results reflect that (a) the eight classifiers we examined are all boosted under our ST-TLF, where 
SVM
 improves 49.74% considering 
MCC
, as is similar to others; (b) when performing the best training set matching, the accuracy of the method proposed by us is 82.4%, while the experience recommended method is only 41.2%; (c) comparing the 12 
control methods
, our ST-TLF (with BayesNet), against the best contrast method P15-NB, improves the average 
MCC
 by 18.84%.
Conclusions:
Our framework ST-TLF with various classifiers can work well in CVDP. The training set selection method we proposed can effectively match the best training set for the current version, breaking through the limitation of relying on experience recommendation, which has been ignored in other studies. Also, ST-TLF can efficiently elevate the CVDP performance compared with 
random forest
 and 12 
control methods
.",21 Mar 2025,7,"The framework ST-TLF enhances cross-version defect prediction with strong performance improvements over state-of-the-art approaches, providing an effective training set selection method."
https://www.sciencedirect.com/science/article/pii/S0950584922000866,A search-based framework for automatic generation of testing environments for cyber–physical systems,September 2022,Information and Software Technology,Not Found,Dmytro=Humeniuk: dmytro.humeniuk@polymtl.ca; Foutse=Khomh: Not Found; Giuliano=Antoniol: Not Found,"Abstract
Background:
Many modern cyber–physical systems incorporate 
computer vision technologies
, complex sensors and advanced control software, allowing them to interact with the environment autonomously. Examples include drone swarms, self-driving vehicles, autonomous robots, etc. Testing such systems poses numerous challenges: not only should the system inputs be varied, but also the surrounding environment should be accounted for. A number of tools have been developed to test the system model for the possible inputs falsifying its requirements. However, they are not directly applicable to autonomous cyber–physical systems, as the inputs to their models are generated while operating in a virtual environment.
Aims:
In this paper, we aim to design a search-based framework, named AmbieGen, for generating diverse fault-revealing test scenarios for autonomous cyber–physical systems. The scenarios represent an environment in which an 
autonomous agent
 operates. The framework should be applicable to generating different types of environments.
Methods:
To generate the test scenarios, we leverage the NSGA-II algorithm with two objectives. The first objective evaluates the deviation of the observed system’s behaviour from its expected behaviour. The second objective is the test case diversity, calculated as a Jaccard distance with a reference test case. To guide the first objective we are using a simplified system model rather than the full model. The full model is used to run the system in the simulation environment and can take substantial time to execute (several minutes for one scenario). The simplified system model is derived from the full model and can be used to get an 
approximation
 of the results obtained from the full model without running the simulation.
Results:
We evaluate AmbieGen on three scenario generation 
case studies
, namely a smart-thermostat, a robot 
obstacle avoidance
 system, and a vehicle lane-keeping assist system. For all the 
case studies
, our approach outperforms the available baselines in fault revealing and several other metrics such as the diversity of the revealed faults and the proportion of valid test scenarios.
Conclusion:
AmbieGen could find scenarios, revealing failures for all the three autonomous agents considered in our 
case studies
. We compared three configurations of AmbieGen: based on a single objective genetic algorithm, multi-objective, and random search. Both single and multi objective configurations outperform the random search. Multi objective configuration can find the individuals of the same quality as the single objective, producing more unique test scenarios in the same time budget. Our framework can be used to generate virtual environments of different types and complexity and reveal the system’s faults early in the design stage.",21 Mar 2025,9,"AmbieGen framework significantly outperforms baselines in generating diverse fault-revealing test scenarios for autonomous cyber–physical systems, showing promise in revealing failures early in the design stage."
https://www.sciencedirect.com/science/article/pii/S0950584922000908,What do developers consider magic literals? A smalltalk perspective,September 2022,Information and Software Technology,Not Found,N.=Anquetil: nicolas.anquetil@inria.fr; J.=Delplanque: Not Found; S.=Ducasse: Not Found; O.=Zaitsev: Not Found; C.=Fuhrman: Not Found; Y.-G.=Guéhéneuc: Not Found,"Abstract
Context:
Literals are 
constant values
 (numbers, strings, etc.) used in the source code. 
Magic literals
 are such values used without an explicit explanation of their meaning. Such undocumented values may hinder source-code comprehension, negatively impacting maintenance. Relatively little literature can be found on the subject beyond the usual (and very old) recommendation of avoiding literals and preferring named constants. Yet, magic literals are still routinely found in source code.
Objective:
We studied literal values in source code to understand when they should be considered magic or not (i.e., acceptable).
Methods:
First, we perform a 
qualitative
 study of magic literals, to establish why and under which conditions they are considered harmful. We formalize hypotheses about the reasoning behind how literals are considered magic. Second, we perform a 
quantitative
 study on seven real systems ranging from small (a few classes) to large (thousands of classes). We report the literals’ types (number, string, Boolean, …), their 
grammatical function
 (e.g., argument in a call, operand in an expression, value assigned, …), or the purpose of the code in which they appear (test methods, regular code). Third, we report on another study involving 26 programmers who analyzed about 24,000 literals, to understand which ones they consider magic. Finally, we evaluate the 
hypotheses
 defining specific conditions under which literals are acceptable.
Results:
We show that (1) literals still exist and are relatively frequent (found in close to 50% of the methods considered); (2) they are more frequent in test methods (in 80% of test methods); (3) to a large extent, they were considered acceptable (only 25% considered magic); and (4) the hypotheses concerning acceptable literals are valid to various degrees.
Conclusion:
We thus pave the way to future research on magic literals, for example, with tools that could help developers deciding if a literal is acceptable.",21 Mar 2025,8,"The study on literal values in source code and magic literals can have a practical impact on code maintenance in early-stage ventures, helping developers make informed decisions on acceptable literals."
https://www.sciencedirect.com/science/article/pii/S0950584922001008,Cleaning ground truth data in software task assignment,September 2022,Information and Software Technology,Not Found,K. Ayberk=Tecimer: ayberk.tecimer@tum.de; Eray=Tüzün: eraytuzun@cs.bilkent.edu.tr; Cansu=Moran: cansu.moran@ug.bilkent.edu.tr; Hakan=Erdogmus: hakane@andrew.cmu.edu,"Abstract
Context:
In the context of 
collaborative software development
, there are many 
application areas
 of task assignment such as assigning a developer to fix a bug, or assigning a code reviewer to a pull request. Most task assignment techniques in the literature build and evaluate their models based on datasets collected from real projects. The techniques invariably presume that these datasets reliably represent the “ground truth”. In a project dataset used to build an automated task assignment system, the recommended assignee for the task is usually assumed to be the best assignee for that task. However, in practice, the task assignee may not be the best possible task assignee, or even a sufficiently qualified one.
Objective:
We aim to clean up the ground truth by removing the samples that are potentially problematic or suspect with the assumption that removing such samples would reduce any systematic labeling bias in the dataset and lead to 
performance improvements
.
Method:
We devised a debiasing method to detect potentially problematic samples in task assignment datasets. We then evaluated the method’s impact on the performance of seven task assignment techniques by comparing the Mean Reciprocal Rank (MRR) scores before and after debiasing. We used two different task assignment applications for this purpose: Code Reviewer Recommendation (CRR) and Bug Assignment (BA).
Results:
In the CRR application, we achieved an average MRR improvement of 18.17% for the three learning-based techniques tested on two datasets. No significant improvements were observed for the two optimization-based techniques tested on the same datasets. In the BA application, we achieved a similar average MRR improvement of 18.40% for the two learning-based techniques tested on four different datasets.
Conclusion:
Debiasing the ground truth data by removing suspect samples can help improve the performance of learning-based techniques in software task assignment applications.",21 Mar 2025,9,"The debiasing method for task assignment datasets can significantly improve the performance of learning-based techniques, which can be beneficial for startups relying on automated task assignment systems."
https://www.sciencedirect.com/science/article/pii/S095058492200091X,The state of the art in measurement-based experiments on the mobile web,September 2022,Information and Software Technology,"Measurement-based experiment, Mobile web, Systematic mapping study",Omar=de Munk: o.de.munk@student.vu.nl; Gian Luca=Scoccia: gianluca.scoccia@univaq.it; Ivano=Malavolta: i.malavolta@vu.nl,"Abstract
Context:
Nowadays the majority of all worldwide Web traffic comes from mobile devices, as we tend to primarily rely on the browsers installed on our smartphones and tablets (e.g., Chrome for 
Android
, Safari for iOS) for accessing 
online services
. A market of such a large scale leads to an extremely fierce competition, where it is of 
paramount importance
 that the developed mobile Web apps are of high quality, e.g., in terms of performance, energy consumption, security, usability. In order to objectively assess the quality of mobile Web apps, practitioners and researchers are conducting experiments based on the measurement of run-time metrics such as 
battery discharge
, CPU and memory usage, number and type of network requests, etc.
Objective:
The objective of this work is to identify, classify, and evaluate the state of the art of conducting measurement-based experiments on the mobile Web. Specifically, we focus on (i) which metrics are employed during experimentation, how they are measured, and how they are analyzed; (ii) the platforms chosen to run the experiments; (iii) what subjects are used; (iv) the used tools and environments under which the experiments are run.
Method:
We apply the 
systematic mapping
 methodology. Starting from a search process that identified 786 potentially relevant studies, we selected a set of 33 primary studies following a rigorous 
selection procedure
. We defined and applied a classification framework to them to extract data and gather relevant insights.
Results:
This work contributes with (i) a classification framework for measurement-based experiments on the mobile Web; (ii) a systematic map of current research on the topic; (iii) a discussion of emergent findings and challenges, and resulting implications for future research.
Conclusion:
This study provides a rigorous and replicable map of the state of the art of conducting measurement-based experiments on the mobile Web. Its results can benefit researchers and practitioners by presenting common techniques, empirical practices, and tools to properly conduct measurement-based experiments on the mobile Web.",21 Mar 2025,7,"The evaluation of measurement-based experiments on the mobile Web provides valuable insights for practitioners and researchers working on mobile Web apps, potentially improving the quality of apps developed by early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000891,A literature review on optimization techniques for adaptation planning in adaptive systems: State of the art and research directions,September 2022,Information and Software Technology,Not Found,Elia=Henrichs: elia.henrichs@uni-hohenheim.de; Veronika=Lesch: veronika.lesch@uni-wuerzburg.de; Martin=Straesser: martin.straesser@uni-wuerzburg.de; Samuel=Kounev: samuel.kounev@uni-wuerzburg.de; Christian=Krupitzer: christian.krupitzer@uni-hohenheim.de,"Abstract
Context:
Recent developments in modern IT systems including 
internet of things
, edge/fog computing, or cyber–physical systems support intelligent and seamless interaction between users and systems. This requires a reaction to changes in their environment or the system. Adaptive systems provide mechanisms for these reactions.
Objective:
To implement this functionality, several approaches for the planning of adaptations exist that rely on rules, 
utility functions
, or advanced techniques, such as 
machine learning
. As the adaptation space with possible options is often extensively huge, optimization techniques might support efficient determination of the adaptation space and identify the system’s optimal configuration. With this paper, we provide a 
systematic review
 of adaptation planning as the optimization target.
Method:
In this paper, we review which optimization techniques are applied for adaptation planning in adaptive systems using a systematic literature review approach.
Results:
We reviewed 115 paper in detail out of an initial search set of 9,588 papers. Our analysis reveals that learning techniques and 
genetic algorithms
 are by far dominant; in total, heuristics (anytime learning) are more frequently applied as exact algorithms. We observed that around 57% of the approaches target multi-objectiveness and around 30% integrate 
distributed optimization
. As last dimension, we focused on situation-awareness, which is only supported by two approaches.
Conclusion:
In this paper, we provide an overview of the current state of the art of approaches that rely on optimization techniques for planning adaptations in adaptive systems and further derive 
open research
 challenges, in particular regarding the integration of 
distributed optimization
 and situation-awareness.",21 Mar 2025,6,The systematic review of optimization techniques for planning adaptations in adaptive systems offers useful information but may have less immediate impact on European early-stage ventures compared to other abstracts.
https://www.sciencedirect.com/science/article/pii/S0950584922000854,Self-adaptive systems: A systematic literature review across categories and domains,August 2022,Information and Software Technology,Not Found,Terence=Wong: terence.wong@adelaide.edu.au; Markus=Wagner: markus.wagner@adelaide.edu.au; Christoph=Treude: christoph.treude@unimelb.edu.au,"Abstract
Context:
Championed by IBM’s vision of 
autonomic computing
 paper in 2003, the 
autonomic computing
 research field has seen increased research activity over the last 20 years. Several conferences (SEAMS, SASO, ICAC) and workshops (SISSY) have been established and have contributed to the 
autonomic computing
 
knowledge base
 in search of a new kind of system — a self-adaptive system (SAS). These systems are characterized by being context-aware and can act on that awareness. The actions carried out could be on the system or on the context (or environment). The underlying goal of a SAS is the sustained achievement of its goals despite changes in its environment.
Objective:
Despite a number of literature reviews on specific aspects of SASs ranging from their requirements to 
quality attributes
, we lack a systematic understanding of the current state of the art.
Method:
This paper contributes a 
systematic literature review
 into self-adaptive systems using the dblp computer science 
bibliography
 as a database. We filtered the records systematically in successive steps to arrive at 293 relevant papers. Each paper was critically analyzed and categorized into an attribute matrix. This matrix consisted of five categories, with each category having multiple attributes. The attributes of each paper, along with the summary of its contents formed the basis of the literature review that spanned 30 years (1990–2020).
Results:
We characterize the maturation process of the research area from theoretical papers over practical implementations to more holistic and generic approaches, frameworks, and exemplars, applied to areas such as networking, web services, and robotics, with much of the recent work focusing on 
IoT
 and 
IaaS
.
Conclusion:
While there is an ebb and flow of application domains, domains like bio-inspired approaches, security, and cyber–physical systems showed promise to grow heading into the 2020s.",21 Mar 2025,5,The systematic literature review on self-adaptive systems contributes to understanding the field's current state but may have limited direct impact on practical applications for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584922000660,Combining multiple granularity variability in a software product line approach for web engineering,August 2022,Information and Software Technology,"Annotations, Composition, Feature models, SPL, Variability, Web engineering",Jose-Miguel=Horcas: horcas@lcc.uma.es; Alejandro=Cortiñas: alejandro.cortinas@udc.es; Lidia=Fuentes: lff@lcc.uma.es; Miguel R.=Luaces: miguel.luaces@udc.es,"Abstract
Context:
Web engineering involves managing a high diversity of artifacts implemented in different languages and with different levels of 
granularity
. Technological companies usually implement variable artifacts of Software Product Lines (SPLs) using annotations, being reluctant to adopt hybrid, often complex, approaches combining composition and annotations despite their benefits.
Objective:
This paper proposes a combined approach to support fine and coarse-grained variability for web artifacts. The proposal allows web developers to continue using annotations to handle fine-grained variability for those artifacts whose variability is very difficult to implement with a composition-based approach, but obtaining the advantages of the composition-based approach for the coarse-grained variable artifacts.
Methods:
A combined approach based on feature modeling that integrates annotations into a generic composition-based approach. We propose the definition of compositional and annotative variation points with custom-defined semantics, which is resolved by a scaffolding-based derivation engine. The approach is evaluated on a real-world web-based SPL by applying a set of variability metrics, as well as discussing its quality criteria in comparison with annotations, compositional, and combined existing approaches.
Results:
Our approach effectively handles both fine and coarse-grained variability. The mapping between the feature model and the web artifacts promotes the traceability of the features and the uniformity of the variation points regardless of the 
granularity
 of the web artifacts.
Conclusions:
Using well-known techniques of SPLs from an architectural point of view, such as feature modeling, can improve the design and maintenance of variable web artifacts without the need of introducing complex approaches for implementing the underlying variability.",21 Mar 2025,7,The proposed combined approach for handling variability in web artifacts using feature modeling shows effectiveness and can benefit early-stage ventures by improving design and maintenance without introducing complex approaches.
https://www.sciencedirect.com/science/article/pii/S0950584922000763,Improving Stack Overflow question title generation with copying enhanced CodeBERT model and bi-modal information,August 2022,Information and Software Technology,Not Found,Fengji=Zhang: zhangfengji@whu.edu.cn; Xiao=Yu: xiaoyu@whut.edu.cn; Jacky=Keung: jacky.keung@cityu.edu.hk; Fuyang=Li: fyli@whut.edu.cn; Zhiwen=Xie: xiezhiwen@whu.edu.cn; Zhen=Yang: zhyang8-c@my.cityu.edu.hk; Caoyuan=Ma: macaoyuan@whu.edu.cn; Zhimin=Zhang: zhangzhimin@whu.edu.cn,"Abstract
Context:
Stack Overflow is very helpful for software developers who are seeking answers to programming problems. Previous studies have shown that a growing number of questions are of low quality and thus obtain less attention from potential answerers. Gao et al. proposed an LSTM-based model (i.e., BiLSTM-CC) to automatically generate question titles from the code snippets to improve the question quality. However, only using the code snippets in the question body cannot provide sufficient information for title generation, and LSTMs cannot capture the long-range dependencies between tokens.
Objective:
This paper proposes CCBERT, a 
deep learning
 based novel model to enhance the performance of question title generation by making full use of the bi-modal information of the entire question body.
Method:
CCBERT follows the encoder–decoder paradigm and uses CodeBERT to encode the question body into hidden representations, a stacked Transformer decoder to generate predicted tokens, and an additional copy attention layer to refine the output distribution. Both the encoder and decoder perform the multi-head self-attention operation to better capture the long-range dependencies. This paper builds a dataset containing around 200,000 high-quality questions filtered from the data officially published by Stack Overflow to verify the effectiveness of the CCBERT model.
Results:
CCBERT outperforms all the 
baseline models
 on the dataset. Experiments on both code-only and low-resource datasets show the superiority of CCBERT with less 
performance degradation
. The human evaluation also shows the excellent performance of CCBERT concerning both readability and correlation criteria.
Conclusion:
CCBERT is capable of automatically capturing the bi-modal 
semantic information
 from the entire question body and 
parsing
 the long-range dependencies to achieve better performance. Therefore, CCBERT is an effective approach for generating Stack Overflow question titles.",21 Mar 2025,9,"CCBERT model outperforms baseline models, showing superior performance in question title generation and human evaluation criteria. This can bring significant value to startups by enhancing the quality of questions on platforms like Stack Overflow."
https://www.sciencedirect.com/science/article/pii/S0950584922000775,How ReadMe files are structured in open source Java projects,August 2022,Information and Software Technology,Not Found,Yuyang=Liu: yuyang@cs.toronto.edu; Ehsan=Noei: e.noei@utoronto.ca; Kelly=Lyons: kelly.lyons@utoronto.ca,"Abstract
Context:
Recent studies on 
open source platforms
, such as GitHub, provide insights into how developers engage with software artifacts such as 
ReadMe
 files. Since 
ReadMe
 files are usually the first item users interact with in a repository, it is important that 
ReadMe
 files provide users with the information needed to engage with the corresponding repository.
Objective:
We investigate and compare 
ReadMe
 files of open source Java projects on GitHub in order to (i) determine the degree to which 
ReadMe
 files are aligned with the official guidelines, (ii) identify the common patterns in the structure of 
ReadMe
 files, and (iii) characterize the relationship between 
ReadMe
 file structure and popularity of associated repositories.
Method:
We apply statistical analyzes and 
clustering methods
 on 14,901 Java repositories to identify structural patterns of 
ReadMe
 files and the relationship of 
ReadMe
 file structure to repository stars.
Results:
While the majority of 
ReadMe
 files do not align with the GitHub guidelines, repositories whose 
ReadMe
 files follow the GitHub guidelines tend to receive more stars. We identify 32 clusters of common 
ReadMe
 file structures and the features associated with each structure. We show that projects with 
ReadMe
 files that contain project name, usage information, installation instructions, license information, code snippets, or links to images tend to get more stars.
Conclusion:
ReadMe
 file structure shares a statistically significant relationship with popularity as measured by number of stars; however, the most frequent 
ReadMe
 file structures are associated with less popular repositories on GitHub. Our findings can be used to understand the importance of 
ReadMe
 file structures and their relationship with popularity.",21 Mar 2025,8,The analysis of ReadMe files on GitHub repositories and their correlation with popularity can provide valuable insights for startups to improve their project documentation structure and increase visibility. This study offers practical implications for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584922000672,An empirical study of emoji use in software development communication,August 2022,Information and Software Technology,Not Found,Shiyue=Rong: shiyuer@uci.edu; Weisheng=Wang: weishew@uci.edu; Umme Ayda=Mannan: mannanu@oregonstate.edu; Eduardo Santana=de Almeida: esa@rise.com.br; Shurui=Zhou: shuruiz@ece.utoronto.ca; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
Similar to 
social media platforms
, people use emojis in software development related communication to enrich the context and convey additional emotion. With the increasing emoji use in software development-related communication, it has become important to understand why software developers are using emojis and their impact.
Objective:
Gaining a 
deeper understanding
 is essential because the intention of emoji usage might be affected by the demographics and experience of developers; also, frequency and the distribution of emoji usage might change depending on the activity, stage of the development, and nature of the conversation, etc.
Methods:
We present a large-scale empirical study on the intention of emoji usage conducted on 2,712 
Open Source Software
 (OSS) projects. We build a 
machine learning
 model to automate classifying the intentions behind emoji usage in 39,980 posts. We also surveyed 60 open-source software developers from 17 countries to understand developers’ perceptions of why and when emojis are used.
Results:
Our results show that we can classify the intention of emoji usage with high accuracy (AUC of 0.97). In addition, the results indicate that developers use emoji for varying intentions, and emoji usage intention changes throughout a conversation.
Conclusion:
Our study opens a new avenue in 
Software Engineering
 research related to automatically identifying the intention of the emoji use that can help improve the communication efficiency and help project maintainers monitor and ensure the quality of communication. Another thread of future research could look into what intentions of emoji usage or what kind of emojis are more likely to attract users and how that is associated with emoji usage diffusion in different levels (threads, projects, etc.)",21 Mar 2025,6,"The study on emoji usage in software development communication provides interesting insights, but the direct impact on startups might be limited. However, understanding emoji usage intentions can still help improve communication efficiency in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000787,Preventing technical debt with the TAP framework for Technical Debt Aware Management,August 2022,Information and Software Technology,Not Found,Marion=Wiese: marion.wiese@uni-hamburg.de; Paula=Rachow: paula.rachow@uni-hamburg.de; Matthias=Riebisch: matthias.riebisch@uni-hamburg.de; Julian=Schwarze: schwarze.julian@guj.de,"Abstract
Context:
Technical Debt (TD) is a metaphor for technical problems that are not visible to users and customers but hinder developers in their work, making future changes more difficult. TD is often incurred due to tight project deadlines and can make future changes more costly or impossible. Project Management usually focuses on customer benefits and pays less attention to their IT systems’ internal quality. TD prevention should be preferred over 
TD repayment
 because subsequent refactoring and re-engineering are expensive.
Objective:
This paper evaluates a framework focusing on both TD prevention and 
TD repayment
 in the context of agile-managed projects. The framework was developed and applied in an IT unit of a publishing house. The unique contribution of this framework is the integration of TD management into project management.
Method:
The evaluation was performed as a comparative 
case study
 based on ticket statistics and two structured surveys. The surveys were conducted in the observed IT unit using the framework and a comparison unit not using the framework. The first survey targeted team members, the second one IT managers.
Results:
The evaluation shows that in this IT unit the TAP framework led to a raised awareness for the incurrence of TD. Decisions to incur TD are intentional, and TD is repaid timelier. Unintentional TD incurred by unconscious decisions is prevented. Furthermore, better communication and better planning of the project pipeline can be observed.
Conclusion:
We provide an insight into practitioners’ ways to identify, monitor, prevent and repay TD. The presented framework includes a feasible method for TD prevention despite tight timelines by making TD repayment part of project management.",21 Mar 2025,8,"The TAP framework for managing Technical Debt in agile projects shows positive results in raising awareness, preventing unintentional TD, and improving communication and planning. This framework could offer practical benefits for European early-stage ventures to manage technical debt effectively."
https://www.sciencedirect.com/science/article/pii/S0950584922000039,Multi-objective integer programming approaches to Next Release Problem — Enhancing exact methods for finding whole pareto front,July 2022,Information and Software Technology,Not Found,Shi=Dong: dongshi@mail.ustc.edu.cn; Yinxing=Xue: yxxue@ustc.edu.cn; Sjaak=Brinkkemper: s.brinkkemper@uu.nl,"Abstract
Context:
Project planning is a crucial part of software engineering, it involves selecting requirements to develop for the next release. How to make a good release plan is an optimization problem to maximize the goal of revenue under the condition of cost, time, or other aspects, namely Next Release Problem (NRP). 
Genetic
 and exact algorithms are used since it was proposed.
Objective:
We model NRP as bi-objective (revenue, cost) and tri-objective (revenue, cost, urgency) form, and investigate whether exact methods could solve bi-objective and tri-objective instances more efficiently.
Methods:
The state-of-art integer linear programming (ILP) approach to the bi-objective NRP is 
ε
-constraint for finding all non-dominate solutions. To improve its efficiency, we employ CWMOIP (Constrained Weighted Multi-Objective Integer Programming) and I-EC (improved 
ε
-constraint) for solving bi-objective instances. In tri-objective form, we introduce SolRep, an ILP method that optimizes the 
reference points
 from sampling, for finding solutions subset within a short time. NSGA-II is implemented as the 
evolutionary algorithm
 for the comparison with former methods and it adopts the seeding mechanism.
Results
: I-EC can find all non-dominated solutions with better performance than both 
ε
-constraint and CWMOIP on all instances except for one. I-EC reduces solving time by 19.7% (large instances) and 91.5% (small instances) on average separately compared with 
ε
-constraint. SolRep can find evenly distributed solutions and exceed NSGA-II illustrated by several indicators (such as HyperVolume) on tri-objective instances. And each method has its merit in the aspect of speed and number of the solutions.
Conclusion:
(1) The I-EC can solve all non-dominated solutions with better performance than the state-of-art exact method. (2) SolRep solves large tri-objective instances with more non-dominated solutions and solves small instances with less time compared with seeded NSGA-II. (3) Seeded NSGA-II shows its advantage on the number of non-dominated solutions on smaller tri-objective instances.",21 Mar 2025,8,"The research addresses a practical problem in software engineering with a focus on revenue optimization, cost reduction, and efficiency in release planning. The use of state-of-the-art algorithms and methods shows potential for practical application in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000556,A DQN-based agent for automatic software refactoring,July 2022,Information and Software Technology,Not Found,Hamidreza=Ahmadi: h_ahmadi@comp.iust.ac.ir; Mehrdad=Ashtiani: m_ashtiani@iust.ac.ir; Mohammad Abdollahi=Azgomi: azgomi@iust.ac.ir; Raana=Saheb-Nassagh: r_sahebnassagh@comp.iust.ac.ir,"Abstract
Context
Nowadays, technical debt has become a very important issue in 
software project management
. The main mechanism to repay this debt is through refactoring. Refactoring software projects usually comes at a high cost. As a result, researchers have always looked for ways to minimize this cost, and a good potential candidate to reduce the cost of a process is to automate it.
Objective
One of the automatic software refactoring methods that recently has received a lot of attention is based on search-based software engineering (SBSE) methods. Although because of comprehensiveness and versatility 
SBSE
 is considered an appropriate method for automatic refactoring, it has its downsides, the most important of which are the uncertainty of the results and the exponential execution time.
Method
In this research, a solution is proposed inspired by search-based refactoring while taking advantage of exploitation in 
reinforcement learning
 techniques. This work aims to solve the uncertainty problems and execution time for large programs. In the proposed approach, the problem of uncertainty is solved by targeting the selection of refactoring actions used in the search-based approach. Also, due to the reduction of the dependency between the choice of the appropriate refactoring and its execution time, the time problem in large software refactoring has been greatly improved.
Results
Amongst the performed evaluations and specifically for the refactoring of the largest 
case study
, the proposed approach managed to increase the accuracy to more than twice of the 
SBSE
 refactoring approaches, while reducing the execution time of refactoring by more than 98%.
Conclusion
The results of the tests show that with increasing the volume and size of the software, the performance of the proposed approach also improves compared to the methods based on SBSE, both in terms of reducing technical debt and speeding up the 
refactoring process
.",21 Mar 2025,9,"The study offers a solution to automate software refactoring, reducing costs and time significantly. The proposed approach shows promising results in terms of accuracy and speed, which could benefit European early-stage ventures looking to minimize technical debt."
https://www.sciencedirect.com/science/article/pii/S0950584922000374,Investigating replication challenges through multiple replications of an experiment,July 2022,Information and Software Technology,Not Found,Daniel Amador=dos Santos: daniel.amador@email.com; Eduardo Santana=de Almeida: esa@dcc.ufba.br; Iftekhar=Ahmed: iftekha@uci.edu,"Abstract
Context:
As Empirical 
Software Engineering
 grows in maturity and number of publications, more replications are needed to provide a solid grounding to the evidence found through prior research. However, replication studies are scarce in general and some topics suffer more than others with such scarcity. On top, the challenges associated with replicating empirical studies are not well understood.
Objective:
In this study, we aim to fill this gap by investigating difficulties emerging when replicating an experiment.
Method:
We used participants with distinct backgrounds to play the role of a research group attempting to replicate an experimental study addressing Highly-Configurable Systems. Seven external close replications in total were performed. After obtaining the quantitative replication results, a 
focus group session
 was applied to each group inquiring about the replication experience. We used the grounded theory’s constant comparison method for the qualitative analysis.
Results:
We have seen in our study that, in the replications performed, most results hold when comparing them with the baseline. However, the participants reported many difficulties in replicating the original study, mostly related to the lack of clarity of the instructions and the presence of defects on replication artifacts. Based on our findings, we provide recommendations that can help mitigate the problems reported.
Conclusions:
The quality of replication artifacts and the lack of clear instructions might impact an experiment replication. We advocate having good quality replication instructions and well-prepared laboratory packages to foster and enable researchers to perform better replications.",21 Mar 2025,6,"The investigation on replicating experiments in Empirical Software Engineering provides valuable insights, but the practical implications for startups may be limited. Still, the recommendations for clearer instructions and quality replication artifacts can be helpful for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000532,API-m-FAMM: A focus area maturity model for API Management,July 2022,Information and Software Technology,"API Management, Maturity model, Focus area maturity models",Michiel=Overeem: michiel.overeem@afas.nl; Max=Mathijssen: max.mathijssen@afas.nl; Slinger=Jansen: slinger.jansen@uu.nl,"Abstract
Context:
Organizations are increasingly connecting 
software applications
 using 
Application Programming Interfaces
 (APIs) to share data, services, functionality, and even complete business processes. However, the creation and management of APIs is non-trivial. Aspects such as traffic management, community engagement, documentation, and version management are often rushed afterthoughts.
Objective:
In this research, we present and evaluate a focus area maturity model for API Management (API-m-FAMM). A focus area maturity model can be used to establish the maturity level of an organization in a specific functional domain described through a number of areas. The API-m-FAMM addresses the areas 
Lifecycle Management
, Security, Performance, Observability, Community, and Commercial.
Method:
The model is constructed using established methods for the design of a focus area maturity model. It is grounded in literature and practice, and was developed and evaluated through a 
systematic literature Review
, eleven expert interviews, and five 
case studies
 at software producing organizations.
Result:
The model is described in detail, and its application is illustrated by six 
case studies
.
Conclusions:
The evaluations are reported on, and show that the API-m-FAMM is an efficient tool for aiding organizations in gaining a better understanding of their current implementation of API management practices, and provides them with guidance towards higher levels of maturity. The detailed description of the construction of the API-m-FAMM gives researchers an example to further support the available methodologies, specifically how to combine design science research with these methodologies. Additionally, this study’s unique case study design shows that maturity models can be successfully deployed in practice with minimal involvement of researchers. The focus area maturity model for API Management is maintained on 
www.maturitymodels.org
, allowing practitioners to benefit from its useful insights.",21 Mar 2025,7,"The focus area maturity model for API Management offers a structured approach for organizations to improve their API practices. While the model provides guidance for maturity, its direct impact on early-stage ventures may vary, but the detailed description can be beneficial for those looking to enhance their API management."
https://www.sciencedirect.com/science/article/pii/S0950584922000544,Aligned metric representation based balanced multiset ensemble learning for heterogeneous defect prediction,July 2022,Information and Software Technology,Not Found,Haowen=Chen: hwc_zzu@126.com; Yuming=Zhou: zhouyuming@nju.edu.cn; Bing=Li: bingli@whu.edu.cn; Baowen=Xu: bwxu@nju.edu.cn,"Abstract
Context:
Heterogeneous 
defect prediction
 (HDP) refers to the 
defect prediction
 across projects with different metrics. Most existing HDP methods map source and target data into a common metric space where each dimension has no actual meaning, which weakens their 
interpretability
. Besides, HDP always suffers from the 
class imbalance problem
.
Objective:
For deficiencies of current HDP methods, we intend to propose a novel HDP approach that can reduce the heterogeneity of source and target data and deal with 
imbalanced data
 while retaining the actual meaning for each dimension of constructed common metric space.
Method:
We propose an Aligned Metric Representation based Balanced Multiset 
Ensemble learning
 (BMEL+ AMR) approach for HDP. AMR consists of shared, source-specific, and target-specific metrics. It is built by learning the translation from shared to specific metrics and reducing the distribution difference. To deal with 
imbalanced data
, we design BMEL that constructs multiple balanced subsets for source data and produces an aggregated classifier for predicting labels of target data.
Result:
Experimental results on 22 public projects indicate that (1) among all competing methods, BMEL+AMR achieves the best performance on all indicators except 
Popt
, followed by AMR; (2) compared with AMR, the introduction of BMEL improves the performance on non-effort-aware indicators statistically significantly except 
F1-score
; compared with BMEL, the introduction of AMR improves the performance throughout all indicators statistically significantly.
Conclusion:
BMEL+AMR can effectively improve HDP performance by eliminating heterogeneity and dealing with imbalanced data, and AMR is helpful to explain the prediction model.",21 Mar 2025,8,"The proposed HDP approach addresses issues of heterogeneity and class imbalance, providing a potential solution for improved defect prediction across projects. The practical implications for startups lie in more accurate defect prediction, which can be crucial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000507,"SE
M: A model for software effort estimation using pre-trained embedding models",July 2022,Information and Software Technology,Not Found,Eliane Maria=De Bortoli Fávero: elianedb@utfpr.edu.br; Dalcimar=Casanova: dalcimar@utfpr.edu.br; Andrey Ricardo=Pimentel: andrey@inf.ufpr.br,"Abstract
Context:
Software effort estimation from requirements texts, presents many challenges, mainly in getting viable features to infer effort. The most recent 
Natural Language Processing
 (NLP) initiatives for this purpose apply context-less embedding models, which are often not sufficient to adequately discriminate each analyzed sentence. Contextualized pre-trained embedding models have emerged quite recently and have been shown to be far more effective than context-less models in representing textual features.
Objective:
This paper proposes evaluating the effectiveness of pre-trained embedding models, to explore a more effective technique for representing textual requirements, which are used to infer effort estimates by analogy.
Method:
Generic pre-trained models went through a fine-tuning process for both approaches — context-less and contextualized. The generated models were used as input in the applied 
deep learning
 architecture, with linear output. The results were very promising, realizing that contextualized pre-trained embedding models can be used to estimate software effort based only on requirements texts.
Results:
We highlight the results obtained to apply the contextualized pre-trained model 
BERT
 with fine-tuning, applied in a single repository containing different projects, whose 
Mean Absolute Error
 (MAE) value is 4.25 and the standard deviation is only 0.17. This represents a result very positive when compared to similar works.
Conclusion:
The main advantages of the proposed estimation method are reliability, the possibility of generalization, speed, and low computational cost. Such advantages are provided by the fine-tuning process, enabling to infer effort estimation for new or existing requirements.",21 Mar 2025,9,The proposed method of using contextualized pre-trained embedding models for software effort estimation shows promising results and can have a significant impact on improving the accuracy and efficiency of estimating effort for startups.
https://www.sciencedirect.com/science/article/pii/S095058492200060X,Locality-based security bug report identification via active learning,July 2022,Information and Software Technology,Not Found,Xiuting=Ge: dg20320002@smail.nju.edu.cn; Chunrong=Fang: fangchunrong@nju.edu.cn; Meiyuan=Qian: mf20320109@smail.nju.edu.cn; Yu=Ge: 920397425@qq.com; Mingshuang=Qing: qingms@mails.swust.edu.cn,"Abstract
Context:
Security 
bug report
 (SBR) identification is a crucial way to eliminate security-critical vulnerabilities during software development.
Objective:
In recent years, many approaches have utilized supervised machine learning (SML) techniques in the SBR identification. However, such approaches often require a large number of labelled bug reports, which are often hard to obtain in practice. Active learning is a potential approach to reducing the manual labelling cost while maintaining 
good performance
. Nevertheless, the existing active learning-based SBR identification approach still yields poor performance due to ignoring the locality in bug reports.
Method:
To address the above problems, we propose locality-based SBR identification via active learning. Our approach recommends a small part of instances based on locality in bug reports, asks for their labels, and learns the SBR classifier. Specifically, our approach relies on the locality to construct the initial training set, which is designed to address how to start during active learning. Moreover, our approach applies the locality into the query process, which is designed to improve which instance should be queried next during active learning.
Result:
We conduct experiments on large-scale bug reports (nearly 125K) from six real-world projects. In comparison with three state-of-the-art SML-based and active learning-based SBR identification approaches, our approach can obtain the maximum values of F-Measure (0.8176) and AUC (0.8631). Moreover, our approach requires 16.60% to 71.40% of all bug reports when achieving the 
optimal performance
 in these six projects, which improves three approaches from 9.82% to 64.19% on average.
Conclusion:
As shown from the experimental results, our approach can be more effective and efficient to identify SBRs than the existing approaches.",21 Mar 2025,8,"The locality-based SBR identification approach using active learning shows improved performance and efficiency in identifying security bugs, which can benefit startups in ensuring the security of their software products."
https://www.sciencedirect.com/science/article/pii/S0950584922000611,Mind the product owner: An action research project into agile release planning,July 2022,Information and Software Technology,"Canonical action research, Agile release planning, Product owner",Konsta=Kantola: konsta.kantola@finago.com; Jari=Vanhanen: Not Found; Jussi=Tolvanen: Not Found,"Abstract
Context:
This paper studies agile release planning in a software development organization with 13 development teams. It is important for software development organizations to be able to plan work in an efficient way that supports development work.
Objective:
The research aims to understand issues within agile release planning in the studied organization, and to make some improvement to the agile release planning practices there.
Method:
The study followed canonical 
action research
 methodology completing one cycle of diagnosis, action planning, intervention, evaluation, and learning. Qualitative methods were used during these phases to identify preliminary issues, to support the choice of action, and the evaluation of those actions.
Results:
The research identified issues of strain on the role of Product Owners. Sources of strain in the organization include changing priorities, the effort required to build up domain competence for new projects, and external pressure to push out new features. Additionally, there was difficulty for people participating in agile release planning to suggest improvements to the used practices due in part to the complexity and scale of planning practices in a multi-team development organization. The actions taken as part of the research provided ways for Product Owners to share knowledge between themselves, to better affect the working practices in the organization, and promoted a sense of team spirit between the Product Owners.
Conclusion:
Organizations should be mindful of their Product Owners when looking at their release planning practices. Problems for Product Owners are problems in planning for the whole organization. Having an active, collective, and structured channel for continuous improvement for Product Owners can help drive improvements to agile release planning.",21 Mar 2025,7,"The research on agile release planning provides insights into improving planning practices for development teams, which can be valuable for startups in enhancing their release planning efficiency and effectiveness."
https://www.sciencedirect.com/science/article/pii/S0950584922000623,A Systematic Literature Review on prioritizing software test cases using Markov chains,July 2022,Information and Software Technology,Not Found,Gerson=Barbosa: gerson.barbosa@unesp.br; Érica Ferreira=de Souza: ericasouza@utfpr.edu.br; Luciana Brasil Rebelo=dos Santos: lurebelo@ifsp.edu.br; Marlon=da Silva: marlon.silva@ifsp.edu.br; Juliana Marino=Balera: juliana.balera@inpe.br; Nandamudi Lankalapalli=Vijaykumar: vijay.nl@inpe.br,"Abstract
Context:
Software Testing is a costly activity since the size of the test case set tends to increase as the construction of the software evolves. Test Case Prioritization (TCP) can reduce the effort and cost of software testing. TCP is an activity where a subset of the existing test cases is selected in order to maximize the possibility of finding defects. On the other hand, 
Markov Chains
 representing a reactive system, when solved, can present the occupation time of each of their states. The idea is to use such information and associate priority to those test cases that consist of states with the highest probabilities.
Objective:
The objective of this paper is to conduct a survey to identify and understand key initiatives for using 
Markov Chains
 in TCP. Aspects such as approaches, developed techniques, programming languages, analytical and simulation results, and validation tests are investigated.
Methods:
A Systematic Literature Review (SLR) was conducted considering studies published up to July 2021 from five different databases to answer the three research questions.
Results:
From SLR, we identified 480 studies addressing 
Markov Chains
 in TCP that have been reviewed in order to extract relevant information on a set of research questions.
Conclusion:
The final 12 studies analyzed use 
Markov Chains
 at some stage of test case prioritization in a distinct way, that is, we found that there is no strong relationship between any of the studies, not only on how the technique was used but also in the context of the application. Concerning the fields of application of this subject, 6 forms of approach were found: Controlled Markov Chain, Usage Model, Model-Based Test, 
Regression Test
, Statistical Test, and 
Random Test
. This demonstrates the versatility and robustness of the tool. A large part of the studies developed some prioritization tool, being its validation done in some cases analytically and in others numerically, such as: Measure of the software specification, Optimal Test Transition Probabilities, Adaptive Software Testing, Automatic Prioritization, 
Ant Colony Optimization
, Model Driven approach, and Monte Carlo Random Testing.",21 Mar 2025,6,"The survey on using Markov Chains in Test Case Prioritization offers valuable insights, but the applicability and impact on startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000635,Continuous verification of system of systems with collaborative MAPE-K pattern and probability model slicing,July 2022,Information and Software Technology,Not Found,Jiyoung=Song: jysong@se.kaist.ac.kr; Jeehoon=Kang: Not Found; Sangwon=Hyun: Not Found; Eunkyoung=Jee: Not Found; Doo-Hwan=Bae: Not Found,"Abstract
The phenomenon of cooperation among independent systems to achieve common goals has been growing. In this regard, the concept of 
system of systems
 (SoS), wherein numerous independent systems cooperate with each other, has been proposed. The key characteristic of an SoS is the 
operational and managerial (O/M) independence
 of each 
constituent system
 (CS). Each CS of a 
collaborative SoS
 with high O/M independence provides different levels of 
internal-knowledge
 sharing and is entitled to voluntary participation in the SoS (
i.e.
, 
dynamic reconfiguration
). To increase goal-achievement rate, we need to verify SoS considering the knowledge-sharing and 
dynamic reconfiguration
 constraints.
The 
dynamic reconfiguration
 of SoSs can be managed using 
continuous verification
, which involves environment monitoring, modeling systems for operation in changing environments, and verifying the model runtimes. However, O/M independence introduces the following challenges: (1) the low knowledge-sharing level causes inaccurate modeling, which leads to inaccurate verification results, and (2) dynamic reconfiguration requires frequent re-verification at runtime, which incurs high verification costs.
In this paper, we propose a continuous-verification-of-SoS (CVSoS) approach to solve these two challenges. To address the low knowledge-sharing level, we propose the 
collaborative MAPE-K
 pattern. The key to collaborative MAPE-K is the retrieval of knowledge from the other collaborating CSs. To address dynamic reconfiguration, we propose a new slicing algorithm for SoS models. This algorithm promotes 
synchronization
 dependence
, which is essential for representing interactions between CSs. Furthermore, we demonstrate the accuracy of this algorithm.
We evaluated CVSoS across multiple SoS domains, which revealed that the SoS goal-achievement rate increases by up to 64% using the collaborative MAPE-K pattern and that slicing the benchmark and SoS models improved the verification time by an average of 67%.",21 Mar 2025,5,"The proposed continuous-verification-of-SoS approach addresses important challenges in system of systems verification, but the practical value for startups may be less immediate compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000593,The practical roles of enterprise architecture artifacts: A classification and relationship,July 2022,Information and Software Technology,Not Found,Svyatoslav=Kotusev: kotusev@kotusev.com; Sherah=Kurnia: Not Found; Rod=Dilnutt: Not Found,"Abstract
Context
Enterprise architecture (EA) is a description of an enterprise from an integrated business and IT perspective. EA is typically defined as a comprehensive blueprint of an organization covering its business, data, applications and technology domains and consisting of diverse EA artifacts. EA has numerous potential stakeholders and 
usage scenarios
 in organizations. However, the existing EA literature does not offer any consistent theories explaining the practical roles of individual EA artifacts and fails to explain how exactly different types of EA artifacts are used in practice.
Objective
This study intends to explore the roles of different EA artifacts in organizations and develop a generic descriptive theory explaining these roles. The theory purports to cover various properties of EA artifacts as well as the relationships between them.
Method
The research method of this study follows two consecutive phases: theory construction and theory validation. First, theory construction is based on the qualitative in-depth analysis of five case organizations with established EA practices. Next, theory validation includes confirmatory interviews with ten EA experts.
Results
This study develops a descriptive theory explaining the roles of different EA artifacts in an EA practice. The resulting theory defines six general types of EA artifacts (Considerations, Standards, Visions, Landscapes, Outlines and Designs, CSVLOD) and explains their type-specific practical roles, including their 
informational contents
, typical usage, ensuing organizational benefits and interrelationships with each other.
Conclusions
This study presents the first systematic theory describing the usage of EA artifacts in organizations. Our theory facilitates better theoretical understanding of the concept of EA and also provides evidence-based solutions to the commonly reported practical problems with EA. This study suggests that the EA research community should focus on studying individual EA artifacts instead of studying EA in general and calls for further research on EA artifacts and their usage as part of EA practices.",21 Mar 2025,7,"The study offers a systematic theory explaining the roles of EA artifacts in organizations, providing evidence-based solutions to common problems. It can enhance theoretical understanding and practical implementation of EA."
https://www.sciencedirect.com/science/article/pii/S0950584922000647,CASMS: Combining clustering with attention semantic model for identifying security bug reports,July 2022,Information and Software Technology,Not Found,Xiaoxue=Ma: xiaoxuema3-c@my.cityu.edu.hk; Jacky=Keung: jacky.keung@cityu.edu.hk; Zhen=Yang: zhyang8-c@my.cityu.edu.hk; Xiao=Yu: xiaoyu@whut.edu.cn; Yishu=Li: yishuli5-c@my.cityu.edu.hk; Hao=Zhang: hzhang339-c@my.cityu.edu.hk,"Abstract
Context:
Inappropriate public disclosure of security 
bug reports
 (SBRs) is likely to attract malicious attackers to invade software systems; hence being able to detect SBRs has become increasingly important for software maintenance. Due to the 
class imbalance problem
 that the number of non-security 
bug reports
 (NSBRs) exceeds the number of SBRs, insufficient training information, and weak performance robustness, the existing techniques for identifying SBRs are still less than desirable.
Objective:
This prompted us to overcome the challenges of the most advanced SBR detection methods.
Method:
In this work, we propose the CASMS approach to efficiently alleviate the imbalance problem and predict bug reports. CASMS first converts bug reports into weighted 
word embeddings
 based on 
t
f
−
i
d
f
 and 
w
o
r
d
2
v
e
c
 techniques. Unlike the previous studies selecting the NSBRs that are the most dissimilar to SBRs, CASMS then automatically finds a certain number of diverse NSBRs via the Elbow method and 
k
-means clustering algorithm. Finally, the selected NSBRs and all SBRs train an effective Attention CNN–BLSTM model to extract contextual and sequential information.
Results:
The experimental results have shown that CASMS is superior to the three baselines (i.e., FARSEC, SMOTUNED, and LTRWES) in assessing the overall performance (
g
-measure) and correctly identifying SBRs (
recall
), with improvements of 4.09%–24.26% and 10.33%–36.24%, respectively. The best results are easily obtained under the limited ratio ranges of the two-class training set (1:1 to 3:1), with around 20 experiments for each project. By evaluating the robustness of CASMS via the standard deviation indicator, CASMS is more stable than LTRWES.
Conclusion:
Overall, CASMS can alleviate the data imbalance problem and extract more semantic information to improve performance and robustness. Therefore, CASMS is recommended as a practical approach for identifying SBRs.",21 Mar 2025,9,The CASMS approach addresses a crucial problem of security bug detection with superior performance compared to existing methods. The practical implications of this research are high for software maintenance and security.
https://www.sciencedirect.com/science/article/pii/S0950584922000659,Successful combination of database search and snowballing for identification of primary studies in systematic literature studies,July 2022,Information and Software Technology,"Systematic literature reviews, Hybrid search, Snowballing, Scopus",Claes=Wohlin: claes.wohlin@bth.se; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Katia=Romero Felizardo: katiascannavino@utfpr.edu.br; Emilia=Mendes: emilia.mendes@bth.se,"Abstract
Background:
A good search strategy is essential for a successful systematic literature study. Historically, database searches have been the norm, which was later complemented with snowball searches. Our conjecture is that we can perform even better searches if combining these two search approaches, referred to as a hybrid search strategy.
Objective:
Our main objective was to compare and evaluate a hybrid search strategy. Furthermore, we compared four alternative hybrid search strategies to assess whether we could identify more cost-efficient ways of searching for relevant primary studies.
Methods:
To compare and evaluate the hybrid search strategy, we replicated the search procedure in a 
systematic literature review
 (SLR) on industry–academia collaboration in 
software engineering
. The SLR used a more “traditional” approach to searching for relevant articles for an SLR, while our replication was executed using a hybrid search strategy.
Results:
In our evaluation, the hybrid search strategy was superior in identifying relevant primary studies. It identified 30% more primary studies and even more studies when focusing only on peer-reviewed articles. To embrace individual viewpoints when assessing research articles and minimise the risk of missing primary studies, we introduced two new concepts, 
wild cards
 and 
borderline articles
, when performing systematic literature studies.
Conclusions:
The hybrid search strategy is a strong contender for being used when performing systematic literature studies. Furthermore, alternative hybrid search strategies may be viable if selected wisely in relation to the start set for snowballing. Finally, the two new concepts were judged as essential to cater for different individual judgements and to minimise the risk of excluding primary studies that ought to be included.",21 Mar 2025,8,"The hybrid search strategy proposed in this study shows significant improvement in identifying relevant primary studies, offering a more cost-efficient and effective search approach for systematic literature studies."
https://www.sciencedirect.com/science/article/pii/S0950584922000581,Quantum computing challenges in the software industry. A fuzzy AHP-based approach,July 2022,Information and Software Technology,"Fuzzy analytic hierarchy process (F-AHP), Software process automation, Multiple-criteria decision-making (MCDM), Quantum software requirement, Quantum computing",Usama=Awan: usama.awan@lut.fi; Lea=Hannola: Not Found; Anushree=Tandon: Not Found; Raman Kumar=Goyal: Not Found; Amandeep=Dhir: Not Found,"Abstract
Context
The current technology revolution has posed unexpected challenges for the software 
industry
. In recent years, the field of 
quantum computing
 (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise 
software engineering
. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented.
Objective
The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy 
analytic hierarchy process
 (F-AHP).
Method
First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers.
Results
The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC.
Conclusion
By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges.",21 Mar 2025,6,"The study identifies critical challenges in the adoption of quantum computing technologies in the software industry using F-AHP. The implications for academia and practitioners are significant, but the practical impact may be more limited."
https://www.sciencedirect.com/science/article/pii/S0950584922000568,Toward successful DevSecOps in software development organizations: A decision-making framework,July 2022,Information and Software Technology,"DevOps, DevSecOps, Challenges, Multivocal literature review, Fuzzy analytical hierarchy process",Muhammad Azeem=Akbar: azeem.akbar@lut.fi; Kari=Smolander: kari.smolander@lut.fi; Sajjad=Mahmood: smahmood@kfupm.edu.sa; Ahmed=Alsanad: aasanad@ksu.edu.sa,"Abstract
Context
Development and Operations (DevOps) is a methodology that aims to establish collaboration between programmers and operators to automate the continuous delivery of new software to reduce the 
development life cycle
 and produce quality software. Development, Security, and Operations (DevSecOps) is developing the DevOps concept, which integrates security methods into a 
DevOps process
. DevSecOps is a software development process where security is built in to ensure application confidentiality, integrity, and availability.
Objective
This paper aims to identify and prioritize the challenges associated with implementing the DevSecOps process.
Method
We performed a multivocal literature review (MLR) and conducted a questionnaire-based survey to identify challenges associated with DevSecOps-based projects. Moreover, interpretive structure modeling (ISM) was applied to study the relationships among the core categories of the challenges. Finally, we used the fuzzy technique for 
order preference
 by similarity to an ideal solution (TOPSIS) to prioritize the identified challenges associated with DevSecOps projects.
Results
We identified 18 challenges for the DevSecOps process and mapped them to 10 core categories. The ISM results indicate that the “standards” category has the most decisive influence on the other nine core categories of the identified challenges. Moreover, the fuzzy TOPSIS indicates that “lack of secure coding standards,” “lack of automated testing tools for security in DevOps,” and “ignorance in static testing for security due to lack of knowledge” are the highest priority challenges for the DevSecOps paradigm.
Conclusion
Organizations using DevOps should consider the identified challenges in developing secure software.",21 Mar 2025,8,"The paper identifies and prioritizes challenges in implementing the DevSecOps process, offering insights for organizations to develop secure software. The use of MLR, ISM, and TOPSIS adds value to the research."
https://www.sciencedirect.com/science/article/pii/S0950584922000428,An end-to-end deep learning system for requirements classification using recurrent neural networks,July 2022,Information and Software Technology,Not Found,Osamah=AlDhafer: Not Found; Irfan=Ahmad: irfan.ahmad@kfupm.edu.sa; Sajjad=Mahmood: Not Found,"Abstract
Context:
Existing requirements 
classification approaches
 mainly use lexical and syntactical features to classify requirements using both traditional 
machine learning
 and 
deep learning
 approaches with promising results. However, the existing techniques depend on word and sentence structures and employ preprocessing and feature engineering techniques to classify requirements from textual natural language documents. Moreover, existing studies deal with requirements classification as binary or 
multiclass classification
 problems and not as multilabel classification, although a given requirement can belong to multiple classes at the same time.
Objective:
The objective of this study is to classify requirements into functional and different non-functional types with minimal preprocessing and to model the task as a multilabel classification problem.
Method:
In this paper, we use Bidirectional Gated 
Recurrent Neural Networks
 (BiGRU) to classify requirements using raw text. We investigated two different approaches: (i) using word sequences as tokens and (ii) using character sequences as tokens.
Results:
Experiments conducted on the publicly available PROMISE and 
EHR
 datasets show the effectiveness of the presented techniques. We achieve state-of-the-art results on most of the tasks using word sequences as tokens.
Conclusion:
Requirements can be effectively classified into functional and different non-functional categories using the presented recurrent neural networks-based deep 
learning system
, which involves minimal text prepossessing and no feature engineering.",21 Mar 2025,9,"The study introduces a novel approach using BiGRU to classify requirements, achieving state-of-the-art results. The minimal preprocessing and no feature engineering required make it highly practical for startups to implement in their projects."
https://www.sciencedirect.com/science/article/pii/S0950584922000350,Defining adaptivity and logical architecture for engineering (smart) self-adaptive cyber–physical systems,July 2022,Information and Software Technology,Not Found,Ana=Petrovska: ana.petrovska@tum.de; Stefan=Kugele: stefan.kugele@thi.de; Thomas=Hutzelmann: t.hutzelmann@tum.de; Theo=Beffart: theo.beffart@tum.de; Sebastian=Bergemann: sebastian.bergemann@tum.de; Alexander=Pretschner: alexander.pretschner@tum.de,"Abstract
Context:
Modern cyber–physical systems (CPSs) are embedded in the physical world and intrinsically operate in a continuously changing and uncertain environment or 
operational context
. To meet their business goals and preserve or even improve specific adaptation goals, besides the variety of run-time uncertainties and changes to which the CPSs are exposed—the systems need to self-adapt.
Objective:
The current literature in this domain still lacks a precise definition of what self-adaptive systems are and how they differ from those considered non-adaptive. Therefore, in order to answer 
how
 to engineer self-adaptive CPSs or self-adaptive systems in general, we first need to answer 
what
 is adaptivity, correspondingly self-adaptive systems.
Method:
In this paper, we first formally define the notion of adaptivity. Second, within the frame of the formal definitions, we propose a logical architecture for engineering decentralised self-adaptive CPSs that operate in dynamic, uncertain, and partially observable operational contexts. This logical architecture provides a structure and serves as a foundation for the implementation of a class of self-adaptive CPSs.
Results:
First, our results show that in order to answer if a system is adaptive, the right framing is necessary: the system’s adaptation goals, 
its context
, and the time period in which the system is adaptive. Second, we discuss the benefits of our architecture by comparing it with the MAPE-K conceptual model.
Conclusion:
Commonly accepted definitions of adaptivity and self-adaptive systems are necessary for work in this domain to be compared and discussed since the same terms are often used with different semantics. Furthermore, in modern self-adaptive CPSs, which operate in dynamic and uncertain contexts, it is insufficient if the adaptation logic is specified during the system’s design, but instead, the adaptation logic itself needs to adapt and “learn” during run-time.",21 Mar 2025,8,The paper addresses the need for precise definitions of self-adaptive systems in CPSs and proposes a logical architecture. This has practical value for startups developing CPSs as it provides a structured framework for self-adaptation in dynamic environments.
https://www.sciencedirect.com/science/article/pii/S0950584921002457,Tailoring the Scrum framework for software development: Literature mapping and feature-based support,June 2022,Information and Software Technology,Not Found,Luciano A.=Garcia: lucianogarcia11@hotmail.com; Edson=OliveiraJr: edson@din.uem.br; Marcelo=Morandini: m.morandini@usp.br,"Abstract
Context:
Literature faces the lack of studies relating which characteristics of the Scrum framework are adapted. Understanding such variations is useful for prospective software development projects and guiding teams at conducting Scrum 
customizations
.
Objective:
We aimed at identifying how the Scrum framework has been adapted to the context of 
Agile software development
 projects and how adaptations might be represented to aid researchers and practitioner at analyzing Scrum processes deployed or to be deployed.
Method:
We carried out a 
systematic mapping study
 in five electronic sources, 11 journals and 15 conferences/workshops. We submitted the 281 returned studies to various filters, which resulted in 50 studies with data extracted, analyzed, and quality evaluated.
Results:
SMS provides a panorama on the Scrum characteristics adapted to roles, events, and artifacts. We decided to adopt feature models for hierarchically accommodating found Scrum adaptations as it supports adaptations in the form of variability. We evaluated the resulting feature model with practitioners from different companies in the perspective of Perceived Usefulness and Perceived Ease of Use considering the Technology Acceptance Model (TAM). Therefore, we demonstrated the produced feature model aids users to better visualize and understand the documented Scrum adaptations.
Conclusions:
The panorama on Scrum adaptations and the problems during Scrum adoption are discussed to providing a means to practically understand and tailor (configure) such adaptations. Such adaptations are an essential source of information on the variety of Scrum elements, thus researchers and practitioners may take the results of this work as a guide to understand how different adaptations occur in different contexts during software development. In addition, the conceived feature model is an important asset to guide such users at selecting Scrum characteristics and respective adaptations to perform. The feature model also promotes reuse of knowledge gathered up from several different 
information sources
.",21 Mar 2025,7,The systematic study on Scrum adaptations and the development of a feature model to aid in understanding adaptations will benefit startups in Agile software development projects. It provides a practical guide for customizing Scrum processes.
https://www.sciencedirect.com/science/article/pii/S0950584922000246,Detecting privacy requirements from User Stories with NLP transfer learning models,June 2022,Information and Software Technology,Not Found,Francesco=Casillo: fcasillo@unisa.it; Vincenzo=Deufemia: deufemia@unisa.it; Carmine=Gravino: gravino@unisa.it,"Abstract
Context:
To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems.
Objective:
We present an approach to decrease privacy risks during 
agile software development
 by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile 
Requirement Engineering
 (RE).
Methods:
The proposed approach combines 
Natural Language Processing
 (NLP) and linguistic resources with 
deep learning algorithms
 to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and 
syntactic
 structure of the text. This information is then processed by a pre-trained 
convolutional neural network
, which paved the way for the implementation of a 
Transfer Learning
 technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories.
Results:
The experimental results show that 
deep learning algorithms
 allow to obtain better predictions than those achieved with conventional (shallow) 
machine learning methods
. Moreover, the application of 
Transfer Learning
 allows to considerably improve the accuracy of the predictions, ca. 10%.
Conclusions:
Our study contributes to encourage 
software engineering
 researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models.",21 Mar 2025,6,The approach proposed for detecting privacy-related information in agile software development using NLP and deep learning has practical implications for startups concerned with privacy risks. Automating privacy detection can save time and effort in compliance.
https://www.sciencedirect.com/science/article/pii/S0950584922000209,Featherweight assisted vulnerability discovery,June 2022,Information and Software Technology,"Model interpretability, Vulnerability prediction, Identifier splitting, Source code vocabulary, Software security",David=Binkley: binkley@cs.loyola.edu; Leon=Moonen: Not Found; Sibren=Isaacman: Not Found,"Abstract
Predicting vulnerable 
source code
 helps to focus the attention of a developer, or a program analysis technique, on those parts of the code that need to be examined with more scrutiny. Recent work proposed the use of function names as semantic cues that can be learned by a 
deep neural network
 (DNN) to aid in the hunt for vulnerability of functions.
Combining identifier splitting, which we use to split each function name into its constituent words, with a novel frequency-based algorithm, we explore the extent to which the words that make up a function’s name can be used to predict potentially vulnerable functions. In contrast to the 
lightweight
 prediction provided by a DNN considering only function names, avoiding the need for a DNN provides 
featherweight
 prediction. The underlying idea is that function names that contain certain “dangerous” words are more likely to accompany vulnerable functions. Of course, this assumes that the frequency-based algorithm can be properly tuned to focus on truly dangerous words.
Because it is more transparent than a DNN, which behaves as a “black box” and thus provides no insight into the rationalization underlying its decisions, the frequency-based algorithm enables us to investigate the inner workings of the DNN. If successful, this investigation into what the DNN does and does not learn will help us train more effective future models.
We empirically evaluate our approach on a heterogeneous dataset containing over 73
 
000 functions labeled vulnerable, and over 950
 
000 functions labeled benign. Our analysis shows that words alone account for a significant portion of the DNN’s classification ability. We also find that words are of greatest value in the datasets with a more homogeneous vocabulary. Thus, when working within the scope of a given project, where the vocabulary is unavoidably homogeneous, our approach provides a cheaper, potentially complementary, technique to aid in the hunt for source-code vulnerabilities. Finally, this approach has the advantage that it is viable with orders of magnitude less 
training data
.",21 Mar 2025,5,"The exploration of predicting vulnerable functions using function names and frequency-based algorithm provides insights into DNN behavior. While it offers potential cost-effective options for vulnerability prediction, the practical implementation for startups may require further validation."
https://www.sciencedirect.com/science/article/pii/S0950584922000258,An empirical study on self-admitted technical debt in modern code review,June 2022,Information and Software Technology,Not Found,Yutaro=Kashiwa: kashiwa@ait.kyushu-u.ac.jp; Ryoma=Nishikawa: Not Found; Yasutaka=Kamei: Not Found; Masanari=Kondo: Not Found; Emad=Shihab: Not Found; Ryosuke=Sato: Not Found; Naoyasu=Ubayashi: Not Found,"Abstract
Technical debt is a sub-optimal state of development in projects. In particular, the type of technical debt incurred by developers themselves (e.g., comments that mean the implementation is imperfect and should be replaced with another implementation) is called self-admitted technical debt (SATD). In theory, technical debt should not be left for a long period because it accumulates more cost over time, making it more difficult to process. Accordingly, developers have traditionally conducted code reviews to find technical debt. In fact, we observe that many SATD comments are often introduced during modern code reviews (MCR) that are light-weight reviews with web applications. However, it is uncertain about the nature of SATD comments that are introduced in the review process: impact, frequency, characteristics, and triggers. Herein, this study empirically examines the relationship between SATD and MCR.
Our 
case study
 of 156,372 review records from the Qt and OpenStack systems shows that (i) review records involving SATD are about 6%–7% less likely to be accepted by reviews than those without SATD; (ii) review records involving SATD tend to require two to three more revisions compared with those without SATD; (iii) 28–48% of SATD comments are introduced during code reviews; (iv) SATD during reviews works for communicating between authors and reviewers; and (v) 20% of the SATD comments are introduced due to reviewers’ requests.",21 Mar 2025,9,"This abstract provides valuable insights into the relationship between self-admitted technical debt and modern code reviews, which can greatly impact early-stage ventures by improving code quality and developer efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584922000349,A checklist for the evaluation of software process line approaches,June 2022,Information and Software Technology,Not Found,Halimeh=Agh: agh.halime@gmail.com; Félix=García: Not Found; Mario=Piattini: Not Found,"Abstract
Context
A Software 
Process Line
 (SPrL) can help organisations to construct bespoke software development processes for specific project situations by reusing core assets. However, as there are diverse approaches for SPrL Engineering (SPrLE), this necessitates proper assistance to organisations in selecting the SPrL approach best suited to their needs.
Objective
This paper aims to identify an 
evaluation checklist
 that can be used for evaluating SPrLs.
Method
The checklist was constructed in five stages: first, relevant aspects for managing process variability in the context of SPrLs were identified; based on these, research questions were then formed in the second stage. In the third stage, to answer the research questions, a literature review was conducted that focused on analysing 39 primary studies. In the fourth stage, the checklist was built by synthesising the literature results. In the fifth stage, the checklist was applied to two SPrL approaches as a 
proof of concept
.
Results
The checklist includes seven main aspects, including the 
modelling language
 used, the type of the approach based on the number of artefacts produced, the language constructs provided for 
variability modelling
, the process perspectives covered, the tool used for supporting the SPrL approach, the variability-specific features provided to support process variability throughout the SPrL lifecycle, and the empirical evaluation conducted to evaluate the approach.
Conclusion
The checklist can be used by organisations to compare SPrLs and then select the most suitable SPrL approach; furthermore, it can be used by researchers to propose novel SPrL approaches that consider important aspects for variability management throughout the SPrL lifecycle. Although we have provided an example of the use of the checklist to compare SPrLs, an empirical evaluation of the checklist is required to get feedback from the organisations regarding the strengths and weaknesses of the checklist.",21 Mar 2025,7,"The checklist proposed in this abstract can be a useful tool for organisations and researchers to evaluate Software Process Lines, potentially benefiting early-stage ventures in selecting suitable approaches for software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584922000210,Predicting the precise number of software defects: Are we there yet?,June 2022,Information and Software Technology,Not Found,Xiao=Yu: xiaoyu@whut.edu.cn; Jacky=Keung: jacky.keung@cityu.edu.hk; Yan=Xiao: dcsxan@nus.edu.sg; Shuo=Feng: shuofeng5-c@my.cityu.edu.hk; Fuyang=Li: fyli@whut.edu.cn; Heng=Dai: daiheng726@163.com,"Abstract
Context:
Defect Number Prediction (DNP) models can offer more benefits than classification-based 
defect prediction
. Recently, many researchers proposed to employ regression algorithms for DNP, and found that the algorithms achieve low 
Average Absolute Error
 (AAE) and high Pred(0.3) values. However, since the defect datasets generally contain many non-defective modules, even if a DNP model predicts the number of defects in all modules as zero, the AAE value of the model will be low and Pred(0.3) value will be high. Therefore, the 
good performance
 of the regression algorithms in terms of AAE and Pred(0.3) may be questioned due to the imbalanced distribution of the number of defects.
Objective:
To revisit the impact of regression algorithms for predicting the precise number of defects.
Method:
We examine the practical effects of 12 widely-used regression algorithms, two data resampling algorithm (SmoteR and ROS), and three 
ensemble learning algorithms
 (gradient boosting regression, 
AdaBoost
.R2, and Bagging), one feature selection method (information gain) and one parameter optimization method (grid search) for predicting the precise number of defects on the 18 PROMISE datasets. We propose to evaluate the AAE and Pred(0.3) values for the modules with different numbers of defects separately.
Results:
The AAE values for defective modules are very high and the Pred(0.3) values are very low, i.e., the regression algorithms are very inaccurate for predicting the precise number of defects in defective modules.
Conclusion:
The problem of predicting the precise number of defects via regression algorithms is far from being solved. We recommend that software testers use regression algorithms to rank modules for testing 
resource allocation
, rather than predict the precise number of defects to evaluate the 
software reliability
 and maintenance effort. In addition, most existing DNP studies employing the whole AAE and Pred(0.3) values of all modules as the 
evaluation metrics
 for the proposed DNP algorithms should be revisited.",21 Mar 2025,6,"While this abstract sheds light on the limitations of regression algorithms for defect number prediction, the findings may have limited immediate practical impact on European early-stage ventures unless they are heavily focused on software testing and maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584922000337,Speeding up constraint-based program repair using a search-based technique,June 2022,Information and Software Technology,Not Found,Jooyong=Yi: jooyong@unist.ac.kr; Elkhan=Ismayilzada: elkhan@unist.ac.kr,"Abstract
Context:
Constraint-based program repair has been developed as one of the main techniques for automated program repair. Given a buggy program and a test suite, constraint-based program repair first extracts a repair constraint 
φ
, and then synthesizes a patch satisfying 
φ
. Since a patch is synthesized in a correct-by-construction manner (rather than compiling and testing each repair candidate source code), the constraint-based approach, in theory, requires less runtime overhead than the G&V approach. Nevertheless, the performance of existing constraint-based approaches is still suboptimal.
Objective:
In this work, we propose a novel technique to expedite constraint-based program repair. We aim to boost runtime performance without sacrificing repairability and patch quality.
Method:
The existing constraint-based program repair searches for a patch specification in an unguided manner. We introduce a novel guided search algorithm based on 
MCMC
 sampling.
Results:
Our experimental results for the 50 buggy versions of 5 real-world subjects (i.e., 
Libtiff
, 
PHP
, 
GMP
, 
Gzip
, and 
Wireshark
) show that our method named 
FAngelix
 is on average an order of magnitude faster than 
Angelix
 (a state-of-the-art constraint-based program repair tool), showing up to 23 times speed-up. This speed-up is achieved without sacrificing repairability and patch quality.
Conclusion:
This paper proposes a novel technique that expedites constraint-based program repair, using a search-based technique based on 
MCMC
 sampling. Our experimental results show the promise of our approach.",21 Mar 2025,8,The proposed novel technique for expediting constraint-based program repair in this abstract could have practical value for early-stage ventures by potentially reducing software development time and improving patch quality.
https://www.sciencedirect.com/science/article/pii/S0950584922000404,A Delphi study to recognize and assess systems of systems vulnerabilities,June 2022,Information and Software Technology,"Delphi, Expert judgment, Security, Systems of systems",Miguel A.=Olivero: molivero@us.es; Antonia=Bertolino: antonia.bertolino@isti.cnr.it; Francisco José=Dominguez-Mayo: fjdominguez@us.es; Ilaria=Matteucci: ilaria.matteucci@iit.cnr.it; María José=Escalona: mjescalona@us.es,"Abstract
Context
System of Systems (SoS) is an emerging paradigm by which independent systems collaborate by sharing resources and processes to achieve objectives that they could not achieve on their own. In this context, a number of emergent behaviors may arise that can undermine the security of the 
constituent systems
.
Objective
We apply the Delphi method with the aims to improve our understanding of SoS security and related problems, and to investigate their possible causes and remedies.
Method
Experts on SoS expressed their opinions and reached consensus in a series of rounds by following a structured questionnaire.
Results
The results show that the experts found more consensus in disagreement than in agreement about some SoS characteristics, and on how SoS vulnerabilities could be identified and prevented.
Conclusions
From this study we learn that more work is needed to reach a shared understanding of SoS vulnerabilities, and we leverage expert feedback to outline some future research directions.",21 Mar 2025,5,"While the Delphi method applied to understand System of Systems security in this abstract is interesting, the practical impact on European early-stage ventures may be limited as it focuses more on theoretical understanding and future research directions."
https://www.sciencedirect.com/science/article/pii/S0950584922000362,Towards privacy compliance: A design science study in a small organization,June 2022,Information and Software Technology,Not Found,Ze Shi=Li: lize@uvic.ca; Colin=Werner: Not Found; Neil=Ernst: Not Found; Daniela=Damian: Not Found,"Abstract
Context:
Complying with privacy regulations has taken on new importance with the introduction of the EU’s 
General Data Protection Regulation
 (GDPR) and other privacy regulations. Privacy measures are becoming a paramount requirement demanding software organizations’ attention as recent 
privacy breaches
 such as the Capital One data breach affected millions of customers. Software organizations, however, struggle with achieving privacy compliance. In particular, there is a lack of research into the organizational practices and challenges involved in compliance, particularly for 
small and medium enterprises
 (SMEs), which represent a sizeable portion of organizations. Many SMEs use a continuous 
software engineering
 (CSE) approach, which introduces additional adoption and application challenges. For example, the fast pace of CSE makes it harder for SMEs that are already more resource constrained to prioritize non-functional requirements such as privacy.
Objective:
This paper aims to fill a gap in the under-researched area of continuous compliance with privacy requirements in practice, by investigating how a continuous practicing SME dealt with GDPR compliance.
Method:
Using design science, we conducted an in-depth ethnographically informed study over the span of 16 months and iteratively developed two artifacts to help address the organization’s challenges in addressing GDPR compliance.
Results:
We identified 3 main challenges that our collaborating organization experienced when trying to comply with the GDPR. To help mitigate the challenges, we developed two design science artifacts, which include a list of privacy requirements that operationalized the GDPR principles for automated verification, and an automated testing tool that helps to verify these privacy requirements. We validated these artifacts through close collaboration with our partner organization and applying our artifacts to the partner organization’s system.
Conclusions:
We conclude with a discussion of opportunities and obstacles in leveraging CSE to achieve continuous compliance with the GDPR. We also highlight the importance of building a shared understanding of privacy non-functional requirements and how 
risk management
 plays an important role in an organization’s GDPR compliance.",21 Mar 2025,8,"The research on continuous compliance with GDPR for SMEs addresses a current and pressing issue in the European startup ecosystem, providing practical solutions and insights."
https://www.sciencedirect.com/science/article/pii/S0950584922000398,Leveraging execution traces to enhance traceability links recovery in BPMN models,June 2022,Information and Software Technology,Not Found,Raúl=Lapeña: rlapena@usj.es; Francisca=Pérez: mfperez@usj.es; Óscar=Pastor: opastor@pros.upv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Traceability Links
 Recovery has been a topic of interest for many years, resulting in techniques that perform traceability based on the linguistic clues of the software artifacts under study. However, 
BPMN
 models tend to present an overall lack of linguistic clues when compared to code-based artifacts or code generation models. Hence, TLR becomes a harder task when performed among requirements and 
BPMN
 models.
Objective:
This paper proposes a novel approach, called METRA, that leverages the execution traces of BPMN to expand the BPMN models. The expansion of the BPMN models enhances their linguistic clues, bridging the language between BPMN models and other software artifacts, and improving the TLR process between requirements and BPMN models.
Methods:
The proposed approach is evaluated through a real-world industrial 
case study
, comparing its outcomes against two state-of-the-art baselines, TLR and LORE. The paper also evaluates the combination of METRA with LORE against the rest of the approaches, including standalone METRA. The evaluation process generates a report of measurements (precision, recall, f-measure, and MCC), over which a statistical analysis is conducted.
Results:
Results show that approaches based on METRA maintain the excellent precision results obtained by baseline approaches (74.2% for METRA, 78.8% for METRA+LORE), whilst also improving the recall results from the unacceptable values obtained by the baselines to good values (72.4% for METRA, 73.9% for METRA+LORE). Moreover, according to the statistical analysis, the differences in the results obtained by the evaluated approaches are statistically significant.
Conclusions:
This paper opens a novel field of work in TLR by analyzing the improvement of the TLR process through the inclusion of linguistic clues present in execution traces, and discusses ideas for further research that can delve into this promising direction explored by our work.",21 Mar 2025,7,"The proposed approach METRA improves traceability links in BPMN models, which can benefit certain startups working with such models, but the impact may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492200026X,Context2Vector: Accelerating security event triage via context representation learning,June 2022,Information and Software Technology,Not Found,Jia=Liu: Not Found; Runzi=Zhang: runzi_zhang@163.com; Wenmao=Liu: Not Found; Yinghua=Zhang: Not Found; Dujuan=Gu: Not Found; Mingkai=Tong: Not Found; Xingkai=Wang: Not Found; Jianxin=Xue: Not Found; Huanran=Wang: Not Found,"Abstract
Context:
Security teams are overwhelmed by thousands of alerts and events everyday, which are comprehensively collected for threat analysis in 
security operations center
. Although methods based on rules, intelligence and data mining are utilized, the alert fatigue situation is still a challenging problem, slowing down the overall threat investigation process.
Objective:
‘Event polysemy’ phenomenon broadly exists in large-scale event dataset, which means that events of the same category can reveal different purposes in different contexts. This paper aims at exploring, revealing and evaluating the latent patterns embedding in the event contexts, to gain insight on context semantics and reduce manual intervention in event triage tasks.
Method:
A context 
representation learning
 based method, named Context2Vector, is proposed. Contexts are extracted from multiple behavioral views. Then, both dense event representations and sparse topic representations are learnt at the same time and in the same space. A human-in-the-loop topic annotation process is involved and finally, a context deviation detection based method is integrated to generate explainable and informative labels for automated context semantic decoding.
Results:
Various experiments are conducted on a enterprise-scale event dataset. The topic annotation, context related feature importance and top-N event ranking evaluation results show that Context2Vector outperforms traditional methods on the high-risk event identification problems, improving the attacker recall rate by up to 2.25 times within limited events to be investigated.
Conclusion:
It is concluded that event contexts imply practicable and abundant information in regard to behaviors and intents of real threat actors. More precise profiling of network entities can be extracted from contexts, compared to rules, intelligence, and 
anomaly detectors
 used in practice.",21 Mar 2025,9,"The Context2Vector method for event context analysis and threat detection provides significant value to startups dealing with security operations, offering tangible improvements and efficiency gains."
https://www.sciencedirect.com/science/article/pii/S0950584922000180,An evaluation of the effectiveness of personalization and self-adaptation for e-Health apps,June 2022,Information and Software Technology,"Self-adaptive systems, Personalization, Reference architecture, Mobile apps, e-Health",Eoin Martino=Grua: e.m.grua@vu.nl; Martina=De Sanctis: martina.desanctis@gssi.it; Ivano=Malavolta: i.malavolta@vu.nl; Mark=Hoogendoorn: m.hoogendoorn@vu.nl; Patricia=Lago: p.lago@vu.nl,"Abstract
Context.
There are many e-Health mobile apps on the apps store, from apps to improve a user’s lifestyle to mental coaching. Whilst these apps might consider user context when they give their interventions, prompts, and encouragements, they still tend to be rigid 
e.g.,
 not using user context and experience to tailor themselves to the user.
Objective.
To better engage and tailor to the user, we have previously proposed a Reference Architecture for enabling self-adaptation and 
AI
 personalization in e-Health mobile apps. In this work we evaluate the end users’ perception, usability, performance impact, and energy consumption contributed by this Reference Architecture.
Method.
We do so by implementing a Reference Architecture compliant app and conducting two experiments: a user study and a measurement-based experiment.
Results.
Although limited in the number of participants, the results of our user study show that usability of the Reference Architecture compliant app is similar to the control app. Users’ perception was found to be positively influenced by the compliant app when compared to the control group. Results of our measurement-based experiment showed some differences in performance and energy consumption measurements between the two apps. The differences are, however, deemed minimal.
Conclusions.
Our experiments show promising results for an app implemented following our proposed Reference Architecture. This is preliminary evidence that the use of personalization and self-adaptation techniques can be beneficial within the domain of e-Health apps.",21 Mar 2025,6,The evaluation of a Reference Architecture for e-Health apps shows potential benefits but may have a more limited scope of impact compared to other abstracts.
https://www.sciencedirect.com/science/article/pii/S0950584922000416,Prioritization of model smell refactoring using a covariance matrix-based adaptive evolution algorithm,June 2022,Information and Software Technology,Not Found,Amjad=AbuHassan: amjad.abuhassan@najah.edu; Mohammad=Alshayeb: alshayeb@kfupm.edu.sa; Lahouari=Ghouti: lghouti@psu.edu.sa,"Abstract
Context
The 
refactoring process
 enhances the 
software design
 by modifying the structure of design parts impaired with 
model smells
 without altering the overall software behavior. However, handling these smells without proper prioritization will not produce the anticipated effects.
Objective
In this paper, we solve the prioritization of the model smell refactoring using a multi-objective optimization (MOO) algorithm called the multi-objective (MO) 
covariance matrix
 adaptation evolution strategy (MO
CMA-ES). Our formulation relies on the refactoring of 
unified modeling language
 (UML) 
class diagrams
 to mitigate the negative effect of design smells.
Method
We treat the prioritization problem as a real-valued MOO where we propose novel data encoding procedures. We use two conflicting objectives, quality, and 
maintainability
, to balance the refactoring. We first build a new solution representation that guarantees smell fixing and eliminates the rejection limitation. Furthermore, we suggest a custom mapping scheme to properly encode real-valued quantities using unique representations. For performance evaluation purposes, we developed a large custom dataset with more than 30,000 class records, using seven popular open-source software projects. A novel relative coverage metric is proposed to mitigate the limitations associated with the standard coverage. For benchmarking purposes, we also consider an improved version of the nondominated sorting 
genetic algorithm
 (NSGA-II(.
Results
The reported performance scores confirm the superiority of the MO
CMA-ES algorithm over NSGA-II. The former successfully identified the refactoring sequences that lead to the best improvements in software quality and 
maintainability
 while it is able to fix all identified design smells. These improvements are quantified in terms of hypervolume, coverage, spacing metrics, and 
execution time
.
Conclusion
The MO
CMA-ES attained the highest average maximum quality score of 1149 while keeping the average 
maintainability
 at the lowest score of 13.8. In all experiment settings, the proposed solution leads to longer refactoring sequences at no additional computational cost.",21 Mar 2025,8,"The use of MOO algorithm for model smell refactoring in UML class diagrams offers practical value to startups in software design, demonstrating effective solutions to common challenges."
https://www.sciencedirect.com/science/article/pii/S095058492200057X,Predicting reliability of software in industrial systems using a Petri net based approach: A case study on a safety system used in nuclear power plant,June 2022,Information and Software Technology,Not Found,Kuldeep=Kumar: Not Found; Sumit=Not Found: Not Found; Sandeep=Kumar: sandeep.garg@cs.iitr.ac.in; Lalit Kumar=Singh: Not Found; Alok=Mishra: Not Found,"Abstract
Context
Software reliability
 prediction in the early stages of development can be propitious in many ways. The combinatorial models used to predict reliability using architectures such as fault trees, binary decision diagrams, etc. have limitations in modeling complex system behavior. On the other hand, state-based models such as 
Markov chains
 suffer from the state-space explosion problem, and they need 
transition probability
 among different system states to measure reliability. These probabilities are usually assumed or are obtained from the operational profile for which the system should be used in the field.
Objective
The objective of this paper is to present a method for predicting the reliability of software in industrial systems using a generalized stochastic 
Petri nets
 based approach. The key idea is to violate the assumption of state transition probabilities in the Markov chain. The state transition probabilities are calculated using Petri net transitions’ throughput by performing stationary analysis under the consideration to identify and handle dead markings in the Petri net.
Method
Initially, a generalized stochastic Petri net of the system under consideration is generated from the standard system's specification. Thereafter, dead markings are identified in the Petri net which are further removed to perform steady-state analysis. At last, a Markov model is generated based on the 
reachability
 graph of the Petri net, which is further used to predict the system reliability.
Results
The presented method has been applied to a safety-critical system, Shut Down System-1, of a 
nuclear power plant
, which is operational in the Canada 
Deuterium
 Uranium reactor. The predicted reliability of the system using this method is 99.99966% which has been validated using the specified system requirements. To further validate and generalize the results, sensitivity analysis is performed by varying different system parameters.
Conclusions
The method discussed in this paper presents a step of performing structural analysis on the Petri net of the system under consideration to identify and handle dead markings on the Petri net. It further handles the issue of assuming transition probabilities among the system states by calculating them using Petri net transitions’ throughput.",21 Mar 2025,9,The method presented for predicting software reliability in industrial systems using Petri nets offers a practical approach that can have a significant impact on early-stage ventures by ensuring system reliability in critical environments.
https://www.sciencedirect.com/science/article/pii/S0950584922000179,Short communication: Evolution of secondary studies in software engineering,May 2022,Information and Software Technology,"Systematic review, Mapping study, Qualitative study, Experience of authors",David=Budgen: david.budgen@durham.ac.uk; Pearl=Brereton: o.p.brereton@keele.ac.uk,"Abstract
Context:
Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by 
software engineering
 in 2004, many have been undertaken by researchers.
Objective:
To assess how the role of secondary studies in software engineering has evolved.
Methods:
We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their 
type
 (e.g. mapping study), their 
research focus
 (quantitative/qualitative and practice/methodological), as well as the experience of the first authors.
Results:
Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study.
Conclusion:
Existing guidelines
 largely focus upon quantitative 
systematic reviews
. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies.",21 Mar 2025,6,"While the evolution of secondary studies in software engineering is important, the practical implications for early-stage ventures may not be as direct. Still, the insights can be valuable for researchers and practitioners alike."
https://www.sciencedirect.com/science/article/pii/S0950584922000234,Translating quality-driven code change selection to an instance of multiple-criteria decision making,May 2022,Information and Software Technology,Not Found,Christos P.=Lamprakos: cplamprakos@microlab.ntua.gr; Charalampos=Marantos: hmarantos@microlab.ntua.gr; Miltiadis=Siavvas: siavvasm@iti.gr; Lazaros=Papadopoulos: lpapadop@microlab.ntua.gr; Angeliki-Agathi=Tsintzira: angeliki.agathi.tsintzira@gmail.com; Apostolos=Ampatzoglou: ampatzoglou@uom.edu.gr; Alexander=Chatzigeorgiou: achat@uom.edu.gr; Dionysios=Kehagias: diok@iti.gr; Dimitrios=Soudris: dsoudris@microlab.ntua.gr,"Abstract
Context:
The definition and assessment 
of software quality
 have not converged to a single specification. Each team may formulate its own notion of quality and tools and methodologies for measuring it. Software quality can be improved via code changes, most often as part of a software maintenance loop.
Objective:
This manuscript contributes towards providing decision support for code change selection given a) a set of preferences on a software product’s qualities and b) a pool of heterogeneous code changes to select from.
Method:
We formulate the problem as an instance of Multiple-Criteria Decision Making, for which we provide both an abstract flavor and a prototype implementation. Our prototype targets energy efficiency, technical debt and dependability.
Results:
This prototype achieved inconsistent results, in the sense of not always recommending changes reflecting the decision maker’s preferences. Encouraged from some positive cases and cognizant of our prototype’s shortcomings, we propose directions for future research.
Conclusion:
This paper should thus be viewed as an imperfect first step towards quality-driven, code change-centered decision support and, simultaneously, as a curious yet pragmatic enough gaze on the road ahead.",21 Mar 2025,4,The decision support for code change selection contributes to software quality but the inconsistent results and prototype limitations may hinder immediate practical application for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921002378,A systematic literature review on counterexample explanation,May 2022,Information and Software Technology,Not Found,Arut Prakash=Kaleeswaran: arutprakash.kaleeswaran@de.bosch.com; Arne=Nordmann: arne.nordmann@de.bosch.com; Thomas=Vogel: thomas.vogel@informatik.hu-berlin.de; Lars=Grunske: grunske@informatik.hu-berlin.de,"Abstract
Context:
Safety is of 
paramount importance
 for cyber–physical systems in domains such as automotive, robotics, and avionics. Formal methods such as model checking are one way to ensure the safety of cyber–physical systems. However, adoption of formal methods in industry is hindered by 
usability issues
, particularly the difficulty of understanding model checking results.
Objective:
We want to provide an overview of the state of the art for counterexample explanation by investigating the contexts, techniques, and evaluation of research approaches in this field. This overview shall provide an understanding of current and guide future research.
Method:
To provide this overview, we conducted a systematic literature review. The survey comprises 116 publications that address counterexample explanations for model checking.
Results:
Most primary studies provide counterexample explanations graphically or as traces, minimize counterexamples to reduce complexity, localize errors in the models expressed in the input formats of 
model checkers
, support 
linear temporal logic
 or computation tree logic specifications, and use 
model checkers
 of the Symbolic Model Verifier family. Several studies evaluate their approaches in safety-critical domains with industrial applications.
Conclusion:
We notably see a lack of research on counterexample explanation that targets probabilistic and real-time systems, leverages the explanations to domain-specific models, and evaluates approaches in user studies. We conclude by discussing the adequacy of different types of explanations for users with varying domain and formal methods expertise, showing the need to support laypersons in understanding model checking results to increase adoption of formal methods in industry.",21 Mar 2025,7,The overview of counterexample explanations in model checking provides valuable insights for enhancing safety in cyber-physical systems. This can be particularly useful for early-stage ventures in safety-critical domains.
https://www.sciencedirect.com/science/article/pii/S0950584922000167,Response time evaluation of mobile applications combining network protocol analysis and information fusion,May 2022,Information and Software Technology,Not Found,Pan=Liu: Not Found; Yihao=Li: yihao.li@ldu.edu.cn,"Abstract
The 
response time
 of a mobile application (app), especially a mobile stock trading app, is an important factor that affects customer satisfaction. However, it is considerably difficult to accurately evaluate the performance of mobile apps owing to numerous real-world settings such as operating systems, hardware, and test environments. This paper presents a novel method to evaluate the 
response time
 of mobile apps on different 
mobile phones
 through combining network protocol analysis and information fusion. To make the 
data collected
 from the mobile app more reliable and credible, we recruited some volunteers to collect data on their 
mobile phones
. Then we used the network protocol analysis method to obtain the response time of the mobile app on a mobile phone. Next, we adopted information fusion technology using the rank-score 
characteristic function
 to evaluate the response time of mobile apps on different mobile phones. Experiments were conducted to evaluate our approach on three types of mobile apps. The results showed that the proposed method can effectively evaluate the response time of mobile apps with low cost.",21 Mar 2025,5,The novel method for evaluating response time of mobile apps is interesting but the focus on performance evaluation may have limited immediate practical value for early-stage ventures compared to other abstracts.
https://www.sciencedirect.com/science/article/pii/S0950584921002433,A unifying framework for the systematic analysis of Git workflows,May 2022,Information and Software Technology,Not Found,Julio César=Cortés Ríos: juliocesar.cortesrios@manchester.ac.uk; Suzanne M.=Embury: suzanne.m.embury@manchester.ac.uk; Sukru=Eraslan: seraslan@metu.edu.tr,"Abstract
Context:
Git is a popular distributed version control system that provides flexibility and robustness for software development projects. Several workflows have been proposed to codify the way project contributors work collaboratively with Git. Some workflows are highly prescriptive while others allow more leeway but do not provide the same level of code quality assurance, thus, preventing their comparison to determine the most suitable for a specific set of requirements, or to ascertain if a workflow is being properly followed.
Objective:
In this paper, we propose a novel feature-based framework for describing Git workflows, based on a study of 26 existing instances. The framework enables workflows’ comparison, to discern how, and to what extent, they exploit Git capabilities for 
collaborative software development
.
Methods:
The framework uses feature-based modelling to map Git capabilities, regularly expressed as contribution guidelines, and a set of features that can be impartially applied to all the workflows considered. Through this framework, each workflow was characterised based on their publicly available descriptions. The characterisations were then vectorised and processed using 
hierarchical clustering
 to determine workflows’ similarities and to identify which features are most popular, and more relevant for discriminatory purposes.
Results:
Comparative analysis evidenced that some workflows claiming to be closely related, when described and then characterised, turned out to have more differences than similarities. The analysis also showed that most workflows focus on the branching and code integration strategies, whilst others emphasise subtle differences from other popular workflows or describe a specific development route and are, thus, widely reused.
Conclusion:
The characterisation and 
clustering analysis
 demonstrated that our framework can be used to compare and analyse Git workflows.",21 Mar 2025,7,"The proposal of a novel feature-based framework for describing Git workflows can have a significant impact on improving collaborative software development practices, especially for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921002469,"Drivers, barriers and impacts of digitalisation in rural areas from the viewpoint of experts",May 2022,Information and Software Technology,"68-02, 68U35, 68N99",Alessio=Ferrari: alessio.ferrari@isti.cnr.it; Manlio=Bacco: Not Found; Kirsten=Gaber: Not Found; Andreas=Jedlitschka: Not Found; Steffen=Hess: Not Found; Jouni=Kaipainen: Not Found; Panagiota=Koltsida: Not Found; Eleni=Toli: Not Found; Gianluca=Brunori: Not Found,"Abstract
Context:
The domain of rural areas, including rural communities, 
agriculture
, and 
forestry
, is going through a process of deep digital transformation. 
Digitalisation
 can have positive impacts on 
sustainability
 in terms of greater environmental control, and community prosperity. At the same time, it can also have disruptive effects, with the 
marginalisation
 of actors that cannot cope with the change. When developing a novel system for rural areas, requirements engineers should carefully consider the specific socio-economic characteristics of the domain, so that potential positive effects can be maximised, while mitigating 
negative impacts
.
Objective:
The goal of this paper is to support requirements engineers with a reference catalogue of 
drivers
, 
barriers
 and potential 
impacts
 associated to the introduction of novel ICT solutions in rural areas.
Method:
To this end, we interview 30 cross-disciplinary experts in digitalisation of rural areas, and we analyse the transcripts to identify common themes.
Results:
According to the experts, main 
drivers
 are economic, with the possibility of reducing costs, and regulatory, as institutions push for more precise tracing and monitoring of production; 
barriers
 are the limited connectivity, but also distrust towards technology and other socio-cultural aspects; positive 
impacts
 are socio-economic (e.g., reduction of manual labour, greater productivity), while negative ones include potential dependency from technology, with loss of hands-on expertise, and marginalisation of certain actors (e.g., 
small farms
, subjects with limited education).
Conclusion:
This paper contributes to the literature with a domain-specific catalogue that characterises digitalisation in rural areas. The catalogue can be used as a reference baseline for 
requirements elicitation
 endeavours in rural areas, to support domain analysis prior to the development of novel solutions, as well as fit-gap analysis for the adaptation of existing technologies.",21 Mar 2025,5,"The support provided for requirements engineers with a reference catalogue of drivers, barriers, and potential impacts associated with the introduction of novel ICT solutions in rural areas can be beneficial, but may not have as direct impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584922000015,Predicting vulnerability inducing function versions using node embeddings and graph neural networks,May 2022,Information and Software Technology,Not Found,Sefa Eren=Şahin: sahinsef@itu.edu.tr; Ecem Mine=Özyedierler: ozyedierlere@itu.edu.tr; Ayse=Tosun: tosunay@itu.edu.tr,"Abstract
Context:
Predicting software vulnerabilities over code changes is a difficult task due to obtaining real vulnerability data and their associated code fixes from software projects as software organizations are often reluctant to report those.
Objective:
We aim to propose a vulnerability prediction model that runs after every code change, and identifies vulnerability inducing functions in that version. We also would like to assess the success of node and token based source code representations over 
abstract syntax trees
 (ASTs) on predicting vulnerability inducing functions.
Method:
We train 
neural networks
 to represent 
node embeddings
 and token embeddings over ASTs in order to obtain feature representations. Then, we build two 
Graph Neural Networks
 (GNNs) with 
node embeddings
, and compare them against 
Convolutional Neural Network
 (CNN) and 
Support Vector Machine
 (SVM) with token representations.
Results:
We report our empirical analysis over the change history of vulnerability inducing functions of 
Wireshark
 project. GraphSAGE model using source code representation via ASTs achieves the highest AUC rate, while 
CNN models
 using token representations achieves the highest recall, precision and F1 measure.
Conclusion:
Representing functions with their structural information extracted from ASTs, either in token form or in complete graph form, is great at predicting vulnerability inducing function versions. Transforming source code into token frequencies as a natural language text fails to build successful models for vulnerability prediction in a real software project.",21 Mar 2025,8,"The vulnerability prediction model using neural networks and graph neural networks to assess vulnerability inducing functions can be highly valuable for improving software security practices, particularly for startups looking to secure their software products."
https://www.sciencedirect.com/science/article/pii/S0950584921002470,Enhancing software modularization via semantic outliers filtration and label propagation,May 2022,Information and Software Technology,Not Found,Kaiyuan=Yang: Not Found; Junfeng=Wang: wangjf@scu.edu.cn; Zhiyang=Fang: Not Found; Peng=Wu: Not Found; Zihua=Song: Not Found,"Abstract
Context:
Software systems’ modular structure often drifts from the intended design throughout evolution. To improve the modular structure of a software system, the 
software clustering
 technology aiming to partition a software system into meaningful modules is demanding. Many 
clustering approaches
 rely on semantic information, which cluster software entities that use similar vocabulary. However, the existence of semantic outliers obstructing the 
clustering process
 is hardly considered.
Objective:
To overcome the existence of semantic outliers, this paper proposes a two-stage 
software clustering
 approach named EVOL (Enhancing Via Outliers filtration and Label propagation).
Methods:
A feature density-based 
outliers detecting
 algorithm is used to compute the 
local outlier factor
 of each feature. Accordingly, we filter out the semantic outliers and cluster remaining high-quality features to construct a partition skeleton; After that, assign each outlier into a suitable cluster by label propagation.
Results:
To assess the effectiveness of the proposed approach, this paper conducts experiments on six folders from Mozilla Firefox and other four software systems, referring to the original design concepts and modular structure provided by the developers. The average of the 
evaluation metric
 MoJoFM shows significant improvement from 6% to 35% over the other six state-of-art 
clustering techniques
. The results demonstrate that the filtration of the outliers facilitates the 
clustering results
, and label propagation could place the outliers into a suitable cluster.
Conclusion:
In this paper, we propose EVOL, a new software clustering approach that considers semantic outliers filtration and label propagation. The experiment results show that the proposed approach EVOL can be very useful to enhance the quality of the software modularization.",21 Mar 2025,6,"The proposal of a two-stage software clustering approach to overcome semantic outliers can be useful for improving the modular structure of software systems, but its impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000064,Empirically developed framework for building trust in distributed agile teams,May 2022,Information and Software Technology,Not Found,Sulabh=Tyagi: sulabhtyagi2k@yahoo.co.in; Ritu=Sibal: Not Found; Bharti=Suri: Not Found,"Abstract
Background:
Organizations are adopting agile practices in distributed software development in order to develop quality software in less time. Using 
agile software development
 in distributed set up has its own set of challenges pertaining to 
face to face interactions
, collaboration, time zone and cultural differences. A strong presence of trust helps to overcome these challenges. A relatively lesser number of empirical studies on multidimensional perspective of trust in distributed 
agile software development
 has motivated this study.
Objective:
This study aims to develop a comprehensive framework to build trust in distributed agile teams.
Method:
This study is based on Grounded Theory research methodology which involves 40 agile practitioners from diverse domains belonging to 19 different software organizations located across seven different countries. Besides, observations in two different software organizations were also performed to gather data. Data has been gathered in the form of semi-structured interviews and field notes.
Result:
Qualitative data analysis
 resulted into five different contributing categories for building trust amongst distributed agile teams. These categories represent multidimensional perspectives that influence trust building amongst agile team members working across different parts of the world.
Conclusion:
This study culminates into a framework for building trust in distributed agile teams. The proposed framework has been developed empirically and has five components that influence trust building. These components are related to working environment, leadership, organizational, personal and socio cultural perspectives. The multidimensional perspective of trust was investigated from an agile practitioners view through their real-life project experiences. Organizations and software practitioners may utilize the results of this study to create a hospitable environment for building trust while practicing agile in a distributed environment.",21 Mar 2025,4,"While the framework for building trust in distributed agile teams is valuable for software development in distributed setups, its direct impact on early-stage ventures may not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584922000027,An automated test data generation method for void pointers and function pointers in C/C++ libraries and embedded projects,May 2022,Information and Software Technology,Not Found,Lam Nguyen=Tung: tunglam@vnu.edu.vn; Hoang-Viet=Tran: thv@vnu.edu.vn; Khoi Nguyen=Le: khoi.n.le@vnu.edu.vn; Pham Ngoc=Hung: hungpn@vnu.edu.vn,"Abstract
Automated test data generation for unit testing C/C++ functions using concolic methods is well-known for improving software quality while reducing human testing effort. However, there have been only a few researches related to generating test data for void pointers and 
function pointers
 which are commonly used in C/C++ libraries and embedded projects. This paper proposes a concolic-based method named VFP (
V
oid and 
F
unction 
P
ointers test data generation) to generate test data for void pointers and 
function pointers
. The key idea of VFP method is to preprocess the 
source code
 of the project under test to find all possible types of void pointers and references of function pointers. These types and references are used in the initial test data generating phase of the concolic 
testing method
. VFP method is implemented in VFP verification tool to test on various C/C++ libraries and embedded projects. The experimental results show that VFP significantly improves the coverage of the generated test data in comparison with existing methods.",21 Mar 2025,8,"The proposed method VFP addresses a gap in automated test data generation for void pointers and function pointers in C/C++ libraries and embedded projects, improving software quality and test coverage."
https://www.sciencedirect.com/science/article/pii/S0950584922000192,Personalizing label prediction for GitHub issues,May 2022,Information and Software Technology,Not Found,Jun=Wang: 20194227028@stu.suda.edu.cn; Xiaofang=Zhang: xfzhang@suda.edu.cn; Lin=Chen: lchen@nju.edu.cn; Xiaoyuan=Xie: xxie@whu.edu.cn,"Abstract
Context:
Automated label prediction tools can help developers manage and categorize issues on GitHub. However, different open-source projects use various forms of labels with the same meaning. Previous label prediction methods mainly solve the problem of the synonymous labels by manual preprocessing rules, but these preprocessing rules can only identify synonyms with the same prefix or suffix.
Objective:
These factors inspire us to propose a method to identify these synonymous labels automatically and recommend personalized labels for different open-source projects.
Method:
In this paper, we propose a Personalizing Label Prediction framework for Issues named PLPI. PLPI identifies labels with similar meanings by representing labels as 
semantic vectors
 and applying 
clustering methods
. PLPI can predict personalized labels from the 
existing labels
 in the open-source project.
Result:
We conduct a comprehensive study to compare seven commonly adopted labeling models with our approach. The experimental results demonstrate the advantages of our approach. Finally, we show some representative examples and discuss the visualization results of synonyms clustering by dimension reduction.
Conclusion:
The experimental results show that our method PLPI can improve label prediction performance and provide personalized label recommendation results for different open-source projects.",21 Mar 2025,7,"PLPI framework offers a personalized label prediction method for GitHub open-source projects, enhancing issue categorization and improving label prediction performance."
https://www.sciencedirect.com/science/article/pii/S0950584921002391,Inferring data model from service interactions for response generation in service virtualization,May 2022,Information and Software Technology,Not Found,Md. Arafat=Hossain: mdarafathossain@swin.edu.au; Jiaojiao=Jiang: jiaojiao.jiang@unsw.edu.au; Jun=Han: jhan@swin.edu.au; Muhammad Ashad=Kabir: akabir@csu.edu.au; Jean-Guy=Schneider: jeanguy.schneider@deakin.edu.au; Chengfei=Liu: cliu@swin.edu.au,"Abstract
Context:
 Service 
virtualization
 has become a popular tool to provide testing environments for highly connected enterprise software systems. It enables the enterprise system under test to interact with and obtain responses from model-based service emulations instead of the actual services they use in production environments, providing accessibility and realness. Existing approaches consider only the 
control dependencies
 between messages (
i.e.
, the service’s control model) and do not consider the relationships between data values of the messages (
i.e.
, the service’s data model), limiting the accuracy of service emulation.
Objective:
 In this paper, we present an approach to deriving the service’s data model from its interaction traces and using it in determining the payloads for 
response messages
, therefore achieving more accurate service emulation.
Method:
 The derivation of a service’s data model is achieved by discovering the data entities and their key attribute(s) from the service interaction messages. It is then used, together with the control model, to synthesize 
response messages
 for incoming 
request messages
 at runtime. While the control model help to identify the types of responses, the data model keeps track of the changes to the service’s data entities and provides the basis for populating accurate payloads for the responses.
Results:
 A number of experiments have been conducted on message traces collected from a range of stateful and stateless services. With the use of both the control and data models in 
response generation
, our approach consistently outperforms the existing state-of-the-art approaches. In particular, it generates 100% identical responses (compared to actual services) for most of the datasets, while the highest accuracy achieved by existing approaches was 88%.
Conclusion:
 The experimental results have shown that the inferred data model provides an effective means in determining the payloads for response messages, significantly improving the accuracy of service emulation and providing more realistic testing environments.",21 Mar 2025,9,"The approach for deriving service's data model improves service emulation accuracy by considering data values in addition to control dependencies, resulting in more realistic testing environments."
https://www.sciencedirect.com/science/article/pii/S0950584921002445,Feature toggles as code: Heuristics and metrics for structuring feature toggles,May 2022,Information and Software Technology,Not Found,Rezvan=Mahdavi-Hezaveh: rmahdav@ncsu.edu; Nirav=Ajmeri: Not Found; Laurie=Williams: Not Found,"Abstract
Context:
Using feature toggles is a technique to turn a feature either on or off in program code by checking the value of a variable in a 
conditional statement
. This technique is increasingly used by software practitioners to support continuous integration and continuous delivery (CI/CD). However, using feature toggles may increase code complexity, create dead code, and decrease the quality of a codebase.
Objective:
The goal of this research is to aid software practitioners in structuring feature toggles in the codebase by proposing and evaluating a set of heuristics and corresponding complexity, 
comprehensibility
, and 
maintainability
 metrics based upon an empirical study of open source repositories.
Method:
We identified 80 GitHub repositories that use feature toggles in their 
development cycle
. We conducted a qualitative analysis using 60 of the 80 repositories to identify heuristics and metrics. Then, we conducted a survey of practitioners of 80 repositories to obtain their feedback that the proposed heuristics can be used to guide the structure of feature toggles and to reduce technical debt. We also conducted a 
case study
 of the all 80 repositories to analyze relations between heuristics and metrics.
Results:
From the qualitative analysis, we proposed 7 heuristics to guide structuring feature toggles and identified 12 metrics to support the principles embodied in the heuristics. Our survey result shows that practitioners agree that managing feature toggles is difficult, and using identified heuristics can reduce technical debt. Based on our 
case study
, we find a relationship between the adoption of heuristics and the values of metrics.
Conclusions:
Our results support that practitioners should have self-descriptive feature toggles, use feature toggles sparingly, avoid duplicate code in using feature toggles, and ensure complete removal of a feature toggle.",21 Mar 2025,6,"The research provides heuristics and metrics to aid software practitioners in structuring feature toggles in codebases, addressing the complexities and potential technical debt associated with their use."
https://www.sciencedirect.com/science/article/pii/S0950584922000222,Collaboration in software ecosystems: A study of work groups in open environment,May 2022,Information and Software Technology,Not Found,Zhifei=Chen: chenzhifei@njust.edu.cn; Wanwangying=Ma: wwyma@smail.nju.edu.cn; Lin=Chen: lchen@nju.edu.cn; Wei=Song: wsong@njust.edu.cn,"Abstract
Context:
As a particular type of software ecosystem, an 
open source software
 ecosystem (OSSECO) is a collection of interdependent 
open source software
 (OSS) projects which are developed and evolve together. Events happening within an OSSECO inherently involve the collaboration of participants from multiple OSS projects, forming a temporary work group. However, it is still unclear how different members of a work group collaborate to fix cross-project bugs, a typical event in the maintenance of OSSECOs.
Objective:
This study aims to investigate the characteristics of collaboration within a work group when fixing cross-project bugs in an OSSECO. It involves the participants from the upstream (which caused the bugs) and the downstream (which were affected by the bugs) OSS projects.
Method:
We conducted our study on 236 cross-project bugs from the scientific Python ecosystem, involving 571 participants and 91 OSS projects, to understand open collaboration within a work group. We established a quantitative analysis to investigate the members of a work group, along with a qualitative analysis to understand the roles of the members from different OSS communities.
Results:
The results show that: (1) A typical work group is constituted of four to eight members from the core development teams of the two OSS communities. More members concern with the upstream OSS projects and few can make active contributions to both sides; (2) Distinct responsibilities are taken by the two OSS communities, with the downstream members as the problem-finders and the upstream members as the decision-makers or gatekeepers.
Conclusions:
Our findings reveal the collaborative mechanism and the responsibility allocation between the upstream and downstream OSS communities in the ecosystems.",21 Mar 2025,7,The study on collaboration within work groups in fixing cross-project bugs in OSSECOs contributes to understanding open collaboration dynamics between upstream and downstream OSS communities.
https://www.sciencedirect.com/science/article/pii/S0950584922000040,Ambiguity in user stories: A systematic literature review,May 2022,Information and Software Technology,Not Found,Anis R.=Amna: AnisRahmawati.Amna@UGent.be; Geert=Poels: Geert.Poels@UGent.be,"Abstract
Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, 
syntactic
, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.",21 Mar 2025,4,"While addressing an important issue, ambiguity in user stories, the abstract focuses more on research gaps and future potential rather than providing practical solutions for startups."
https://www.sciencedirect.com/science/article/pii/S0950584922000052,Proactive hybrid learning and optimisation in self-adaptive systems: The swarm-fleet infrastructure scenario,May 2022,Information and Software Technology,Not Found,Christian=Krupitzer: christian.krupitzer@uni-hohenheim.de; Christian=Gruhl: Not Found; Bernhard=Sick: Not Found; Sven=Tomforde: Not Found,"Abstract
Context:
Smart and adaptive Systems, such as self-adaptive and self-organising (SASO) systems, typically consist of a large set of highly autonomous and heterogeneous subsystems that are able to adapt their behaviour to the requirements of ever-changing, dynamic environments. Their successful operation is based on appropriate modelling of the internal and external conditions.
Objective:
The control problem for establishing a near-to-optimal coordinated behaviour of systems with multiple, potentially conflicting objectives is either approached in a distributed (i.e., fully autonomous by the autonomous subsystems) or in a centralised way (i.e. one instance controlling the optimisation and planning process). In the distributed approach, selfish behaviour and being limited to local knowledge may lead to sub-optimal 
system behaviour
, while the 
centralised approach
 ignores the 
autonomy
 and the coordination efforts of parts of the system.
Method:
In this article, we present a concept for a hybrid (i.e., integrating a central optimisation with a distributed decision-making process) 
system management
 that combines local 
reinforcement learning
 and self-awareness mechanisms of fully autonomous subsystems with external system-wide planning and optimisation of adaptation freedom that steers the behaviour dynamically by issuing plans and guidelines augmented with incentivisation schemes.
Results:
This work addresses the inherent uncertainty of the dynamic 
system behaviour
, the local autonomous and context-aware learning of subsystems, and proactive control based on adaptiveness. We provide the ‘swarm-fleet infrastructure’ – a self-organised taxi service established by autonomous, privately-owned cars – as a 
testbed
 for structured comparison of systems.
Conclusion:
The ‘swarm-fleet infrastructure’ supports the advantages of a proactive hybrid self-adaptive and self-organising system operation. Further, we provide a system model to combine the system-wide optimisation while ensuring local decision-making through 
reinforcement learning
 for individualised configurations.",21 Mar 2025,8,"The concept of hybrid system management for self-adaptive systems presents practical implications for startups working on AI-driven technologies, providing a structured approach to optimize system behavior."
https://www.sciencedirect.com/science/article/pii/S0950584921002317,A systematic process for Mining Software Repositories: Results from a systematic literature review,April 2022,Information and Software Technology,Not Found,M.=Vidoni: melina.vidoni@anu.edu.au,"Abstract
Context:
Mining Software Repositories
 (MSR) is a growing area of 
Software Engineering
 (SE) research. Since their emergence in 2004, many investigations have analysed different aspects of these studies. However, there are no guidelines on how to conduct systematic MSR studies. There is a need to evaluate how MSR research is approached to provide a framework to do so systematically.
Objective:
To identify how MSR studies are conducted in terms of repository selection and 
data extraction
. To uncover potential for improvement in directing systematic research and providing guidelines to do so.
Method:
A 
systematic literature review
 of MSR studies was conducted following the guidelines and 
template
 proposed by Mian et al. (which refines those provided by Kitchenham and Charters). These guidelines were extended and revised to provide a framework for systematic MSR studies.
Results:
MSR studies typically do not follow a systematic approach for repository selection, and many do not report selection or 
data extraction
 protocols. Furthermore, few manuscripts discuss threats to the study’s validity due to the selection or data extraction steps followed.
Conclusions:
Although MSR studies are evidence-based research, they seldom follow a systematic process. Hence, there is a need for guidelines on how to conduct systematic MSR studies. New guidelines and a 
template
 have been proposed, consolidating related studies in the MSR field and strategies for systematic literature reviews.",21 Mar 2025,6,"The systematic review of MSR studies offers valuable insights and proposes guidelines, which can be beneficial for startups working on software development, but lacks direct impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492100224X,How far are we from reproducible research on code smell detection? A systematic literature review,April 2022,Information and Software Technology,"Software engineering, Code smells, Reproducibility, Reproducible research",Tomasz=Lewowski: tomasz.lewowski@pwr.edu.pl; Lech=Madeyski: lech.madeyski@pwr.edu.pl,"Abstract
Context:
Code smells are symptoms of wrong design decisions or coding shortcuts that may increase defect rate and decrease 
maintainability
. Research on code smells is accelerating, focusing on code smell detection and using code smells as 
defect predictors
. Recent research shows that even between software developers, agreement on what constitutes a code smell is low, but several publications claim the high performance of detection algorithms—which seems counterintuitive, considering that algorithms should be taught on data labeled by developers.
Objective:
This paper aims to investigate the possible reasons for the inconsistencies between studies in the performance of applied 
machine learning algorithms
 compared to developers. It focuses on the reproducibility of existing studies.
Methods:
A systematic literature review was performed among conference and journal articles published between 1999 and 2020 to assess the state of reproducibility of the research performed in those papers. A quasi-gold standard procedure was used to validate the search. Modeling process descriptions, reproduction scripts, data sets, and techniques used for their creation were analyzed.
Results:
We obtained data from 46 publications. 22 of them contained a detailed description of the modeling process, 17 included any reproduction data (data set, results, or scripts) and 15 used existing data sets. In most of the publications, analyzed projects were hand-picked by the researchers.
Conclusion:
Most studies do not include any form of an online reproduction package, although this has started to change recently—8% of analyzed studies published before 2018 included a full reproduction package, compared to 22% in years 2018–2019. Ones that do include a package usually use a research group website or even a personal one. Dedicated archives are still rarely used for data packages. We recommend that researchers include complete reproduction packages for their studies and use well-established research data archives instead of their own websites.",21 Mar 2025,5,"Investigating inconsistencies in code smells research, while important for software development quality, does not provide immediate practical value for startups in early stages."
https://www.sciencedirect.com/science/article/pii/S0950584921002147,"Software security patch management - A systematic literature review of challenges, approaches, tools and practices",April 2022,Information and Software Technology,Not Found,Nesara=Dissanayake: nesara.madugodasdissanayakege@adelaide.edu.au; Asangi=Jayatilaka: asangi.jayatilaka@adelaide.edu.au; Mansooreh=Zahedi: mansooreh.zahedi@unimelb.edu.au; M. Ali=Babar: ali.babar@adelaide.edu.au,"Abstract
Context:
Software security patch management purports to support the process of patching known software 
security vulnerabilities
. Patching 
security vulnerabilities
 in large and complex systems is a hugely challenging process that involves multiple stakeholders making several interdependent technological and socio-technical decisions. Given the increasing recognition of the importance of software security patch management, it is important and timely to systematically review and synthesise the relevant literature on this topic.
Objective:
This paper aims at systematically reviewing the state of the art of software security patch management to identify the socio-technical challenges in this regard, reported solutions (i.e., approaches, tools, and practices), the rigour of the evaluation and the industrial relevance of the reported solutions, and to identify the gaps for future research.
Method:
We conducted a systematic literature review of 72 studies published from 2002 to March 2020, with extended coverage until September 2020 through forward snowballing.
Results:
We identify 14 socio-technical challenges in software security patch management, 18 solution approaches, tools and practices mapped onto the software security patch management process. We provide a mapping between the solutions and challenges to enable a reader to obtain a holistic overview of the gap areas. The findings also reveal that only 20.8% of the reported solutions have been rigorously evaluated in industrial settings.
Conclusion:
Our results reveal that 50% of the common challenges have not been directly addressed in the solutions and that most of them (38.9%) address the challenges in one phase of the process, namely vulnerability scanning, assessment and prioritisation. Based on the results that highlight the important concerns in software security patch management and the lack of solutions, we recommend a list of future research directions. This study also provides useful insights about different opportunities for practitioners to adopt new solutions and understand the variations of their practical utility.",21 Mar 2025,7,"The systematic review of software security patch management highlights socio-technical challenges and gaps for future research, which can directly impact startups focusing on software security and system maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584921002287,Impact of software development processes on the outcomes of student computing projects: A tale of two universities,April 2022,Information and Software Technology,"Software engineering, Comparative study, Capstone project, Student projects, Education, Computer science education",Rafal=Włodarski: Not Found; Aneta=Poniszewska-Marańda: aneta.poniszewska-maranda@p.lodz.pl; Jean-Remy=Falleri: Not Found,"Abstract
Context:
Project-based courses are more and more commonly used as an opportunity to teach students structured methods of developing software. Two well-known approaches in this area – traditional and Agile – have been successfully applied to drive academic projects. However too often the default is still to have no organizational process at all. While a large variety of software development life-cycle models exists, little guidance is available on which one to choose to fit the context of working with students.
Objective:
This paper assesses the impact of iterative, sequential and “hands-off” development approaches on the success of student computing projects. A structured, metric-based assessment scheme was applied to investigate team productivity, teamwork and the quality of the final product.
Method:
Empirical evidence was collected during a controlled experiment carried out at two engineering schools in Europe. More than 100 students at Bachelor’s and Master’s levels participated in the research, with varied software development and teamwork skill sets.
Results:
Similar patterns were observed among both sets of subjects, with iterative teams demonstrating the highest productivity and superior team cohesion but a decline in the quality of the final product. Sequential development led to a considerable improvement in the external 
quality characteristics
 of the software produced, owing to the method’s stress on design activities.
Conclusion:
The findings of this study will be of use to educators interested in applying software development processes to student groupwork. A set of guidelines is provided for applying a structured way of working in a project-based course.",21 Mar 2025,6,The study provides guidelines for educators working on student group projects which can be valuable for startups focused on educational technology solutions.
https://www.sciencedirect.com/science/article/pii/S0950584921002020,Erratum: Leveraging Flexible Tree Matching to repair broken locators in web automation scripts,April 2022,Information and Software Technology,Not Found,Sacha=Brisset: sacha.brisset@hotmail.fr; Romain=Rouvoy: romain.rouvoy@univ-lille.fr; Lionel=Seinturier: lionel.seinturier@inria.fr; Renaud=Pawlak: renaud.pawlak@gmail.com,"Abstract
Web applications are constantly evolving to integrate new features and fix reported bugs. Even an imperceptible change can sometimes entail significant modifications of the 
Document Object Model
 (DOM), which is the underlying model used by browsers to render all the elements included in a web application. Scripts that interact with web applications (
e.g.
 web test scripts, crawlers, or robotic process automation) rely on this continuously evolving DOM which means they are often particularly fragile. More precisely, the major cause of breakages observed in automation scripts are 
element locators
, which are identifiers used by automation scripts to navigate across the DOM. When the DOM evolves, these identifiers tend to break, thus causing the related scripts to no longer locate the intended target elements.
For this reason, several contributions explored the idea of automatically repairing broken locators on a page. These works attempt to repair a given broken locator by scanning all elements in the new DOM to find the most similar one. Unfortunately, this approach fails to scale when the complexity of web pages grows, leading either to long 
computation times
 or incorrect element repairs. This article, therefore, adopts a different perspective on this problem by introducing a new locator repair solution that leverages 
tree
 
matching algorithms
 to relocate broken locators. This solution, named 
Erratum
, implements a holistic approach to reduce the element 
search space
, which greatly eases the locator repair task and drastically improves repair accuracy. We compare the robustness of 
Erratum
 on a large-scale benchmark composed of realistic and synthetic mutations applied to popular web applications currently deployed in production. Our empirical results demonstrate that 
Erratum
 outperforms the accuracy of WATER, a state-of-the-art solution, by 67%.",21 Mar 2025,8,"The development of 'Erratum' solution to repair broken locators in web applications can be highly beneficial for startups relying on web automation tools, improving efficiency and accuracy."
https://www.sciencedirect.com/science/article/pii/S0950584921002342,A model-driven approach to reengineering processes in cloud computing,April 2022,Information and Software Technology,Not Found,Mahdi=Fahmideh: mahdi.fahmideh@usq.edu.au; John=Grundy: Not Found; Ghassan=Beydoun: Not Found; Didar=Zowghi: Not Found; Willy=Susilo: Not Found; Davoud=Mougouei: Not Found,"Abstract
Context
The 
reengineering process
 of large data-intensive legacy 
software applications
 (“legacy applications” for brevity) to cloud platforms involves different interrelated activities. These activities are related to planning, architecture design, re-hosting/lift-shift, 
code refactoring
, and other related ones. In this regard, the 
cloud computing
 literature has seen the emergence of different methods with a disparate point of view of the same underlying legacy application 
reengineering process
 to cloud platforms. As such, the effective interoperability and tailoring of these methods become problematic due to the lack of integrated and consistent standard models.
Objective
We design, implement, and evaluate a novel framework called 
MLSAC (Migration of Legacy Software Applications to the Cloud)
. The core aim of MLSAC is to facilitate the sharing and tailoring of reengineering methods for migrating legacy applications to cloud platforms. MLSAC achieves this by using a collection of coherent and empirically tested cloud-specific method fragments from the literature and practice. A metamodel (or meta-method) together with corresponding 
instantiation
 guidelines is developed from this collection. The metamodel can also be used to create and maintain bespoke reengineering methods in a given scenario of reengineering to cloud platforms.
Approach
MLSAC is underpinned by a metamodeling approach that acts as a representational layer to express reengineering methods. The design and evaluation of MLSAC are informed by the guidelines from the 
design science research
 approach.
Results
Our framework is an accessible guide of what legacy-to-cloud reengineering methods can look like. The efficacy of the framework is demonstrated by modeling real-world reengineering scenarios and obtaining user feedback. Our findings show that the framework provides a fully-fledged domain-specific, yet platform-independent, foundation for the semi-automated representing, maintaining, sharing, and tailoring reengineering methods. MLSAC contributes to the state of the art of 
cloud computing
 and model-driven 
software engineering
 literature through (a) providing a collection of mainstream method fragments for incorporate into various scenarios of reengineering processes and (b) enabling a basis for consistent creation, representation, and maintenance of reengineering methods and processes within the cloud computing community.",21 Mar 2025,7,The MLSAC framework for migrating legacy software applications to cloud platforms can offer startups efficient solutions for restructuring legacy applications in a cloud environment.
https://www.sciencedirect.com/science/article/pii/S0950584921002366,"Prioritizing user concerns in app reviews – A study of requests for new features, enhancements and bug fixes",April 2022,Information and Software Technology,Not Found,Saurabh=Malgaonkar: Not Found; Sherlock A.=Licorish: sherlock.licorish@otago.ac.nz; Bastin Tony Roy=Savarimuthu: tony.savarimuthu@otago.ac.nz,"Abstract
Context
: App developers spend exhaustive manual efforts towards the identification and prioritization of informative end-user reviews. Informative reviews are those that express end-users’ requests for new features, bug fixes and possible enhancements. 
Problem Statement
: While prior studies have proposed approaches to convert app reviews into 
actionable knowledge
, these are limited in utility due to being domain knowledge dependent or manually-based.
Objective
: In this study, in order to facilitate app maintenance and evolution cycles, we develop two novel automated prioritization techniques to rank informative reviews, and also compare their performances.
Method
: We developed the techniques comprising of entropy, frequency, TF-IDF and sentiment scoring methods using reviews from four popular apps comprising more than 1000 informative reviews in each app. Time and accuracy metrics were then used to measure the performance of our techniques. We performed evaluations where the ranking outcomes generated by our techniques were compared to those provided by regular app users and developers using two rounds of evaluations (internal and external evaluations).
Results
: Our results show that the time required for prioritization was in the range of 17–25 s and the accuracy of prioritization was in the range of 73–90%.
Conclusion
: These are promising outcomes when compared to prior work, where our outcomes were 4% and 185% better in terms of accuracy and time respectively. Thus, it is anticipated that our proposed techniques could support app developers in identifying and prioritizing informative reviews.",21 Mar 2025,9,"The automated prioritization techniques for app review ranking can significantly enhance the app maintenance and evolution cycles for startups, improving productivity and decision-making processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002330,An exploratory study of bug prediction at the method level,April 2022,Information and Software Technology,Not Found,Ran=Mo: moran@mail.ccnu.edu; Shaozhi=Wei: wsz@mails.ccnu.edu.cn; Qiong=Feng: qiongfeng@njust.edu.cn; Zengyang=Li: zengyangli@mail.ccnu.edu.cn,"Abstract
Context:
During the past decades, researchers have proposed numerous studies to predict bugs at different 
granularity
 levels, such as the file level, package level, module level, etc. However, the prediction models at the method level are rarely investigated.
Objective:
In this paper, we investigate to predict bug-prone methods based on method-level 
code metrics
 or history measures, and analyze the prediction importance of each metric.
Method:
To proceed our study, we first propose a series of 
code metrics
 and history measures for conducting method-level bug predictions. Next, we compare the performance of different types of prediction models. Finally, we conduct analyses about the prediction power of each metric, based on which, we further analyze whether we can simplify the prediction models.
Results:
Through our evaluation on eighteen large-scale projects, we have presented: (1) conducting method-level bug prediction has potentials of saving a large portion of effort on code reviews and inspections; (2) models using the proposed code metrics or history measures could achieve a good prediction performance; (3) the prediction importance of each metric distributes differently; (4) a highly simplified prediction model could be derived by just using a few important metrics.
Conclusion:
This study presents how to systematically build models for predicting bug-prone methods, and provides empirical evidence for developers to best select metrics to build method-level bug prediction models.",21 Mar 2025,7,"The study on predicting bug-prone methods and identifying important metrics can assist startups in improving software quality assurance processes, potentially saving time and effort on code reviews."
https://www.sciencedirect.com/science/article/pii/S0950584921002354,Consistent or not? An investigation of using Pull Request Template in GitHub,April 2022,Information and Software Technology,Not Found,Mengxi=Zhang: Not Found; Huaxiao=Liu: liuhuaxiao@jlu.edu.cn; Chunyang=Chen: Not Found; Yuzhou=Liu: Not Found; Shuotong=Bai: Not Found,"Abstract
Context:
The arbitrary usage of pull requests in GitHub may bring many issues such as incomplete, verbose, and duplicated descriptions, which hinder the development and maintenance of the project. Thus, GitHub proposed the Pull Request 
Template
 (PRT) in 2016 so that developers could edit the pull request in a relevant consistent manner. However, whether the PRT has been widely applied to GitHub and what impact it might bring remain little known. Such uninformed cases may affect the efficiency of cooperative development.
Objective:
In this work, we conduct an empirical study on large-scale repositories to explore whether the PRT has been widely applied and what impact it can bring to the GitHub community.
Method:
This work aims to answer four research questions. The first is a statistical experiment with the aim of analyzing the current status of PRTs. The second is an explored experiment, which aims at probing which repositories are suitable to adopt the PRT. The third is the measurement evaluation experiment, focusing on discussing what impact the PRT can bring. The last is an online survey to explain why few PRTs have been adopted. Notably, both the second and third questions are conducted a mixed quantitative and qualitative analysis.
Results and conclusion:
In this work, we find that only 1.2% of repositories contain the PRT in GitHub, and such repositories are mostly in high popularity and contain a large number of PRs. Besides, contributors are willing to accept the PRT that requires pivotal information, including description, test, and check_list. Meanwhile, the PRT can assist developers to manage repositories, reflecting in less time for reviewing, fewer duplicated pull requests, and almost non-existentially invalid comments. Finally, we survey 527 well-reputed developers to explain why few repositories adopt the PRT, and further provide some actionable suggestions.",21 Mar 2025,8,The study on the impact of Pull Request Template on GitHub repositories can benefit early-stage ventures by improving development efficiency and cooperation.
https://www.sciencedirect.com/science/article/pii/S0950584921002408,Indexing source code and clone detection,April 2022,Information and Software Technology,Not Found,Zdenek=Tronicek: tronicek@tarleton.edu,"Abstract
Context:
Searching 
source code
 is a common task in code recommendation systems as well as in many other areas. Clone detection is used in software maintenance and bug detection.
Objective:
The paper introduces an algorithm for building the index structure of 
abstract syntax trees
. When the index structure is built, a pattern tree can be found in time linear in the length of the pattern. Furthermore, the paper describes 
DrDup2
 and 
DrDupLex
, two open-source tools that use the index structure to find Type-2 clones.
Method:
The index structure presented in this paper is based on the trie, which is a fundamental 
data structure
 in computer science. Evaluation of the presented clone detectors is done on BigCloneBench, which is a well-established benchmark for clone detection.
Results:
Comparison with three state-of-the-art clone detectors (
NiCad
, 
CloneWorks
 and 
SourcererCC
) shows that 
DrDup2
 and 
DrDupLex
 are able to beat them in precision, recall and running time.
Conclusion:
The presented index structure can be used for example to speed up searching for code fragments in code recommendation systems. It is also shown that it can be used to detect Type-2 clones with high precision and recall.",21 Mar 2025,5,"The algorithm for building index structures of abstract syntax trees can be beneficial for code recommendation systems, but its practical impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921002421,VUDENC: Vulnerability Detection with Deep Learning on a Natural Codebase for Python,April 2022,Information and Software Technology,Not Found,Laura=Wartschinski: wartschinski@informatik.hu.berlin.de; Yannic=Noller: noller@informatik.hu.berlin.de; Thomas=Vogel: thomas.vogel@informatik.hu.berlin.de; Timo=Kehrer: kehrer@informatik.hu.berlin.de; Lars=Grunske: grunske@informatik.hu.berlin.de,"Abstract
Context:
Identifying potential vulnerable code is important to improve the security of our software systems. However, the manual detection of software vulnerabilities requires expert knowledge and is time-consuming, and must be supported by automated techniques.
Objective:
Such automated 
vulnerability detection
 techniques should achieve a high accuracy, point developers directly to the vulnerable code fragments, scale to real-world software, generalize across the boundaries of a specific software project, and require no or only moderate setup or configuration effort.
Method:
In this article, we present 
Vudenc
 (Vulnerability Detection with 
Deep Learning
 on a Natural Codebase), a deep learning-based 
vulnerability detection
 tool that automatically learns features of vulnerable code from a large and real-world Python codebase. 
Vudenc
 applies a word2vec model to identify semantically similar code tokens and to provide a vector representation. A network of long-short-term memory cells (LSTM) is then used to classify vulnerable code token sequences at a fine-grained level, highlight the specific areas in the source code that are likely to contain vulnerabilities, and provide confidence levels for its predictions.
Results:
To evaluate 
Vudenc
, we used 1,009 vulnerability-fixing commits from different GitHub repositories that contain seven different types of vulnerabilities (SQL injection, XSS, Command injection, XSRF, 
Remote code execution
, Path disclosure, Open redirect) for training. In the experimental evaluation, 
Vudenc
 achieves a recall of 78%–87%, a precision of 82%–96%, and an F1 score of 80%–90%. 
Vudenc
’s code, the datasets for the vulnerabilities, and the Python corpus for the word2vec model are available for reproduction.
Conclusions:
Our experimental results suggest that 
Vudenc
 is capable of outperforming most of its competitors in terms of vulnerably detection capabilities on real-world software. Comparable accuracy was only achieved on synthetic benchmarks, within single projects, or on a much coarser level of 
granularity
 such as entire 
source code files
.",21 Mar 2025,9,"Vudenc, a vulnerability detection tool, can greatly enhance the security of software systems for European early-stage ventures by automating the identification of vulnerabilities with high accuracy."
https://www.sciencedirect.com/science/article/pii/S0950584921002081,iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms,April 2022,Information and Software Technology,Not Found,Mohammad=Hamdaqa: mhamdaqa@polymtl.ca; Lucas Alberto Pineda=Met: Not Found; Ilham=Qasse: Not Found,"Abstract
Context:
Smart contracts
 play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of 
contractual rules
. In addition to business knowledge, coding a smart contract requires strong 
technical knowledge
 in a multiplex of new and rapidly changing domain-specific languages and 
blockchain
 platforms.
Objectives:
The goal of this paper is to assist developers in 
building smart
 contracts independently from the language or the target 
blockchain
 platform. In which, we present our second-generation smart contract language iContractML 2.0.
Methods:
We follow a feature-oriented approach to analyze three different 
blockchain
 platforms and propose an enhanced reference model and a modeling framework for smart contracts (iContractML 2.0). Then, we evaluate the coverage and extensibility of iContractML 2.0, first through mapping the concepts of the reference models to the constructs within each of the platforms used in devising the reference model, and second through mapping its concepts to a new smart contract language not previously considered. Finally, we demonstrate the capabilities of iContractML 2.0 using five 
case studies
 from different business domains.
Results:
iContractML 2.0 extends our first generation language to support 
DAML
, which is another standardized language for smart contracts. This makes iContractML 2.0 supports the platforms that 
DAML
 support by extension. Moreover, iContractML 2.0 supports generating the structural and deployment artifacts in addition to the smart contract behavior by implementing templates for some of the common functions. The results of evaluating the generality of the iContractML 2.0 reference model show that it is 91.7% lucid and 72.2% laconic. Moreover, the reference model is able to capture all the elements of the new language with 83.3% of the components which have a direct one-to-one mapping.
Conclusion:
iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple 
blockchain
 platforms.",21 Mar 2025,7,"iContractML 2.0 provides a framework for building smart contracts independently from language or blockchain platform, which can be valuable for startups working with smart contracts."
https://www.sciencedirect.com/science/article/pii/S095058492100241X,HyMap: Eliciting hypotheses in early-stage software startups using cognitive mapping,April 2022,Information and Software Technology,Not Found,Jorge=Melegati: jorge.melegati@unibz.it; Eduardo=Guerra: eduardo.guerra@unibz.it; Xiaofeng=Wang: xiaofeng.wang@unibz.it,"Abstract
Context:
 Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation, an approach based on validating assumptions about the software product through data obtained from diverse techniques, like A/B tests or interviews, is valuable for these companies. Relying on data rather than opinions reduces the chance of developing unnecessary products or features, improving the likelihood of success, especially in early development stages, when implementing unnecessary features represents a higher risk for companies’ survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. Since the first step of the approach is to define hypotheses, testable statements about the software product features, based on which software development teams will create experiments, eliciting hypotheses is a natural first step to develop practices. 
Objective:
 We aim to develop a systematic technique for identifying hypotheses in early-stage software startups to support experimentation in these companies and, consequently, improve their software products. 
Methods:
 We followed a Design Science approach consisting of an artifact 
construction process
, divided in three phases, and an evaluation within three startups. 
Results:
 We developed the HyMap, a hypotheses 
elicitation
 technique based on 
cognitive mapping
. It consists of a process conducted by a facilitator using pre-defined questions, supported by a visual language to depict a cognitive map representing the founder’s understanding of the product. Our evaluation showed that founders perceived the artifacts as clear, easy to use, and useful leading to hypotheses and facilitating their idea’s visualization. 
Conclusion:
 From a theoretical perspective, our study provides a better understanding of the guidance founders use to develop their startups and, from a practical point of view, a technique to identify hypotheses in early-stage software startups.",21 Mar 2025,6,"The systematic technique for identifying hypotheses in early-stage software startups can help improve software products, though its direct impact on European ventures may be moderate."
https://www.sciencedirect.com/science/article/pii/S095058492100238X,Introduction to the Special Issue on value and waste in software engineering,April 2022,Information and Software Technology,Not Found,Matthias=Galster: Not Found; Clemente=Izurieta: Not Found; Carolyn=Seaman: Not Found,"Abstract
In the context of software engineering, “value” and “waste” can mean different things to different stakeholders. While traditionally value and waste have been considered from a business or economic point of view, there has been a trend in recent years towards a broader perspective that also includes wider human and societal values. This Special Issue explores value and waste aspects in all areas of software engineering, including identifying, quantifying, reasoning about, and representing value and waste, driving value and avoiding waste, and managing value and waste. In this editorial we provide an introduction to the topic and provide an overview of the contributions included in this Special Issue.",21 Mar 2025,4,"The abstract discusses a broader perspective on value and waste in software engineering, but the practical impact on European early-stage ventures is not clearly addressed."
https://www.sciencedirect.com/science/article/pii/S0950584921002263,When should we (not) use the mean magnitude of relative error (MMRE) as an error measure in software development effort estimation?,March 2022,Information and Software Technology,Not Found,Magne=Jørgensen: magnej@simula.no; Torleif=Halkjelsvik: torleif@simula.no; Knut=Liestøl: knut@ifi.uio,"Abstract
Context
The mean magnitude of relative error (MMRE) is an error measure frequently used to evaluate and compare the estimation performance of prediction models and software professionals.
Objective
This paper examines conditions for proper use of MMRE in effort estimation contexts.
Method
We apply research on scoring functions to identify the type of estimates that minimizes the expected value of the MMRE.
Results
We show that the MMRE is a proper error measure for estimates of the most likely (mode) effort, but not for estimates of the median or mean effort, provided that the effort usage is approximately log-normally distributed, which we argue is a reasonable assumption in many software development contexts. The relevance of the findings is demonstrated on real-world software development data.
Conclusion
MMRE is not a proper measure of the accuracy of estimates of the median or mean effort, but may be used for the accuracy evaluation of estimates of most likely effort.",21 Mar 2025,6,"The abstract provides insights into the proper use of error measures in software development, which can have a positive impact on European startups striving for accurate estimation and performance evaluation."
https://www.sciencedirect.com/science/article/pii/S0950584921001865,A comparison of machine learning algorithms on design smell detection using balanced and imbalanced dataset: A study of God class,March 2022,Information and Software Technology,"Software quality, Design smell detection, Machine learning, God class, Balanced data",Khalid=Alkharabsheh: khalidkh@bau.edu.jo; Sadi=Alawadi: sadi.alawadi@it.uu.se; Victor R.=Kebande: victor.kebande@bth.se; Yania=Crespo: yania@infor.uva.es; Manuel=Fernández-Delgado: manuel.fernandez.delgado@usc.es; José A.=Taboada: joseangel.taboada@usc.es,"Abstract
Context:
Design smell detection has proven to be a significant activity that has an aim of not only enhancing the software quality but also increasing its life cycle.
Objective:
This work investigates whether 
machine learning approaches
 can effectively be leveraged for 
software design
 smell detection. Additionally, this paper provides a comparatively study, focused on using balanced datasets, where it checks if avoiding dataset balancing can be of any influence on the accuracy and behavior during design smell detection.
Method:
A set of experiments have been conducted-using 28 
Machine Learning
 classifiers aimed at detecting God classes. This experiment was conducted using a dataset formed from 12,587 classes of 24 software systems, in which 1,958 classes were manually validated.
Results:
Ultimately, most classifiers obtained high performances,-with Cat Boost showing a higher performance. Also, it is evident from the experiments conducted that data balancing does not have any significant influence on the accuracy of detection. This reinforces the application of machine learning in real scenarios where the data is usually imbalanced by the inherent nature of design smells.
Conclusions:
Machine learning approaches can effectively be used as a leverage for God class detection. While in this paper we have employed SMOTE technique for data balancing, it is worth noting that there exist other methods of data balancing and with other design smells. Furthermore, it is also important to note that application of those other methods may improve the results, in our experiments SMOTE did not improve God class detection.
The results are not fully generalizable because only one design smell is studied with projects developed in a single programming language, and only one balancing technique is used to compare with the imbalanced case. But these results are promising for the application in real design smells detection scenarios as mentioned above and the focus on other measures, such as Kappa, ROC, and MCC, have been used in the assessment of the 
classifier behavior
.",21 Mar 2025,9,"The abstract delves into the application of machine learning for design smell detection, showcasing high performances and discussing the significance of data balancing. This can benefit early-stage ventures in improving software quality and development processes."
https://www.sciencedirect.com/science/article/pii/S0950584921002056,An information theoretic notion of software testability,March 2022,Information and Software Technology,Not Found,Krishna=Patel: krishna.patel@sheffield.ac.uk; Robert M.=Hierons: r.hierons@sheffield.ac.uk; David=Clark: david.clark@ucl.ac.uk,"Abstract
Context:
In software testing, Failed 
Error Propagation
 (FEP) is the situation in which a faulty program state occurs during the execution of the system under test (SUT) but this does not lead to incorrect output. It is known that FEP can adversely affect software testing and this has resulted in associated information 
theoretic measures
.
Objective:
To devise measures that can be used to assess the 
testability
 of the SUT. By testability, we mean how likely it is that a faulty program state, that occurs during testing, will lead to incorrect output. Previous work has considered a single program point rather than an entire program.
Method:
New, more fine-grained, measures were devised. Experiments were used to evaluate these and the previously defined measures (Squeeziness and Normalised Squeeziness). The experiments assessed how well these measures correlated with an estimate of the probability of FEP occurring during testing. Mutants were used to estimate this probability.
Results:
A strong rank correlation was found between several of the measures and the probability of FEP. Importantly, this included the Normalised Squeeziness of the whole SUT, which is simpler to compute, or estimate, than most of the other measures considered. Additional experiments found that the measures were relatively insensitive to the choice of mutants and also test suite.
Conclusion:
There is scope to use information 
theoretic measures
 to estimate how prone an SUT is to FEP. As a result, there is potential to use such measures to prioritise testing or estimate how much testing an SUT might require.",21 Mar 2025,7,"The abstract introduces measures to assess testability in software testing, which can aid European startups in evaluating the reliability and effectiveness of their software products."
https://www.sciencedirect.com/science/article/pii/S0950584921002068,Refactoring embedded software: A study in healthcare domain,March 2022,Information and Software Technology,Not Found,Paraskevi=Smiari: psmiari@uowm.gr; Stamatia=Bibi: sbibi@uowm.gr; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Elvira-Maria=Arvanitou: e.arvanitou@uom.edu.gr,"Abstract
Context
In 
embedded software
 industry, stakeholders usually promote run-time properties (e.g., performance, energy efficiency, etc.) as quality drivers, which in many cases leads to a compromise at the levels of design-time qualities (e.g., 
maintainability
, reusability, etc.). Such a compromise does not come without a cost; since embedded systems need heavy maintenance cycles. To assure effective bug-fixing, shorten the time required for releasing updates, a refactoring of the software codebase needs to take place regularly. Objective: This study aims to investigate how refactorings are applied in ES industry; and propose a systematic approach that can guide refactoring through a 3-step process for refactoring: (a) planning; (b) design; and (c) evaluation.
Method
The aforementioned goals were achieved by conducting a single case study in a company that develops medical applications for bio-impedance devices; and follows a rather systematic 
refactoring process
 in periodic timestamps. Three 
data collection approaches
 have been used: surveys, interviews (10 practitioners), and artifact analysis (51 refactoring activities).
Results
The results of the study suggest that: (a) 
maintainability
 and reusability are the design-time quality attributes that motivate the refactoring of Embedded Software (ES), with 30% of the participants considering them as of “Very High” importance; (b) the refactorings that are most frequently performed are “Extract Method”, “Replace Magic Number with Constant” and “Remove Parameter”. We note that the “Extract Method” refactoring has an applicability of more than over 80%; and (c) to evaluate the 
refactoring process
 engineers use tools producing structural metrics, internal standards, and reviews.
Conclusions
The outcomes of this study can be useful to both researchers and practitioners, in the sense that the former can focus their efforts on aspects that are meaningful to industry, whereas the latter are provided with a systematic refactoring process.",21 Mar 2025,8,"The abstract presents a systematic approach for refactoring in the embedded software industry, addressing quality attributes like maintainability and reusability. This can provide valuable guidance for European early-stage ventures seeking to improve their software development practices."
https://www.sciencedirect.com/science/article/pii/S0950584921002251,"Relative estimates of software development effort: Are they more accurate or less time-consuming to produce than absolute estimates, and to what extent are they person-independent?",March 2022,Information and Software Technology,Not Found,Magne=Jørgensen: magnej@simula.no; Eban=Escott: Not Found,"Abstract
Context
Estimates of software development effort may be given as judgments of relationships between the use of efforts on different tasks-that is, as relative estimates. The use of relative estimates has increased with the introduction of story points in 
agile software development
 contexts.
Objective
This study examines to what extent relative estimates are likely to be more accurate or less time-consuming to produce than absolute software development effort estimates and to what extent relative estimates can be considered developer-independent.
Method
We conducted two experiments. In the first experiment, we collected estimates from 102 professional software developers estimating the same tasks and randomly allocated to providing relative estimates in story points or absolute estimates in work-hours. In the second experiment, we collected the actual efforts of 20 professional software developers completing the same 5 programming tasks and used these to analyze the variance in relative efforts.
Results
The results from the first experiment indicates that the relative estimates were less accurate than the absolute estimates, and that the time consumed completing the estimation work was higher for those using relative estimation, even when only considering developers with extensive 
prior experience
 in story point–based estimation for both tasks. The second experiment revealed that the relative effort was far from developer-independent, especially for the least productive developers. This suggests that relative estimates to a large extent are developer-dependent.
Conclusions
Although there may be good reasons for the continued use of relative estimates, we interpret our results as not supporting that the use of relative estimates is connected with higher estimation accuracy or less time consumed on producing the estimates. Neither do our results support a high degree of developer-independence in relative estimates.",21 Mar 2025,3,The study shows that relative estimates are less accurate and time-consuming. This could impact startups relying on agile methodologies.
https://www.sciencedirect.com/science/article/pii/S0950584921002123,An Adaptive Penalty based Parallel Tabu Search for Constrained Covering Array Generation,March 2022,Information and Software Technology,Not Found,Yan=Wang: Not Found; Huayao=Wu: hywu@nju.edu.cn; Xintao=Niu: Not Found; Changhai=Nie: Not Found; Jiaxi=Xu: Not Found,"Abstract
Context:
The generation of the optimal constrained covering arrays is a key challenge in the research field of combinatorial testing, where a variety of Constrained Covering Array Generation (CCAG) algorithms have been developed. However, existing algorithms typically reuse 
constraint solver
 or forbidden tuple-based techniques to handle constraints, which might restrict their potentials on finding smaller arrays.
Objective:
This work dedicates to exploring more effective constraint handling techniques for CCAG, so that the sizes of constrained covering arrays can be further minimized.
Methods:
We propose a novel Adaptive Penalty based Parallel Tabu Search (APPTS) algorithm to address the CCAG problem. 
APPTS
 incorporates a penalty term into the fitness function to handle the constrained 
search space
, and employs an adaptive penalty mechanism to dynamically adjust the penalty weight in different search phases. Moreover, 
APPTS
 adopts Java Parallel Stream to compute the fitness values of candidate solutions to speed up the generation process.
Results:
The performance of APPTS is evaluated against three alternative tabu search-based algorithms (with different penalty and 
parallelization
 mechanisms), and seven state-of-the-art algorithms for CCAG. The results demonstrate the superiority of APPTS over these existing algorithms. In particular, APPTS finds 22 new upper bounds on the sizes of 2-way and 3-way constrained covering arrays.
Conclusion:
The adaptive penalty mechanism provides an effective choice for handling constraints in CCAG, and the 
parallelization
 can help APPTS reduce the generation cost.",21 Mar 2025,9,"The proposed algorithm improves constrained covering arrays generation, which can benefit startups in software testing efficiency and optimization."
https://www.sciencedirect.com/science/article/pii/S095058492100210X,Developers’ viewpoints to avoid bug-introducing changes,March 2022,Information and Software Technology,Not Found,Jairo=Souza: jrmcs@ic.ufal.br; Rodrigo=Lima: Not Found; Baldoino=Fonseca: Not Found; Bruno=Cartaxo: Not Found; Márcio=Ribeiro: Not Found; Gustavo=Pinto: Not Found; Rohit=Gheyi: Not Found; Alessandro=Garcia: Not Found,"Abstract
Context:
During software development, developers can make assumptions that guide their development practices to avoid bug-introducing changes. For instance, developers may consider that code with low test coverage is more likely to introduce bugs; and thus, focus their attention on that code to avoid bugs, neglecting other factors during the software development process. However, there is no knowledge about the relevance of these assumptions for developers.
Objective:
This study investigates the developers’ viewpoints on the relevance of certain assumptions to avoid bug-introducing changes. In particular, we analyze which assumptions developers can make during software development; how relevant these assumptions are for developers; the common viewpoints among developers regarding these assumptions; and the main reasons for developers to put more/less relevance for some assumptions.
Method:
We applied the Q-methodology, a mixed-method from the psychometric spectrum, to investigate the relevance of assumptions and extract the developers’ viewpoints systematically. We involved 41 developers analyzing 41 assumptions extracted from literature and personal interviews.
Results:
We identified five viewpoints among developers regarding their assumptions around bug-introducing changes. Despite the differences among the viewpoints, there is also consensus, for example, regarding the importance of being aware of changes invoking high number of features. Moreover, developers rely on personal and technical reasons to put relevance on some assumptions.
Conclusion:
These findings are valuable knowledge for practitioners and researchers towards future research directions and development practices improvements.",21 Mar 2025,6,Understanding developers' assumptions to avoid bug-introducing changes can help startups enhance their development practices and reduce bugs.
https://www.sciencedirect.com/science/article/pii/S095058492100207X,Summarizing source code with hierarchical code representation,March 2022,Information and Software Technology,Not Found,Ziyi=Zhou: zhouziyi@mail.ecust.edu.cn; Huiqun=Yu: yhq@ecust.edu.cn; Guisheng=Fan: Not Found; Zijie=Huang: Not Found; Xingguang=Yang: Not Found,"Abstract
Context
Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area. Data-driven code summarization models based on 
neural networks
 have proliferated in recent few years.
Objective
Almost all of existing 
neural models
 are built upon the 
granularity
 of token or 
AST
 node. This has several drawbacks: a) Code summarization requires high-level knowledge of code while token representations are limited to provide a global view; b) Such approaches can hardly model the hierarchy of code; c) Long input codes challenge such models to handle long-range dependencies due to the large number of tokens and 
AST
 nodes.
Method
To address these issues, we propose a novel framework to utilize hierarchical representation of code to generate better summaries. We consider two levels of code hierarchy: token-level and statement-level. Our framework contains a pair of customized encoder-decoder models for tokens and AST of code respectively. Each of them has a hierarchical encoder that aims to extract both token and statement-level code features, and an attentional decoder with the ability to attend to those different levels of representation during decoding. They are then combined to predict summaries via 
ensemble learning
.
Results
We conduct extensive experiments to evaluate our models on a large Java corpus. The experimental results show that our approach outperforms several state-of-the-art baselines by a substantial margin.
Conclusion
In conclusion, our approach could better learn global information of code and shift attention between important statements during summary generation. With the help of hierarchical attention, the models are able to locate keywords more accurately in a top-down way. Ensemble learning is also proved to be an effective way to benefit from multiple input sources.",21 Mar 2025,8,"The hierarchical model for code summarization outperforms existing models, which can be valuable for startups in code documentation and understanding."
https://www.sciencedirect.com/science/article/pii/S0950584921002275,Task assignment to counter the effect of developer turnover in software maintenance: A knowledge diffusion model,March 2022,Information and Software Technology,Not Found,Vahid=Etemadi: Not Found; Omid=Bushehrian: bushehrian@sutech.ac.ir; Gregorio=Robles: Not Found,"Abstract
Context:
Developer churn is the overall turnover in a software organization’s staff. Existing developers leave and new ones join the project. Retaining the knowledge of the software 
source code
 among the development team in such scenarios is an essential factor to keep the software maintenance cost as low as possible. 
Knowledge diffusion
 is an activity that could mitigate the 
negative impact
 of developer churn, while a task assignment strategy could pay an important role to attain good knowledge diffusion among the team members and effectively lower the likelihood of knowledge loss.
Objective:
In this work, a self-adaptive task assignment (SATA) approach is proposed that adaptively switches between cost-oriented and diffusion-oriented strategies over subsequent rounds of task assignments.
Method:
An entropy-based model is applied to estimate the current conditions of the development team from the knowledge concentration perspective. This model is assisted by a learning 
automata
 and 
evolutionary algorithms
 to offer smart assignments.
Results:
The experimental results show that, particularly in teams with medium churn rates, applying an entropy-aware task assignment model can reduce the total maintenance cost up to slightly over 50%, provided that the knowledge demands in the team over successive rounds of task assignment remain stationary. There are also improvements in terms of the projects’ 
bus factor
 which prevent the project to lose its key knowledge. Even for projects where there is no saving in maintenance costs, SATA results in knowledge being more distributed among the developers, resulting in a more resilient project.
Conclusion:
SATA improves the long-term 
sustainability
 of development teams with developer turnover. Projects and their managers can hence rely on it when there is the risk of knowledge loss due to developer turnover.",21 Mar 2025,7,"The self-adaptive task assignment approach can reduce maintenance costs and improve knowledge diffusion in development teams, benefiting startups with developer turnover."
https://www.sciencedirect.com/science/article/pii/S0950584921002111,Review4Repair: Code review aided automatic program repairing,March 2022,Information and Software Technology,Not Found,Faria=Huq: 1505052.fh@ugrad.cse.buet.ac.bd; Masum=Hasan: masum@ra.cse.buet.ac.bd; Md Mahim Anjum=Haque: mahim@vt.edu; Sazan=Mahbub: 1505020.sm@ugrad.cse.buet.ac.bd; Anindya=Iqbal: anindya@cse.buet.ac.bd; Toufique=Ahmed: tfahmed@ucdavis.edu,"Abstract
Context:
Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the 
source code
 of the software. These tools mostly exploit 
historical data
 of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers’ expertise and experience are reflected in these suggestions.
Objective:
The natural language instructions scripted on the review comments are enormous sources of information about the bug’s nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the 
performance improvement
 of repair techniques using code review comments.
Method:
We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques.
Results:
We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques.
Conclusion:
We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort.",21 Mar 2025,8,"The study presents significant improvements in fixing programming bugs by utilizing code review comments which can help developers save time and effort, making it valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921002135,Automated data function extraction from textual requirements by leveraging semi-supervised CRF and language model,March 2022,Information and Software Technology,Not Found,Mingyang=Li: Not Found; Lin=Shi: shilin@iscas.ac.cn; Yawen=Wang: Not Found; Junjie=Wang: Not Found; Qing=Wang: Not Found; Jun=Hu: Not Found; Xinhua=Peng: Not Found; Weimin=Liao: Not Found; Guizhen=Pi: Not Found,"Abstract
Context:
Function Point Analysis (FPA) provides an objective, comparative measure for size estimation in the early stage of software development. When practicing FPA, analysts typically abide by the following steps: data function (DF) extraction, transactional function extraction, function type classification and adjustment factor determination. However, due to lack of approach and tool support, these steps are usually conduct by human efforts in practice. Related approaches can hardly be applied in the FPA due to the following three challenges, i.e., FPA rule-driven extraction, domain-specific 
parsing
, and expensive labeled resources.
Objective:
In this paper, we aim to automate the extraction of DFs, which is the starting and fundamental step in FPA.
Method:
We propose an automated approach named DEX to extract data functions from textual requirements. Specifically, DEX introduces the popularly-used 
conditional random field
 (CRF) model to predict the boundary of a data function. Besides, DEX employs the bootstrapping-based algorithm and DF-oriented 
language model
 to further boost the performance.
Results:
We evaluate DEX from two aspects: evaluation on a real industrial dataset and a manual review by domain experts. The evaluation on the real industrial dataset shows that DEX could achieve 80% precision, 84% recall, and 82% F1, and outperforms three state-of-the-art baselines. The expert review suggests that DEX could increase 16% precision and 13% recall, compared with those produced by engineers.
Conclusion:
DEX could achieve promising results under a small number of labeled requirements and outperform the state-of-the-art approaches. Moreover, DEX could help engineers produce more accurate and complete DFs in the industrial environment.",21 Mar 2025,6,"Automating the extraction of data functions in Function Point Analysis could provide efficiency gains, but the impact on European early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921002044,Layout merging with relative positioning in Concern-Oriented Reuse hierarchies,March 2022,Information and Software Technology,Not Found,Hyacinth=Ali: hyacinth.ali@mail.mcgill.ca; Gunter=Mussbacher: gunter.mussbacher@mcgill.ca,"Abstract
Context:
The advent of modeling in 
software engineering
, like other engineering fields, has revolutionized the formalism and pace of software development. However, 
software applications
 are not built from scratch, instead, other existing software artifacts are reused and combined with new artifacts. This notion of 
software reuse
 has been in existence for decades. When structural models such as 
class diagrams
 are reused, the reusing and reused models often need to be merged and the result visualized to the modeler. However, layout mechanisms such as GraphViz, JGraphX, and other related layout tools do not retain the original layout and rather arbitrarily layout the merged models. Therefore, important information that corresponds to the mental map of a modeler and is conveyed by the specific layout is currently lost.
Objective:
This paper aims to establish layout algorithms to retain the original layout information from a set of individual but interrelated models after they are merged during 
software reuse
 to preserve a modeler’s mental map of the models.
Method:
In this work, rpGraph uses the 
relative positioning
 of model elements to retain the general layout of a single reusing model and a single reused model (two-model merge). Additionally, rpGraph integrates its two-model merge approach into a multi-model merge in a reuse hierarchy to preserve the general topology of several interrelated models. Our findings are evaluated with 20 example single-model reuses from a library of reusable software model artifacts. We further carry out a 
case study
 in a reuse hierarchy framework, Concern Oriented Reuse (CORE), where rpGraph is applied to the layout of reusable artifacts, which result from a merge of several individual models.
Result:
A comparison of the merged layouts of rpGraph, GraphViz, and JGraphX shows that rpGraph performs better in terms of retaining the original layouts.
Conclusion:
Considering 
relative positioning
 during model merge increases the degree with which original layouts can be preserved.",21 Mar 2025,7,"Retaining original layout information during software reuse can benefit developers in understanding and maintaining reused models, showing practical value for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921002093,Facilitating the co-evolution of semantic descriptions in standards and models,March 2022,Information and Software Technology,Not Found,Philip=Makedonski: makedonski@cs.uni-goettingen.de; Jens=Grabowski: grabowski@cs.uni-goettingen.de,"Abstract
Context:
Standardised specifications for sophisticated technologies are subdivided in multiple documents maintained by different working groups, typically accompanied by models and other formalised artefacts. As the specifications and the models evolve, ensuring their consistency at scale becomes challenging.
Objective:
While previous work developed a methodology for facilitating the co-evolution of models and standards, based on the Network Function 
Virtualisation
 (NFV) Information Model (IM) and models extracted from the related standardised specifications, the methodology focused on structural aspects only. This article refines the methodology, enabling the alignment of 
semantic descriptions
 of 
information elements
 and attributes, both across specifications and across 
information elements
.
Method:
To enable the alignment of 
semantic descriptions
, we extend the methodology by using statistical and visual analyses of terms used in the specifications. The underlying meta-model for the information extracted from the specifications is extended to accommodate the capturing of additional semantic information.
Results:
We report on our experiences with the application of a prototypical implementation of the methodology during the continued alignment and maintenance of the IM and the related standardised specifications. More than 400 potential inconsistencies were identified, leading to more than 100 contributions, some of which addressed multiple findings. Feedback from the working group provided insights on how to refine the methodology further.
Conclusions:
Models shall play a more central role and be better integrated throughout the specification development and implementation processes, helping to ensure and maintain consistency among specifications. Our experiences may provide useful insights into ongoing and future initiatives where similar challenges are faced.",21 Mar 2025,5,"Aligning semantic descriptions of information elements in standardized specifications may have limited immediate impact on early-stage ventures, affecting the score."
https://www.sciencedirect.com/science/article/pii/S0950584921002305,Programming language implementations for context-oriented self-adaptive systems,March 2022,Information and Software Technology,Not Found,Nicolás=Cardozo: n.cardozo@uniandes.edu.co; Kim=Mens: kim.mens@uclouvain.be,"Abstract
Context
The context-oriented programming paradigm is designed to enable self-adaptation, or dynamic behavior modification of software systems, in response to changes in their surrounding environment. Contextoriented programming offers an adaptation model, from a programming language perspective, that maintains a clean modularisation between the application and adaptation logic, as well as between the components providing adaptations.
Objective
We use three implementation techniques for context-oriented programming languages to assess their appropriateness to foster self-adaptive systems. These approaches take advantage of the capabilities offered by the host programming language to realize self-adaptation as proposed by context-oriented languages.
Method
We evaluate each of these approaches by assessing their modularity and complexity when defining adaptations, and by comparing their run-time performance on a simple benchmark.
Results
Our results show a higher modularity than that for common architecture based self-adaptive systems, while maintaining comparable performance.
Conclusion
We conclude that context-oriented programming is an appropriate paradigm to realize self-adaptation.",21 Mar 2025,7,Assessing implementation techniques for context-oriented programming for self-adaptive systems presents valuable insights for startups in adapting to changing environments.
https://www.sciencedirect.com/science/article/pii/S0950584921002329,An onboarding model for integrating newcomers into agile project teams,March 2022,Information and Software Technology,Not Found,Peggy=Gregory: ajgregory@uclan.ac.uk; Diane E.=Strode: diane.strode@whitireia.ac.nz; Helen=Sharp: helen.sharp@open.ac.uk; Leonor=Barroca: leonor.barroca@open.ac.uk,"Abstract
Context
A stable team is deemed optimal for 
agile software development
 project success; however, all teams change membership over time. Newcomers joining an agile project team must rapidly assimilate into the organisational and project environment. They must do this while learning how to contribute effectively and without seriously interrupting project progress.
Objective
This paper addresses how newcomers integrate into an established agile project team and how agile practices assist with onboarding.
Method
A single, qualitative 
case study
 approach was used, investigating a co-located agile project team in a large IT department who regularly onboard inexperienced newcomers. Analysis was abductive, consisting of inductive coding and theming using categories from an existing onboarding theory.
Results
We describe the team's onboarding practices and adjustments and present an agile onboarding model that encompasses onboarding activities, individual adjustments, and workplace adjustments.
Conclusions
A mixture of general and specific agile onboarding practices contribute to successful onboarding in an agile team. We provide 
practical guidelines
 to improve onboarding practice in agile teams. Our major new contribution is an extended model of onboarding for agile teams.",21 Mar 2025,7,"The paper addresses a practical issue on how newcomers integrate into agile teams, providing guidelines for improved onboarding practices which can be beneficial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001889,Does it matter who pays back Technical Debt? An empirical study of self-fixed TD,March 2022,Information and Software Technology,Not Found,Jie=Tan: j.tan@rug.nl; Daniel=Feitosa: d.feitosa@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
Technical Debt (TD) can be paid back either by those that incurred it or by others. We call the former self-fixed TD, and it can be particularly effective, as developers are experts in their own code and are well-suited to fix the corresponding TD issues.
Objective:
The goal of our study is to investigate self-fixed technical debt, especially the extent in which TD is self-fixed, which types of TD are more likely to be self-fixed, whether the remediation time of self-fixed TD is shorter than non-self-fixed TD and how development behaviors are related to self-fixed TD.
Method:
We report on an empirical study that analyzes the self-fixed issues of five types of TD (i.e., Code, Defect, Design, Documentation and Test), captured via 
static analysis
, in more than 44,000 commits obtained from 20 Python and 16 Java projects of the Apache Software Foundation.
Results:
The results show that about half of the fixed issues are self-fixed and that the likelihood of contained TD issues being self-fixed is negatively correlated with project size, the number of developers and total issues. Moreover, there is no significant difference of the survival time between self-fixed and non-self-fixed issues. Furthermore, developers are more keen to pay back their own TD when it is related to lower code level issues, e.g., Defect Debt and Code Debt. Finally, developers who are more dedicated to or knowledgeable about the project contribute to a higher chance of self-fixing TD.
Conclusions:
These results can benefit both researchers and practitioners by aiding the prioritization of TD remediation activities and refining strategies within development teams, and by informing the development of TD 
management tools
.",21 Mar 2025,8,"The study on self-fixed technical debt provides insights that can aid in prioritizing TD remediation activities and refining strategies within development teams, benefiting startups looking to manage technical debt effectively."
https://www.sciencedirect.com/science/article/pii/S095058492100183X,A mapping study on documentation in Continuous Software Development,February 2022,Information and Software Technology,"Systematic mapping studies, Systematic reviews, Continuous Software Development, Lean, Agile, DevOps, Documentation",Theo=Theunissen: theo.theunissen@gmail.com; Uwe=van Heesch: Not Found; Paris=Avgeriou: Not Found,"Abstract
Context:
With an increase in Agile, Lean, and 
DevOps
 software methodologies over the last years (collectively referred to as Continuous Software Development (CSD)), we have observed that documentation is often poor.
Objective:
This work aims at collecting studies on documentation challenges, documentation practices, and tools that can support documentation in CSD.
Method:
A 
systematic mapping study
 was conducted to identify and analyze research on documentation in CSD, covering publications between 2001 and 2019.
Results:
A total of 63 studies were selected. We found 40 studies related to documentation practices and challenges, and 23 studies related to tools used in CSD. The challenges include: informal documentation is hard to understand, documentation is considered as waste, productivity is measured by working software only, documentation is out-of-sync with the software and there is a short-term focus. The practices include: non-written and informal communication, the usage of development artifacts for documentation, and the use of architecture frameworks. We also made an inventory of numerous tools that can be used for documentation purposes in CSD. Overall, we recommend the usage of executable documentation, modern tools and technologies to retrieve information and transform it into documentation, and the practice of minimal documentation upfront combined with detailed design for knowledge transfer afterwards.
Conclusion:
It is of 
paramount importance
 to increase the quantity and quality of documentation in CSD. While this remains challenging, practitioners will benefit from applying the identified practices and tools in order to mitigate the stated challenges.",21 Mar 2025,9,"The work on documentation challenges and practices in Continuous Software Development is highly relevant for startups adopting Agile and Lean methodologies, offering recommendations that can improve documentation quality and quantity."
https://www.sciencedirect.com/science/article/pii/S0950584921001932,Patchworking: Exploring the code changes induced by vulnerability fixing activities,February 2022,Information and Software Technology,Not Found,Gerardo=Canfora: canfora@unisannio.it; Andrea=Di Sorbo: disorbo@unisannio.it; Sara=Forootani: forootani@unisannio.it; Matias=Martinez: matias.martinez@uphf.fr; Corrado A.=Visaggio: visaggio@unisannio.it,"Abstract
Context:
Identifying and repairing vulnerable code is a critical software maintenance task. Change impact analysis plays an important role during software maintenance, as it helps software maintainers to figure out the potential effects of a change before it is applied. However, while the 
software engineering
 community has extensively studied techniques and tools for performing impact analysis of change requests, there are no approaches for estimating the impact when the change involves the resolution of a vulnerability bug.
Objective:
We hypothesize that similar vulnerabilities may present similar strategies for patching. More specifically, our work aims at understanding whether the class of the vulnerability to fix may determine the type of impact on the system to repair.
Method:
To verify our conjecture, in this paper, we examine 524 security patches applied to vulnerabilities belonging to ten different weakness categories and extracted from 98 different open-source projects written in Java.
Results:
We obtain empirical evidence that vulnerabilities of the same types are often resolved by applying similar code transformations, and, thus, produce almost the same impact on the codebase.
Conclusion:
On the one hand, our findings open the way to better management of software maintenance activities when dealing with software vulnerabilities. Indeed, vulnerability class information could be exploited to better predict how much code will be affected by the fixing, how the 
structural properties
 of the code (i.e., complexity, coupling, cohesion, size) will change, and the effort required for the fix. On the other hand, our results can be leveraged for improving automated strategies supporting developers when they have to deal with security flaws.",21 Mar 2025,6,"The investigation on vulnerability patching strategies provides useful insights for software maintenance activities, especially for startups concerned with identifying and repairing vulnerable code."
https://www.sciencedirect.com/science/article/pii/S0950584921001920,The impact of the distance metric and measure on SMOTE-based techniques in software defect prediction,February 2022,Information and Software Technology,Not Found,Shuo=Feng: shuo.feng@hotmail.com; Jacky=Keung: jacky.keung@cityu.edu.hk; Peichang=Zhang: pzhang@szu.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Miao=Zhang: miazhang9-c@my.cityu.edu.hk,"Abstract
Context:
In software 
defect prediction
, SMOTE-based techniques are widely adopted to alleviate the 
class imbalance problem
. SMOTE-based techniques select instances close in the distance to synthesize minority class instances, ensuring few noise instances are generated.
Objective:
However, recent studies show that selecting instances far away effectively increases the diversity and alleviates the overgeneralization brought by SMOTE-based techniques. To investigate the relationship between the distance of the selected instances and the performances of SMOTE-based techniques, we carry out this study.
Method:
We first conduct experiments to empirically investigate the impact of the distance between the instances on the performances of three common SMOTE-based techniques. Based on the experimental result, we improve a recently proposed oversampling technique-SMOTUNED.
Results:
The experimental results on five common classifiers across 30 imbalanced datasets from the PROMISE repository show that (1) the selection of the distance metric has little impact on the performances of SMOTE-based techniques, (2) as long as the number of synthesized noise instances is not beyond the noise-resistant ability of classifiers, the overall performances measured by AUC and 
b
a
l
a
n
c
e
 of SMOTE-based techniques are not significantly affected by the distance between instances, and (3) the 
probability of detection
 (
p
d
) and the 
probability of false alarm
 (
p
f
) values of SMOTE-based techniques are significantly affected by the distance between the selected instances. The larger the distance between the selected instances is, the lower the 
p
d
 and 
p
f
 values SMOTE-based techniques obtain. The performance of the improved SMOTUNED is similar to that of the original SMOTUNED, but the improved SMOTUNED dramatically decreases the 
execution time
 of the original SMOTUNED.
Conclusion:
By controlling the distance, different 
p
d
 and 
p
f
 values can be obtained. The diversity of SMOTE-based techniques can be improved, and the overgeneralization can be avoided.",21 Mar 2025,5,"The study on SMOTE-based techniques for software defect prediction, while informative, may have limited immediate practical impact for early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001890,Classifying issue reports according to feature descriptions in a user manual based on a deep learning model,February 2022,Information and Software Technology,"Deep learning, Classification, Issue reports, User manual, Software features, Data-based software engineering, Convolution neural network, Recurrent neural network, Machine learning",Heetae=Cho: Not Found; Seonah=Lee: saleese@gnu.ac.kr; Sungwon=Kang: Not Found,"Abstract
Context
Issue reports are documents with which users report problems and state their opinions on a software system. Issue reports are useful for software maintenance, but managing them requires developers’ considerable manual effort. To reduce such effort, previous studies have mostly suggested methods for automatically classifying issue reports. However, most of those studies classify issue reports according to issue types, based only on whether the report is relevant to a bug, whether the report is duplicated, or whether the issue is functional or nonfunctional.
Objective
In this paper, we intend to link issue reports and a user manual and so propose a deep learning model-based method that classifies issue reports according to software features that are described in the user manual in order to help developers relate issue reports to features to make changes to a software system.
Method
In order to classify issue reports according to the feature descriptions in a user manual, our method uses a 
deep learning technique
 with a 
word embedding
 technique. The key insight in our method is that the sections of a user manual that describe software features contain the words and sentences similar to those in issue reports. Based on the insight, we construct a 
classification model
 that learns the feature descriptions (i.e. sections) in a user manual and classifies issue reports according to the feature descriptions.
Results
We evaluate the proposed method by comparing its classification performance with that of the state-of-the-art method, TicketTagger. The experimental results show that the proposed method yields 10% ∼ 24% higher classification f1-score than that of TicketTagger. We also experiment with two deep learning models and four word embedding techniques and find out that the 
Convolution Neural Network model
 with FastText (or GloVe) yields the best performance.
Conclusion
Our study shows the feasibility of classifying issue reports according to software features, which can be the basis for successive studies to classify issue reports into software features.",21 Mar 2025,8,"The proposed method of linking issue reports to user manuals using deep learning shows potential to help developers relate issue reports to features and make changes to a software system, with a significant improvement in classification performance over existing methods."
https://www.sciencedirect.com/science/article/pii/S0950584921001944,Towards cost-effective API deprecation: A win–win strategy for API developers and API users,February 2022,Information and Software Technology,Not Found,Chia Hung=Kao: chkao@nttu.edu.tw; Cheng-Ying=Chang: Not Found; Hewijin Christine=Jiau: Not Found,"Abstract
API
 deprecation, which enables 
API
 developers to assist API users in 
migration tasks
, has been widely employed in API removal management. However, mismanaged API deprecation will cause unnecessary cost and bring negligible benefit to API users. Cost-effective investments in API deprecation become challenges for API developers. In this work, an iterative model for cost-effective investments in API deprecation is developed. The model provides a data-driven mechanism for API developers to iteratively make investments in API deprecation. A tool named 
AWARE
 (
A
 
W
in–win 
A
ssistant for API 
RE
moval management) is also developed for API developers to accurately assess the benefit from the perspective of API 
usage statistics
. Based on the prioritized benefit, API developers can allocate appropriate resources on API deprecation. A 
case study
 is performed to evaluate the effectiveness of the iterative model with AWARE. The evaluation result shows that the cost paid by API developers can be reduced significantly while the benefit brought to API users can be increased. A win–win strategy for API deprecation can be achieved.",21 Mar 2025,6,"The iterative model for cost-effective investments in API deprecation and the AWARE tool provide a practical approach for API developers to manage API removal, leading to reduced costs and increased benefits, with a win-win strategy for API deprecation."
https://www.sciencedirect.com/science/article/pii/S0950584921001956,SHSE: A subspace hybrid sampling ensemble method for software defect number prediction,February 2022,Information and Software Technology,Not Found,Haonan=Tong: Not Found; Wei=Lu: Not Found; Weiwei=Xing: Not Found; Bin=Liu: Not Found; Shihai=Wang: wangshihai@buaa.edu.cn,"Abstract
Context:
Software defect
 number prediction (SDNP) helps allocate limited testing resources by ranking software modules according to the predicted defect numbers. However, the highly skewed distribution of defects greatly degrades the performance of SDNP models by preventing SDNP models from ranking software modules accurately.
Objective:
This paper introduces a novel subspace 
hybrid sampling
 ensemble (SHSE) method based on feature subspace construction, 
hybrid sampling
, and 
ensemble learning
 for building high-performance SDNP models.
Method:
Specifically, we first construct a series of feature subspace to ensure the diversity of 
base
 learners. In each of feature subspace, we then use the proposed hybrid sampling method to balance the training subset without losing too much information and introducing lots of noisy data caused by only using undersampling or oversampling techniques. Finally, we train each 
base
 learner and combine them by using the proposed weighted ensemble strategy. Experiments are performed on 27 public defect datasets. We compare SHSE with five state-of-the-art resampling-based models and four zero-inflated/hurdle models in terms of the ranking 
performance measure
 fault-percentile-average (FPA). To demonstrate the effectiveness of SHSE, two statistical 
testing methods
 including Wilcoxon Signed-rank test and Scott–Knott 
Effect Size
 Difference test are utilized. Cliff’s 
δ
 is also computed for quantifying the difference when there is significant difference between SHSE and each baseline.
Results:
The experimental results show that SHSE significantly outperforms the baselines and improves the performance over each baseline with as least medium effect size on most datasets. On average, SHSE improves the performance over the resampling-based methods by 8.7%
∼
14.4% and the zero-inflate/hurdle models by 10.3%
∼
15.2%.
Conclusion:
It can be concluded that SHSE is a more promising alternative for software defect number prediction.",21 Mar 2025,7,"The SHSE method for software defect number prediction addresses the challenges of highly skewed defect distributions and significantly outperforms existing models, showing promise as an alternative for more accurate prediction of software defects."
https://www.sciencedirect.com/science/article/pii/S0950584921002032,Early prediction for merged vs abandoned code changes in modern code reviews,February 2022,Information and Software Technology,Not Found,Khairul=Islam: Not Found; Toufique=Ahmed: Not Found; Rifat=Shahriyar: Not Found; Anindya=Iqbal: Not Found; Gias=Uddin: gias.uddin@ucalgary.ca,"Abstract
Context:
The modern 
code review process
 is an integral part of the current software development practice. Considerable effort is given here to inspect code changes, find defects, suggest an improvement, and address the suggestions of the reviewers. In a 
code review process
, several iterations usually take place where an author 
submits
 code changes and a reviewer gives feedback until is happy to accept the change. In around 12% cases, the changes are abandoned, eventually wasting all the efforts.
Objective:
In this research, our objective is to design a tool that can predict whether a code change would be merged or abandoned at an early stage to reduce the waste of efforts of all stakeholders (e.g., program author, reviewer, project management, etc.) involved. The real-world demand for such a tool was formally identified by a study by Fan et al. (2018).
Method:
We have mined 146,612 code changes from the code reviews of three large and popular open-source software and trained and tested a suite of supervised 
machine learning
 classifiers, both shallow and deep learning-based. We consider a total of 25 features in each code change during the training and testing of the models. The features are divided into five dimensions: reviewer, author, project, text, and code.
Results:
The best performing model named PredCR (Predicting Code Review), a LightGBM-based classifier achieves around 85% AUC score on average and relatively improves the state-of-the-art (Fan et al., 2018) by 14%–23%. In our extensive empirical study involving PredCR on the 146,612 code changes from the three software projects, we find that (1) The new features like reviewer dimensions that are introduced in PredCR are the most informative. (2) Compared to the baseline, PredCR is more effective towards reducing bias against new developers. (3) PredCR uses 
historical data
 in the code review repository and as such the performance of PredCR improves as a software system evolves with new and more data.
Conclusion:
PredCR can help save time and effort by helping developers/code reviewers to prioritize the code changes that they are asked to review. Project management can use PredCR to determine how code changes can be assigned to the code reviewers (e.g., select code changes that are more likely to be merged for review before the changes that might be abandoned).",21 Mar 2025,9,"The PredCR tool for predicting code changes that will be merged or abandoned at an early stage using machine learning classifiers achieves high AUC scores and improvements over existing methods, offering significant time and effort savings for developers and project management."
https://www.sciencedirect.com/science/article/pii/S0950584921001919,Understanding in-app advertising issues based on large scale app review analysis,February 2022,Information and Software Technology,Not Found,Cuiyun=Gao: gaocuiyun@hit.edu.cn; Jichuan=Zeng: jczeng@cse.cuhk.edu.hk; David=Lo: davidlo@smu.edu.sg; Xin=Xia: xin.xia@monash.edu; Irwin=King: king@cse.cuhk.edu.hk; Michael R.=Lyu: lyu@cse.cuhk.edu.hk,"Abstract
Context:
In-app advertising closely relates to app revenue. Reckless ad integration could adversely impact app quality and 
user experience
, leading to loss of income. It is very challenging to balance the ad revenue and 
user experience
 for app developers.
Objective:
Towards tackling the challenge, we conduct a study on analyzing user concerns about in-app advertisement.
Method:
Specifically, we present a large-scale analysis on ad-related user feedback. The large user feedback data from App Store and Google Play allow us to summarize ad-related app issues comprehensively and thus provide practical ad integration strategies for developers. We first define common ad issues by manually labeling a statistically 
representative sample
 of ad-related feedback, and then build an automatic classifier to categorize ad-related feedback. We study the relations between different ad issues and user ratings to identify the ad issues poorly scored by users. We also explore the fix durations of ad issues across platforms for extracting insights into prioritizing ad issues for ad maintenance.
Results:
(1) We summarize 15 types of ad issues by manually annotating 903 out of 36,309 ad-related user reviews. From a statistical analysis of 36,309 ad-related reviews, we find that users care most about the number of unique ads and ad display frequency during usage. (2) Users tend to give relatively lower ratings when they report the security and notification related issues. (3) Regarding different platforms, we observe that the distributions of ad issues are significantly different between App Store and Google Play. (4) Some ad issue types are addressed more quickly by developers than other ad issues.
Conclusion:
We believe the findings we discovered can benefit app developers towards balancing ad revenue and user experience while ensuring app quality.",21 Mar 2025,5,"The study on in-app advertising and user concerns provides insights into ad-related issues and user ratings, but the impact on startups and early-stage ventures may be limited compared to the other abstracts focusing on software development and prediction models."
https://www.sciencedirect.com/science/article/pii/S0950584921001907,The use of incentives to promote technical debt management,February 2022,Information and Software Technology,"Technical debt, Software development, Software incentive programs, Empirical study",Terese=Besker: Terese.Besker@ri.se; Antonio=Martini: antonima@ifi.uio.no; Jan=Bosch: Jan.Bosch@chalmers.se,"Abstract
Context
When developing software, it is vitally important to keep the level of technical debt down since, based on several studies, it has been well established that technical debt can lower the development productivity, decrease the developers' morale and 
compromise
 the overall quality of the software, among others. However, even if researchers and practitioners working in today's software development industry are quite familiar with the concept of technical debt and its related negative consequences, there has been no empirical research focusing specifically on how software managers actively communicate and manage the need to keep the level of technical debt as low as possible.
Objective
This study aims to understand how software companies give incentives to manage technical debt. This is carried out by exploring how companies encourage and reward practitioners for actively keeping the level of technical debt down add whether the companies use any 
forcing
 or 
penalising
 initiatives when managing technical debt.
Method
As a first step, this paper reports the results of both an online survey providing quantitative data from 258 participants and interviews with 32 software practitioners. As a second step, this study sets out to specifically provide a detailed assessment of additional and in-depth analysis of technical debt management strategies based on an encouraging mindset and attitude from both managers and technical roles to understand 
how, when and by whom
 such strategies are adopted in practice.
Results
Our findings show that having a technical debt management strategy (specially based on encouragement) can significantly impact the amount of technical debt related to the software.
Conclusion
The result indicates that there is considerable unfulfilled potential to influence how software practitioners can further limit and reduce technical debt by adopting a strategy based explicitly on an encouraging mindset from managers where they also specifically dedicate time and resources for technical debt remediation activities.",21 Mar 2025,8,"This abstract addresses a key issue in software development - technical debt management. The study provides valuable insights on strategies to reduce technical debt and highlights the impact on software quality, which can benefit early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584921001877,Towards a taxonomy of code review smells,February 2022,Information and Software Technology,Not Found,Emre=Doğan: emredogan7@outlook.com; Eray=Tüzün: Not Found,"Abstract
Context:
Code review is a crucial step of the 
software development life cycle
 in order to detect possible problems in 
source code
 before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the 
code review process
, many companies and 
open source software
 (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective 
code review process
. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are 
code review (CR) smells
.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight 
OSS projects
.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the 
OSS projects
 are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.",21 Mar 2025,9,"Code review is crucial for software quality, and this study focuses on identifying bad practices in the process. The findings of this study can have a significant impact on improving code review practices in early-stage ventures and startups, making it highly relevant and valuable."
https://www.sciencedirect.com/science/article/pii/S0950584921001816,An approach to explore sequential interactions in cognitive activities of software engineering,January 2022,Information and Software Technology,Not Found,Joelma=Choma: jh.choma@hotmail.com; Eduardo M.=Guerra: guerraem@gmail.com; Tiago S.=da Silva: silvadasilva@unifesp.br; Luciana M.=Zaina: lzaina@ufscar.br,"Abstract
Context
: The study of cognitive aspects around software activities can provide valuable insights to improve 
software engineering
 practices. Objective: This paper presents an approach based on distributed cognition and sequential analysis to explore cognitive activities in the software development context by analyzing the interactions between software practitioners and the resources used to support them. Method: We conducted nine laboratory-based observation sessions to record qualitative audio/video data of interactions between the study participants and at-hand resources during the planning and managing of 
software analytics
 tasks. Results: The interaction strategies of the resources model included 21 emergent actions, and the sequential analysis revealed two different patterns of interaction over time. Conclusion: Our approach has been useful for evaluating how well an artifact works to support a team in 
software analytics
 activities. Furthermore, it can be applied to explore and discover interaction patterns in different software activities.",21 Mar 2025,6,"The study on cognitive aspects in software development provides interesting insights, but the direct impact on practical applications for early-stage ventures and startups may be limited. However, the approach could still offer valuable perspectives for software engineering practices."
https://www.sciencedirect.com/science/article/pii/S0950584921001543,Challenges and solutions when adopting DevSecOps: A systematic review,January 2022,Information and Software Technology,Not Found,Roshan N.=Rajapakse: roshan.rajapakse@adelaide.edu.au; Mansooreh=Zahedi: mansooreh.zahedi@adelaide.edu.au; M. Ali=Babar: ali.babar@adelaide.edu.au; Haifeng=Shen: Haifeng.Shen@acu.edu.au,"Abstract
Context:
DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge.
Objective:
This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future.
Method:
We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data.
Results:
We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied.
Conclusions:
We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.",21 Mar 2025,9,"The study on DevSecOps adoption challenges and solutions is highly relevant in the current software development landscape. It can offer valuable insights for startups looking to integrate security practices into DevOps, making it a high-impact study for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001580,Relationships between software architecture and source code in practice: An exploratory survey and interview,January 2022,Information and Software Technology,Not Found,Fangchao=Tian: tianfangchao@whu.edu.cn; Peng=Liang: liangp@whu.edu.cn; Muhammad Ali=Babar: ali.babar@adelaide.edu.au,"Abstract
Context
Software Architecture (SA) and Source Code (SC) are two intertwined artefacts that represent the interdependent design decisions made at different levels of abstractions - High-Level (HL) and Low-Level (LL). An understanding of the relationships between SA and SC is expected to bridge the gap between SA and SC for supporting maintenance and evolution of software systems.
Objective
We aimed at exploring practitioners’ understanding about the relationships between SA and SC.
Method
We used a mixed-method that combines an online survey with 87 respondents and an interview with 8 participants to collect the views of practitioners from 37 countries about the relationships between SA and SC.
Results
Our results reveal that: practitioners mainly discuss five features of relationships between SA and SC; a few practitioners have adopted dedicated approaches and tools in the literature for identifying and analyzing the relationships between SA and SC despite recognizing the importance of such information for improving a system's quality attributes, especially 
maintainability
 and reliability. It is felt that cost and effort are the major impediments that prevent practitioners from identifying, analyzing, and using the relationships between SA and SC.
Conclusions
The results have empirically identified five features of relationships between SA and SC reported in the literature from the perspective of practitioners and a systematic framework to manage the five features of relationships should be developed with dedicated approaches and tools considering the cost and benefit of maintaining the relationships.",21 Mar 2025,7,"The exploration of relationships between software architecture and source code can benefit software maintenance and evolution. While the findings may not directly impact startups, the insights on improving system quality attributes could still offer value to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001579,Automatically inferring user behavior models in large-scale web applications,January 2022,Information and Software Technology,Not Found,Saeedeh Sadat Sajjadi=Ghaemmaghami: sajjadig@ualberta.ca; Seyedeh Sepideh=Emam: emam@ualberta.ca; James=Miller: jimm@ualberta.ca,"Abstract
Context
Inferring a behavioral model from users’ navigation patterns in a web application helps application providers to understand their users’ interests. It is essential to automatically identify and generate such models as the volume of daily interactions with applications are enormous.
Objective
The goal of this paper is to incrementally generate such an automated user behavior model with no instrumentation for understanding users’ interests in large-scale mobile and desktop applications.
Method
We propose an approach to fully automate the behavioral model generation for large-scale web applications. Our proposed solution infers a reward augmented behavioral model using a reinforcement learning method by 1) dynamically generating a set of probabilistic Markov models from the users’ interactions, 2) augmenting the state of the model with reward values. Our analysis engine of the proposed solution evaluates the evolving properties of interaction patterns against the inferred behavioral models using probabilistic model checking.
Results
We evaluate the utility of our approach by using it on a large-scale mobile and desktop application. In order to show that it is assigning meaningful reward values, we compare these values with results from Google Analytics (as a state-of-the-art approach). Empirical results indicate that our approach is not only compatible with the results from Google Analytics, but also can provide information in situations, where Google Analytics data is not available.
Conclusion
In this paper, we present a novel stochastic approach to (1) generate user behavioral models for mobile and desktop web applications, (2) automatically calculate the states’ rewards, (3) annotate and analyze the models to verify their quantitative properties, and (4) address many limitations found in existing approaches.",21 Mar 2025,8,"The paper presents a novel approach to generating user behavioral models for large-scale web applications, addressing limitations found in existing approaches. This can have a significant impact on understanding user interests and improving application performance."
https://www.sciencedirect.com/science/article/pii/S0950584921001695,Supporting refactoring of BDD specifications—An empirical study,January 2022,Information and Software Technology,"Refactoring, Normalized Compression Distance (NCD), Normalized Compression Similarity (NCS), Reuse, Similarity ratio (SR), BDD, Behavior-driven development, Specifications, Testing",Mohsin=Irshad: mohsin.irshad@bth.se; Jürgen=Börstler: Not Found; Kai=Petersen: Not Found,"Abstract
Context:
Behavior-driven development (BDD) is a variant of test-driven development where specifications are described in a structured domain-specific natural language. Although refactoring is a crucial activity of BDD, little research is available on the topic.
Objective:
To support practitioners in refactoring BDD specifications by (1) proposing semi-automated approaches to identify 
refactoring candidates
; (2) defining refactoring techniques for BDD specifications; and (3) evaluating the proposed identification approaches in an industry context.
Method:
Using Action Research, we have developed an approach for identifying refactoring candidates in BDD specifications based on two measures of similarity and applied the approach in two projects of a large software organization. The accuracy of the measures for identifying refactoring candidates was then evaluated against an approach based on 
machine learning
 and a manual approach based on practitioner perception.
Results:
We proposed two measures of similarity to support the identification of refactoring candidates in a BDD specification base; (1) normalized compression similarity (NCS) and (2) 
similarity ratio
 (SR). A semi-automated approach based on NCS and SR was developed and applied to two industrial cases to identify refactoring candidates. Our results show that our approach can identify candidates for refactoring 6o times faster than a manual approach. Our results furthermore showed that our measures accurately identified refactoring candidates compared with a manual identification by software practitioners and outperformed an ML-based text 
classification approach
. We also described four types of refactoring techniques applicable to BDD specifications; merging candidates, restructuring candidates, deleting duplicates, and renaming specification titles.
Conclusion:
Our results show that NCS and SR can help practitioners in accurately identifying BDD specifications that are suitable candidates for refactoring, which also decreases the time for identifying refactoring candidates.",21 Mar 2025,7,"The paper provides valuable insights into refactoring BDD specifications, proposing semi-automated approaches that can significantly decrease the time for identifying refactoring candidates. This can benefit practitioners in improving software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584921001804,How resource utilization influences UI responsiveness of Android software,January 2022,Information and Software Technology,Not Found,Jiaojiao=Fu: jjfu15@fudan.edu.cn; Yaohui=Wang: 17210240047@fudan.edu.cn; Yangfan=Zhou: zyf@fudan.edu.cn; Xin=Wang: xinw@fudan.edu.cn,"Abstract
Context:
The rapid responsiveness of smartphones is critical to 
user experience
. Excessive 
resource utilization
 is typically considered as one of the major factors leading to laggy-UI. Much work focuses on modifying the design of 
Android
 systems and software to reduce their 
resource utilization
. However, laggy-UI is still quite common on 
Android
 devices, especially the low-end ones. One reason is that developers still lack a clear understanding about how the utilization of various resources (
e.g.
, CPU and memory) affects Android responsiveness, which leads to the inadequacy of existing 
performance optimization
 measures.
Objective:
The objective of this paper is to obtain a systematical understanding of how the utilization of various resources (
e.g.
, CPU and memory) affects Android responsiveness. Then accordingly, we aim to figure out the root cause(s) of laggy-UI.
Methods:
First, we conduct a set of controlled experiments on two Android devices with a stress test tool. Second, we further test 36 real-life Android software to study whether the competition of resource(s), the root factor(s) causing laggy-UI, is severe in real-life scenarios.
Results:
The experimental results show that CPU competition is the root cause and other resources have no observable impact on Android responsiveness, except in extreme cases, 
e.g.
, utilization reaches almost 100%. We also find out CPU competition is quite common for existing Android software when it is running in the background.
Conclusion:
Through stress testing and real-life Android software testing, this work unveils that CPU competition should be the main problem to be solved. Our experimental results deepen and update previous perceptions of resources’ impact on Android responsiveness. Based on these findings, we provide a set of suggestions for designing high-performance Android systems and software, and effective 
performance optimization
 tools.",21 Mar 2025,6,"The research provides a systematic understanding of how resource utilization affects Android responsiveness, with practical suggestions for designing high-performance Android systems and software. This can be valuable for developers looking to enhance user experience on Android devices."
https://www.sciencedirect.com/science/article/pii/S0950584921001853,Engineering Web Augmentation software: A development method for enabling end-user maintenance,January 2022,Information and Software Technology,Not Found,Diego=Firmenich: dfirmenich@tw.unp.edu.ar; Sergio=Firmenich: Not Found; Gustavo=Rossi: Not Found; Manuel=Wimmer: Not Found; Irene=Garrigós: Not Found; César=González-Mora: Not Found,"Abstract
Nowadays, end-users are able to adapt Web applications when some of their requirements have not been taken into account by developers. One possible way to do adaptations is by using Web Augmentation techniques. Web Augmentation allows end-users to modify the Web sites’ user interfaces once these are loaded on the client-side, i.e., in the browser. They achieve these adaptations by developing and/or installing Web browser plugins (“augmenters”) that modify the user interface with new functionalities. This particular kind of software artifacts requires 
special attention
 regarding maintenance as–in most cases–they depend on third-party resources, such as HTML pages. When these resources are upgraded, unexpected results during the augmentation process may occur. Many communities have arisen around Web Augmentation, and today there are large repositories where developers share their augmenters; end-users may give feedback about existing augmentations and even ask for new ones. Maintenance is a key phase in the augmenters’ life-cycle, and currently, this task falls (as usual) on the developers. In this paper, we present a participatory approach for allowing end-users without programming skills to participate in the augmenters’ maintenance phase. In order to allow this, we also provide support for the development phase to bootstrap a first version of the augmenter and to reduce the load on developers in both phases, development and maintenance. We present an analysis of more than eight thousand augmenters, which helped us devise the approach. Finally, we present an experiment with 48 participants to validate our approach.",21 Mar 2025,5,"The paper introduces a participatory approach for end-users to participate in the maintenance of Web augmenters, which can be beneficial in reducing the load on developers. While important for end-user involvement, the practical impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S095058492100152X,Introduction to the Special Issue on: Grey Literature and Multivocal Literature Reviews (MLRs) in software engineering,January 2022,Information and Software Technology,Not Found,Vahid=Garousi: v.garousi@qub.ac.uk; Michael=Felderer: michael.felderer@uibk.ac.at; Mika V.=Mäntylä: mika.mantyla@oulu.fi,"Abstract
In parallel to academic (peer-reviewed) literature (e.g., journal and conference papers), an enormous extent of grey literature (GL) has accumulated since the inception of software engineering (SE). GL is often defined as “literature that is not formally published in sources such as books or journal articles”, e.g., in the form of trade magazines, online blog-posts, technical reports, and online videos such as tutorial and presentation videos. GL is typically produced by SE practitioners. We have observed that researchers are increasingly using and benefitting from the knowledge available within GL. Related to the notion of GL is the notion of Multivocal Literature Reviews (MLRs) in SE, i.e., a MLR is a form of a Systematic Literature Review (SLR) which includes knowledge and/or evidence from the GL in addition to the peer-reviewed literature. MLRs are useful for both researchers and practitioners because they provide summaries of both the state-of-the-art and -practice in a given area. MLRs are popular in other fields and have started to appear in SE community. It is timely then for a Special Issue (SI) focusing on GL and MLRs in SE. From the pool of 13 submitted papers, and after following a rigorous peer review process, seven papers were accepted for this SI. In this introduction we provide a brief overview of GL and MLRs in SE, and then a brief summary of the seven papers published in this SI.",21 Mar 2025,4,"The abstract discusses the importance of grey literature and Multivocal Literature Reviews in software engineering. While informative for researchers, the direct practical impact on early-stage ventures, especially startups, may be less significant."
https://www.sciencedirect.com/science/article/pii/S0950584921001506,A closer look at process-based simulation with stackless coroutines,January 2022,Information and Software Technology,Not Found,Dorian=Weber: weber@informatik.hu-berlin.de; Paula=Wiesner: wiesnerp@informatik.hu-berlin.de; Joachim=Fischer: fischer@informatik.hu-berlin.de,"Abstract
Context
Validating discrete-event 
computer simulations
 for a particular problem domain often involves the help of a domain expert. This means that a certain structural closeness between the simulator’s inner workings and the modeled system is needed in order to allow the expert to follow the implementation in analogy. Process-based simulation imposes an object-oriented view onto a modeled system which allows for a high degree of structural closeness in most cases. In comparison, event-based simulation requires a procedural definition with a relatively low degree of structural closeness for many cases, but outperforms the process-based approach both in terms of performance and portability. Recent advances in compiler technology have introduced a portable way of rewriting thread-based code into event-based code, effectively providing the means to implement portable green-threads in compiled system languages.
Objective
This work aims to cover the historical, mechanical, and implementation specific aspects as well as practical measurements of runtime performance of a library based solution to process-based discrete-event simulation in comparison to alternative solutions.
Method
We explain how to use the stackless coroutines introduced into the 
Rust
 programming language to implement a minimal simulator core and discuss aesthetic as well as performance implications through systematic benchmarking using the three simulation scenarios 
Barbershop
, 
Car Ferry
 and 
Dining Philosophers
 by comparing their implementations to equivalent ones in the simulation language 
SLX
 and the 
C


++
 library 
ODEMx
.
Results
Our results indicate that stackless coroutines enable structurally equivalent formulations to pure process-based simulations while still delivering close to equivalent or – depending on the use-case – even superior performance and portability compared to the aforementioned solutions.
Conclusion
We show that stackless coroutines can be used to bridge the gap between process- and event-based simulators, affording modelers a level of abstraction close to the former approach while delivering the performance and portability of the latter one.",21 Mar 2025,7,"The research presents a practical approach to improving the performance and portability of process-based discrete-event simulation, which can be beneficial for early-stage ventures in optimizing their simulation processes."
https://www.sciencedirect.com/science/article/pii/S095058492100149X,Cronista: A multi-database automated provenance collection system for runtime-models,January 2022,Information and Software Technology,Not Found,Owen=Reynolds: 180200041@aston.ac.uk; Antonio=García-Domínguez: a.garcia-dominguez@aston.ac.uk; Nelly=Bencomo: n.bencomo@aston.ac.uk,"Abstract
Context:
Decision making by software systems that face uncertainty needs tracing to support 
understandability
, as accountability is crucial. While logging has been essential to support explanations and 
understandability
 of behaviour, several issues still persist, such as the high cost for managing large logs, not knowing what to log, and the inability of logging techniques to relate events to each other or to specific occurrences of high-level activities in the system.
Objective:
Cronista
 is an alternative to logging for systems that act on top of runtime models. Instead of targeting the running systems, 
Cronista
 automatically collects the provenance of changes made to the runtime models, which aim at leveraging high-level representations, to produce more concise historical records. The provenance graphs capture causal links between those changes and the activities of the system, which are used to investigate issues.
Method:
Cronista
’s architecture is described with the current design and the implementation of its high-level components for single-machine, multi-threaded systems. 
Cronista
 is applied to a traffic-simulation 
case study
. The trade-offs of two different storage solutions are evaluated, i.e. the CDO 
model repositories
, and JanusGraph 
graph databases
.
Results:
Integrating 
Cronista
 into the 
case study
 requires only minor code changes. 
Cronista
 collected provenance graphs for the simulations as they ran, using both CDO and JanusGraph. Both solutions highlighted the cause of a seeded defect in the system. For the longer executions, both CDO and JanusGraph showed negligible overhead on the simulation times. Querying and visualisation tools were more user-friendly in JanusGraph than in CDO.
Conclusion:
Cronista
 demonstrates the feasibility of recording fine-grained provenance for the evolution of runtime models, while using it to investigate issues. User convenience and resource requirements need to be balanced. The paper present how the available technologies studied offer different trade-offs to satisfy the balance required.",21 Mar 2025,6,"The alternative to logging presented in this research offers a more concise way of recording system changes, which can be valuable for startups dealing with large logs and needing to trace system behavior for accountability purposes."
https://www.sciencedirect.com/science/article/pii/S0950584921001701,Improving Agile Software Development using User-Centered Design and Lean Startup,January 2022,Information and Software Technology,Not Found,Maximilian=Zorzetti: maximilian.zorzetti@acad.pucrs.br; Ingrid=Signoretti: ingrid.manfrim@acad.pucrs.br; Larissa=Salerno: larissa.salerno@acad.pucrs.br; Sabrina=Marczak: sabrina.marczak@pucrs.br; Ricardo=Bastos: bastos@pucrs.br,"Abstract
Context:
Agile methods have limitations concerning problem understanding and solution finding, which can cause organizations to push misguided products and accrue waste. Some authors suggest combining agile methods with discovery-oriented approaches to overcome this, with notable candidates being User-Centered Design (UCD) and Lean Startup, a combination of which there is yet not a demonstrated, comprehensive study on how it works.
Objective:
To characterize a development approach combination of 
Agile Software Development
, UCD, and Lean Startup; exposing how the three approaches can be intertwined in a single 
development process
 and how they affect development.
Method:
We conducted a 
case study
 with two industry software development teams that use this combined approach, investigating them through interviews, observation, focus groups, and a workshop during a nine-month period in which they were stationed in a custom-built development lab.
Results:
The teams are made up of user advocates, business advocates, and solution builders; while their development approach emphasizes experimentation by making heavy use of build-measure-learn cycles. The combined approach promotes a problem-oriented mindset, encouraging team members to work together and engage with the entire 
development process
, actively discovering stakeholders needs and how to fulfill them. Each of its approaches provide a unique contribution to the development process: UCD fosters empathy with stakeholders and enables teams to better understand the problem they are tasked with solving; Lean Startup introduces experimentation as the guiding force of development; and 
Extreme Programming
 (the teams’ agile method) provides support to experimentation and achieving technical excellence.
Conclusion:
The combined approach pushes teams to think critically throughout the development effort. Our practical example provides insight on its essence and might inspire industry practitioners to seek a similar development approach based on the same precepts.",21 Mar 2025,8,"The study on combining Agile Software Development, UCD, and Lean Startup provides insights on a comprehensive development approach that promotes problem-solving and critical thinking, valuable for early-stage ventures seeking effective development methodologies."
https://www.sciencedirect.com/science/article/pii/S0950584921001841,A model-driven framework to support strategic agility: Value-added perspective,January 2022,Information and Software Technology,"Strategic agility, IT governance, Strategic value, Model-driven development, Agile, Agility, i* framework, Scaled agile framework, SAFe, MoDrIGo, StratAMoDrIGo",Konstantinos=Tsilionis: Not Found; Yves=Wautelet: yves.wautelet@kuleuven.be,"Abstract
Context:
The Covid-19 pandemic has shown the entire world that the habits of work, freedom, and consumption can change quickly and significantly for an undetermined amount of time. A dynamic environment as such, prompts organizations to move fast in order to leverage changing circumstances as sources of opportunity rather than deadly threats. Drastic changes in work organization, consumption habits, compliance, etc., may require firms to quickly adopt new technology delivering all sorts of added value.
Objective:
The development and adoption of new technology – structurally impacting the way the organization conducts its activities – requires a considerable amount of effort in a short time frame, thus rendering it a governance decision where the alignment of the technology’s adoption and use to the long term strategy needs to be evaluated. The short time frame requiring fast response implies that agility should not remain a development or management/operational concept but should also be adopted onto the strategic layer.
Method:
Design Science Research (DSR) has been applied to build-up a framework supporting strategic agility in a model-driven fashion called 
Strategic Agile Model Driven IT Governance
 (
StratAMoDrIGo
). The relevance, rigor and design cycles of DSR have been applied and presented.
Results:
StratAMoDrIGo is based on the identification of sources of value for the organization’s strategy, its stakeholders and the users of the implemented/adopted technology. Relevant concepts are consolidated in an ontology of which the application uses the 
NFR
 Model at strategic-level and the i* Strategic Rationale Model at management-level. The proposal is applied on the case of an hospital facing the Covid-19 pandemic.
Conclusion:
The value brought by strategic opportunities’ adoption to the organization, stakeholders and users can be evaluated 
ex ante
 through conceptual models.",21 Mar 2025,5,"The framework presented for strategic agility in technology adoption can provide guidance for startups looking to align technology decisions with long-term strategies, but may require more adaptation for practical implementation in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001828,Human values in software development artefacts: A case study on issue discussions in three Android applications,January 2022,Information and Software Technology,Not Found,Arif=Nurwidyantoro: Arif.Nurwidyantoro@monash.edu; Mojtaba=Shahin: Mojtaba.Shahin@monash.edu; Michel R.V.=Chaudron: m.r.v.chaudron@tue.nl; Waqar=Hussain: Waqar.Hussain@monash.edu; Rifat=Shams: Rifat.Shams@monash.edu; Harsha=Perera: Harsha.Perera@monash.edu; Gillian=Oliver: Gillian.Oliver@monash.edu; Jon=Whittle: Jon.Whittle@data61.csiro.au,"Abstract
Context:
Human values such as inclusion, privacy, and accessibility need to be considered during software development to attract and maintain users. However, little effort has been made to study human values consideration in software development, particularly in software development artefacts.
Objective:
Issue discussion is potentially a rich source for human values analysis because it is a common place for users and developers to share and communicate their concerns. This paper aims to investigate the extent to which human values are discussed and whether the presence of values differs across projects.
Method:
We carried out a 
case study
 to discover human values in 1,097 issues collected from three 
Android
 projects: Signal, K-9, and Focus.
Results:
We identified 20 value themes and proposed a contextualised 
software engineering
 description for each of them. The analysis shows that privacy, freedom, usability, and efficiency were the prevalent value themes in the issue discussions of these three projects. Meanwhile, Self-direction - Action and Security - Personal are the common prevalent human values found in the projects. Moreover, we found that a statement of values from the apps and their functionalities could contribute to the presence of values.
Conclusion:
The results suggest that human values are present in software development artefacts, for which automated tools can be developed to extract and classify human values from them.",21 Mar 2025,6,"The investigation on human values in software development artifacts offers valuable insights for startups aiming to consider user values in their software products, potentially guiding the development of user-centric solutions."
https://www.sciencedirect.com/science/article/pii/S0950584921001348,On preserving the behavior in software refactoring: A systematic mapping study,December 2021,Information and Software Technology,Not Found,Eman Abdullah=AlOmar: eman.alomar@mail.rit.edu; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Christian=Newman: cdnvse@rit.edu,"Abstract
Context:
Refactoring is the art of modifying the design of a system without altering its behavior. The idea is to reorganize variables, classes and methods to facilitate their future adaptations and comprehension. As the concept of behavior preservation is fundamental for refactoring, several studies, using 
formal verification
, language transformation and dynamic analysis, have been proposed to monitor the execution of 
refactoring operations
 and their impact on the program semantics. However, there is no existing study that examines the available behavior preservation strategies for each refactoring operation.
Objective:
This paper identifies behavior preservation approaches in the research literature.
Method:
We conduct, in this paper, a 
systematic mapping study
, to capture all existing behavior preservation approaches that we classify based on several criteria including their methodology, applicability, and their degree of automation.
Results:
The results indicate that several behavior preservation approaches have been proposed in the literature. The approaches vary between using formalisms and techniques, developing automatic refactoring safety tools, and performing a manual analysis of the source code.
Conclusion:
Our taxonomy reveals that there exist some types of 
refactoring operations
 whose behavior preservation is under-researched. Our classification also indicates that several possible strategies can be combined to better detect any violation of the program semantics.",21 Mar 2025,7,"The study on behavior preservation approaches in refactoring operations can benefit early-stage ventures by providing insights into maintaining program semantics, which is crucial for startups developing and modifying software."
https://www.sciencedirect.com/science/article/pii/S0950584921001488,Developing Mobile Applications Via Model Driven Development: A Systematic Literature Review,December 2021,Information and Software Technology,Not Found,Md.=Shamsujjoha: md.shamsujjoha@monash.edu; John=Grundy: john.grundy@monash.edu; Li=Li: li.li@monash.edu; Hourieh=Khalajzadeh: hourieh.khalajzadeh@monash.edu; Qinghua=Lu: qinghua.lu@data61.csiro.au,"Abstract
Context:
Mobile applications (known as “apps”) usage continues to rapidly increase, with many new apps being developed and deployed. However, developing a mobile app is challenging due to its dependencies on devices, technologies, platforms, and deadlines to reach the market. One potential approach is to use 
M
odel 
D
riven 
D
evelopment (MDD) techniques that simplify the app 
development process
, reduce complexity, increase abstraction level, help achieve scalable solutions and maximize cost-effectiveness and productivity.
Objective:
This paper systematically investigates what 
MDD
 techniques and methodologies have been used to date to support mobile app development and how these techniques have been employed, to identify key benefits, limitations, gaps and future research potential.
Method:
A Systematic Literature Review approach was used for this study based on a formal protocol. The rigorous search protocol identified a total of 1,042 peer-reviewed academic research papers from four major 
software engineering
 databases. These papers were subsequently filtered, and 55 high quality relevant studies were selected for analysis, synthesis, and reporting.
Results:
We identified the popularity of different applied 
MDD
 approaches, supporting tools, artifacts, and evaluation techniques. Our analysis found that architecture, domain model, and code generation are the most crucial purposes in MDD-based app development. Three qualities – productivity, scalability and reliability – can benefit from these modeling strategies. We then summarize the key collective strengths, limitations, gaps from the studies and made several future recommendations.
Conclusion:
There has been a steady interest in MDD approaches applied to mobile app development over the years. This paper guides future researchers, developers, and stakeholders to improve app development techniques, ultimately that will help end-users in having more effective apps, especially when some recommendations are addressed, e.g., taking into account more human-centric aspects in app development.",21 Mar 2025,9,"The investigation of Model Driven Development (MDD) techniques for mobile app development is highly relevant and practical for European early-stage ventures, as it offers insights into reducing complexity, increasing productivity, and achieving scalable solutions in app development."
https://www.sciencedirect.com/science/article/pii/S0950584921001312,Blurring boundaries: Toward the collective empathic understanding of product requirements,December 2021,Information and Software Technology,"Requirements understanding and validation, Empathy-driven development, Product team organisation, Collective sensemaking, Constructivist Grounded Theory",Robert C.=Fuller: rfuller@ece.ubc.ca; Philippe=Kruchten: pbk@ece.ubc.ca,"Abstract
Context
Many software product companies create cross-functional development teams that own a product or a defined set of features. These product teams often require a deep and collective understanding of the product domain, a rich context within which to understand the product requirements and to make decisions throughout the 
development process
.
Objective
Little is known about what supports or impedes these teams in collectively achieving this 
deep understanding
. This paper identifies certain organisational conditions that impact teams in this respect.
Method
Using Constructivist 
Grounded Theory method
, we studied 18 teams across seven software companies creating products for a diverse range of markets.
Results
The study found certain organisational and planning process factors play a significant role in whether product development teams have the potential to collectively develop deep domain understanding. These factors also impact individual and development team dynamics.
Conclusions
We identify two essential metaphorical dynamics, broadening the lens and blurring boundaries, that cross-functional product teams employ in order to fully embrace product ownership, visioning, and planning towards achieving this rich context for understanding product requirements. We also conclude that the highly specialised nature of many organisational models and development processes is contraindicated for cross-functional product development teams in achieving this deep collective understanding and we call for a revisiting of conventional organisational and product planning practices for software product development.",21 Mar 2025,6,"The exploration of organisational conditions impacting product development teams' deep understanding can be insightful for startups, although the direct practical implications for early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058492100135X,Topic modeling for feature location in software models: Studying both code generation and interpreted models,December 2021,Information and Software Technology,Not Found,Francisca=Pérez: mfperez@usj.es; Raúl=Lapeña: rlapena@usj.es; Ana C.=Marcén: acmarcen@usj.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
In the last 20 years, the research community has increased its attention to the use of 
topic modeling
 for software maintenance and evolution tasks in code. Topic modeling is a popular and promising information retrieval technique that represents topics by word probabilities. 
Latent Dirichlet Allocation
 (LDA) is one of the most popular 
topic modeling
 methods. However, the use of topic modeling in model-driven software development has been largely neglected. Since software models have less noise (implementation details) than software code, software models might be well-suited for topic modeling.
Objective:
This paper presents our LDA-guided evolutionary approach for feature location in software models. Specifically, we consider two types of software models: models for code generation and interpreted model.
Method:
We evaluate our approach considering two real-world industrial 
case studies
: code-generation models for train control software, and interpreted models for a commercial 
video game
. To study the impact on the results, we compare our approach for feature location in models against random search and a baseline based on Latent Semantic Indexing, which is a popular information retrieval technique. In addition, we perform a statistical analysis of the results to show that this impact is significant. We also discuss the results in terms of the following aspects: data 
sparsity
, implementation complexity, calibration, and stability.
Results:
Our approach significantly outperforms the baseline in terms of recall, precision and F-measure when it comes to interpreted models. This is not the case for code-generation models.
Conclusions:
Our analysis of the results uncovers a recommendation towards results improvement. We also show that calibration approaches can be transferred from code to models. The findings of our work with regards to the compensation of instability have the potential to help not only feature location in models, but also in code.",21 Mar 2025,8,"The LDA-guided evolutionary approach for feature location in software models presents a valuable technique for startups in software development, offering a potentially significant impact on improving feature location in models and code."
https://www.sciencedirect.com/science/article/pii/S0950584921001336,Evaluating the influence of scope on feature location,December 2021,Information and Software Technology,Not Found,África=Domingo: adomingo@usj.es; Jorge=Echeverría: jecheverria@usj.es; Óscar=Pastor: opastor@dsic.upv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Feature Location (FL) is a widespread technique that is used to maintain and evolve a software product. FL is also helpful in reengineering a family of software products into a Software Product Line (SPL). Despite the popularity of FL, there is no study that evaluates the influence of scope (single product or product family) when engineers perform FL.
Objective:
The goal of this paper is to compare the performance, productivity, and perceived difficulty of manual FL when scope changes from a single product to a 
product family
.
Method:
We conducted a crossover experiment to compare the performance, productivity, and perceived difficulty of manual FL when scope changes. The 
experimental objects
 are extracted from a real-world SPL that uses a Domain-Specific Language to generate the firmware of its products.
Results:
Performance and productivity decrease significantly when engineers locate features in a 
product family
 regardless of their experience. For these variables the impact of the FL Scope is medium–large. On contrast, for perceived difficulty, the magnitude of the difference is moderate and is not significant.
Conclusions:
While performance and productivity decrease significantly when engineers locate features in a product family, the difficulty they perceive does not predict the significant worsening of the results. Our work also identifies strengths and weaknesses in FL. This can help in developing better FL approaches and test cases for evaluation.",21 Mar 2025,6,"The comparison of feature location performance in single products versus product families provides useful insights, but the direct practical implications for early-stage ventures may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001373,Feature-based insight for forks in social coding platforms,December 2021,Information and Software Technology,Not Found,Hamzeh=Eyal Salman: hamzehmu@mutah.edu.jo,"Abstract
Context:
Recently, fork-based development has shown to be an easy and straightforward technique to reuse the 
source code
 of existing projects (upstream projects and their forks) in 
open source communities
 (for example, GitHub) and industry. This technique allows developers to tailor the existing forks to build their applications and thus reduce the development’s burden.
Objective:
However, when the number of forks of a given repository increases, it is difficult to manually maintain and keep track of the development activities across all existing forks. Consequently, this leads to redundant development activities and to lose the efforts of the developers and maintainers. In this article, an automatic approach is proposed to overcome the above-mentioned problems.
Method:
The proposed approach incorporates a mathematical research technique called formal 
concept analysis
 with other proposed algorithms.
Results:
To evaluate the effectiveness of the proposed approach, it is applied on four software projects from different domains and sizes. The results show that the proposed approach gives promising results according to well-known metrics in the subject.
Conclusion:
Also, it significantly outperforms the existing state-of-the-art and gives developers in 
open source communities
 and industry a development overview about forks of a given repository.",21 Mar 2025,7,The proposed automatic approach to managing forks in open source communities can significantly benefit early-stage ventures by improving development efficiency and reducing redundant activities.
https://www.sciencedirect.com/science/article/pii/S0950584921001361,Guiding the selection of research methodology in industry–academia collaboration in software engineering,December 2021,Information and Software Technology,"Research methodology, Selecting research methodology, Design Science, Action Research, Technology Transfer Model, Industry–academia collaboration",Claes=Wohlin: claes.wohlin@bth.se; Per=Runeson: per.runeson@cs.lth.se,"Abstract
Background:
The literature concerning research methodologies and methods has increased in 
software engineering
 in the last decade. However, there is limited guidance on selecting an appropriate research methodology for a given research study or project.
Objective:
Based on a selection of research methodologies suitable for software engineering research in collaboration between industry and academia, we present, discuss and compare the methodologies aiming to provide guidance on which research methodology to choose in a given situation to ensure successful industry–academia collaboration in research.
Method:
Three research methodologies were chosen for two main reasons. Design Science and Action Research were selected for their usage in software engineering. We also chose a model emanating from software engineering, i.e., the Technology Transfer Model. An overview of each methodology is provided. It is followed by a discussion and an illustration concerning their use in industry–academia collaborative research. The three methodologies are then compared using a set of criteria as a basis for our guidance.
Results:
The discussion and comparison of the three research methodologies revealed general similarities and distinct differences. All three research methodologies are easily mapped to the general research process describe–solve–practice, while the main driver behind the formulation of the research methodologies is different. Thus, we guide in selecting a research methodology given the primary research objective for a given research study or project in collaboration between industry and academia.
Conclusions:
We observe that the three research methodologies have different main objectives and differ in some characteristics, although still having a lot in common. We conclude that it is vital to make an informed decision concerning which research methodology to use. The presentation and comparison aim to guide selecting an appropriate research methodology when conducting research in collaboration between industry and academia.",21 Mar 2025,8,Providing guidance on selecting research methodologies for successful industry-academia collaboration in software engineering can be valuable for startups looking to engage in research and development partnerships.
https://www.sciencedirect.com/science/article/pii/S0950584921001440,Prioritizing code documentation effort: Can we do it simpler but better?,December 2021,Information and Software Technology,Not Found,Shiran=Liu: Not Found; Zhaoqiang=Guo: Not Found; Yanhui=Li: yanhuili@nju.edu.cn; Hongmin=Lu: Not Found; Lin=Chen: Not Found; Lei=Xu: Not Found; Yuming=Zhou: zhouyuming@nju.edu.cn; Baowen=Xu: Not Found,"Abstract
Context
. Due to time or economic pressures, code developers are often unable to write documents for all modules in a project. Recently, a supervised artificial neural network (ANN) approach is proposed to prioritize documentation effort “to ensure that sections of code important to 
program comprehension
 are thoroughly explained”.
Objective
. However, as a supervised approach, there is a need to use labeled 
training data
 to train the prediction model, which may not easy to obtain in practice. Furthermore, it is unclear whether the ANN approach is generalizable, as it is only evaluated on several small data sets collected from API libraries.
Method
. In this paper, we propose an unsupervised approach based on improved PageRank to prioritize documentation effort. This approach identifies “important” modules only based on the dependence relationships between modules in a project. As a result, the PageRank approach does not need any 
training data
 to build the prediction model.
Results
. In order to evaluate the effectiveness of the PageRank approach, we use six additional large data sets collected from two larger libraries and four applications to conduct the experiment. The experimental results show that the PageRank approach is superior to the state-of-the-art ANN approach.
Conclusion
. Due to the simplicity and effectiveness, we advocate that the PageRank approach should be used as an easy-to-implement baseline in future research on documentation effort prioritization, and any newly proposed approach should be compared with it to demonstrate its effectiveness.",21 Mar 2025,8,The unsupervised PageRank approach for prioritizing documentation effort offers a practical and efficient solution for startups dealing with time and resource constraints in software development.
https://www.sciencedirect.com/science/article/pii/S0950584921001452,UX work in software startups: A thematic analysis of the literature,December 2021,Information and Software Technology,Not Found,Jullia=Saad: julliasaad01@gmail.com; Suéllen=Martinelli: suellen.martinelli@estudante.ufscar.br; Leticia S.=Machado: leticia.smachado@gmail.com; Cleidson R.B.=de Souza: cleidson.desouza@acm.org; Alexandre=Alvaro: alvaro@ufscar.br; Luciana=Zaina: lzaina@ufscar.br,"Abstract
Context:
Startups are new and fast-growing innovative businesses. These companies also deal with uncertain market conditions and work under constant time and business pressures. Although 
User Experience
 (UX) has been widely adopted in the software industry, this has not been a reality in the context of software startups yet. Several factors might influence whether, which, and how UX is adopted by software startups.
Objective:
The objective of this paper is to investigate in the literature how software startups work with UX and to discover the relationship between software development practices and UX in startups.
Methodology:
Our methodology is composed of three main activities: (1) mapping the literature seeking publications on UX work, 
software engineering
, and startups, which resulted in 21 relevant publications; (2) a thematic analysis based on the output of step 1 (i.e., the relevant literature); and (3) refining the themes found out in step 2 and the design of their relationships to explain the link between UX work and software startups.
Results:
The challenges, opportunities, and practices associated with UX in the context of software startups reported by the literature were organized in a set of themes. As a result, seven themes were defined so as to identify needs and opportunities related to UX work in startups. In addition, we synthesize open questions from the literature and suggest new ones to further research directions about the adoption of UX by software startups.
Conclusion:
Our findings demonstrate that software startups require an approach to UX that is more adherent to the startups’ dynamic and disruptive nature. We also suggest emerging 
open research
 questions which should be answered to promote the evolution of UX as applied to software startups.",21 Mar 2025,9,Investigating the relationship between software development practices and user experience in startups can be crucial for early-stage ventures aiming to enhance user satisfaction and product usability.
https://www.sciencedirect.com/science/article/pii/S0950584921001464,Sequential coding patterns: How to use them effectively in code recommendation,December 2021,Information and Software Technology,Not Found,Luiz Laerte Nunes=da Silva: luiznunes@id.uff.br; Troy Costa=Kohwalter: troy@ic.uff.br; Alexandre=Plastino: plastino@ic.uff.br; Leonardo Gresta Paulino=Murta: leomurta@ic.uff.br,"Abstract
Context:
Some programming constructs frequently appear together in different parts of the code, representing sequential coding patterns throughout the project. These sequential coding patterns can be mined from the project repository and, whenever the code a developer is writing coincides with the beginning of a sequential pattern, the remainder of this pattern can be suggested to the developer. This is equivalent to the usual Code Completion, which suggests 
syntactic
 structures based on the line being programmed. However, instead of providing 
syntactic
 suggestions for completing the current line, such feature suggests code snippets containing multiple lines.
Objective:
This paper contributes with an in-depth study on how code pattern recommendation can be used effectively.
Method:
We answer three research questions through a quantitative study using a robust experimental infrastructure with a corpus of five open-source projects: (1) “In a code recommendation, how many frequent coding patterns should be presented?”, (2) “What is the impact of filtering sequential patterns by their confidence?”, and (3) “Does the effectiveness of the sequential coding patterns degrade over time?”.
Results:
Our study shows that it is possible to achieve correctness above 80% when using suggestions with the highest confidence values and that a threshold confidence of 30% generally provides better outcomes. Furthermore, it shows that frequent code pattern completion effectiveness tends to degrade 50 commits after the patterns have been mined.
Conclusion:
We could observe that: (1) the top five ranked suggestions are the ones that deliver the best results; (2) the code recommendations that deliver the best results are the ones with the highest confidence values; and (3) the code recommendation performance degrades as the source code evolves because patterns become outdated.",21 Mar 2025,7,"The study on code pattern recommendation can provide startups with valuable insights on improving code quality and productivity, which are essential for their growth and success."
https://www.sciencedirect.com/science/article/pii/S0950584921001476,Technical debt payment and prevention through the lenses of software architects,December 2021,Information and Software Technology,Not Found,Boris=Pérez: br.perez41@uniandes.edu.co; Camilo=Castellanos: Not Found; Darío=Correal: Not Found; Nicolli=Rios: Not Found; Sávio=Freire: Not Found; Rodrigo=Spínola: Not Found; Carolyn=Seaman: Not Found; Clemente=Izurieta: Not Found,"Abstract
Context:
Architectural decisions
 are considered one of the most common sources of technical debt (TD). Thus, it is necessary to understand how TD is perceived by software architects, particularly, the practices supporting the elimination of debt items from projects, and the practices used to reduce the chances of TD occurrence.
Objective:
This paper investigates the most commonly used practices to pay off TD and to prevent debt occurrence in software projects from the architect’s point of view.
Method:
We used the available data from InsighTD, which is a globally distributed family of industrial surveys on the causes, effects, and management of TD. We analyze responses from a corpus of 72 software architects from Brazil, Chile, Colombia, and the United States.
Results:
Results showed that refactoring (30.2%) was the main practice related to TD payment, followed by design improvements (14.0%). Refactoring, design improvements, and test improvements are the most cited payment practices among cases of code, design and test debt. Concerning the TD preventive practices, we find that having a well-defined architecture and design is the most cited practice (13.6%), followed by having a well-defined scope and requirements. This last practice is the most cited one for expert software architects. Finally, when comparing preventive practices among the three major roles derived from the survey (software architects, engineer roles, and management roles), we found that none of the roles shared the most cited practice, meaning that each role had its worries and focus on different strategies to reduce TD’s presence in the software.
Conclusion:
The lists of TD payment and prevention practices can guide software teams by having a catalog of practices to keep debt controlled or reduced.",21 Mar 2025,8,"The study provides valuable insights into practices to manage technical debt in software projects, which can be highly beneficial for early-stage ventures aiming to maintain code quality and reduce debt."
https://www.sciencedirect.com/science/article/pii/S0950584921001518,An automatic methodology for the quality enhancement of requirements using genetic algorithms,December 2021,Information and Software Technology,Not Found,Daniel=Adanza Dopazo: 100371746@alumnos.uc3m.es; Valentín=Moreno Pelayo: vmpelayo@kr.inf.uc3m.es; Gonzalo=Génova Fuster: ggenova@uc3m.es,"Abstract
Context
The set of requirements for any project offers common ground where the client and the company agree on the most important features and limitations of the project. Having a set of requirements of the highest possible quality is of enormous importance; benefits include improving project quality, understanding client needs better, reducing costs, and predicting project schedules and results with greater accuracy.
Objective
This paper's primary goal is to create a methodology that can provide effective and efficient solutions for modifying poor requirements integrated into a full-fledged system, extracting the main features of each requirement, assessing their quality at an expert level, and, finally, enhancing the quality of the requirements.
Method
In the first step, a 
machine learning algorithm
 is implemented to classify requirements based on quality and identify those that are the likeliest to be problematic. In the second step, the 
genetic algorithm
 generated solutions to enhance the quality of the requirements identified as inferior.
Results
The results of the 
genetic algorithm
 are compared with the theoretically optimal solution. The paper demonstrates the significant flexibility of genetic algorithms, which create a wide variety of solutions and can adapt to any type of classifier. From the initial dataset of requirements, the genetic algorithm finds the optimal solution in 85% of cases after 10 iterations and achieves 59.8% success after only one iteration.
Conclusions
Genetic algorithms are promising tools for 
requirements engineering
 by delivering benefits such as saving costs, automating tasks, and providing more solid and efficient planning in any project through the generation of new solutions.",21 Mar 2025,6,"The methodology for improving requirements quality can benefit startups by enhancing project understanding, reducing costs, and predicting schedules more accurately, but the practical implementation may be challenging for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001531,A longitudinal study of the impact of refactoring in android applications,December 2021,Information and Software Technology,Not Found,Oumayma=Hamdi: Not Found; Mel Ó=Cinnéide: Not Found; Mohamed Wiem=Mkaouer: Not Found,"Abstract
Context:
Mobile applications have to continuously evolve in order to meet new user requirements and technological changes. Addressing these constraints may lead to poor implementation and design choices, known 
code smells
. 
Code refactoring
 is a key practice that is employed to ensure that the intent of a code change is properly achieved without compromising internal software quality. While previous studies have investigated the impact of refactoring on traditional code smells in 
desktop applications
, little attention has been paid to the impact of refactoring activities in mobile application development.
Objective:
We aim to develop a broader understanding of the impact of refactoring activities on 
Android
 and traditional code smells in 
Android
 apps.
Method:
We conduct a longitudinal empirical study by analyzing the evolution history of five open-source Android apps comprising 652 releases and exhibiting a total of 9,600 
refactoring operations
. We consider 15 common Android smell types and 10 common traditional Object-Oriented (OO) code smell types to provide a broad overview of the relationship between refactoring and code smells.
Results:
We find that code smells are widespread across 
Android applications
, but smelly classes are not particularly targeted by refactoring activities and, when they are, it is rare for refactoring to actually remove a smell.
Conclusions:
These somewhat surprising results indicate that it is critical to understand better the real quality issues that Android developers face, and to develop a model of code smells and refactoring that can better address their needs in practice.",21 Mar 2025,7,"The research on refactoring activities in Android apps can offer useful guidance to startups on maintaining code quality and addressing code smells, potentially aiding in the development of robust mobile applications."
https://www.sciencedirect.com/science/article/pii/S0950584921001555,DeepBackground: Metamorphic testing for Deep-Learning-driven image recognition systems accompanied by Background-Relevance,December 2021,Information and Software Technology,Not Found,Zhiyi=Zhang: Not Found; Pu=Wang: Not Found; Hongjing=Guo: Not Found; Ziyuan=Wang: wangziyuan@njupt.edu.cn; Yuqian=Zhou: Not Found; Zhiqiu=Huang: Not Found,"Abstract
Context:
Recently, advances in 
Deep Learning
 (DL) have promoted the development of DL-driven image recognition systems in various fields, such as medical treatment, face detection, etc., almost achieving the same level of performance as the human brain. Nevertheless, using DL-driven image recognition systems in these safety-critical domains requires ensuring the accuracy and the stability of these systems. Recent research in this direction mainly focuses on using the image transformations for the overall image to detect the inconsistency of image recognition systems. However, the influence of the image background region (
i
.
e
.
, the region of the image other than the target object) on the recognition result of the systems and the robustness evaluation of the systems are not considered.
Objective:
To evaluate the robustness of DL-driven image recognition systems about image background region changes, this paper introduces DeepBackground, a novel metamorphic 
testing method
 for DL-driven image recognition systems.
Method:
First, we define a new metric, termed Background-Relevance (BRC) to assess the influence degree of the image background region on the recognition result of the image recognition systems. DeepBackground defines a series of domain-specific metamorphic relations (MRs) combined with BRC and automatically generates many follow-up test images based on these MRs. Finally, DeepBackground detects the inconsistency of these systems and evaluates their robustness about image background changes according to BRC.
Results:
Our empirical validation on 3 commercial image recognition services and 6 popular 
convolutional neural networks
 (CNNs) models shows that DeepBackground can not only evaluate the robustness of these image recognition systems about image background changes according to BRC, but also can detect their inconsistent behaviors.
Conclusion:
DeepBackground is capable of automatically generating high-quality test input images to detect the inconsistency of the image recognition systems, and evaluating the robustness of these systems about image background changes according to BRC.",21 Mar 2025,9,"The DeepBackground method for evaluating robustness of image recognition systems provides practical value in ensuring system accuracy and stability, which can be crucial for startups utilizing image recognition technology in various domains."
https://www.sciencedirect.com/science/article/pii/S0950584921001567,HYDRA: Feedback-driven black-box exploitation of injection vulnerabilities,December 2021,Information and Software Technology,Not Found,Manuel=Leithner: mleithner@sba-research.org; Bernhard=Garn: bgarn@sba-research.org; Dimitris E.=Simos: dsimos@sba-research.org,"Abstract
Context:
Injection vulnerabilities
 remain an omnipresent threat to web application security. These issues arise when user-supplied input is included in commands constructed by the application without applying adequate validation and filtering, permitting attackers to modify the resulting instructions.
Objective:
Tools used in real-world security assessments commonly employ a static list of 
malicious input
 strings to be submitted to the system under test (SUT) to gauge the presence of vulnerabilities. However, sanitizing filters may cause these simulated attacks to fail, even if they only mitigate a subset of potentially harmful values. This may result in a false sense of security. This work introduces HYDRA, a feedback-driven black-box security testing approach for the exploitation of injection vulnerabilities. It is capable of constructing inputs designed to evade such imperfect filters while allowing users to define and rank 
output contexts
, abstract locations in the output of the SUT that are associated with desirable semantics (for instance, allowing the execution of JavaScript code).
Method:
Starting with an innocuous initial input string that is submitted to the SUT and appears anywhere in the output, HYDRA identifies the initial output context. It extends the input string with the goal of reaching contexts that are deemed ”better” according to domain knowledge. This process continues until an ”ideal” output context is reached, usually corresponding to an exploit that impacts the security of the SUT. In addition to this dynamic approach, we present a static variant based on combinatorial security testing. We instantiate our approach by targeting cross-site scripting (XSS) vulnerabilities, detailing the unique challenges posed by HTML 
parsing
, and implement this application of HYDRA in a prototype tool.
Results:
The evaluation shows that our implementation is able to evade faulty filters and is effective at identifying injection vulnerabilities while remaining more flexible than existing approaches by allowing users to define desirable output contexts.
Conclusion:
Based on the results of our evaluation, we are confident that including the HYDRA approach in security assessments will increase the number of identified XSS vulnerabilities, particularly those that are difficult to exploit. We anticipate that an application to other classes of vulnerabilities such as 
SQL
 injections will significantly advance the state of the art.",21 Mar 2025,7,"The HYDRA approach for security testing of web applications addresses injection vulnerabilities effectively, offering a valuable tool for startups to enhance their application security, though the complexity of implementation may pose challenges."
https://www.sciencedirect.com/science/article/pii/S095058492100166X,MS-QuAAF: A generic evaluation framework for monitoring software architecture quality,December 2021,Information and Software Technology,Not Found,Salim=Kadri: salim8359@yahoo.fr; Sofiane=Aouag: Sofiane.Aouag@gmail.com; Djalal=Hedjazi: Djalal.Hedjazi@gmail.com,"Abstract
Context
In a highly competitive software market, architecture quality is one of the key 
differentiators
 between software systems. Many quantitative and qualitative evaluation frameworks were proposed to measure architecture. However, qualitative evaluation lacks statistical significance, whereas quantitative methods are designed for evaluating 
specific quality attributes
, such as modifiability and performance. Besides, the assessment covers usually a single development stage, either at the design stage or at the implementation stage.
Objective
A lack of generic frameworks that can support the assessment of a broad set of attributes and ensure continuous evaluation by covering the main development stages is addressed. Accordingly, this paper presents MS-QuAAF, a 
quantitative assessment
 framework destined for evaluating software architecture through a set of generic metrics.
Method
The 
quantitative evaluation
 consists of checking architecture facets mapped to quality attributes against the early specified meta-models. This process starts by analyzing rules infringements and calculating architecture defects after accomplishing the design stage. Second, the assigned responsibilities supposed to promote stakeholders’ quality attributes are assessed quantitatively at the end of the implementation stage. Third, the final evaluation report is generated.
Results
We made specifically three main contributions. First, the proposed metrics within the framework are generic, which means that the framework has the ability to assess any inputted quality. Second, the framework proposes a set of evaluation services capable of assessing the architecture at two main development stages, which are design and implementation. Third, we proposed a 
quantitative assessment
 tree within the framework called the Responsibilities Satisfaction Tree (RST) that uses 
NFR
 responsibilities nodes to evaluate the implemented architectures.
Conclusion
The conducted experiment showed that the framework is capable of evaluating quality attributes based on architecture specification using the proposed metrics. Furthermore, these metrics contributed to enhancing architecture quality during the development stages by notifying architects of the discovered anomalies.",21 Mar 2025,9,The development of a quantitative assessment framework for evaluating software architecture at various development stages can significantly impact early-stage ventures by enhancing architecture quality and notifying architects of anomalies.
https://www.sciencedirect.com/science/article/pii/S0950584921000938,Assessing test artifact quality—A tertiary study,November 2021,Information and Software Technology,"Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance",Huynh Khanh Vi=Tran: huynh.khanh.vi.tran@bth.se; Michael=Unterkalmsteiner: michael.unterkalmsteiner@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Nauman bin=Ali: nauman.ali@bth.se,"Abstract
Context:
Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases.
Objective:
We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives.
Methods:
We have carried out a systematic literature review to identify and analyze existing secondary studies on 
quality aspects
 of software testing artifacts.
Results:
We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the 
quality attributes
 and measurements based on findings in the literature and ISO/IEC 25010:2011.
Conclusion:
The test artifact 
quality model
 presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furthermore, the model can also be used as a framework for documenting context characteristics to make 
research results
 more accessible for research and practice.",21 Mar 2025,7,The comprehensive model for test case/suite quality can provide valuable insights for startups to improve their testing artifacts and enhance the overall quality of their software.
https://www.sciencedirect.com/science/article/pii/S0950584921001270,The impact of using biased performance metrics on software defect prediction research,November 2021,Information and Software Technology,Not Found,Jingxiu=Yao: JingxiuYao@buaa.edu.cn; Martin=Shepperd: martin.shepperd@brunel.ac.uk,"Abstract
Context:
Software engineering
 researchers have undertaken many experiments investigating the potential of software 
defect prediction
 algorithms. Unfortunately some widely used performance metrics are known to be problematic, most notably F1, but nevertheless F1 is widely used.
Objective:
To investigate the potential impact of using F1 on the validity of this large body of research.
Method:
We undertook a 
systematic review
 to locate relevant experiments and then extract all 
pairwise comparisons
 of 
defect prediction
 performance using F1 and the unbiased Matthews 
correlation coefficient
 (MCC).
Results:
We found a total of 38 primary studies. These contain 12,471 pairs of results. Of these comparisons, 21.95% changed direction when the MCC metric is used instead of the biased F1 metric. Unfortunately, we also found evidence suggesting that F1 remains widely used in 
software defect
 prediction research.
Conclusion:
We reiterate the concerns of statisticians that the F1 is a problematic metric outside of an information retrieval context, since we are concerned about both classes (defect-prone and not defect-prone units). This inappropriate usage has led to a substantial number (more than one fifth) of erroneous (in terms of direction) results. Therefore we urge researchers to (i) use an unbiased metric and (ii) publish detailed results including 
confusion matrices
 such that alternative analyses become possible.",21 Mar 2025,5,"The investigation of the impact of using F1 metrics on software defect prediction research, while important, may have less immediate practical value for startups compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921001154,A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models,November 2021,Information and Software Technology,Not Found,Kunsong=Zhao: Not Found; Zhou=Xu: zhouxullx@cqu.edu.cn; Meng=Yan: Not Found; Tao=Zhang: Not Found; Dan=Yang: Not Found; Wei=Li: Not Found,"Abstract
Context:
Software crash is a serious form of the software failure, which often occurs during the software development and maintenance process. As the stack trace reported when the software crashes contains a wealth of information about crashes, recent work utilized 
classification models
 with the collected features from stack traces and 
source code
 to predict whether the fault causing the crash resides in the stack trace. This could speed-up the crash localization task.
Objective:
As the quality of features can affect the performance of the constructed 
classification models
, researchers proposed to use feature selection methods to select a representative feature subset to build models by replacing the original features. However, only limited feature selection methods and classification models were taken into consideration for this issue in previous work. In this work, we look into this topic deeply and find out the best feature selection method for crash fault residence prediction task.
Method:
We study the performance of 24 feature selection techniques with 21 classification models on a benchmark dataset containing crash instances from 7 real-world software projects. We use 4 indicators to evaluate the performance of these feature selection methods which are applied to the classification models.
Results:
The experimental results show that, overall, a probability-based feature selection, called Symmetrical Uncertainty, performs well across the studied classification models and projects. Thus, we recommend such a feature selection method to preprocess the crash instances before constructing classification models to predict the crash fault residence.
Conclusion:
This work conducts a large-scale empirical study to investigate the impact of feature selection methods on the performance of classification models for the crashing fault residence prediction task. The results clearly demonstrate that there exist significant performance differences among these feature selection techniques across different classification models and projects.",21 Mar 2025,8,"The study on feature selection methods for crash fault residence prediction task can offer startups valuable guidance on improving the performance of their classification models, impacting the quality of their software."
https://www.sciencedirect.com/science/article/pii/S0950584921000951,"Does shortening the release cycle affect refactoring activities: A case study of the JDT Core, Platform SWT, and UI projects",November 2021,Information and Software Technology,Not Found,Olivier=Nourry: oliviern@posl.ait.kyushu-u.ac.jp; Yutaro=Kashiwa: kashiwa@ait.kyushu-u.ac.jp; Yasutaka=Kamei: kamei@ait.kyushu-u.ac.jp; Naoyasu=Ubayashi: ubayashi@ait.kyushu-u.ac.jp,"Abstract
Context:
Several large-scale companies such as Google and Netflix chose to adopt short release cycles (e.g., rapid releases) in recent years. Although this allows these companies to provide updates and features faster for their users, it also causes developers to have less time to dedicate to development activities other than feature development.
Objective:
In this paper, we investigate how refactoring activities were impacted by the adoption of shorter releases.
Method:
We extract all refactorings applied over a period of two years during traditional yearly releases and almost two years during shorter quarterly releases in three Eclipse projects. We then analyze both time periods’ refactoring activities to understand how refactoring activities can be impacted by shortening the release cycles.
Results:
We observe reduced refactoring activities in one project and a decrease in more complex 
refactoring operations
 after shortening the release cycles. We also find that weekly efforts dedicated to refactoring activities was lower across all projects after shortening the release cycles.
Conclusion:
Shorter releases may impact software development tasks such as refactoring in unintended ways. Not applying specific types of refactoring may also affect the software’s quality in the long term. Using this 
case study
 and past work on shorter releases, potential short release adopters can now better plan their transition to shorter releases knowing which areas of development may be affected.",21 Mar 2025,6,"The investigation on how shorter release cycles impact refactoring activities, while interesting, may have limited immediate practical impact on early-stage ventures compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000999,RBAC protection-impacting changes identification: A case study of the security evolution of two PHP applications,November 2021,Information and Software Technology,Not Found,Marc-André=Laverdière: marc-andre.laverdiere-papineau@polymtl.ca; Karl=Julien: karl.julien@polymtl.ca; Ettore=Merlo: ettore.merlo@polymtl.ca,"Abstract
Abstract:
Web applications often use Role-Based Access Control (RBAC) to restrict operations and protect security 
sensitive information
 and resources.
Context:
Web applications’ RBAC security may be affected by 
source code
 changes between releases. Developers should re-validate their application prior to release, but this may be labor and resource-intensive.
Objective:
Among all changes between two versions during software evolution, we define Protection-Impacting Changes (PICs) as changed statements that potentially alter privilege protection of other statement(s). PICs may focus the attention of developers towards root cause candidates for security protection changes, especially when these protection changes are unexpected.
Method:
The proposed automated 
static analysis
 identifies PICs between two versions of an application. It is based on the interprocedural 
flow graph
 
reachability
 analysis of security checks and statements.
Results:
We examined the software evolution of two PHP web applications. We examined 210 versions of WordPress, and 192 versions of MediaWiki. Additional experiments have been performed on 19 fix commits corresponding to Common Vulnerabilities and Exposures CVEs from WordPress. They are presented and discussed in this paper and show that PICs contain 98.2% of the CVE oracle root causes.
Conclusion:
PICs represent overall only 8% and 2% of total code changes, respectively for WordPress and MediaWiki. PICs may help developers to focus onto a smaller number of candidate security-related problems, during software evolution. Consequently, developers may re-validate application security and perform repairs more efficiently.",21 Mar 2025,8,"The proposed automated analysis for identifying Protection-Impacting Changes (PICs) can significantly help developers focus on security-related issues during software evolution, thus improving application security efficiently."
https://www.sciencedirect.com/science/article/pii/S0950584921001051,A decision model for programming language ecosystem selection: Seven industry case studies,November 2021,Information and Software Technology,"Programming language ecosystem selection, Decision model, Industry case study, Software production, Multi-criteria decision-making, Decision support system",Siamak=Farshidi: s.farshidi@uva.nl; Slinger=Jansen: slinger.jansen@uu.nl; Mahdi=Deldar: m.deldar@datakavosh.com,"Abstract
Context:
Software development is a continuous decision-making process that mainly relies on the software engineer’s experience and intuition. One of the essential decisions in the early stages of the process is selecting the best fitting programming language ecosystem based on the project requirements. A significant number of criteria, such as developer availability and consistent documentation, in addition to the number of available options in the market, lead to a challenging decision-making process. As the selection of programming language ecosystems depends on the application to be developed and its environment, a decision model is required to analyze the selection problem using systematic identification and evaluation of potential alternatives for a development project.
Method:
Recently, we introduced a framework to build decision models for technology selection problems in software production. Furthermore, we designed and implemented a decision support system that uses such decision models to support software engineers with their decision-making problems. This study presents a decision model based on the framework for the programming language ecosystem selection problem.
Results:
The decision model has been evaluated through seven real-world 
case studies
 at seven software development companies. The case study participants declared that the approach provides significantly more insight into the programming language ecosystem selection process and decreases the decision-making process’s time and cost.
Conclusion:
With the decision model, software engineers can more rapidly evaluate and select programming language ecosystems. Having the knowledge in the decision model readily available supports software engineers in making more efficient and effective decisions that meet their requirements and priorities. Furthermore, such reusable knowledge can be employed by other researchers to develop new concepts and solutions for future challenges.",21 Mar 2025,6,"The decision model for programming language ecosystem selection provides valuable insights and decreases decision-making time and cost, offering practical benefits to software engineers."
https://www.sciencedirect.com/science/article/pii/S0950584921001038,Automatic patch linkage detection in code review using textual content and file location features,November 2021,Information and Software Technology,Not Found,Dong=Wang: wang.dong.vt8@is.naist.jp; Raula Gaikovina=Kula: Not Found; Takashi=Ishio: Not Found; Kenichi=Matsumoto: Not Found,"Abstract
Context:
Contemporary code review tools are a popular choice for 
software quality assurance
. Using these tools, reviewers are able to post a 
linkage
 between two patches during a review discussion. Large development teams that use a review-then-commit model risk being unaware of these linkages.
Objective:
Our objective is to first explore how patch linkage impacts the review process. We then propose and evaluate models that detect patch linkage based on realistic time intervals.
Method:
First, we carry out an 
exploratory study
 on three 
open source projects
 to conduct linkage impact analysis using 942 manually classified linkages. Second, we propose two techniques using textual and file location similarity to build detection models and evaluate their performance.
Results:
The study provides evidence of latency in the linkage notification. We show that a patch with the Alternative Solution linkage (i.e., patches that implement similar functionality) undergoes a quicker review and avoids additional revisions after the team has been notified, compared to other linkage types. Our detection model experiments show promising recall rates for the Alternative Solution linkage (from 32% to 95%), but precision has room for improvement.
Conclusion:
Patch linkage detection is promising, with likely improvements if the practice of posting linkages becomes more prevalent. From our implications, this paper lays the groundwork for future research on how to increase patch linkage awareness to facilitate efficient reviews.",21 Mar 2025,7,"The models proposed for detecting patch linkage in code review processes show promising results, potentially improving efficiency and awareness for large development teams."
https://www.sciencedirect.com/science/article/pii/S0950584921001166,ALBFL: A novel neural ranking model for software fault localization via combining static and dynamic features,November 2021,Information and Software Technology,Not Found,Xi=Xiao: xiaox@sz.tsinghua.edu.cn; Yuqing=Pan: 66panyuqing@sina.com; Bin=Zhang: bin.zhang@pcl.ac.cn; Guangwu=Hu: hugw@sziit.edu.cn; Qing=Li: liq8@sustech.edu.cn; Runiu=Lu: lurn@sz.singhua.edu.cn,"Abstract
Context
Automatic 
software fault
 localization serves as a significant purpose in helping developers solve bugs efficiently. Existing approaches for software 
fault localization
 can be categorized into static methods and dynamic ones, which have improved the fault locating ability greatly by analyzing static features from the 
source code
 or tracking dynamic behaviors during the runtime respectively. However, the accuracy of 
fault localization
 is still unsatisfactory.
Objective
To enhance the capability of detecting software faults with the statement 
granularity
, this paper puts forward ALBFL, a novel neural ranking model that combines the static and dynamic features, which obtains excellent fault 
localization accuracy
. Firstly, ALBFL learns the 
semantic features
 of the 
source code
 by a transformer encoder. Then, it exploits a self-attention layer to integrate those static features and dynamic features. Finally, those integrated features are fed into a LambdaRank model, which can list the suspicious statements in 
descending order
 by their ranked scores.
Method
The experiments are conducted on an authoritative dataset (i.e., Defect4J), which includes 5 open-source projects, 357 faulty programs in total. We evaluate the effectiveness of ALBFL, effectiveness of combining features, effectiveness of model components and aggregation on method level.
Result
The results reflect that ALBFL identifies triple more faulty statements than 11 traditional 
SBFL methods
 and outperforms 2 state-of-the-art approaches by on average 14% on ranking faults in the first position.
Conclusions
To improve the precision of automatic 
software fault
 localization, ALBFL combines 
neural network
 ranking model equipped with the self-attention layer and the transformer encoder, which can take full use of various techniques to judge whether a code statement is fault-inducing or not. Moreover, the 
joint
 architecture of ALBFL is capable of training the integration of these features under various strategies so as to improve accuracy further. In the future, we plan to exploit more features so as to improve our method's efficiency and accuracy.",21 Mar 2025,9,"ALBFL, a novel neural ranking model for software fault localization, significantly outperforms traditional methods and state-of-the-art approaches, showcasing a high impact on improving fault localization accuracy."
https://www.sciencedirect.com/science/article/pii/S0950584921001300,Crowdsourced test report prioritization considering bug severity,November 2021,Information and Software Technology,Not Found,Yao=Tong: Not Found; Xiaofang=Zhang: xfzhang@suda.edu.cn,"Abstract
In crowdsourced testing, a large number of test reports will be generated in a short time. How to efficiently inspect these reports becomes one of the critical steps in the testing process. In recent years, many automated techniques like clustering, classification, and prioritization have emerged to provide an automated inspection order over test reports. Even though these methods have achieved 
good performance
, they did not consider the priority to image and text information. Simultaneously, existing prioritization approaches only focus on the rate of detecting faults but ignore the severity of the faults. In fact, bug severity is a vital indicator that the users provide to flag the 
criticality
 of a bug, so developers can then use it to set their priority for the resolution process. For these reasons, this paper presents a novel prioritization approach for crowdsourcing test reports. It extracts features from text and screenshot information of the test reports, uses the hash technique to index test reports, and finally designs a prioritization algorithm. To validate our approach, we conducted experiments on six industrial projects. The results and the hypotheses analysis show that our approach can detect all faults faster in a limited time and can prioritize reports that have higher severity faults compared with the existing methods.",21 Mar 2025,5,"The prioritization approach for crowdsourced test reports considers both text and image information, as well as bug severity, but the impact may be limited compared to other abstracts in terms of practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001257,Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction,November 2021,Information and Software Technology,Not Found,Shuo=Feng: shuofeng5-c@my.cityu.edu.hk; Jacky=Keung: jacky.keung@cityu.edu.hk; Xiao=Yu: xiaoyu@whut.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Miao=Zhang: miazhang9-c@my.cityu.edu.hk,"Abstract
Context:
In practice, software datasets tend to have more non-defective instances than defective ones, which is referred to as the 
class imbalance problem
 in 
software defect
 prediction (SDP). Synthetic Minority Oversampling TEchnique (SMOTE) and its variants alleviate the 
class imbalance problem
 by generating synthetic defective instances. SMOTE-based oversampling techniques were widely adopted as the baselines to compare with the newly proposed oversampling techniques in SDP. However, randomness is introduced during the procedure of SMOTE-based oversampling techniques. If the performance of SMOTE-based oversampling techniques is highly unstable, the conclusion drawn from the comparison between SMOTE-based oversampling techniques and the newly proposed techniques may be misleading and less convincing.
Objective:
This paper aims to investigate the stability of SMOTE-based oversampling techniques. Moreover, a series of stable SMOTE-based oversampling techniques are proposed to improve the stability of SMOTE-based oversampling techniques.
Method:
Stable SMOTE-based oversampling techniques reduce the randomness in each step of SMOTE-based oversampling techniques by selecting defective instances in turn, distance-based selection of 
K
 neighbor instances, and evenly distributed interpolation. Besides, we mathematically prove and also empirically investigate the stability of SMOTE-based and stable SMOTE-based oversampling techniques on four common classifiers across 26 datasets in terms of AUC, 
b
a
l
a
n
c
e
, and 
MCC
.
Results:
The analysis of SMOTE-based and stable SMOTE-based oversampling techniques shows that the performance of stable SMOTE-based oversampling techniques is more stable and better than that of SMOTE-based oversampling techniques. The difference between the worst and best performances of SMOTE-based oversampling techniques is up to 23.3%, 32.6%, and 204.2% in terms of AUC, 
b
a
l
a
n
c
e
, and 
MCC
, respectively.
Conclusion:
Stable SMOTE-based oversampling techniques should be considered as a drop-in replacement for SMOTE-based oversampling techniques.",21 Mar 2025,7,"The investigation of stable SMOTE-based oversampling techniques can have a significant impact on improving software defect prediction, which is crucial for early-stage ventures relying on software quality."
https://www.sciencedirect.com/science/article/pii/S0950584921001282,Architectural design decisions that incur technical debt — An industrial case study,November 2021,Information and Software Technology,"Technical debt, Architectural design decisions, Architectural knowledge, Architectural technical debt",Mohamed=Soliman: m.a.m.soliman@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl; Yikun=Li: yikun.li@rug.nl,"Abstract
Context:
During software development, some 
architectural design decisions
 incur technical debt, either deliberately or inadvertently. These have serious impact on the quality of a software system, and can cost significant time and effort to be changed. While current research efforts have explored general concepts of architectural design decisions and technical debt separately, debt-incurring architectural design decisions have not been specifically explored in practice.
Objective:
In this 
case study
, we explore debt-incurring architectural design decisions (DADDs) in practice. Specifically, we explore the main types of DADDs, why and how they are incurred in a software system, and how practitioners deal with these types of design decisions.
Method:
We performed interviews and a focus group with practitioners working in embedded and enterprise software companies, discussing their concrete experience with such architectural design decisions.
Results:
We provide the following contributions: 1) A categorization for the types of DADDs, which extend a current ontology on architectural design decisions. 2) A process on how deliberate DADDs are made in practice. 3) A conceptual model which shows the relationships between the causes and triggers of inadvertent DADDs. 4) The main factors that influence the way of dealing with DADDs.
Conclusion:
The results can support the development of new approaches and tools for Architecture Technical Debt management from the perspective of Design Decisions. Moreover, they support future research to capture architecture knowledge related to DADDs.",21 Mar 2025,6,Exploring debt-incurring architectural design decisions provides valuable insights for startups to manage technical debt and enhance the quality of their software systems.
https://www.sciencedirect.com/science/article/pii/S0950584921001269,“Won’t We Fix this Issue?” Qualitative characterization and automated identification of wontfix issues on GitHub,November 2021,Information and Software Technology,"Issue tracking, Issue management, Empirical study, Machine learning",Sebastiano=Panichella: panc@zhaw.ch; Gerardo=Canfora: canfora@unisannio.it; Andrea=Di Sorbo: disorbo@unisannio.it,"Abstract
Context
: Addressing user requests in the form of 
bug reports
 and Github issues represents a crucial task of any successful software project. However, user-submitted issue reports tend to widely differ in their quality, and developers spend a considerable amount of time handling them.
Objective
: By collecting a dataset of around 6,000 issues of 279 GitHub projects, we observe that developers take significant time (i.e., about five months, on average) before labeling an issue as a wontfix. For this reason, in this paper, we empirically investigate the nature of wontfix issues and methods to facilitate issue management process.
Method
: We first manually analyze a sample of 667 wontfix issues, extracted from heterogeneous projects, investigating the common reasons behind a “wontfix decision”, the main characteristics of wontfix issues and the potential factors that could be connected with the time to close them. Furthermore, we experiment with approaches enabling the prediction of wontfix issues by analyzing the titles and descriptions of reported issues when submitted.
Results and conclusion
: Our investigation sheds some light on the wontfix issues’ characteristics, as well as the potential factors that may affect the time required to make a “wontfix decision”. Our results also demonstrate that it is possible to perform prediction of wontfix issues with high average values of precision, recall, and F-measure (90%–93%).",21 Mar 2025,5,"Understanding and predicting 'wontfix' issues can aid startups in streamlining their issue management processes, although the immediate impact might be more limited."
https://www.sciencedirect.com/science/article/pii/S0950584921001294,On the value of encouraging gender tolerance and inclusiveness in software engineering communities,November 2021,Information and Software Technology,Not Found,Elijah=Zolduoarrati: Not Found; Sherlock A.=Licorish: sherlock.licorish@otago.ac.nz,"Abstract
Context
The recent spike in the growth of online communities is a testament to the technological advancements of the 21st century. People with shared interests, problems, and solutions can now engage via online groups, including the 
software engineering
 community. There is evidence, however, to suggest females are often underrepresented in such online communities, and especially those that are technology related. This comes at a great loss to these communities, and for 
software engineering
 in particular. Females, like males, add much value to the field of software engineering.
Objective
Limited evidence exists to quantify the value of males and females in the software engineering process or relevant communities. This insight could inform evidence-driven inclusiveness strategies. Accordingly, we sought to better understand 
gender differences
 in the Stack Overflow community in order to delineate the value of 
gender diversity
 in the field of software engineering.
Method
This study used 
archival data
 from Stack Overflow over an 11-year period, comprising records from 9.5 million contributors. We employed quantitative and qualitative approaches to examine the role of gender in terms of contributors’ orientation, attitudes, and 
knowledge sharing
 patterns.
Results
The results indicate female contributors on Stack Overflow differed significantly from males in relation to their orientation, attitudes, and 
knowledge sharing
 patterns. We observe that female contributors tend to have a more cooperative orientation. Additionally, females expressed a more supportive and collective outlook and were more willing to share knowledge than their male counterparts.
Conclusion
The software engineering community would benefit from gender tolerance and inclusiveness to promote a knowledge sharing culture. In this regard, 
gender diversity
 should be encouraged for the value it brings to Stack Overflow and the field of software engineering.",21 Mar 2025,4,"Addressing gender diversity in software engineering communities is important, but the immediate practical impact on early-stage ventures may be less direct."
https://www.sciencedirect.com/science/article/pii/S0950584921001324,The organization of software teams in the quest for continuous delivery: A grounded theory approach,November 2021,Information and Software Technology,Not Found,Leonardo=Leite: leofl@ime.usp.br; Gustavo=Pinto: Not Found; Fabio=Kon: Not Found; Paulo=Meirelles: Not Found,"Abstract
Context:
To accelerate time-to-market and improve customer satisfaction, software-producing organizations have adopted continuous delivery practices, impacting the relations between development and infrastructure professionals. Yet, no substantial literature has substantially tackled how the software industry structures the organization of development and infrastructure teams.
Objective:
In this study, we investigate how software-producing organizations structure their development and infrastructure teams, specifically how is the division of labor among these groups and how they interact.
Method:
After brainstorming with 7 
DevOps
 experts to better formulate our research and procedures, we collected and analyzed data from 37 semi-structured interviews with IT professionals, following Grounded Theory guidelines.
Results:
After a careful analysis, we identified four common organizational structures: (1) siloed departments, (2) classical 
DevOps
, (3) cross-functional teams, and (4) platform teams. We also observed that some companies are transitioning between these structures.
Conclusion:
The main contribution of this study is a theory in the form of a taxonomy that organizes the found structures along with their properties. This theory could guide researchers and practitioners to think about how to better structure development and infrastructure professionals in software-producing organizations.",21 Mar 2025,3,"While studying the structure of development and infrastructure teams is valuable, the direct impact on early-stage ventures may be more theoretical than immediately applicable."
https://www.sciencedirect.com/science/article/pii/S0950584921000914,On the impact of Continuous Integration on refactoring practice: An exploratory study on TravisTorrent,October 2021,Information and Software Technology,Not Found,Islem=Saidani: islem.saidani.1@ens.etsmlt.ca; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Fabio=Palomba: fpalomba@unisa.it,"Abstract
Context:
The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption.
Objective:
We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied.
Method:
We collect a corpus of 99,545 commits and 89,926 
refactoring operations
 extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA).
Results:
Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context.
Conclusion:
Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems.",21 Mar 2025,6,"The study provides insights on the impact of Continuous Integration on refactoring practices, which can be valuable for early-stage ventures looking to improve software quality and maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584921000902,Predicting long-time contributors for GitHub projects using machine learning,October 2021,Information and Software Technology,"Long-time contributor, GitHub, GHTorrent, BigQuery, Machine learning models",Vijaya Kumar=Eluri: eluri@gwu.edu; Thomas A.=Mazzuchi: mazzu@gwu.edu; Shahram=Sarkani: sarkani@gwu.edu,"Abstract
Context:
Many organizations develop software systems using 
open source software
 (OSS), which is risky due to the high possibility of losing support. Contributors are critical for the survival of 
OSS projects
, but very few new contributors remain with OSS projects to become long-time contributors (LTCs). Identification of factors that contribute to become an LTC can help OSS project owners utilize limited resources to retain new contributors.
Objective:
In this paper, we investigate whether we can effectively predict new contributors to OSS repositories becoming long time contributors based on repository and contributor meta-data collected from GitHub.
Method:
We construct a dataset containing 70,899 observations from 888 most popular repositories with 56,766 contributors. Each observation represents a contributor who joined the repository and is categorized as either an LTC or a non-LTC, depending on whether their project tenure is longer than 3 years. Each observation has 31 features that are calculated using the information of the new contributor and the repository when a new contributor joins the project. We build several 
machine learning
 models, including 
naive Bayes
, k-nearest neighbor, 
logistic regression
, 
decision tree
, and 
random forest
 to predict LTC validated using 10-fold cross-validation. We compare our best model with state of the art model in terms of precision, recall, F1-score, Matthews 
correlation coefficient
 (MCC), and area under the curve (AUC).
Results:
In 10-fold cross-validation, the precision, recall, F1-score, MCC, and AUC of our best model (random forest) are 0.695, 0.079, 0.140, 0.226, and 0.913, respectively. These values are 27.29%, 92.68%, 86.67%, 56.94%, and 0.55%, respectively better than the best 
baseline state
 of the art model (random forest).
Conclusion:
Compared to state of the art models, the models built using our approach use less than 50% features (31 vs 63), have no wait time of one month after the contributor joins to predict future LTC status, and produce better results.",21 Mar 2025,8,"The research offers a practical approach to predicting long-time contributors to OSS repositories, which can significantly benefit European startups relying on open source software for development."
https://www.sciencedirect.com/science/article/pii/S0950584921000963,Why many challenges with GUI test automation (will) remain,October 2021,Information and Software Technology,Not Found,Michel=Nass: michel.nass@bth.se; Emil=Alégroth: emil.alegroth@bth.se; Robert=Feldt: robert.feldt@chalmers.se,"Abstract
Context:
Automated testing is ubiquitous in modern software development and used to verify requirement conformance on all levels of system abstraction, including the system’s graphical user interface (GUI). GUI-based test automation, like other automation, aims to reduce the cost and time for testing compared to alternative, manual approaches. Automation has been successful in reducing costs for other forms of testing (like unit- or integration testing) in industrial practice. However, we have not yet seen the same convincing results for automated GUI-based testing, which has instead been associated with multiple 
technical challenges
. Furthermore, the software industry has struggled with some of these challenges for more than a decade with what seems like only limited progress.
Objective:
This systematic literature review takes a longitudinal perspective on GUI test automation challenges by identifying them and then investigating why the field has been unable to mitigate them for so many years.
Method:
The review is based on a final set of 49 publications, all reporting empirical evidence from practice or industrial studies. Statements from the publications are synthesized, based on a thematic coding, into 24 challenges related to GUI test automation.
Results:
The most reported challenges were mapped chronologically and further analyzed to determine how they and their proposed solutions have evolved over time. This chronological mapping of reported challenges shows that four of them have existed for almost two decades.
Conclusion:
Based on the analysis, we discuss why the key challenges with GUI-based test automation are still present and why some will likely remain in the future. For others, we discuss possible ways of how the challenges can be addressed. Further research should focus on finding solutions to the identified 
technical challenges
 with GUI-based test automation that can be resolved or mitigated. However, in parallel, we also need to acknowledge and try to overcome non-technical challenges.",21 Mar 2025,4,"The study highlights the challenges in GUI test automation, which may not directly impact early-stage ventures but can provide valuable insights for startups dealing with GUI-based applications."
https://www.sciencedirect.com/science/article/pii/S0950584921000987,Metamorphic testing of OpenStreetMap,October 2021,Information and Software Technology,"Metamorphic testing, Quality of maps, OpenStreetMap",Jesús M.=Almendros-Jiménez: jalmen@ual.es; Antonio=Becerra-Terón: abecerra@ual.es; Mercedes G.=Merayo: mgmerayo@fdi.ucm.es; Manuel=Núñez: mn@sip.ucm.es,"Abstract
Context:
OpenStreetMap represents a collaborative effort of many different and unrelated users to create a free map of the world. Although contributors follow some general guidelines, unsupervised additions are prone to include erroneous information. Unfortunately, it is impossible to automatically detect most of these issues because there does not exist an 
oracle
 to evaluate whether the information is correct or not. Metamorphic testing has shown to be very useful in assessing the correctness of very heterogeneous artifacts when oracles are not available.
Objective:
The main goal of our work is to provide a (fully implemented) framework, based on metamorphic testing, that will support the analysis of the information provided in OpenStreetMap with the goal of detecting 
faulty
 information.
Method:
We defined a general metamorphic testing framework to deal with OpenStreetMap. We identified a set of 
good
 metamorphic relations. In order to have as much automation as possible, we paid 
special attention
 to the automatic selection of 
follow-up inputs
 because they are fundamental to diminish manual testing. In order to assess the usefulness of our framework, we applied it to analyze maps of four cities in different continents. The rationale is that we would be dealing with different problems created by different contributors.
Results:
We obtained experimental evidence that shows the potential value of our framework. The application of our framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories.
Conclusion:
The experiments showed the usefulness of our framework to identify potential issues in the information appearing in OpenStreetMap. Although our metamorphic relations are very helpful, future users of the framework might identify other relations to deal with specific situations not covered by our relations. Since we provide a general pattern to define metamorphic relations, it is relatively easy to extend the existing framework. In particular, since all our metamorphic relations are implemented and the code is freely available, users have a pattern to implement new relations.",21 Mar 2025,7,"The framework using metamorphic testing for assessing information in OpenStreetMap can be beneficial for startups relying on location-based services and data quality, offering practical value in error detection and verification."
https://www.sciencedirect.com/science/article/pii/S0950584921001014,Automating user-feedback driven requirements prioritization,October 2021,Information and Software Technology,Not Found,Fitsum Meshesha=Kifetew: kifetew@fbk.eu; Anna=Perini: perini@fbk.eu; Angelo=Susi: susi@fbk.eu; Aberto=Siena: siena@fbk.eu; Denisse=Muñante: denisse_yessica.munante_arzapalo@telecom-sudparis.eu; Itzel=Morales-Ramirez: imoralesr@iingen.unam.mx,"Abstract
Context:
Feedback from end users of 
software applications
 is a valuable resource in understanding what users request, what they value, and what they dislike. Information derived from user-feedback can support software evolution activities, such as requirements prioritization. User-feedback analysis is still mostly performed manually by practitioners, despite growing research in automated analysis.
Objective:
We address two issues in automated user-feedback analysis: (i) most of the existing automated analysis approaches that exploit 
linguistic analysis
 assume that the vocabulary adopted by users (when expressing feedback) and developers (when formulating requirements) are the same; and (ii) user-feedback analysis techniques are usually experimentally evaluated only on some user-feedback dataset, not involving assessment by potential software developers.
Method:
We propose an approach, 
ReFeed
, that computes, for each requirement, the set of related user-feedback, and from such user-feedback extracts quantifiable properties which are relevant for prioritizing the requirement. The extracted properties are propagated to the 
related requirements
, based on which ranks are computed for each requirement. 
ReFeed
 relies on domain knowledge, in the form of an ontology, helping mitigate the gap in the vocabulary of end users and developers. The effectiveness of 
ReFeed
 is evaluated on a realistic requirements prioritization scenario in two experiments involving graduate students from two different universities.
Results:
ReFeed
 is able to synthesize reasonable priorities for a given set of requirements based on properties derived from user-feedback. The implementation of 
ReFeed
 and related resources are publicly available.
Conclusion:
The results from our studies are encouraging in that using only three properties of user-feedback, 
ReFeed
 is able to prioritize requirements with reasonable accuracy. Such automatically determined prioritization could serve as a 
good starting point
 for requirements experts involved in the task of prioritizing requirements Future studies could explore additional user-feedback properties to improve the effectiveness of computed priorities.",21 Mar 2025,9,"The approach for automated user-feedback analysis addresses key issues and can greatly benefit startups by improving requirements prioritization based on user input, leading to more user-centric software development."
https://www.sciencedirect.com/science/article/pii/S0950584921000975,Convergence rate of Artificial Neural Networks for estimation in software development projects,October 2021,Information and Software Technology,"Software effort estimation, Convergence rate, Taguchi Orthogonal Arrays, Artificial Neural Networks design, Fuzzification, Clustering",Dragica=Rankovic: drankovic@raf.rs; Nevena=Rankovic: Not Found; Mirjana=Ivanovic: Not Found; Ljubomir=Lazic: Not Found,"Abstract
Context:
Nowadays, companies are investing in brand new software, given that fact they always need help with 
estimating software
 development, effort, costs, and the period of time needed for completing the software itself. In this paper, four different architectures of 
Artificial Neural Networks
 (ANN), as one of the most desired tools for predicting and estimating effort in software development, were used.
Objective:
This paper aims to determine the 
convergence rate of
 each of the proposed ANNs, when obtaining the minimum 
relative error
, first depending on the cost effect function, then on the nature of the data on which the training, testing, and validation is performed.
Method:
Magnitude 
relative error
 (MRE) is calculated based on Taguchi’s orthogonal plans for each of these four proposed 
ANN architectures
. The 
fuzzification
 method, five different datasets, the 
clustering method
 for input values of each dataset, and prediction were used to achieve the best model for estimation.
Results:
Based on performed parts of the experiment, it can be concluded that the convergence rate of each proposed architecture depends on the cost effect function and the nature of projects in different datasets. By following the prediction throughout all experimental parts, it can be further confirmed that ANN-L36 gave the best results in this proposed approach.
Conclusion:
The main advantages of this model are as follows: the number of iterations is less than 10, shortened effort estimation time thanks to convergence rate, simple architecture of each proposed ANN, large coverage of different values of actual project efficiency, and minimal MMRE. This model can also serve as an idea for the construction of a tool that would be able to reliably, efficiently and accurately estimate the effort when developing various software projects.",21 Mar 2025,7,The research on predicting and estimating effort in software development using Artificial Neural Networks has practical value for European early-stage ventures in improving project efficiency and estimation accuracy.
https://www.sciencedirect.com/science/article/pii/S0950584921001002,Visual Resume: Exploring developers’ online contributions for hiring,October 2021,Information and Software Technology,Not Found,Sandeep Kaur=Kuttal: sandeep-kuttal@utulsa.edu; Xiaofan=Chen: Not Found; Zhendong=Wang: Not Found; Sogol=Balali: Not Found; Anita=Sarma: Not Found,"Abstract
Context:
Recruiters and practitioners are increasingly relying on online activities of developers to find a suitable candidate. Past empirical studies have identified technical and soft skills that managers use in online peer 
production sites
 when making hiring decisions. However, finding candidates with relevant skills is a labor-intensive task for managers, due to the sheer amount of information online peer 
production sites
 contain.
Objective:
We designed a profile aggregation tool—Visual Resume—that aggregates contribution information across two types of peer production sites: a code hosting site (GitHub) and a technical Q&A forum (Stack Overflow). Visual Resume displays summaries of developers’ contributions and allows easy access to their contribution details. It also facilitates 
pairwise comparisons
 of candidates through a card-based design. We present the motivation for such a design and design guidelines for creating such recruitment tool.
Methods:
We performed a scenario-based evaluation to identify how participants use developers’ online contributions in peer production sites as well as how they used Visual Resume when making hiring decisions.
Results:
Our analysis helped in identifying the technical and soft skill cues that were most useful to our participants when making hiring decisions in online production sites. We also identified the information features that participants used and the ways the participants accessed that information to select a candidate.
Conclusions:
Our results suggest that Visual Resume helps in participants evaluate cues for technical and soft skills more efficiently as it presents an aggregated view of candidate’s contributions, allows drill down to details about contributions, and allows easy comparison of candidates via movable cards that could be arranged to match participants’ needs.",21 Mar 2025,8,"The Visual Resume tool for aggregating developer contributions from online peer production sites can significantly impact recruitment processes for startups, making it a valuable tool for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921001129,Code smell detection using feature selection and stacking ensemble: An empirical investigation,October 2021,Information and Software Technology,Not Found,Amal=Alazba: aalazba@ksu.edu.sa; Hamoud=Aljamaan: hjamaan@kfupm.edu.sa,"Abstract
Context:
Code smell detection is the process of identifying code pieces that are poorly designed and implemented. Recently more research has been directed towards machine learning-based approaches for code smells detection. Many classifiers have been explored in the literature, yet, finding an effective model to detect different code smells types has not yet been achieved.
Objective:
The main objective of this paper is to empirically investigate the capabilities of stacking heterogeneous ensemble model in code smell detection.
Methods:
Gain feature selection technique was applied to select relevant features in code smell detection. Detection performance of 14 
individual classifiers
 was investigated in the context of two class-level and four method-level code smells. Then, three stacking ensembles were built using all 
individual classifiers
 as 
base classifiers
, and three different meta-classifiers (LR, SVM and DT).
Results:
GP, MLP, DT and SVM(Lin) classifiers were among the best performing classifiers in detecting most of the code smells. On the other hand, SVM(Sig), NB(B), NB(M), and SGD were among the least accurate classifiers for most smell types. The stacking ensemble with LR and SVM meta-classifiers achieved a consistent high detection performance in class-level and method-level code smells compared to all individual models.
Conclusion:
This paper concludes that the detection performance of the majority of individual classifiers varied from one code smell type to another. However, the detection performance of the stacking ensemble with LR and SVM meta-classifiers was consistently superior over all individual classifiers in detecting different code smell types.",21 Mar 2025,6,"Investigating stacking heterogeneous ensemble models for code smell detection provides insights that can benefit startups in improving their code quality, making it moderately valuable for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000926,Icon2Code: Recommending code implementations for Android GUI components,October 2021,Information and Software Technology,Not Found,Yanjie=Zhao: Not Found; Li=Li: Li.Li@monash.edu; Xiaoyu=Sun: Not Found; Pei=Liu: Not Found; John=Grundy: Not Found,"Abstract
Context:
Event-driven programming plays a crucial role in implementing GUI-based software systems such as 
Android
 apps. However, such event-driven code is inherently challenging to design and implement correctly. Despite a significant amount of research to help developers efficiently implement such software, improved approaches are still needed to assist developers in better handling events and associated callback methods.
Objective:
This work aims at inventing an intelligent recommendation system for helping app developers efficiently and effectively implement 
Android
 GUI components.
Methods:
To achieve the aforementioned objective, we introduce in this work a novel approach called Icon2Code. Given an icon or UI widget provided by designers as input, Icon2Code first searches from a large-scale app database to locate similar icons used in existing popular apps. It then learns from the implementation of these similar apps and leverages a collaborative filtering model to select and recommend the most relevant APIs.
Results:
Our approach can achieve an 81% success rate when only five recommended APIs are considered, and a 94% success rate if twenty results are considered, based on ten-fold cross-validation with a large-scale dataset containing over 45,000 icons and their code implementations.
Conclusion:
It is feasible to automatically recommend code implementations for Android GUI components and Icon2Code is useful and effective in helping achieve such an objective.",21 Mar 2025,9,"The intelligent recommendation system Icon2Code for Android GUI components offers a practical solution that can greatly assist app developers in the early stages, making it highly valuable for European startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000872,TIDY: A PBE-based framework supporting smart transformations for entity consistency in PowerPoint,October 2021,Information and Software Technology,Not Found,Shuguan=Liu: liu_shuguan@126.com; Huiyan=Wang: cocowhy1013@gmail.com; Chang=Xu: changxu@nju.edu.cn,"Abstract
Context:
Programming by Example (PBE) is increasingly assisting human users by recognizing and executing repetitive tasks, such as text editing and spreadsheet manipulation. Yet, existing work falls short on dealing with rich-formatted documents like PowerPoint (PPT) files, when examples are few and collecting them is intrusive.
Objective:
This article presents 
TIDY
, a PBE-based framework, to assist automated entity transformations for their layout and style consistency in rich-formatted documents like PowerPoint, in a way adaptive to entity contexts and flexible with user selections.
Methods:
TIDY
 achieves this by examining entities’ operation histories, and proposes a two-stage framework to first identify user intentions behind histories and then make wise next-operation recommendations for users, in order to maintain the entity consistency for rich-formatted documents.
Results:
We implemented 
TIDY
 as a prototype tool and integrated it into PowerPoint as a plug-in module. We experimentally evaluated 
TIDY
 with real-world user operation data. The evaluation reports that 
TIDY
 achieved promising effectiveness with a hit rate of 77.3% on average, which was stably holding for a variety of editing tasks. Besides, 
TIDY
 took only marginal time overhead, costing several to several tens of milliseconds, to complete each recommendation.
Conclusion:
TIDY
 assists users to complete repetitive tasks in rich-formatted documents by non-intrusive user intention recognition and smart next-operation recommendations, which is effective and practically useful.",21 Mar 2025,8,"The TIDY framework for assisting automated entity transformations in rich-formatted documents like PowerPoint addresses a practical need for user-friendly document editing, making it a valuable tool for European startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000859,Analyzing privacy policies through syntax-driven semantic analysis of information types,October 2021,Information and Software Technology,Not Found,Mitra Bokaei=Hosseini: mbokaeihossein@stmarytx.edu; Travis D.=Breaux: breaux@cs.cmu.edu; Rocky=Slavin: Rocky.Slavin@utsa.edu; Jianwei=Niu: Jianwei.Niu@utsa.edu; Xiaoyin=Wang: xiaoyin.wang@utsa.edu,"Abstract
Context:
Several government laws and app markets, such as Google Play, require the disclosure of app data practices to users. These data practices constitute critical privacy requirements statements, since they underpin the app’s functionality while describing how various personal information types are collected, used, and with whom they are shared.
Objective:
Abstract and ambiguous terminology in requirements statements concerning information types (e.g., “we collect your device information”), can reduce shared understanding among app developers, policy writers, and users.
Method:
To address this challenge, we propose a syntax-driven method that first parses a given information type phrase (e.g. mobile device identifier) into its constituents using a context-free grammar and second infers 
semantic relationships
 between constituents using semantic rules. The inferred 
semantic relationships
 between a given phrase and its constituents generate a hierarchy that models the generality and ambiguity of phrases. Through this method, we infer relations from a lexicon consisting of a set of information type phrases to populate a partial ontology. The resulting ontology is a knowledge graph that can be used to guide requirements authors in the selection of the most appropriate information type terms.
Results:
We evaluate the method’s performance using two criteria: (1) expert assessment of relations between information types; and (2) non-expert preferences for relations between information types. The results suggest 
performance improvement
 when compared to a previously proposed method. We also evaluate the reliability of the method considering the information types extracted from different data practices (e.g., collection, usage, sharing, etc.) in privacy policies for mobile or web-based apps in various app domains.
Contributions:
The method achieves average of 89% precision and 87% recall considering information types from various app domains and data practices. Due to these results, we conclude that the method can be generalized reliably in inferring relations and reducing the ambiguity and abstraction in privacy policies.",21 Mar 2025,8,"The proposed method addresses an important issue of reducing ambiguity in privacy policies, with high precision and recall rates, impacting startups dealing with app data practices."
https://www.sciencedirect.com/science/article/pii/S0950584921000884,Overcoming cultural barriers to being agile in distributed teams,October 2021,Information and Software Technology,Not Found,Darja=Šmite: Darja.Smite@bth.se; Nils Brede=Moe: Nils.B.Moe@bth.se; Javier=Gonzalez-Huerta: Javier.Gonzalez.Huerta@bth.se,"Abstract
Context:
 Agile methods in offshored projects have become increasingly popular. Yet, many companies have found that the use of agile methods in coordination with companies located outside the regions of early agile adopters remains challenging. 
India
 has received particular attention as the leading destination of offshoring contracts due to significant cultural differences between sides of such contracts. Alarming differences are primarily rooted in the hierarchical business culture of Indian organizations and related command-and-control management behavior styles.
Objective:
 In this study, we attempt to understand whether cultural barriers persist in distributed projects in which Indian engineers work with a more empowering Swedish management, and if so, how to overcome them. The present work is an invited extension of a 
conference paper
.
Method:
 We performed a multiple-case study in a mature agile company located in Sweden and a more hierarchical Indian vendor. We 
collected data
 from five group interviews with a total of 34 participants and five workshops with 96 participants in five distributed 
DevOps
 teams, including 36 Indian members, whose preferred behavior in different situations we surveyed.
Results:
 We identified twelve cultural barriers, six of which were classified as impediments to 
agile software development
 practices, and report on the manifestation of these barriers in five 
DevOps
 teams. Finally, we put forward recommendations to overcome the identified barriers and emphasize the importance of 
cultural training
, especially when onboarding new team members.
Conclusions:
 Our findings confirm previously reported behaviors rooted in cultural differences that impede the adoption of agile approaches in offshore collaborations, and identify new barriers not previously reported. In contrast to the existing opinion that cultural characteristics are rigid and unchanging, we found that some barriers present at the beginning of the studied collaboration disappeared over time. Many offshore members reported behaving similarly to their onshore colleagues.",21 Mar 2025,7,"The study identifies cultural barriers in distributed projects involving Indian engineers, providing recommendations to overcome them, which can be valuable for startups collaborating with offshore teams."
https://www.sciencedirect.com/science/article/pii/S0950584921000860,Grey Literature in Software Engineering: A critical review,October 2021,Information and Software Technology,Not Found,Fernando=Kamei: fernando.kenji@ifal.edu.br; Igor=Wiese: Not Found; Crescencio=Lima: Not Found; Ivanilton=Polato: Not Found; Vilmar=Nepomuceno: Not Found; Waldemar=Ferreira: Not Found; Márcio=Ribeiro: Not Found; Carolline=Pena: Not Found; Bruno=Cartaxo: Not Found; Gustavo=Pinto: Not Found; Sérgio=Soares: Not Found,"Abstract
Context:
Grey Literature (GL) recently has grown in 
Software Engineering
 (SE) research since the increased use of online communication channels by software engineers. However, there is still a limited understanding of how SE research is taking advantage of GL.
Objective:
This research aimed to understand how SE researchers use GL in their secondary studies.
Methods:
We conducted a tertiary study of studies published between 2011 and 2018 in high-quality software engineering conferences and journals. We then applied qualitative and 
quantitative analysis
 to investigate 446 potential studies.
Results:
From the 446 selected studies, 126 studies cited GL but only 95 of those used GL to answer a specific research question representing almost 21% of all the 446 secondary studies. Interestingly, we identified that few studies employed specific search mechanisms and used additional criteria for assessing GL. Moreover, by the time we conducted this research, 49% of the GL URLs are not working anymore. Based on our findings, we discuss some challenges in using GL and potential 
mitigation plans
.
Conclusion:
In this paper, we summarized the last 10 years of software engineering research that uses GL, showing that GL has been essential for bringing practical new perspectives that are scarce in traditional literature. By drawing the current landscape of use, we also raise some awareness of related challenges (and strategies to deal with them).",21 Mar 2025,6,"Understanding how software engineering researchers use Grey Literature can offer new perspectives, but the impact on startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000689,Joint feature representation learning and progressive distribution matching for cross-project defect prediction,September 2021,Information and Software Technology,Not Found,Quanyi=Zou: zouquanyi2010@163.com; Lu=Lu: lul@scut.edu.cn; Zhanyu=Yang: yangzhanyu@hotmail.com; Xiaowei=Gu: amyxwgu@163.com; Shaojian=Qiu: qiushaojian@outlook.com,"Abstract
Context:
Cross-Project 
Defect Prediction
 (CPDP) aims to leverage the knowledge from label-rich source software projects to promote tasks in a label-poor target software project. Existing CPDP methods have two major flaws. One is that previous CPDP methods only consider global feature representation and ignores local relationship between instances in the same category from different projects, resulting in ambiguous predictions near the decision boundary. The other one is that CPDP methods based on pseudo-labels assume that the conditional distribution can be well matched at one stroke, when instances of target project are correctly annotated pseudo labels. However, due to the great gap between projects, the pseudo-labels seriously deviate from the real labels.
Objective:
To address above issues, this paper proposed a novel CPDP method named 
Joint
 Feature Representation with Double Marginalized 
Denoising
 
Autoencoders
 (DMDA_JFR).
Method:
Our method mainly includes two parts: joint feature 
representation learning
 and progressive distribution matching. We utilize two novel 
autoencoders
 to jointly learn the global and 
local feature
 representations simultaneously. To achieve progressive distribution matching, we introduce a repetitious pseudo-labels strategy, which makes it possible that distributions are matched after each stack layer learning rather than in one stroke.
Results:
The effectiveness of the proposed method was evaluated through experiments conducted on 10 open-source projects, including 29 software releases from PROMISE repository. Overall, experimental results show that our proposed method outperformed several state-of-the-art baseline CPDP methods.
Conclusions:
It can be concluded that (1) joint deep representations are promising for CPDP compared with only considering global feature representation methods, (2) progressive distribution matching is more effective for adapting probability distributions in CPDP compared with existing CPDP methods based on pseudo-labels.",21 Mar 2025,9,"The proposed CPDP method outperforms state-of-the-art methods in software projects, showing promise in leveraging knowledge from different projects, which can benefit startups in improving defect prediction."
https://www.sciencedirect.com/science/article/pii/S0950584921000823,Leveraging developer information for efficient effort-aware bug prediction,September 2021,Information and Software Technology,Not Found,Yu=Qu: yuq@ucr.edu; Jianlei=Chi: chijianlei@stu.xjtu.edu.cn; Heng=Yin: hengy@ucr.edu,"Abstract
Context:
Software bug prediction techniques can provide informative guidance in 
software engineering
 practices. Over the past 15 years, developer information has been intensively used in bug prediction as features or basic 
data source
 to construct other useful models.
Objective:
Further leverage developer information from a new and straightforward perspective to improve effort-aware bug prediction.
Methods:
We propose to investigate the direct relations between the number of developers and the probability for a file to be buggy. Based on an empirical study on nine open-source Java systems with 32 versions, we observe a widely-existed and interesting tendency: when there are more developers working on a source file, there will be a stronger possibility for this file to be buggy. Based on the observed tendency, we propose an unsupervised algorithm and a supervised equation both called 
top-dev
 to improve effort-aware bug prediction. The key idea is to prioritize the ranking of files, whose number of developers is large, in the suspicious file list generated by effort-aware models.
Results:
Experimental results show that the proposed 
top-dev
 algorithm and equation significantly outperform the unsupervised and supervised 
baseline models
 (ManualUp, 
R
a
d
, 
R
d
d
, 
R
e
e
, CBS+, and 
top-core
). Moreover, the unsupervised 
top-dev
 algorithm is comparable or superior to existing supervised 
baseline models
.
Conclusion:
The proposed approaches are very useful in effort-aware bug prediction practices. Practitioners can use the 
top-dev
 algorithm to generate a high-quality and informative suspicious file list without training complex 
machine learning
 classifiers. On the other hand, when building supervised bug prediction model, the best practice is to combine existing models with the 
top-dev
 equation.",21 Mar 2025,8,"The proposed approach for effort-aware bug prediction using developer information shows significant improvement over baseline models, providing practical guidance for startups in software bug prediction practices."
https://www.sciencedirect.com/science/article/pii/S0950584921000811,"How do developers discuss and support new programming languages in technical Q&A site? An empirical study of Go, Swift, and Rust in Stack Overflow",September 2021,Information and Software Technology,Not Found,Partha=Chakraborty: Not Found; Rifat=Shahriyar: Not Found; Anindya=Iqbal: Not Found; Gias=Uddin: gias.uddin@ucalgary.ca,"Abstract
Context:
New programming languages (e.g., Swift, Go, Rust, etc.) are being introduced to provide a better opportunity for the developers to make software development robust and easy. At the early stage, a programming language is likely to have 
resource constraints
 that encourage the developers to seek help frequently from experienced peers active in Question–Answering (QA) sites such as Stack Overflow (SO).
Objective:
In this study, we have formally studied the discussions on three popular new languages introduced after the inception of SO (2008) and match those with the relevant activities in GitHub whenever appropriate. For that purpose, we have mined 4,17,82,536 questions and answers from SO and 7,846 issue information along with 6,60,965 
repository information
 from Github. Initially, the development of new languages is relatively slow compared to mature languages (e.g., C, C++, Java). The expected outcome of this study is to reveal the difficulties and challenges faced by the developers working with these languages so that appropriate measures can be taken to expedite the generation of relevant resources.
Method:
We have used the 
Latent Dirichlet Allocation
 (LDA) method on SO’s questions and answers to identify different topics of new languages. We have extracted several features of the answer pattern of the new languages from SO (e.g., time to get an accepted answer, time to get an answer, etc.) to study their characteristics. These attributes were used to identify difficult topics. We explored the background of developers who are contributing to these languages. We have created a model by combining Stack Overflow data and issues, repository, user data of Github. Finally, we have used that model to identify factors that affect language evolution.
Results:
The major findings of the study are: (i) migration, data and 
data structure
 are generally the difficult topics of new languages, (ii) the time when adequate resources are expected to be available vary from language to language, (iii) the unanswered question ratio increases regardless of the age of the language, and (iv) there is a relationship between developers’ activity pattern and the growth of a language.
Conclusion:
We believe that the outcome of our study is likely to help the owner/sponsor of these languages to design better features and documentation. It will also help the software developers or students to prepare themselves to work on these languages in an informed way.",21 Mar 2025,8,This study provides valuable insights into the difficulties faced by developers working with new programming languages and offers measures to expedite the generation of relevant resources. It has practical value for startups using these languages.
https://www.sciencedirect.com/science/article/pii/S0950584921000793,From monolithic systems to Microservices: An assessment framework,September 2021,Information and Software Technology,"Microservices, Cloud migration, Software measurement",Florian=Auer: florian.auer@uibk.ac.at; Valentina=Lenarduzzi: valentina.lenarduzzi@lut.fi; Davide=Taibi: davide.taibi@tuni.fi,"Abstract
Context:
Re-architecting 
monolithic systems
 with Microservices-based architecture is a common trend. Various companies are migrating to 
Microservices
 for different reasons. However, making such an important decision like re-architecting an entire system must be based on real facts and not only on gut feelings.
Objective:
The goal of this work is to propose an evidence-based decision support framework for companies that need to migrate to Microservices, based on the analysis of a set of characteristics and metrics they should collect before re-architecting their monolithic system.
Method:
We conducted a survey done in the form of interviews with professionals to derive the assessment framework based on Grounded Theory.
Results:
We identified a set consisting of information and metrics that companies can use to decide whether to migrate to Microservices or not. The proposed assessment framework, based on the aforementioned metrics, could be useful for companies if they need to migrate to Microservices and do not want to run the risk of failing to consider some important information.",21 Mar 2025,9,"The evidence-based decision support framework proposed in this work can be highly beneficial for companies considering migrating to Microservices, aiding in making informed decisions. This has a significant impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S095058492100080X,Source Code Transformations for Improving Security of Time-bounded K-variant Systems,September 2021,Information and Software Technology,Not Found,Berk=Bekiroglu: bbekirog@iit.edu; Bogdan=Korel: Not Found,"Abstract
Context
Source code transformation techniques can improve the security of systems against memory exploitation attacks. As such, the chance of exploitation of 
security vulnerabilities
 can be decreased by using different controlled source code transformation techniques. In K-variant architecture, multiple variants of a program are generated through a controlled source code transformation to improve the security of systems.
Objective
To investigate the effectiveness and practicality of source code program transformations in improving the security of time-bounded K-variant systems for memory exploitation attacks.
Method
The effectiveness of program transformations in improving the security of time-bounded K-variant systems is experimentally investigated for different memory attacks.
Results
The results suggest that generating multiple variants using the presented transformations significantly improves the survivability of time-bounded K-variant systems under memory exploitation attacks.
Conclusion
We conclude that generating multi-variants in time-bounded K-variant systems in accordance with the presented program transformations may improve the security of time-bounded K-variant systems significantly for memory exploitation attacks with a reasonable cost and overhead.",21 Mar 2025,7,The investigation on source code program transformations for improving security against memory exploitation attacks has practical implications for early-stage ventures concerned about system security. The findings can be beneficial for startups.
https://www.sciencedirect.com/science/article/pii/S0950584921000896,Evaluating and comparing memory error vulnerability detectors,September 2021,Information and Software Technology,Not Found,Yu=Nong: yu.nong@wsu.edu; Haipeng=Cai: haipeng.cai@wsu.edu; Pengfei=Ye: pengfei.ye@wsu.edu; Li=Li: Li.Li@monash.edu; Feng=Chen: Feng.Chen@utdallas.edu,"Abstract
Context:
Memory error vulnerabilities have been consequential and several well-known, open-source memory error vulnerability detectors exist, built on static and/or dynamic code analysis. Yet there is a lack of assessment of such detectors based on rigorous, quantitative accuracy and efficiency measures while not being limited to 
specific application domains
.
Objective:
Our study aims to assess and explain the 
strengths
 and weaknesses of state-of-the-art memory error vulnerability detectors based on static and/or dynamic code analysis, so as to inform tool selection by practitioners and future design of better detectors by researchers and tool developers.
Method:
We empirically evaluated and compared five state-of-the-art memory error vulnerability detectors against two benchmark datasets of 520 and 474 C/C++ programs, respectively. We conducted 
case studies
 to gain in-depth explanations of successes and failures of individual tools.
Results:
While generally fast, these detectors had largely varied accuracy across different vulnerability categories and moderate overall accuracy. Complex code (e.g., deep loops and recursions) and data (e.g., deeply embedded linked lists) structures appeared to be common, major barriers. Hybrid analysis did not always outperform purely static or dynamic analysis for memory error 
vulnerability detection
. Yet the evaluation results were noticeably different between the two datasets used. Our 
case studies
 further explained the performance variations among these detectors and enabled additional actionable insights and recommendations for improvements.
Conclusion:
There was no single most effective tool among the five studied. For future research, integrating different techniques is a promising direction, yet simply combining different classes of code analysis (e.g., static and dynamic) may not. For practitioners to choose right tools, making various tradeoffs (e.g., between precision and recall) might be inevitable.",21 Mar 2025,6,"The assessment of memory error vulnerability detectors provides insights for developers and researchers, though the impact on startups may be somewhat limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000835,Phase-wise migration of multiple legacy applications–A graph-theoretic approach,September 2021,Information and Software Technology,Not Found,Rohit=Punnoose: r14012@astra.xlri.ac.in; Supriya Kumar=De: skde@xlri.ac.in,"Abstract
Context
Many organizations undertake large-scale projects of application migration due to availability of scalable and cost-efficient technologies. Such legacy application migration projects are very complex since the process involves in-depth profiling of the applications.
Objective
During the initial profiling phase, it is imperative to understand the underlying complexities of individual applications, as well as the interdependencies among applications in the organization. This analysis phase can take considerable time and effort, depending on number and complexity of the applications. The main goal of this paper is to provide a framework that provides a cost-effective and quick approach to study the interdependencies between legacy applications with minimal prior knowledge of application usage.
Method
In this paper, we propose a framework that uses community 
detection algorithms
 and other established techniques from graph theory, to discover interdependencies of legacy applications within an organization, group these highly interdependent legacy applications in clusters, and finally sequence the clusters for migration to a modern platform. We study the proposed framework through three case studies, using network datasets from a large US organization.
Results
The experimental results from the proposed framework suggests that legacy applications can be grouped into clusters with high interdependencies between each other. Also, the framework shows how organizations can then appropriately sequence the clusters of legacy applications into a phase-wise migration project, thereby reducing migration costs.
Conclusion
The proposed framework provides a valuable design input to organizations on how to determine the interdependencies between the various legacy applications that are in scope for migration to a modern platform. Such large-scale migration projects can be simplified and broken down to use a systematic approach, thereby reducing migration costs and data integrity challenges.",21 Mar 2025,8,The framework proposed for analyzing interdependencies between legacy applications in migration projects offers a cost-effective approach for startups undergoing similar processes. It has practical value for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000720,The do’s and don’ts of infrastructure code: A systematic gray literature review,September 2021,Information and Software Technology,"Infrastructure-as-code, DevOps, Gray literature review",Indika=Kumara: i.p.k.weerasingha.dewage@tue.nl; Martín=Garriga: m.garriga@uvt.nl; Angel Urbano=Romeu: a.urbanoromeu@uvt.nl; Dario=Di Nucci: d.dinucci@uvt.nl; Fabio=Palomba: fpalomba@unisa.it; Damian Andrew=Tamburri: d.a.tamburri@tue.nl; Willem-Jan=van den Heuvel: w.j.a.m.vdnheuvel@uvt.nl,"Abstract
Context:
Infrastructure-as-code (IaC) is the 
DevOps
 tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual 
hardware configuration
 or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the 
relative scarcity
 of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.",21 Mar 2025,8,The abstract provides valuable insights into Infrastructure-as-code (IaC) practices and offers a catalog of best and bad practices for popular IaC languages. This can greatly benefit early-stage ventures looking to improve their development and maintenance processes.
https://www.sciencedirect.com/science/article/pii/S0950584921000847,"Processes, challenges and recommendations of Gray Literature Review: An experience report",September 2021,Information and Software Technology,Not Found,He=Zhang: Not Found; Runfeng=Mao: Not Found; Huang=Huang: Not Found; Qiming=Dai: Not Found; Xin=Zhou: Not Found; Haifeng=Shen: Not Found; Guoping=Rong: ronggp@nju.edu.cn,"Abstract
Context:
Systematic Literature Review
 (SLR), as a tool of Evidence-Based 
Software Engineering
 (EBSE), has been widely used in 
Software Engineering
 (SE). However, for certain topics in SE, especially those that are trendy or 
industry
 driven, academic literature is generally scarce and consequently Gray Literature (GL) becomes a major source of evidence. In recent years, the adoption of Gray Literature Review (GLR) or Multivocal Literature Review (MLR) is rising steadily to provide the state-of-the-practice of a specific topic where SLR is not a viable option.
Objective:
Although some SLR guidelines recommend the use of GL and several MLR guidelines have already been proposed in SE, researchers still have conflicting views on the value of GL and commonly accepted GLR or MLR studies are generally lacking in terms of publication. This experience report aims to shed some light on GLR through a 
case study
 that uses SLR and MLR guidelines to conduct a GLR on an emerging topic in SE to specifically answer the questions related to the reasons of using GL, the processes of conducting GL, and the impacts of GL on review results.
Method:
We retrospect the review process of conducting a GLR on the topic of DevSecOps with reference to Kitchenham’s SLR and Garousi’s MLR guidelines. We specifically reflect on the processes we had to adapt in order to tackle the challenges we faced. We also compare and contrast our GLR with existing MLRs or GLRs in SE to contextualize our reflections.
Results:
We distill ten challenges in nine activities of a GLR process. We provide reasons for these challenges and further suggest ways to tackle them during a GLR process. We also discuss the decision process of selecting a suitable review methodology among SLR, MLR and GLR and elaborate the impacts of GL on our review results.
Conclusion:
Although our experience on GLR is mainly derived from a specific 
case study
 on DevSecOps, we conjecture that it is relevant and would be beneficial to other GLR or MLR studies. We also expect our experience would contribute to future GLR or MLR guidelines, in a way similar to how SLR guidelines learned from the SLR experience report a dozen years ago. In addition, other researchers may find our 
decision making process
 useful before they conduct their own reviews.",21 Mar 2025,6,"The abstract discusses the rising trend of Gray Literature Review (GLR) in Software Engineering, providing insights into the challenges and impacts of using GL. While not directly focused on practical application for startups, the information on GLR can be insightful for ventures in the industry."
https://www.sciencedirect.com/science/article/pii/S0950584921000707,Automated formalization of structured natural language requirements,September 2021,Information and Software Technology,Not Found,Dimitra=Giannakopoulou: dimitra.giannakopoulou@nasa.gov; Thomas=Pressburger: tom.pressburger@nasa.gov; Anastasia=Mavridou: anastasia.mavridou@nasa.gov; Johann=Schumann: johann.m.schumann@nasa.gov,"Abstract
The use of structured natural languages to capture requirements provides a reasonable trade-off between ambiguous natural language and unintuitive 
formal notations
. There are two major challenges in making structured natural language amenable to formal analysis: (1) formalizing requirements as formulas that can be processed by analysis tools and (2) ensuring that the formulas conform to the semantics of the structured natural language. 
fretish
 is a structured natural language that incorporates features from existing research and from NASA applications. Even though 
fretish
 is quite expressive, its underlying semantics is determined by the types of four fields: 
scope
, 
condition
, 
timing
, and 
response
. Each combination of field types defines a template with Real-Time Graphical Interval Logic (RTGIL) semantics. We have developed a framework that constructs temporal logic formulas for each template compositionally, from its fields. The compositional nature of our algorithms facilitates maintenance and extensibility. Our goal is to be inclusive not only in terms of language expressivity, but also in terms of requirements analysis tools that we can interface with. For this reason we generate metric-temporal logic formulas with (1) exclusively future-time operators, over both finite and infinite traces, and (2) exclusively past-time operators. To establish trust in the produced formalizations for each template, our framework: (1) extensively tests the generated formulas against the template semantics and (2) proves equivalence between its past-time and future-time formulas. Our approach is available through the open-source tool 
fret
 and has been used to capture and analyze requirements for a Lockheed Martin Cyber–Physical System challenge.",21 Mar 2025,9,"The abstract presents a structured natural language, fretish, for capturing requirements in software development with a focus on formal analysis. The development of a framework for generating temporal logic formulas can be highly valuable for startups aiming to improve their requirement analysis processes."
https://www.sciencedirect.com/science/article/pii/S095058492100046X,Measuring the cognitive load of software developers: An extended Systematic Mapping Study,August 2021,Information and Software Technology,Not Found,Lucian José=Gonçales: lucianj@edu.unisinos.br; Kleinner=Farias: Not Found; Bruno C.=da Silva: Not Found,"Abstract
Context:
Cognitive load in 
software engineering
 refers to the mental effort users spend while reading software artifacts. The cognitive load can vary according to tasks and across developers. Researchers have measured developers’ cognitive load for different purposes, such as understanding its impact on productivity and software quality. Thus, researchers and practitioners can use cognitive load measures for solving many aspects of 
software engineering
 problems.
Problem:
However, a lack of a classification of dimensions on cognitive load measures in software engineering makes it difficult for researchers and practitioners to obtain research trends to advance 
scientific knowledge
 or apply it in software projects.
Objective:
This article aims to classify different aspects of cognitive load measures in software engineering and identify challenges for further research.
Method:
We conducted a 
Systematic Mapping Study
 (SMS), which started with 4,175 articles gathered from 11 search engines and then narrowed down to 63 primary studies.
Results:
Our main findings are: (1) 43% (27/63) of the primary studies focused on applying a combination of sensors; (2) 81% (51/63) of the selected works were validation studies; (3) 83% (52/63) of the primary studies analyzed cognitive load while developers performed programming tasks. Moreover, we created a 
classification scheme
 based on the answers to our research questions.
Conclusion:
despite the production of a significant amount of studies on cognitive load in software engineering, there are still many challenges to be solved in this particular field for effectively measuring the cognitive load in software engineering. Therefore, this work provided directions for future studies on cognitive load measurement in software engineering.",21 Mar 2025,7,"The abstract addresses the classification of cognitive load measures in software engineering, providing insights into research trends and challenges. While not directly providing tools or solutions for startups, the classification scheme can guide ventures in understanding and mitigating cognitive load issues."
https://www.sciencedirect.com/science/article/pii/S0950584921000690,Automation of systematic literature reviews: A systematic literature review,August 2021,Information and Software Technology,Not Found,Raymon=van Dinter: raymon.vandinter@wur.nl; Bedir=Tekinerdogan: bedir.tekinerdogan@wur.nl; Cagatay=Catal: ccatal@qu.edu.qa,"Abstract
Context
Systematic Literature Review (SLR) studies aim to identify relevant primary papers, extract the required data, analyze, and synthesize results to gain further and broader insight into the investigated domain. Multiple SLR studies have been conducted in several domains, such as 
software engineering
, medicine, and pharmacy. Conducting an SLR is a time-consuming, laborious, and costly effort. As such, several researchers developed different techniques to automate the SLR process. However, a 
systematic overview
 of the current state-of-the-art in SLR automation seems to be lacking.
Objective
This study aims to collect and synthesize the studies that focus on the automation of SLR to pave the way for further research.
Method
A systematic literature review is conducted on published primary studies on the automation of SLR studies, in which 41 primary studies have been analyzed.
Results
This SLR identifies the objectives of automation studies, application domains, automated steps of the SLR, automation techniques, and challenges and solution directions.
Conclusion
According to our study, the leading automated step is the 
Selection of Primary Studies
. Although many studies have provided automation approaches for systematic literature reviews, no study has been found to apply automation techniques in the planning and reporting phase. Further research is needed to support the automation of the other activities of the SLR process.",21 Mar 2025,5,"The abstract focuses on the automation of Systematic Literature Review (SLR) studies, which can streamline the research process for academics and professionals. While automation techniques are valuable, the direct impact on early-stage ventures or startups might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000550,BCI-CFI: A context-sensitive control-flow integrity method based on branch correlation integrity,August 2021,Information and Software Technology,Not Found,Ye=Wang: daguoli415@163.com; Qingbao=Li: Not Found; Zhifeng=Chen: Not Found; Ping=Zhang: Not Found; Guimin=Zhang: Not Found; Zhihui=Shi: Not Found,"Abstract
Context
As part of the arms race, one emerging attack methodology has been control-hijacking attacks, e.g., return-oriented programming (ROP). Control-flow integrity (CFI) is a generic and effective defense against most control-hijacking attacks. However, existing CFI mechanisms have poor security as demonstrated by their equivalence class (EC) sizes, which are sets of targets that CFI policies cannot distinguish. Adversaries can choose an illegitimate control transfer within an EC that is included in the resulting 
CFG
 and incorrectly allowed by CFI protection policies.
Objective
The paper introduces a context-sensitive control-flow integrity method, which aims to improve the security of CFI and prevent ROP attacks.
Method
The paper presents BCI-CFI, a context-sensitive CFI technique based on branch correlation integrity (BCI), which can effectively break down EC sizes and improve the security of CFI. BCI-CFI takes the branch correlation relationship (i.e., a new type of context for CFI) as contextual information to refine the CFI policy and identify the BCI pairs in the target program via 
static analysis
. Furthermore, the paper introduces a state machine M
CFI
 for BCI-CFI to conduct target validation for the indirect control-flow transfer (ICT) instructions in the target program at runtime.
Results
Our results show that, (i) BCI-CFI prevented adversaries from manipulating the control data and launching ROP attacks, (ii) protected both forward and backward ICT in the target program, and improved the security and effectiveness of CFI, and (iii) BCI-CFI introduced a 19.67% runtime overhead on average and a maximum runtime overhead of 31.2%
Conclusion
BCI-CFI is a context-sensitive CFI technique aiming to prevent adversaries from manipulating the control data of the target program to launch ROP attacks. BCI-CFI can reduce EC sizes and improve the security of CFI while incurring a moderate runtime overhead on average.",21 Mar 2025,8,"The proposed context-sensitive CFI technique has the potential to significantly improve the security of early-stage ventures by preventing ROP attacks. The runtime overhead is moderate, making it a practical solution for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000586,BGNN4VD: Constructing Bidirectional Graph Neural-Network for Vulnerability Detection,August 2021,Information and Software Technology,Not Found,Sicong=Cao: Not Found; Xiaobing=Sun: xbsun@yzu.edu.cn; Lili=Bo: Not Found; Ying=Wei: Not Found; Bin=Li: lb@yzu.edu.cn,"Abstract
Context:
Previous studies have shown that existing deep learning-based approaches can significantly improve the performance of 
vulnerability detection
. They represent code in various forms and mine vulnerability features with 
deep learning models
. However, the differences of code representation forms and 
deep learning
 models make various approaches still have some limitations. In practice, their false-positive rate (FPR) and false-negative rate (FNR) are still high.
Objective:
To address the limitations of existing deep learning-based 
vulnerability detection
 approaches, we propose 
BGNN4VD
 (Bidirectional 
Graph Neural Network
 for Vulnerability Detection), a vulnerability detection approach by constructing a Bidirectional Graph Neural-Network (BGNN).
Method:
In Phase 1, we extract the syntax and semantic information of source code through 
abstract syntax tree
 (AST), 
control flow graph
 (CFG), and 
data flow graph
 (DFG). Then in Phase 2, we use vectorized source code as input to Bidirectional Graph Neural-Network (BGNN). In Phase 3, we learn the different features between vulnerable code and non-vulnerable code by introducing backward edges on the basis of traditional Graph Neural-Network (GNN). Finally in Phase 4, a Convolutional Neural-Network (CNN) is used to further extract features and detect vulnerabilities through a classifier.
Results:
We evaluate 
BGNN4VD
 on four popular C/C++ projects from NVD and GitHub, and compare it with four state-of-the-art (
Flawfinder
, 
RATS
, 
SySeVR
, and 
VUDDY
) vulnerab ility detection approaches. Experiment results show that, when compared these baselines, 
BGNN4VD
 achieves 4.9%, 11.0%, and 8.4% improvement in F1-measure, accuracy and precision, respectively.
Conclusion:
The proposed 
BGNN4VD
 achieves a higher precision and accuracy than the state-of-the-art methods. In addition, when applied on the latest vulnerabilities reported by CVE, 
BGNN4VD
 can still achieve a precision at 45.1%, which demonstrates the feasibility of 
BGNN4VD
 in practical application.",21 Mar 2025,9,"The BGNN4VD approach for vulnerability detection shows significant improvement in F1-measure, accuracy, and precision compared to state-of-the-art methods. This can have a high practical value for startups in enhancing the security of their software products."
https://www.sciencedirect.com/science/article/pii/S0950584921000562,An empirical study on clone consistency prediction based on machine learning,August 2021,Information and Software Technology,Not Found,Fanlong=Zhang: izhangfanlong@gmail.com; Siau-cheng=Khoo: khoosc@nus.edu.sg,"Abstract
Context:
Code Clones have been accepted as a common phenomenon in software, thanks to the increasing demand for rapid production of software. The existence of code clones is recognized by developers in the form of 
clone group
, which includes several pieces of clone fragments that are similar to one another. A change in one of these clone fragments may indicate necessary 
“consistent changes”
 are required for the rest of the clones within the same group, which can increase extra maintenance costs. A failure in making such consistent change when it is necessary is commonly known as a “clone consistency-defect”, which can adversely impact software 
maintainability
.
Objective:
Predicting the need for “clone consistent changes” after successful clone-creating or clone-changing operations can help developers maintain clone changes effectively, avoid consistency-defects and reduce maintenance cost.
Method:
In this work, we use several sets of attributes in two scenarios of clone operations (
clone-creating
 and 
clone-changing
), and conduct an empirical study on five different machine-learning methods to assess each of their clone consistency predictability — whether any one of the clone operations will 
require
 or be 
free of
 clone consistency maintenance in future.
Results:
We perform our experiments on 
eight
 open-source projects. Our study shows that such predictions can be reasonably effective both for clone-creating and changing operating instances. We also investigate the use of 
five
 different machine-learning methods for predictions and show that our selected features are effective in predicting the needs of consistency-maintenance across all selected machine-learning methods.
Conclusion:
The empirical study conducted here demonstrates that the models developed by different machine-learning methods with the specified sets of attributes have the ability to perform clone-consistency prediction.",21 Mar 2025,6,"Predicting clone consistency changes can help reduce maintenance costs, but the impact on European early-stage ventures may not be as direct. The study provides valuable insights but may not have an immediate practical application for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000537,From a Scrum Reference Ontology to the Integration of Applications for Data-Driven Software Development,August 2021,Information and Software Technology,Not Found,Paulo Sérgio=Santos Júnior: paulo.junior@ifes.edu.br; Monalessa Perini=Barcellos: monalessa@inf.ufes.edu.br; Ricardo de Almeida=Falbo: falbo@inf.ufes.br; João Paulo A.=Almeida: jpalmeida@ieee.org,"Abstract
Context
Organizations often use different applications to support the Scrum process, including project management tools, source repository and quality assessment tools. These applications store useful data for decision-making. However, data items often remain spread in different applications, each of which adopt different data and behavioral models, posing a barrier for integrated data usage. As a consequence, data-driven decisions in agile development are uncommon, missing valuable opportunities for informed decision making.
Objective
Considering the need to address semantic issues to properly integrate applications that support the agile development process, we aim to provide a common and comprehensive conceptualization about Scrum in the software development context and apply this conceptualization to support application integration.
Method
We have developed the Scrum Reference Ontology (SRO) and used it to semantically integrate Azure DevOps and Clockify.
Results
SRO served as a reference model to build software artifacts in a semantic integration architecture that enables applications to automatically share, exchange and combine data and services. The integrated solution was used in the software development unit of a Brazilian government agency. Results demonstrate that the integrated solution contributed to improving estimates, provided data that helped allocate teams, manage team productivity and project performance, and enabled to identify and fix problems in the Scrum process execution.
Conclusions
SRO can serve as an interlingua for application integration in the context of Scrum-process support. By capturing the conceptualization underlying Scrum, the reference ontology can address semantic conflicts and thereby support the development of integrated data-driven solutions for decision making.",21 Mar 2025,7,"The Scrum Reference Ontology (SRO) could streamline data integration in agile development processes, offering valuable insights for decision-making. While promising, the practical impact on early-stage ventures may require further validation and implementation."
https://www.sciencedirect.com/science/article/pii/S0950584921000665,Exploring the communication functions of comments during bug fixing in Open Source Software projects,August 2021,Information and Software Technology,Not Found,Sandra L.=Ramírez-Mora: sandra.ramirez@ciencias.unam.mx; Hanna=Oktaba: Not Found; Helena=Gómez-Adorno: Not Found; Gerardo=Sierra: Not Found,"Abstract
Context:
Bug fixing is a frequent and important task in 
Open Source Software
 (OSS) development and involves the communication of messages, which can serve for multiple purposes and affect the efficiency and effectiveness of corrective software activities.
Objective:
This work is aimed at studying the communication functions of bug comments and their associations with fast and complete bug fixing in 
OSS
 development.
Method:
Over 500K comments and 89K bugs of 100 
OSS projects
 were extracted from three Issue Tracking Systems. Six thousand comments were manually tagged to create a corpus of communication functions. The extracted comments were automatically tagged using 
machine learning algorithms
 and the corpus of communication functions. Statistical and correlation analyses were performed and the most frequent comments communicated during fast and successful bug fixing were identified.
Results:
Significant differences in the distribution of comments of fixed and not fixed bugs were found. Variations in the distribution of comments of bugs with different fixing time were also found. Referential comments that provided objective information were found to be the most frequent messages. Results showed that the percentages of conative and emotive comments are greater when bugs are resolved without the requested fixes and when fixes are implemented in a long time.
Conclusion:
Associations between communication functions and bug fixing exist. The results of this work could be used to improve corrective tasks in 
OSS
 development and some other specific linguistic aspects should be studied in detail in OSS communities.",21 Mar 2025,5,"Studying bug comments' communication functions can improve bug fixing efficiency, but the direct impact on early-stage ventures may be limited. The results provide insights for OSS development but may not have immediate practical applications for startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000677,Insights on the relationship between decision-making style and personality in software engineering,August 2021,Information and Software Technology,Not Found,Fabiana=Mendes: fabiana.mendes@oulu.fi; Emília=Mendes: emilia.mendes@bth.se; Norsaremah=Salleh: norsaremah@iium.edu.my; Markku=Oivo: markku.oivo@oulu.fi,"Abstract
Context:
Software development involves many activities, and 
decision making
 is an essential one. Various factors can impact a decision-making process, and by understanding such factors, one can improve the process. Since people are the ones making decisions, some human-related aspects are amongst those influencing factors. One such aspect is the decision maker’s personality.
Objective:
This research investigates the relationship between decision-making style and personality within the context of software project development.
Method:
We conducted a survey in a population of Brazilian software engineers to gather data on their personality and decision-making style.
Results:
Data from 63 participants was gathered and resulted in the identification of seven statistically significant correlations between decision-making style and personality (personality factor and personality facets). Furthermore, we built a regression model in which decision-making style (DMS) was the response variable and personality factors the independent variables. The 
backward elimination
 procedure selected only agreeableness to explain 4.2% of DMS variation. The model accuracy was evaluated and deemed good enough. Regarding the moderation effect of demographic variables (age, educational level, experience, and role) on the relationship between DMS and Agreeableness, the analysis showed that only software engineers’ role has such effect.
Conclusion:
This paper contributes toward understanding the relationship between DMS and personality. Results show that the personality variable agreeableness can explain the variation in decision-making style. Furthermore, someone’s role in a software development project can impact the 
strength
 of the relationship between DMS and agreeableness.",21 Mar 2025,7,"This research contributes to understanding the relationship between decision-making style and personality within software project development, which can have practical implications for improving team dynamics and project outcomes."
https://www.sciencedirect.com/science/article/pii/S0950584921000719,A methodology to automatically translate user requirements into visualizations: Experimental validation,August 2021,Information and Software Technology,"Data visualization, Big data analytics, Model-driven development, Requirements engineering, Experimental validation",Ana=Lavalle: alavalle@dlsi.ua.es; Alejandro=Maté: Not Found; Juan=Trujillo: Not Found; Miguel A.=Teruel: Not Found; Stefano=Rizzi: Not Found,"Abstract
Context:
Information visualization
 is paramount for the analysis of Big Data. The volume of data requiring interpretation is continuously growing. However, users are usually not experts in information visualization. Thus, defining the visualization that best suits a determined context is a very challenging task for them. Moreover, it is often the case that users do not have a clear idea of what objectives they are building the visualizations for. Consequently, it is possible that graphics are misinterpreted, making wrong decisions that lead to missed opportunities. One of the underlying problems in this process is the lack of methodologies and tools that non-expert users in visualizations can use to define their objectives and visualizations.
Objective:
The main objectives of this paper are to (i) enable non-expert users in data visualization to communicate their analytical needs with little effort, (ii) generate the visualizations that best fit their requirements, and (iii) evaluate the impact of our proposal with reference to a 
case study
, describing an experiment with 97 non-expert users in data visualization.
Methods:
We propose a methodology that collects user requirements and semi-automatically creates suitable visualizations. Our proposal covers the whole process, from the definition of requirements to the implementation of visualizations. The methodology has been tested with several groups to measure its effectiveness and perceived usefulness.
Results:
The experiments increase our confidence about the utility of our methodology. It significantly improves over the case when users face the same problem manually. Specifically: (i) users are allowed to cover more analytical questions, (ii) the visualizations produced are more effective, and (iii) the overall satisfaction of the users is larger.
Conclusion:
By following our proposal, non-expert users will be able to more effectively express their analytical needs and obtain the set of visualizations that best suits their goals.",21 Mar 2025,9,"The proposal in this paper addresses a critical issue of enabling non-expert users in data visualization to effectively communicate their needs and obtain suitable visualizations, which can significantly impact decision-making processes and overall business outcomes."
https://www.sciencedirect.com/science/article/pii/S0950584921000549,"Motivations, benefits, and issues for adopting Micro-Frontends: A Multivocal Literature Review",August 2021,Information and Software Technology,"Micro-Frontends, Microservices, Web front-end development, Software architectures, Multivocal Literature Review",Severi=Peltonen: severi.peltonen@gmail.com; Luca=Mezzalira: luca.mezzalira@dazn.com; Davide=Taibi: davide.taibi@tuni.fi,"Abstract
Context:
Micro-Frontends are increasing in popularity, being adopted by several large companies, such as DAZN, Ikea, Starbucks and may others. Micro-Frontends enable splitting of monolithic frontends into independent and smaller micro applications. However, many companies are still hesitant to adopt Micro-Frontends, due to the lack of knowledge concerning their benefits. Additionally, provided online documentation is often times perplexed and contradictory.
Objective:
The goal of this work is to map the existing knowledge on Micro-Frontends, by understanding the motivations of companies when adopting such applications as well as possible benefits and issues.
Method:
For this purpose, we surveyed the academic and grey literature by means of the Multivocal Literature Review process, analysing 173 sources, of which 43 reported motivations, benefits and issues.
Results:
The results show that existing architectural options to build web applications are cumbersome if the application and development team grows, and if multiple teams need to develop the same frontend application. In such cases, companies adopted Micro-Frontends to increase team independence and to reduce the overall complexity of the frontend. The application of the Micro-Frontend, confirmed the expected benefits, and Micro-Frontends resulted to provide the same benefits as 
microservices
 on the back end side, combining the development team into a fully cross-functional development team that can scale processes when needed. However, Micro-Frontends also showed some issues, such as the increased payload size of the application, increased code duplication and coupling between teams, and monitoring complexity.
Conclusions:
Micro-Frontends allow companies to scale development according to business needs in the same way microservices do with the back end side. In addition, Micro-Frontends have a lot of overhead and require careful planning if an advantage is achieved by using Micro-Frontends. Further research is needed to carefully investigate this new hype, by helping practitioners to understand how to use Micro-Frontends as well as understand in which contexts they are the most beneficial.",21 Mar 2025,8,"Mapping the existing knowledge on Micro-Frontends and understanding the motivations, benefits, and issues associated with their adoption contributes valuable insights for companies looking to improve their development processes, potentially benefiting European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000574,RLTCP: A reinforcement learning approach to prioritizing automated user interface tests,August 2021,Information and Software Technology,Not Found,Vu=Nguyen: nvu@fit.hcmus.edu.vn; Bach=Le: ldbach@apcs.vn,"Abstract
Context:
User interface testing validates the correctness of an application through visual cues and interactive events emitted in real-world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for full execution.
Objective:
This paper describes a novel test prioritization method called RLTCP whose goal is to maximize the number of test faults detected while reducing the amount of test.
Methods:
We define a weighted coverage graph to model the underlying association among test cases for the user interface testing. Our method combines 
Reinforcement Learning
 (RL) and the coverage graph to prioritize test cases. While RL is found to be suitable for rapidly changing projects with abundant 
historical data
, the coverage graph considers in-depth the event-based aspects of user interface testing and provides a fine-grained level at which the 
RL system
 can gain more insights into individual test cases.
Results:
We experiment and assess the proposed method using nine data sets obtained from two mature web applications, finding that the method outperforms the six, including the state-of-the-art, methods.
Conclusions:
The use of both reinforcement learning and the underlying structure of user interface tests modeled via the coverage has the potential to improve the performance of test prioritization methods. Our study also shows the benefit of using the coverage graph to gain insights into test cases, their relationship and execution history.",21 Mar 2025,8,"The novel test prioritization method proposed in this paper using Reinforcement Learning and coverage graph can enhance the efficiency of user interface testing, which is crucial for software development, especially for startups with limited resources."
https://www.sciencedirect.com/science/article/pii/S0950584921000458,Continuous Systems and Software Engineering for Industry 4.0: A disruptive view,July 2021,Information and Software Technology,Not Found,Elisa Yumi=Nakagawa: elisa@icmc.usp.br; Pablo Oliveira=Antonino: pablo.antonino@iese.fraunhofer.de; Frank=Schnicke: Frank.Schnicke@iese.fraunhofer.de; Peter=Liggesmeyer: peter.liggesmeyer@iese.fraunhofer.de,"Abstract
Context:
Industry 4.0
 has substantially changed the manufacturing processes, leading to smart factories with full 
digitalization
, intelligence, and dynamic production. The need for rigorous and continuous development of highly networked software-intensive 
Industry 4.0
 systems entails great challenges. Hence, Industry 4.0 requires new ways to develop, operate, and evolve these systems accordingly.
Objective:
We introduce the view of 
Continuous Systems
 and 
Software Engineering
 for Industry 4.0 (CSSE I4.0).
Method:
Based on our research and industrial projects, we propose this novel view and its core elements, including continuous twinning, which is also introduced first in this paper. We also discuss the existing industrial engagement and research that could leverage this view for practical application.
Results:
There are still several open issues, so we highlight the most urgent perspectives for future work.
Conclusions:
A disruptive view on how to engineer Industry 4.0 systems must be established to pave the way for the realization of the fourth industrial revolution.",21 Mar 2025,6,"The introduction of Continuous Systems and Software Engineering for Industry 4.0 presents an innovative view that could lead to advancements in manufacturing processes, although the practical impact on early-stage ventures may be more indirect."
https://www.sciencedirect.com/science/article/pii/S0950584921000501,Test case generation for agent-based models: A systematic literature review,July 2021,Information and Software Technology,Not Found,Andrew G.=Clark: agclark2@sheffield.ac.uk; Neil=Walkinshaw: n.walkinshaw@sheffield.ac.uk; Robert M.=Hierons: r.hierons@sheffield.ac.uk,"Abstract
Context:
Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a 
software fault
 may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential.
Objective:
Our objective is to summarise the state-of-the-art techniques for test case generation in agent-based models and identify future research directions.
Method:
We have conducted a systematic literature review in which we pose five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test?
Results:
Out of the 464 
initial search results
, we identified 24 primary publications. Based on these primary publications, we formed a taxonomy to summarise the state-of-the-art techniques for test case generation in agent-based models. Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Furthermore, the majority of techniques cannot test non-functional requirements or “soft goals”.
Conclusions:
This paper reports insights into the key developments and open challenges concerning test case generation in agent-based models that may be of interest to both researchers and practitioners. In particular, we identify the need for test case generation techniques that focus on societal and non-functional behaviour, and a more thorough evaluation using realistic 
case studies
 that feature challenging properties associated with a typical agent-based model.",21 Mar 2025,7,"The research on test case generation for agent-based models is important for ensuring critical decisions are made accurately, especially at societal levels. The identified gaps in current techniques and the call for more thorough evaluations contribute to practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000379,On the generalizability of Neural Program Models with respect to semantic-preserving program transformations,July 2021,Information and Software Technology,Not Found,Md Rafiqul Islam=Rabin: mrabin@uh.edu; Nghi D.Q.=Bui: Not Found; Ke=Wang: Not Found; Yijun=Yu: Not Found; Lingxiao=Jiang: Not Found; Mohammad Amin=Alipour: Not Found,"Abstract
Context:
With the prevalence of publicly available 
source code
 repositories to train 
deep neural network
 models, neural program models can do well in 
source code analysis
 tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen 
source code
 is largely unknown.
Objective:
Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the 
generalizability
 of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures.
Method:
We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art 
neural network
 models for code, namely 
code2vec
, 
code2seq
, and 
GGNN
, to build nine such neural program models for evaluation.
Results:
Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and 
control dependencies
 in programs generalize better than neural program models based only on 
abstract syntax trees
 (ASTs). On the positive side, we observe that as the size of the training dataset grows and diversifies the 
generalizability
 of correct predictions produced by the neural program models can be improved too.
Conclusion:
Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement.",21 Mar 2025,5,The evaluation of generalizability of neural program models is relevant for startups using deep neural networks in source code analysis. The results provide insights for improvement but may have limited immediate practical impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000422,iMER: Iterative process of entity relationship and business process model extraction from the requirements,July 2021,Information and Software Technology,Not Found,Muhammad=Javed: m.javed@uon.edu.au; Yuqing=Lin: yuqing.lin@newcastle.edu.au,"Abstract
Context
Extracting conceptual models, e.g., 
entity relationship model
 or Business Process model, from software requirement document is an essential task in the 
software development life cycle
. Business process model presents a clear picture of required system's functionality. Operations in business process model together with the data entity consumed, help the software developers to understand the database design and operations to be implemented. Researchers have been aiming at automatic extraction of these artefacts from the requirement document.
Objective
In this paper, we present an automated approach to extract the entity relationship and business process models from requirements, which are possibly in different formats such as general requirements, use case specification and user stories. Our approach is based on the efficient 
natural language processing
 techniques.
Method
It is an iterative approach of Models Extraction from the Requirements (iMER). iMER has multiple iterations where each iteration is to address a sub-problem. In the first iteration, iMER extracts the data entities and attributes. Second iteration is to find the relationships between data entities, while extracting 
cardinalities
 is in the third step. Business process model is generated in the fourth iteration, containing the external (actors’) and internal (system's) operations.
Evaluation
To evaluate the performance and accuracy of iMER, experiments are conducted on various formats of the requirement documents. Additionally, we have also evaluated our approaches using the requirement documents which been modified by shuffling the sentences and by merging with other requirements. Comparative study is also performed. The preliminary results show a noticeable improvement.
Conclusion
The iMER is an efficient automated iterative approach that is able to extract the conceptual models from the various formats of requirements.",21 Mar 2025,8,The automated extraction of conceptual models from requirement documents using an iterative approach is highly valuable for startups in software development. The promising results from experiments and comparative studies contribute to practical significance for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000410,Generating feasible protocol test sequences from EFSM models using Monte Carlo tree search,July 2021,Information and Software Technology,Not Found,Ting=Shu: shuting@zstu.edu.cn; Yechao=Huang: 1766254653@qq.com; Zuohua=Ding: zouhuading@hotmail.com; Jinsong=Xia: js_xia@126.com; Mingyue=Jiang: jiang_my@126.com,"Abstract
Context:
Feasible test sequences generation is a key step in protocol conformance testing based on the Extended 
Finite State Machine
 (EFSM) model. To guarantee the feasibility of generated test sequences, transition executability analysis (TEA) technique is widely applied in automatic test derivation. However, the TEA method often suffers from the famous state explosion problem, which has become a major obstacle to its efficient application.
Objective:
In order to mitigate this issue, this paper proposed a novel heuristic TEA method (MTEA) that uses Monte Carlo tree search (MCTS) to guide the TEA tree expansion for efficiently deriving feasible test sequences.
Method:
The approach first provides a framework to apply the MCTS algorithm based on multiple decision subtrees, in the context of test sequence generation for EFSM-specified systems, to more efficiently expanding the TEA tree with huge state space, and thus alleviating the problem of state explosion. To achieve this, we then design a reward function to calculate the fitness of nodes currently being expanded in the TEA tree and heuristically direct the search towards a near-optimal solution. Next, an adaptive reduction mechanism of search budget is also introduced to accelerate the convergence of the analysis. Finally, a MTEA-based algorithm for automatically generating feasible test sequences is presented under a specific transition coverage criterion.
Results:
A detailed 
case study
 on 6 popular EFSMs was carried out to evaluate the effectiveness and efficiency of our method. Experimental results show that the MTEA significantly outperforms Breadth-First-Search based TEA method (BTEA) and the standard MCTS-based method (SMCTS), regarding time and space performance. Compared with the BTEA, SMCTS and random TEA method (RTEA), the success rate of test generation of MTEA (98.14% on average) is approximately 2, 1.85 and 3 times higher, respectively. For successful test derivation, MTEA only needs to explore on average 9.95% of the nodes and consume on average 61.68% of the runtime of the BTEA method.
Conclusion:
The experiments illustrate the promise of our approach for alleviating the state explosion problem in test generation for EFSM-specified systems.",21 Mar 2025,6,"The proposed heuristic TEA method for test sequence generation addresses a key challenge in protocol conformance testing. The significant improvement in success rate and exploration efficiency provides practical value, but the application may be more specialized for certain ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000495,Augmenting commit classification by using fine-grained source code changes and a pre-trained deep neural language model,July 2021,Information and Software Technology,Not Found,Lobna=Ghadhab: lobnaghadhab@gmail.com; Ilyes=Jenhani: ijenhani@pmu.edu.sa; Mohamed Wiem=Mkaouer: mwmvse@rit.edu; Montassar=Ben Messaoud: montassar.benmassaoud@isgs.u-sousse.tn,"Abstract
Context:
Analyzing software maintenance activities is very helpful in ensuring cost-effective evolution and development activities. The categorization of commits into maintenance tasks supports practitioners in making decisions about 
resource allocation
 and managing technical debt.
Objective:
In this paper, we propose to use a pre-trained language neural model, namely BERT (Bidirectional Encoder Representations from Transformers) for the classification of commits into three categories of maintenance tasks — corrective, perfective and adaptive. The proposed commit 
classification approach
 will help the classifier better understand the context of each word in the commit message.
Methods:
We built a balanced dataset of 1793 labeled commits that we collected from publicly available datasets. We used several popular code change distillers to extract fine-grained code changes that we have incorporated into our dataset as additional features to BERT’s word representation features. In our study, a 
deep neural network
 (DNN) classifier has been used as an additional layer to fine-tune the BERT model on the task of commit classification. Several models have been evaluated to come up with a deep analysis of the impact of code changes on the classification performance of each commit category.
Results and conclusions:
Experimental results have shown that the 
DNN model
 trained on BERT’s word representations and Fixminer code changes (DNN@BERT+Fix_cc) provided the best performance and achieved 79.66% accuracy and a macro-average f1 score of 0.8. Comparison with the state-of-the-art model that combines keywords and code changes (RF@KW+CD_cc) has shown that our model achieved approximately 8% improvement in accuracy. Results have also shown that a 
DNN model
 using only BERT’s word representation features achieved an improvement of 5% in accuracy compared to the RF@KW+CD_cc model.",21 Mar 2025,9,The use of pre-trained language neural models for commit classification in software maintenance tasks is highly relevant for startups dealing with code changes. The improved classification performance and comparison with state-of-the-art models offer significant practical value for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584921000525,MEGDroid: A model-driven event generation framework for dynamic android malware analysis,July 2021,Information and Software Technology,Not Found,Hayyan=Hasan: Not Found; Behrouz=Tork Ladani: Ladani@eng.ui.ac.ir; Bahman=Zamani: Not Found,"Abstract
Context
The tremendous growth of 
Android malware
 in recent years is a strong motivation for the vast endeavor in detection and analysis of 
malware
 apps. A prominent approach for this purpose is dynamic analysis in which providing complex interactions with the samples under analysis is a need. Event generation tools are almost used to provide such interactions, but they have deficiencies for effective 
malware analysis
. For example, anti-static and anti-dynamic analysis techniques employed by the 
malware
 prevent event generators to extract sufficient information for generating appropriate events. As a result, they fail to trigger 
malicious payloads
 or obtain high code coverage in most cases.
Objective
In this paper, we aim to present a new framework to improve the event generation process for dynamic analysis of 
Android malware
.
Method
We propose MEGDroid, a 
Model Driven Engineering
 (MDE) framework in which malware-related information is automatically extracted and represented as a domain-specific model. This model, then is used to generate appropriate events for 
malware analysis
 using model-to-model and model-to-code transformations. The proposed model-driven artifacts also provide required facilities to put the 
human in the loop
 for properly taking his/her knowledge into account.
Results
The proposed framework has been realized as an Eclipse plugin and we performed extensive practical analysis on a set of 
malware samples
 selected from the AMD dataset. The experimental results showed that MEGDroid considerably increases the number of triggered 
malicious payloads
 as well as the execution code coverage compared with Monkey and DroidBot, as two state of the art general-purpose and malware specific event generators respectively.
Conclusion
The proposed MDE approach, enhances the event generation process through both automatic event generation and analyzer user involvement who can efficiently direct the process to increase the effectiveness of the generated events considering small amount of information that is extractable from the malware code.",21 Mar 2025,7,"The proposed framework MEGDroid aims to improve event generation for Android malware analysis, showing promising results compared to existing tools. This can have a significant impact on cybersecurity for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000483,Multi-objective software performance optimisation at the architecture level using randomised search rules,July 2021,Information and Software Technology,Not Found,Youcong=Ni: youcongni@foxmail.com; Xin=Du: xindu79@126.com; Peng=Ye: whuyp@126.com; Leandro L.=Minku: Not Found; Mark=Harman: Not Found; Ruliang=Xiao: Not Found,"Abstract
Architecture-based software 
performance optimisation
 can help to find potential performance problems and mitigate their negative effects at an early stage. To automate this optimisation process, rule-based and metaheuristic-based 
performance optimisation
 methods have been proposed. However, existing rule-based methods explore a limited 
search space
, potentially excluding optimal or near-optimal solutions. Most of current metaheuristic-based methods ignore existing practical knowledge of 
performance improvement
, and lead to solutions that are not easily explicable to humans. To address these problems, we propose a novel approach for performance optimisation at the software architecture level named Multiobjective performance Optimisation based on 
Randomised search
 rulEs (MORE). First, we 
design randomised
 search rules (MORE-R) to provide explanation without parameters while benefiting from existing practical knowledge of 
performance improvement
. Second, based on all possible 
composite applications
 of MORE-R, an explicable multi-objective 
optimisation problem
 (MORE-P) is defined to enlarge 
search space
 and enable solutions explicable to architectural stakeholder. Third, a multi-objective evolutionary algorithm (MORE-EA) with an introduced do-nothing rule, innovative encoding and repair mechanism is designed to effectively solve MORE-P. The experiments show that MORE is able to achieve more explicable and higher quality solutions than two state-of-the-art techniques. They also demonstrate the benefits of integrating search-based 
software engineering
 approaches with practical knowledge.",21 Mar 2025,8,"The MORE approach for software performance optimization addresses existing limitations in rule-based and metaheuristic-based methods, providing more explicable and higher quality solutions. This can benefit startups by optimizing software performance effectively."
https://www.sciencedirect.com/science/article/pii/S0950584921000471,A practical algorithm for learning disjunctive abstraction heuristics in static program analysis,July 2021,Information and Software Technology,Not Found,Donghoon=Jeon: donghoon_jeon@korea.ac.kr; Minseok=Jeon: minseok_jeon@korea.ac.kr; Hakjoo=Oh: hakjoo_oh@korea.ac.kr,"Abstract
Context:
The precision and cost of 
static analysis
 are determined by abstraction heuristics (e.g., strategies for abstracting calling contexts, heap locations, etc.), but manually designing effective abstraction heuristics requires a huge amount of engineering effort and domain knowledge. Recently, data-driven 
static analysis
 has emerged to address this challenge by learning such heuristics automatically from a set of training programs.
Objective:
We present a practical algorithm for learning disjunctive abstraction heuristics in data-driven static analysis. We build on a recently proposed approach that can learn nontrivial program properties by disjunctive 
boolean functions
. However, the existing approach is practically limited as it assumes that the most precise abstraction is cheap for the training programs; the algorithm is inapplicable if the most precise abstraction is not scalable. The objective of this paper is to mitigate this limitation.
Method:
Our algorithm overcomes the limitation with two new ideas. It systematically decomposes the learning problem into feasible 
subproblems
, and it can search through the abstraction space from the coarse- to fine-grained abstractions. With this approach, our algorithm is able to learn heuristics when static analysis with the most precise abstraction is not scalable over the training programs.
Results:
We show our approach is effective and generally applicable. We applied our approach to a context-sensitive points-to analysis for Java and a flow-sensitive interval analysis for C. Experimental results show that our algorithm is efficient. For example, our algorithm can learn heuristics for 3-object-sensitive analysis for which the existing learning algorithm is too expensive to learn any useful heuristics.
Conclusion:
Our algorithm makes a state-of-the-art technique for data-driven static analysis more practical.",21 Mar 2025,6,"The algorithm for learning disjunctive abstraction heuristics in data-driven static analysis improves on existing approaches, making it more effective and generally applicable. This could provide valuable insights for European startups dealing with static analysis."
https://www.sciencedirect.com/science/article/pii/S0950584921000513,Analyzing the sensitivity of multi-objective software architecture refactoring to configuration characteristics,July 2021,Information and Software Technology,"Search-based software engineering, Automated refactoring, Software quality, Multi-objective optimization, Genetic algorithms, Software performance engineering, Software performance antipatterns",Vittorio=Cortellessa: vittorio.cortellessa@univaq.it; Daniele=Di Pompeo: daniele.dipompeo@univaq.it,"Abstract
Context:
Software architecture refactoring can be induced by multiple reasons, such as satisfying new functional requirements or improving non-functional properties. Multi-objective optimization approaches have been widely used in the last few years to introduce automation in the 
refactoring process
, and they have revealed their potential especially when quantifiable attributes are targeted. However, the effectiveness of such approaches can be heavily affected by configuration characteristics of the 
optimization algorithm
, such as the composition of solutions.
Objective:
In this paper, we analyze the behavior of 
E
A
S
I
E
R
, which is an Evolutionary Approach for Software archItecturE Refactoring, while varying its configuration characteristics, with the objective of studying its potential to find near-optimal solutions under different configurations.
Method:
In particular, we use two different 
solution space
 inspection algorithms (i.e., 
N
S
G
A
−
I
I
 and 
S
P
E
A
2
) while varying the genome length and the solution composition.
Results:
We have conducted our experiments on a specific 
case study
 modeled in 
Æ
Æmilia
 ADL, on which we have shown the ability of 
E
A
S
I
E
R
 to identify performance-critical elements in the software architecture where refactoring is worth to be applied. Beside this, from the comparison of multi-objective algorithms, 
N
S
G
A
−
I
I
 has revealed to outperform 
S
P
E
A
2
 in most of cases, although the latter one is able to induce more diversity in the proposed solutions.
Conclusion:
Our results show that the 
E
A
S
I
E
R
 thoroughly automated process for software architecture refactoring allows to identify configuration contexts of the 
evolutionary algorithm
 in which multi-objective optimization more effectively finds near-optimal Pareto solutions.",21 Mar 2025,8,"The study on EA-SIER for software architecture refactoring demonstrates its potential to find near-optimal solutions under different configurations, benefiting startups by automating the refactoring process and improving software architecture effectively."
https://www.sciencedirect.com/science/article/pii/S0950584921000434,A field experiment on trialsourcing and the effect of contract types on outsourced software development,June 2021,Information and Software Technology,Not Found,Magne=Jørgensen: magnej@simula.no; Jon=Grov: jon.grov@langs.no,"ABSTRACT
Context
To ensure the success of software projects, it is essential to select skilled developers and to use suitable work contracts.
Objective
This study tests two hypotheses: (i) the use of work-sample testing (trialsourcing) improves the selection of skilled software developers; and (ii) the use of contracts based on hourly payment leads to better software project outcomes than fixed-price contracts.
Method
Fifty-seven software freelancers with relevant experience and good evaluation scores from previous clients were invited to complete a two-hour long trialsourcing task to qualify for a software development project. Thirty-six developers completed the trialsourcing task with acceptable performance, and, based on a 
stratified random
 allocation process, were asked to give a proposal based on an hourly payment or a fixed-price contract. Eight hourly payment-based and eight fixed-priced proposals were accepted. The process and product characteristics of the completion of these 16 projects were collected and analysed.
Results and Conclusion
While the use of trialsourcing may have prevented the selection of developers with insufficient skills, the performance on the trialsourcing task of the selected developers did, to a large extent, not predict their performance on the projects. The use of hourly payments seems to have led to lower costs than fixed-price contracts, but not to improved processes or products. We plan to follow up these results with research on how to design more skill-predictive trialsourcing tasks, and when and why different project contexts give different contract consequences.",21 Mar 2025,5,"The study on trialsourcing and payment contracts in software projects provides insights on developer selection and contract types, but the results show mixed outcomes. While relevant, the practical impact for startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000276,A decentralized blockchain oriented framework for automated bug assignment,June 2021,Information and Software Technology,Not Found,Chetna=Gupta: chetna.gupta@ubi.pt; Mário M=Freire: mario@di.ubi.pt,"ABSTRACT
Context
In large software projects bug fixing is a time-bound, time-consuming, mind-numbing, and challenging task that requires a collaborative effort with multiple developers, separated geographically.
Objective
The objective of this paper is to propose a decentralized automated bug assignment approach to improve the quality of bug assignments to minimize backlogs and overall bug fixing time.
Method
To the best of our knowledge, the literature lacks in studies focusing on how to increase software developer's motivation for efficient bug resolution. It is a novel decentralized 
blockchain
 oriented, transparent auction-based bug assignment framework which uses incentive mechanism as reward and penalty backed by 
blockchain
 technology using 
smart contracts
 for developers motivation. The process allows individual developers to select 
bug reports
, matching their preferences and schedule for which they shall we able to provide 
robust solutions
 with reduced overhead in cost and time of bug fixing.
Results
Results of experimentation and surveys conclude that the proposed method is transparent and effective in bug assignment minimizing overall bug fixing time. The low cost of contract execution demonstrates that it can be used quantitatively and without ambiguity.
Conclusion
The work presented is novel to improve (i) bug assignment (ii) allow individual developers to choose what they like to provide 
robust solutions
 (iii) handles two major issues of differentiating between active and inactive developers and confusion over the assignment of bugs (iv) will further reduce bug-fixing delays and will prevent reassignment problem.",21 Mar 2025,9,The proposed decentralized automated bug assignment approach using blockchain technology is innovative and has the potential to significantly improve bug fixing time and overall efficiency in software development.
https://www.sciencedirect.com/science/article/pii/S0950584921000240,Stakeholder engagement in enterprise architecture practice: What inhibitors are there?,June 2021,Information and Software Technology,Not Found,Sherah=Kurnia: sherahk@unimelb.edu.au; Svyatoslav=Kotusev: kotusev@kotusev.com; Graeme=Shanks: gshanks@unimelb.edu.au; Rod=Dilnutt: rpd@unimelb.edu.au; Simon=Milton: simon.milton@unimelb.edu.au,"Abstract
Context
Enterprise
 architecture (EA) is a collection of artifacts describing various aspects of an organization from an integrated business and IT perspective. EA practice is an organizational activity that implies using EA artifacts for facilitating decision-making and improving business and IT alignment. EA practice involves numerous participants ranging from C-level executives to project teams and effective engagement between these stakeholders and architects is critically important for success. Moreover, many practical problems with EA practice can be also attributed to insufficient engagement between architects and other EA stakeholders. However, the notion of engagement received only limited attention in the EA literature and the problem of establishing engagement has not been intentionally studied.
Objective
This paper intends to explore in detail the problem of achieving effective engagement between architects and other EA stakeholders in an organization, identify the main inhibitors of engagement and present a theoretical model explaining the problem of establishing engagement in practice.
Method
This paper is based on a single in-depth revelatory 
case study
 including nine interviews with different participants of EA practice (e.g. architects and other EA stakeholders) and documentation analysis. It leverages the 
grounded theory method
 to construct a conceptual model explaining the problem of engagement in the studied organization.
Results
This paper identifies 28 direct and indirect inhibitors of engagement and unifies them into a holistic conceptual model addressing the problem of achieving engagement that covers the factors undermining both strategic and initiative-based engagement between architects and other EA stakeholders.
Conclusions
This paper focuses on the notion of engagement and offers arguably the first available theoretical model that explains how typical engagement problems between architects and other stakeholders inhibit the realization of value from EA practice. However, the developed model has a number of limitations and we call for further empirical research on engagement problems in EA practice and coping strategies for addressing these problems.",21 Mar 2025,7,"The exploration of achieving effective engagement between architects and EA stakeholders is valuable, but the practical application and impact on early-stage ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000252,Game industry problems: An extensive analysis of the gray literature,June 2021,Information and Software Technology,Not Found,Cristiano=Politowski: c_polito@encs.concordia.ca; Fabio=Petrillo: fabio@petrillo.com; Gabriel C.=Ullmann: gabriel.cavalheiro@sou.unijui.edu.br; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@concordia.ca,"Abstract
Context:
Given its competitiveness, the video-game 
industry
 has a closed-source culture. Hence, little is known about the problems faced by game developers. However, game developers do 
share information
 about their game projects through postmortems, which describe informally what happened during the projects.
Objective:
The software-engineering research community and game developers would benefit from a state of the problems of the 
video game
 
industry
, in particular the problems faced by game developers, their evolution in time, and their root causes. This state of the practice would allow researchers and practitioners to work towards solving these problems.
Method:
We analyzed 200 postmortems from 1997 to 2019, resulting in 927 problems divided into 20 types. Through our analysis, we described the overall landscape of game industry problems in the past 23 years and how these problems evolved over the years. We also give details on the most common problems, their root causes, and possible solutions. We finally discuss suggestions for future projects.
Results:
We observe that (1) the game industry suffers from management and production problems in the same proportion; (2) management problems decreased over the years, giving space to 
business problems
, while production problems remained constant; (3a) technical and game design problems are decreasing over the years, the latter only after the last decade; (3b) problems related to the team increase over the last decade; (3c) marketing problems are the ones that had the biggest increase over the 23 years compared to other problem types; (4) finally, the majority of the main root causes are related to people, not technologies. Conclusions: In this paper, we provide a state of the practice for researchers to understand and study video-game development problems. We also offer suggestions to help practitioners to avoid the most common problems in future projects.",21 Mar 2025,8,"The state of the practice analysis of video game development problems provides valuable insights for researchers and practitioners in the industry, contributing to potential improvements in game development processes."
https://www.sciencedirect.com/science/article/pii/S095058492100029X,Efilter: An effective fault localization based on information entropy with unlabelled test cases,June 2021,Information and Software Technology,Not Found,Yan=Xiaobo: yxbbuaa@buaa.edu.cn; Liu=Bin: liubin@buaa.edu.cn; Wang=Shihai: wangshihai@buaa.edu.cn; An=Dong: ad14011099@buaa.edu.cn; Zhu=Feng: fenix_zh@126.com; Yang=Yelin: ylyang05@163.com,"Abstract
Context:
Automatic 
fault localization
 is essential to intelligent software system. Most 
fault localization
 techniques assume the test oracle is perfect before debugging, which is hard to exist in practice. In fact, the test suite would contain a number of unlabelled test cases which have been proved to be useful in fault localization. However, due to the execution diversity, not all unlabelled test cases are suitable for fault localization. Selecting inappropriate unlabelled test cases can even weaken the fault localization efficiency.
Objective:
To solve the problem of filtering unlabelled test cases, this work aims to construct a feasible framework to select suitable unlabelled test cases for better fault localization.
Method:
To address this issue, an entropy-based framework Efilter is constructed to filter unlabelled test cases. In Efilter, a Statement-based entropy and Testsuite-based entropy are constructed to measure the localization uncertainty of given test suite. The unlabelled test case with less Statement-based entropy or Testsuite-based entropy compared with its threshold would be selected. Further, the feature integration strategies for both Statement-based entropy and Testsuite-based entropy are given to calculate the 
suspiciousness
 of statements.
Results:
The Efilter efficiency is evaluated across 6 open-source programs and 3 spectrum-based fault localizations. The results reveal that Efilter can improve fault localization efficiency by 18.8% and 16.5% with the Statement-based entropy and the Testsuite-based entropy respectively compared with the strategy without Efilter from the perspective of 
EXAM
 score on average.
Conclusion:
Our results indicate that the Efilter with both the Statement-based entropy and the Testsuite-based entropy can improve the fault localization in the scenario lack of test oracles, serving as an enhancement for fault localization in practice.",21 Mar 2025,6,"The framework for filtering unlabelled test cases to improve fault localization efficiency is useful, but the direct impact on European early-stage ventures may not be as substantial compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584921000446,A process for analysing the energy efficiency of software,June 2021,Information and Software Technology,Not Found,Javier=Mancebo: Javier.Mancebo@uclm.es; Félix=García: Felix.Garcia@uclm.es; Coral=Calero: Coral.Calero@uclm.es,"Abstract
Context
It is essential to be aware of the energy efficiency of software when it is running, so that it can be improved; to that end, energy consumption measurements need to be carried out. To ensure that these measurements are as reliable as possible, it is recommended that a well-defined process be followed.
Objective
To identify how the process for analysing the energy efficiency of software should be carried out (including the definition of the software to be evaluated, the selection of measuring instruments, the analysis and the presentation of results, etc.), in an endeavour to improve the reliability and consistency of the information obtained regarding energy efficiency.
Method
An analysis of related work was carried out, to extract some good practices in measuring energy consumption; based on our experience, a process to analyse the energy efficiency of the software has been defined.
Results
We have defined a process to analyse the energy efficiency of the software. We describe this process through a set of phases that covers all the steps needed to carry out a correct analysis of the energy consumption of the software executed. Moreover, this process was validated with two different studies using different measurement instruments (one with a hardware-based approach and one with a software-based approach) to ensure its applicability to all types of studies with software energy consumption measurement.
Conclusion
The steps to be followed to analyse the energy efficiency of the software need to be established. A new process has hence been defined to improve the reliability and consistency of the measurements. Furthermore, this process facilitates the replicability and comparison of the studies carried out.",21 Mar 2025,7,"The defined process for analyzing the energy efficiency of software is important for improving reliability in energy consumption measurements, but the direct applicability to early-stage ventures might be limited."
https://www.sciencedirect.com/science/article/pii/S0950584921000367,Controlled experimentation in continuous experimentation: Knowledge and challenges,June 2021,Information and Software Technology,"Continuous experimentation, Online controlled experiments, A/B testing, Systematic literature review",Florian=Auer: florian.auer@uibk.ac.at; Rasmus=Ros: rasmus.ros@cs.lth.se; Lukas=Kaltenbrunner: lukas.kaltenbrunner@uibk.ac.at; Per=Runeson: per.runeson@cs.lth.se,"Abstract
Context:
Continuous experimentation and A/B testing is an established industry practice that has been researched for more than 10 years. Our aim is to synthesize the conducted research.
Objective:
We wanted to find the core constituents of a framework for continuous experimentation and the solutions that are applied within the field. Finally, we were interested in the challenges and benefits reported of continuous experimentation.
Methods:
We applied forward snowballing on a known set of papers and identified a total of 128 relevant papers. Based on this set of papers we performed two qualitative narrative syntheses and a thematic synthesis to answer the research questions.
Results:
The framework constituents for continuous experimentation include experimentation processes as well as supportive technical and organizational infrastructure. The solutions found in the literature were synthesized to nine themes, e.g. experiment design, automated experiments, or metric specification. Concerning the challenges of continuous experimentation, the analysis identified cultural, organizational, business, technical, statistical, ethical, and domain-specific challenges. Further, the study concludes that the benefits of experimentation are mostly implicit in the studies.
Conclusion:
The research on continuous experimentation has yielded a large body of knowledge on experimentation. The synthesis of published research presented within include recommended infrastructure and experimentation process models, guidelines to mitigate the identified challenges, and what problems the various published solutions solve.",21 Mar 2025,8,"The research on continuous experimentation provides valuable insights into experimentation processes, challenges, and benefits, offering recommended infrastructure and guidelines for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000409,Challenges and recommendations to publishing and using credible evidence in software engineering,June 2021,Information and Software Technology,"Evidence-based software engineering, EBSE, Credible evidence, Validity, Relevance",Claes=Wohlin: claes.wohlin@bth.se; Austen=Rainer: Not Found,"Abstract
Context:
An evidence-based 
scientific discipline
 should produce, consume and disseminate credible evidence. Unfortunately, mistakes are sometimes made, resulting in the production, consumption and dissemination of invalid or otherwise questionable evidence. In the worst cases, such questionable evidence achieves the status of accepted knowledge. There is, therefore, the need to ensure that producers and consumers seek to identify and rectify such situations.
Objectives:
To raise awareness of the 
negative impact
 of misinterpreting evidence and of propagating that misinterpreted evidence, and to provide guidance on how to improve on the type of issues identified.
Methods:
We use a case-based approach to present and analyse the production, consumption and dissemination of evidence. The cases are based on the literature and our professional experience. These cases illustrate a range of challenges confronting evidence-based researchers as well as the consequences to research when invalid evidence is not corrected in a timely way.
Results:
We use the cases and the challenges to formulate a framework and a set of recommendations to help the community in producing and consuming credible evidence.
Conclusions:
We encourage the community to collectively remain alert to the emergence and dissemination of invalid, or otherwise questionable, evidence, and to proactively seek to identify and rectify it.",21 Mar 2025,5,"While highlighting the negative impact of misinterpreting evidence is important, the practical guidance provided may have limited direct impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000288,Self-Attention Networks for Code Search,June 2021,Information and Software Technology,Not Found,Sen=Fang: Not Found; You-Shuai=Tan: Not Found; Tao=Zhang: tazhang@must.edu.mo; Yepang=Liu: liuyp1@sustech.edu.cn,"Abstract
Context:
Developers tend to search and reuse code snippets from a large-scale codebase when they want to implement some functions that exist in the previous projects, which can enhance the efficiency of software development.
Objective:
As the first deep learning-based code search model, DeepCS outperforms prior models such as Sourcere and CodeHow. However, it utilizes two separate 
LSTM
 to represent code snippets and natural language descriptions respectively, which ignores 
semantic relations
 between code snippets and their descriptions. Consequently, the performance of DeepCS falls into the bottleneck, and thus our objective is to break this bottleneck.
Method:
We propose a self-attention 
joint
 
representation learning
 model, named SAN-CS (
S
elf-
A
ttention 
N
etwork for 
C
ode 
S
earch). Comparing with DeepCS, we directly utilize the self-attention network to construct our code search model. By a weighted average operation, self-attention networks can fully capture the contextual information of code snippets and their descriptions. We first utilize two individual self-attention networks to represent code snippets and their descriptions, respectively, and then we utilize the self-attention network to conduct an extra 
joint
 representation network for code snippets and their descriptions, which can build 
semantic relationships
 between code snippets and their descriptions. Therefore, SAN-CS can break the 
performance bottleneck
 of DeepCS.
Results:
We evaluate SAN-CS on the dataset shared by 
Gu et al.
 and choose two 
baseline models
, DeepCS and CARLCS-CNN. Experimental results demonstrate that SAN-CS achieves significantly better performance than DeepCS and CARLCS-CNN. In addition, SAN-CS has better execution efficiency than DeepCS at the training and testing phase.
Conclusion:
This paper proposes a code search model, SAN-CS. It utilizes the self-attention network to perform the joint attention representations for code snippets and their descriptions, respectively. Experimental results verify the effectiveness and efficiency of SAN-CS.",21 Mar 2025,10,"The proposal of SAN-CS as a code search model that outperforms prior models, with improved performance and efficiency, can directly benefit European early-stage ventures in enhancing software development."
https://www.sciencedirect.com/science/article/pii/S0950584921000264,On the practitioners’ understanding of coupling smells — A grey literature based Grounded-Theory study,June 2021,Information and Software Technology,"Grey literature, Grounded theory, Design smells, Code smells, Coupling smells, Software design quality, Code quality",Apitchaka=Singjai: apitchaka.singjai@univie.ac.at; Georg=Simhandl: georg.simhandl@univie.ac.at; Uwe=Zdun: uwe.zdun@univie.ac.at,"Abstract
Context:
Code and design smells, such as the coupling smells examined in this article, are widely studied. Existing empirical studies reveal gaps between the scientific theory and practice, not yet explained by the scientific literature. Only basic coupling smell detection approaches and metrics seem to have been transferred to practice so far.
Objective:
This article aims to study the current practitioner’s understanding of coupling smells.
Method:
Based on grey literature sources containing practitioner views on coupling smells, we performed a Grounded Theory (GT) study. We used UML-based modeling to precisely encode our findings and performed a 
rigorous analysis
 of our codes and models.
Results:
Our results are defining factors of coupling smells, as well as smell impacts, trade-offs, relationships to other smells, relationships to practices and patterns, and fix options as perceived by practitioners. We further identified gaps in the understanding of coupling smells between science and practice, and derived opportunities and challenges for future scientific work.
Conclusions:
Five lessons are presented as opportunities and challenges for future research. Our results can help scientists to get a better understanding of practitioner concerns, and practitioners to get an overview of the current perception of other practitioners on coupling smells.",21 Mar 2025,7,"The study on current practitioner understanding of coupling smells offers insights into gaps between theory and practice, providing opportunities and challenges for future research that can potentially benefit European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584921000033,"Case Study Research in Software Engineering—It is a Case, and it is a Study, but is it a Case Study?",May 2021,Information and Software Technology,"Case study, Empirical, Misuse, Software engineering",Claes=Wohlin: claes.wohlin@bth.se,"Abstract
Background:
Case studies
 are regularly published in the 
software engineering
 literature, and guidelines for conducting case studies are available. Based on a perception that the label “case study” is assigned to studies that are not case studies, an investigation has been conducted.
Objective:
The aim was to investigate whether or not the label “case study” is correctly used in software engineering research.
Method:
To address the objective, 100 recent articles found through Scopus when searching for case studies in software engineering have been investigated and classified.
Results:
Unfortunately, the perception of misuse of the label “case study” is correct. Close to 50% of the articles investigated were judged as not being case studies according to the definition of a case study.
Conclusions:
We either need to ensure correct use of the label “case study”, or we need another label for its definition. Given that “case study” is a well-established label, it is probably impossible to change the label. Thus, we introduce an alternative definition of case study emphasising its real-life context, and urge researchers to carefully follow the definition of different research methods when presenting their research.",21 Mar 2025,4,"While the investigation into the correct use of the label 'case study' is relevant in academic circles, it may have limited impact on European early-stage ventures in terms of practical application."
https://www.sciencedirect.com/science/article/pii/S0950584920302123,Mobile app privacy in software engineering research: A systematic mapping study,May 2021,Information and Software Technology,Not Found,Fahimeh=Ebrahimi: Not Found; Miroslav=Tushev: Not Found; Anas=Mahmoud: mahmoud@csc.lsu.edu,"Abstract
Context:
 Mobile applications (apps) have become deeply personal, constantly demanding access to privacy-sensitive information in exchange for more personalized 
user experiences
. Such privacy-invading practices have generated major multidimensional privacy concerns among app users.
Objective:
 The research on mobile app privacy has experienced rapid growth over the past decade. This line of research is aimed at systematically exposing the privacy practices of apps and proposing solutions to protect the privacy of mobile app users. In this paper, we conduct a 
systematic mapping study
 of this body of research. Our objectives are to 
a)
 explore trends in 
SE
 app privacy research, 
b)
 categorize existing evidence, and 
c)
 identify potential directions for future research.
Method:
 A 
systematic mapping study
 of 59 Software Engineering (SE) primary studies on mobile app privacy. Our scope is studies published in software engineering venues between 2008 and 2018.
Results:
 Our results show that existing literature can be divided into four main categories: privacy policy, requirements, user perspective, and leak detection. Furthermore, our survey reveals an imbalance between these categories—the majority of existing research focuses on proposing tools for detecting privacy leaks, with fewer studies targeting privacy requirements and policy and even fewer on user perspective.
Conclusions:
 Our survey exposes several gaps in existing research and suggests areas for improvement.",21 Mar 2025,6,"The research on mobile app privacy is essential for startups developing apps, but the impact on European early-stage ventures may be limited by the focus on research gaps rather than practical implementation."
https://www.sciencedirect.com/science/article/pii/S0950584921000021,Spectrum-based multi-fault localization using Chaotic Genetic Algorithm,May 2021,Information and Software Technology,Not Found,Debolina=Ghosh: debolina442@gmail.com; Jagannath=Singh: jagannath.singhfcs@kiit.ac.in,"Abstract
Context:
In the field of 
software engineering
, the most complex and time consuming activity is fault-finding. Due to increasing size and complexity of software, there is a necessity of automated fault detection tool which can detect fault with minimal human intervention. A programmer spends a lot of time and effort on 
software fault
 localization. Various Spectrum Based Fault Localization (SBFL) techniques have already been developed to automate the 
fault localization
 in single-fault software. But, there is a scarcity of 
fault localization
 technique for multi-fault software. In our study, we have found that pure SBFL is not always sufficient for effective fault localization in multi-fault programs.
Objective:
To address the above challenge, we propose an automated framework using Chaos-based 
Genetic Algorithm
 for Multi-fault Localization (CGAML) based on SBFL technique.
Methods:
Traditional 
Genetic Algorithm
 (GA) sometimes stuck in local optima, and it takes more time to converge. Different chaos 
mapping functions
 have been applied to GA for better performance. We have used logistic mapping function to achieve 
chaotic sequence
. The proposed technique CGAML first calculates the 
suspiciousness
 score for each program statement and then assigns ranks according to that score. The statements having smaller rank means there is a high probability of the statements to be faulty.
Results:
Five open-source 
benchmark programs
 are tested to evaluate the efficiency of CGAML technique. The experimental results show CGAML gives better results for both single-fault and multi-fault programs in comparison with existing spectrum-based fault localization techniques.
Conclusion:
E
X
A
M
 metric is used to compare the performance of our proposed technique with other existing techniques. Smaller 
E
X
A
M
 score denotes the higher accuracy of the technique. The proposed framework generates smaller 
E
X
A
M
 score in comparison with other existing techniques. We found that, overall CGAML works on an average 8.5% better than GA for both single-fault and multi-fault software.",21 Mar 2025,9,"The proposed automated framework using Chaos-based Genetic Algorithm for fault localization has a high practical value for startups in software development, improving the efficiency of fault detection in multi-fault programs."
https://www.sciencedirect.com/science/article/pii/S095058492100001X,Bridging the state-of-the-art and the state-of-the-practice of SaaS pricing: A multivocal literature review,May 2021,Information and Software Technology,"Software-as-a-Service, SaaS, Pricing, Software Economics, Software Product Management, Multi-vocal Literature Review",Andrey=Saltan: andrey.saltan@lut.fi; Kari=Smolander: Not Found,"Abstract
Context
Pricing
 is an essential element of software 
business strategy
 and tactics. Informed pricing decision-making requires the involvement of different stakeholders and comprehensive data analysis. Achieving both appears to be challenging, and pricing remains one of the most under-managed processes in the software business. Simultaneously, a coherent 
SaaS
 pricing body of knowledge and verified solutions to assist SaaS providers while designing and implementing pricing are missing.
Objective
There is a lack of integration among different research areas focused on SaaS pricing and, more importantly, between academia and 
industry
. The primary aim of this paper is to clarify this misconception by classifying, thematically analyzing, and putting in correspondent academic state-of-the-art and industrial state-of-the-practice of SaaS pricing.
Method
A multivocal literature review (MLR) approach was used for the study, exploring both “white” literature as well as “grey” literature. The body of literature of 387 
bibliography
 items was collected using a formal protocol. Of these, 57 were white literature items, and 330 were grey. A multistage content analysis process was implemented to classify the rich literature body across multiple dimensions with further mapping, synthesis, and reporting.
Results
A taxonomy of pricing-related concepts was created. It classifies SaaS pricing aspects, affecting factors, and challenges facing SaaS providers. The findings and interpretations are summarized to emphasize the major research themes and practical challenges of SaaS pricing practices’ transformation and provide further research guidelines in this area.
Conclusion
SaaS pricing is a maturing and prominent area of research that requires further investigation. The conducted MLR formed a clear picture of SaaS pricing research and practice and identified different SaaS pricing aspects and affecting factors. The study will enable both scholars and practitioners to assess the current state-of-the-art in research and practice.",21 Mar 2025,7,"The study on SaaS pricing provides valuable insights for startups in the software business, but the focus on classification and analysis may limit direct practical implementation for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302457,Rigorous code review by reverse engineering,May 2021,Information and Software Technology,Not Found,Shaoying=Liu: sliu@hiroshima-u.ac.jp; Honghui=Li: Not Found; Zhouxian=Jiang: Not Found; Xiuru=Li: Not Found; Feng=Liu: Not Found; Yan=Zhong: Not Found,"Abstract
Context:
Agile 
software development methods
 advocate the importance of producing working software without comprehensive documentation. While this approach seems to suit the evolutionary nature of realistic software development for many applications, even including safety-critical systems, it faces two major challenges. One is the lack of a comprehensible specification for code evolution and future maintenance, and the other is the potentially huge cost in code verification.
Objective:
To address this problem, we believe that supporting the efficient production of system specification by reversing the program constructed as the result of an 
agile development
 will be a useful solution. The reverse engineering of specifications from programs will not only help us produce the necessary specification for future program evolution, but more importantly can help us rigorously review the program to detect bugs for the enhancement of program quality.
Method:
In this paper, we put forward a novel method for rigorously reviewing code by reversing it into a comprehensible, formal specification. We elaborate on the principle of translating code into a specification and discuss how the translation process helps detect bugs in programs. We demonstrate how the proposed method works in practice with examples. We also present an experiment to evaluate the performance of the method by comparing it with existing checklist-based inspection.
Conclusions:
How to utilize reverse engineering of formal specifications from programs as a means to review the program for bug detection is an almost unexplored topic in 
software engineering
. In this paper, we have described a specific method called RCRRE to reverse engineering of SOFL formal specifications from code and discussed how the reverse 
engineering process
 can be taken as an effective means to review the program for bug detection. The principle of converting code to a SOFL specification is reflected by a set of translation patterns and a two-step approach to construct a SOFL specification is established. To evaluate the performance, we have carried out an experiment on the effectiveness of our RCRRE method by comparing it with the CBI approach. The result of the experiment indicates that using our RCRRE method can effectively help the reviewer scrutinize the code and therefore find more bugs than the CBI when the reviewer is rather familiar with the SOFL specification language and skills. In the meanwhile, it also shows that the effectiveness of our RCRRE method may be affected in the situation where the reviewer lacks sufficient understanding and experience of SOFL, and using our RCRRE method may in general take a little longer time than the CBI.",21 Mar 2025,8,"The method for reverse engineering formal specifications from programs for bug detection provides a practical solution for startups in software development, improving code review processes for enhanced program quality."
https://www.sciencedirect.com/science/article/pii/S0950584920302147,A graph-based clustering algorithm for software systems modularization,May 2021,Information and Software Technology,Not Found,Babak=Pourasghar: Not Found; Habib=Izadkhah: izadkhah@tabrizu.ac.ir; Ayaz=Isazadeh: Not Found; Shahriar=Lotfi: Not Found,"Abstract
Context:
Clustering algorithms
, as a modularization technique, are used to modularize a program aiming to understand large software systems as well as software refactoring. These algorithms partition the 
source code
 of the software system into smaller and easy-to-manage modules (clusters). The resulting decomposition is called the software system structure (or software architecture). Due to the NP-hardness of the modularization problem, evolutionary 
clustering approaches
 such as the 
genetic algorithm
 have been used to solve this problem. These methods do not make much use of the information and knowledge available in the artifact 
dependency graph
 which is extracted from the 
source code
.
Objective:
To overcome the limitations of the existing modularization techniques, this paper presents a new modularization technique named GMA (Graph-based Modularization Algorithm).
Methods:
In this paper, a new graph-based clustering algorithm is presented for software modularization. To this end, the depth of relationships is used to compute the similarity between artifacts, as well as seven new criteria are proposed to evaluate the quality of a modularization. The similarity presented in this paper enables the algorithm to use graph-theoretic information.
Results:
To demonstrate the applicability of the proposed algorithm, ten folders of Mozilla Firefox with different domains and functions, along with four other applications, are selected. The experimental results demonstrate that the proposed algorithm produces modularization closer to the human expert’s decomposition (i.e., directory structure) than the other existing algorithms.
Conclusion:
The proposed algorithm is expected to help a software designer in the software reverse 
engineering process
 to extract easy-to-manage and understandable modules from 
source code
.",21 Mar 2025,8,"The new graph-based modularization algorithm presented in the paper offers practical benefits for startups in software development, aiding in modularizing source code for better software architecture."
https://www.sciencedirect.com/science/article/pii/S0950584920302111,Understanding Hypotheses Engineering in Software Startups through a Gray Literature Review,May 2021,Information and Software Technology,Not Found,Jorge=Melegati: jmelegatigoncalves@unibz.it; Eduardo=Guerra: eduardo.guerra@unibz.it; Xiaofeng=Wang: xiaofeng.wang@unibz.it,"Abstract
Context
The 
higher availability
 of software usage data and the influence of the Lean Startup led to the rise of experimentation in 
software engineering
, a new approach for development based on experiments to understand the user needs. In the models proposed to guide this approach, the first step is generally to identify, prioritize, and specify the hypotheses that will be tested through experimentation. However, although practitioners have proposed several techniques to handle hypotheses, the scientific literature is still scarce.
Objective
The goal of this study is to understand what activities, as proposed in industry, are entailed to handle hypotheses, facilitating the comparison, creation, and evaluation of relevant techniques.
Methods
We performed a gray literature review (GLR) on the practices proposed by practitioners to handle hypotheses in the context of software startups. We analyzed the identified documents using thematic synthesis.
Results
The analysis revealed that techniques proposed for software startups in practice compress five different activities: elicitation, prioritization, specification, analysis, and management. It also showed that practitioners often classify hypotheses in types and which qualities they aim for these statements.
Conclusion
Our results represent the first description for hypotheses engineering grounded in practice data. This mapping of the state-of-practice indicates how research could go forward in investigating hypotheses for experimentation in the context of software startups. For practitioners, they represent a catalog of available practices to be used in this context.",21 Mar 2025,5,"The study provides valuable insights into handling hypotheses in software startups, but the practical impact may be limited as it focuses on a specific aspect of software development."
https://www.sciencedirect.com/science/article/pii/S095058492030094X,Context-Oriented Behavioral Programming,May 2021,Information and Software Technology,"Behavioral programming, Scenario-based programming, Programming paradigm, Context awareness, Context-oriented programming, Context-Oriented Behavioral Programming",Achiya=Elyasaf: achiya@bgu.ac.il,"Abstract
Context:
Modern systems require programmers to develop code that dynamically adapts to different contexts, leading to the evolution of new 
context-oriented
 programming languages. These languages introduce new software-engineering challenges, such as: how to maintain the separation of concerns of the codebase? how to model the changing behaviors? how to verify the 
system behavior
? and more.
Objective:
This paper introduces 
Context-Oriented Behavioral Programming
 (COBP) — a novel paradigm for developing context-aware systems, centered on natural and incremental specification of context-dependent behaviors. As the name suggests, we combine 
behavioral-programming
 (BP) — a scenario-based modeling paradigm — with context idioms that explicitly specify when scenarios are relevant and what information they need. The core idea is to connect the behavioral model with a data model that represents the context, allowing an intuitive connection between the models via update and select queries. Combining behavioral-programming with context-oriented programming brings the best of the two worlds, solving issues that arise when using each of the approaches in separation.
Methods:
We begin with providing abstract semantics for COBP and two implementations for the semantics, laying the foundations for applying 
reasoning algorithms
 to context-aware behavioral programs. Next, we exemplify the semantics with formal specifications of systems, including a variant of Conway’s 
Game of Life
. Then, we provide two 
case studies
 of real-life context-aware systems (one in robotics and another in IoT) that were developed using this tool. Throughout the examples and 
case studies
, we provide 
design patterns
 and a methodology for coping with the above challenges.
Results:
The case studies show that the proposed approach is applicable for developing real-life systems, and presents measurable advantages over the alternatives — behavioral programming alone and context-oriented programming alone.
Conclusion:
We present a paradigm allowing programmers and system engineers to capture complex context-dependent requirements and align their code with such requirements.",21 Mar 2025,9,"The introduction of COBP presents a novel paradigm with real-world case studies showing advantages over existing approaches, making it highly valuable for European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584921000185,Improving high-impact bug report prediction with combination of interactive machine learning and active learning,May 2021,Information and Software Technology,Not Found,Xiaoxue=Wu: Not Found; Wei=Zheng: wzheng@nwpu.edu.cn; Xiang=Chen: Not Found; Yu=Zhao: Not Found; Tingting=Yu: Not Found; Dejun=Mu: Not Found,"Abstract
Context:
Bug reports
 record issues found during software development and maintenance. A high-impact bug report (HBR) describes an issue that can cause severe damage once occurred after deployment. Identifying HBRs from the bug repository as early as possible is crucial for guaranteeing software quality.
Objective:
In recent years, many machine learning-based approaches have been proposed for HBR prediction, and most of them are based on supervised 
machine learning
. However, the assumption of supervised 
machine learning
 is that it needs a large number of labeled data, which is often difficult to gather in practice.
Method:
In this paper, we propose hbrPredictor, which combines interactive machine learning and active learning to HBR prediction. On the one hand, it can dramatically reduce the number of bug reports required for prediction model training; on the other hand, it improves the diversity and 
generalization ability
 of 
training samples
 via 
uncertainty sampling
.
Result:
We take security bug report (SBR) prediction as an example of HBR prediction and perform a large-scale experimental evaluation on datasets from different open-source projects. The results show: (1) hbrPredictor substantially outperforms the two baselines and obtains the maximum values of F1-score (0.7939) and AUC (0.8789); (2) with the dynamic 
stop criteria
, hbrPredictor could reach its best performance with only 45% and 13% of the total bug reports for small-sized datasets and large-sized datasets, respectively.
Conclusion:
By reducing the number of required 
training samples
, hbrPredictor could substantially save the data labeling effort without decreasing the effectiveness of the model.",21 Mar 2025,8,"The hbrPredictor approach addresses a crucial issue in software development with significant performance improvements, which can greatly benefit European startups by saving data labeling effort."
https://www.sciencedirect.com/science/article/pii/S0950584921000239,Alignment and granularity of requirements and architecture in agile development: A functional perspective,May 2021,Information and Software Technology,"Requirements engineering, Software architecture, Twin Peaks, Alignment, Granularity, Case study, Agile development",Tjerk=Spijkman: tjerk@fizor.io; Sabine=Molenaar: Not Found; Fabiano=Dalpiaz: Not Found; Sjaak=Brinkkemper: Not Found,"Abstract
Context:
Requirements engineering
 and software architecture are tightly linked disciplines. The Twin Peaks model suggests that requirements and architectural components should stay aligned while the system is designed and as the level of detail increases. Unfortunately, this is hardly the case in practical settings.
Objective:
We surmise that a reason for the absence of conjoint evolution is that existing models, such as the Twin Peaks, do not provide concrete guidance for practitioners. We propose the Requirements Engineering for Software Architecture (RE4SA) model to assist in analyzing the alignment and the 
granularity
 of functional requirements and architectural components.
Methods:
After detailing the RE4SA model in notation-independent terms, we propose a concrete instance, called RE4SA-Agile, that connects common artifacts in 
agile development
, such as user stories and features. We introduce metrics that measure the alignment between the requirements and architecture, and we define 
granularity
 smells to pinpoint situation in which the granularity of one high-level requirement or high-level component is not uniform with the norm. We show two applications of RE4SA-Agile, including the use of the metrics, to real-world 
case studies
.
Results:
Our applications of RE4SA-Agile, which were discussed with representatives from the development teams, prove to be able to pinpoint problematic situations regarding the relationship between functional requirements and architecture.
Conclusion:
RE4SA and its metrics can be seen as a first attempt to provide a concrete approach for supporting the application of the Twin Peaks model. We expect future research to apply our metrics to additional cases and to provide variants for RE4SA that support different concrete notations, and extend the perspective beyond the functional perspective of this research, similar to what we did with RE4SA-Agile in this paper.",21 Mar 2025,7,"The RE4SA model provides concrete guidance for practitioners in aligning requirements and architecture, offering practical metrics and real-world case studies, making it valuable for European startups looking to improve software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584920302172,Leveraging Small Sample Learning for Business Process Management,April 2021,Information and Software Technology,Not Found,Martin=Käppel: martin.kaeppel@uni-bayreuth.de; Stefan=Schönig: stefan.schoenig@ur.de; Stefan=Jablonski: stefan.jablonski@uni-bayreuth.de,"Abstract
Context:
 Tool support for business process management (BPM) is improving more and more. Often, 
machine learning techniques
 are used to recognize certain execution patterns, to optimize workflows and to observe or predict processes. Frequently, many organisations cannot meet the fundamental prerequisites of 
machine learning
 methods since less data is recorded and therefore available for analysis. Most 
machine learning techniques
 rely on big and sufficient data source sets that can be analyzed. Small Sample Learning (SSL) tackles the issue of implementing 
machine learning
 methods in environments where only quantitatively insufficient datasets are available. These methods are strongly tailored to computer vision or 
natural language processing
 problems, which is why they are still neglected in the BPM area.
Objective:
 This paper motivates the use of SSL methods in the BPM area and fosters a research stream that is concerned with the transferability to and the application of these methods in the BPM area.
Method:
 We propose a concept for leveraging SSL methods in BPM and illustrate the idea exemplarly in the field of process mining.
Results:
 Reasons for the need of SSL methods in the BPM area and a conceptual approach for transferring existing SSL methods to the BPM area. The feasibility of our apprach is shown by a brief overview of a primary study leveraging SSL methods for process prediction.
Conclusions:
 Many areas of process mining or BPM in general depend on a sufficient amount of (training) data. Often 
small and medium sized companies
 lack ”big data”, which is why advantages of machine learning and data analysis in the context of BPM cannot be applied. Existing methods that deal with insufficient data are very domain-specific and must be transferred to the process mining area respectively the BPM area.",21 Mar 2025,3,"While the paper addresses an important issue of implementing machine learning techniques with insufficient data in the BPM area, the focus may be less relevant for European early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584920302214,A systematic review of scheduling approaches on multi-tenancy cloud platforms,April 2021,Information and Software Technology,Not Found,Ru=Jia: jiaruweiwei@gmail.com; Yun=Yang: yyang@swin.edu.au; John=Grundy: john.grundy@monash.edu; Jacky=Keung: Jacky.Keung@cityu.edu.hk; Li=Hao: liucoolhao@gmail.com,"Abstract
Context:
Scheduling in cloud is complicated as a result of multi-tenancy. Diverse tenants have different requirements, including service functions, response time, QoS and throughput. Diverse tenants require different scheduling capabilities, resource consumption and competition. Multi-tenancy scheduling approaches have been developed for different service models, such as 
Software as a Service
 (SaaS), Platform as a service (PaaS), Infrastructure as a Service (IaaS), and Database as a Service (DBaaS).
Objective:
In this paper, we survey the current landscape of multi-tenancy scheduling, laying out the challenges and complexity of 
software engineering
 where multi-tenancy issues are involved. This study emphasises scheduling policies, cloud provisioning and deployment with regards to multi-tenancy issues. We conduct a systematic literature review of research studies related to multi-tenancy scheduling approaches on cloud platforms determine the primary scheduling approaches currently used and the challenges for addressing key multi-tenancy scheduling issues.
Method:
We adopted a systematic literature review method to search and review many major journal and 
conference papers
 on four major online electronic databases, which address our four predefined research questions. Defining inclusion and exclusion criteria was the initial step before extracting data from the selected papers and deriving answers addressing our enquiries.
Results:
Finally, 53 papers were selected, of which 62 approaches were identified. Most of these methods are developed without cloud layers’ limitation (43.40%) and on SaaS, most of scheduling approaches are oriented to framework design (43.75%).
Conclusion:
The results have demonstrated most of multi-tenancy scheduling solutions can work at any delivery layer. With the difference of tenants’ requirements and functionalities, the choice of cloud service delivery models is changed. Based on our study, designing a multi-tenancy scheduling framework should consider the following 3 factors: computing, QoS and storage resource. One of the potential research foci of multi-tenancy scheduling approaches is on GPU scheduling.",21 Mar 2025,6,"The abstract provides insights into the challenges and complexity of multi-tenancy scheduling in cloud platforms, which could be beneficial for early-stage ventures dealing with cloud services."
https://www.sciencedirect.com/science/article/pii/S0950584920301981,Big Data analytics in Agile software development: A systematic mapping study,April 2021,Information and Software Technology,Not Found,Katarzyna=Biesialska: katarzyna.biesialska@upc.edu; Xavier=Franch: Not Found; Victor=Muntés-Mulero: Not Found,"Abstract
Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing 
development cycles
 through 
data analytics
 is becoming a commodity.
Objective:
Although a myriad of research exists on 
software analytics
 as well as on 
Agile software development
 (ASD) practice on itself, there exists no 
systematic overview
 of the research done on ASD from a 
data analytics
 perspective. Therefore, the objective of this work is to make progress by linking ASD with 
Big Data analytics
 (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole 
ASD lifecycle
. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of 
software analytics
 research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.",21 Mar 2025,8,"The abstract highlights the importance of linking Agile software development with Big Data analytics, which could have a significant impact on early-stage ventures looking to optimize their development cycles."
https://www.sciencedirect.com/science/article/pii/S095058492030238X,Identifying method-level mutation subsumption relations using Z3,April 2021,Information and Software Technology,Not Found,Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Márcio=Ribeiro: marcio@ic.ufal.br; Beatriz=Souza: beatriz.souza@ccc.ufcg.edu.br; Marcio=Guimarães: masg@ic.ufal.br; Leo=Fernandes: leonardo.oliveira@ifal.edu.br; Marcelo=d’Amorim: damorim@cin.ufpe.br; Vander=Alves: valves@unb.br; Leopoldo=Teixeira: lmt@cin.ufpe.br; Baldoino=Fonseca: baldoino@ic.ufal.br,"Abstract
Context:
Mutation analysis is a popular but costly approach to assess the quality of test suites. One recent promising direction in reducing costs of mutation analysis is to identify redundant mutations, i.e., mutations that are subsumed by some other mutations. A previous approach found redundant mutants manually through truth tables but it cannot be applied to all mutations. Another work derives them using automatic test suite generators but it is a time consuming task to generate mutants and tests, and to execute tests.
Objective:
This article proposes an approach to discover redundant mutants by proving 
subsumption relations
 among method-level 
mutation operators
 using weak mutation testing.
Method:
We conceive and encode a theory of 
subsumption relations
 in the Z3 
theorem prover
 for 37 mutation targets (mutations of an expression or statement).
Results:
We automatically identify and prove a number of subsumption relations using Z3, and reduce the number of mutations in a number of mutation targets. To evaluate our approach, we modified 
MuJava
 to include the results of 24 mutation targets and evaluate our approach in 125 classes of 5 large open source popular projects used in prior work. Our approach correctly discards mutations in 75.93% of the cases, and reduces the number of mutations by 71.38%.
Conclusions:
Our approach offers a good balance between the effort required to derive subsumption relations and the effectiveness for the targets considered in our evaluation in the context of strong mutation testing.",21 Mar 2025,7,"The abstract offers a practical approach to reducing the cost of mutation analysis, which could be valuable for startups focusing on software testing and quality assurance."
https://www.sciencedirect.com/science/article/pii/S0950584920302445,Spectral clustering based mutant reduction for mutation testing,April 2021,Information and Software Technology,Not Found,Changqing=Wei: Not Found; Xiangjuan=Yao: yaoxj@cumt.edu.cn; Dunwei=Gong: Not Found; Huai=Liu: Not Found,"Abstract
Context:
Mutation testing techniques, which attempt to construct a set of so-called mutants by seeding various faults into the software under test, have been widely used to generate test cases as well as to evaluate the effectiveness of a test suite. Its popularity in practice is significantly hindered by its high cost, majorly caused by the large number of mutants generated by the technique.
Objective:
It is always a challenging task to reduce the number of mutants while preserving the effectiveness of mutation testing. In this paper, we make use of an intelligent technique, namely 
spectral clustering
, to improve the efficacy of mutant reduction.
Method:
First of all, we give a family of definitions and the method to calculate the distance between mutants according to the weak mutation testing criteria. Then we propose a mutant reduction method based on 
spectral clustering
 (SCMT), including the determination method of the number of clusters, spectral clustering of mutants, and selection of representative mutants.
Results:
The experimental studies based on 12 object programs show that the new approach can significantly reduce the number of mutants without jeopardizing the performance of mutation testing. As compared with other benchmark techniques, the new approach based on weak mutation testing criteria cannot only consistently deliver high effectiveness of mutation testing, but also help significantly reduce the time-cost of mutation testing.
Conclusion:
It is clearly demonstrated that the use of spectral clustering can help enhance the cost-effectiveness of mutation testing. The research reveals some potential research directions for not only mutation testing but also the broad area of software testing.",21 Mar 2025,9,"The abstract introduces an intelligent technique to reduce the number of mutants in mutation testing, which could greatly benefit early-stage ventures by improving cost-effectiveness in testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584920302408,Using mutual information to test from Finite State Machines: Test suite selection,April 2021,Information and Software Technology,Not Found,Alfredo=Ibias: aibias@ucm.es; Manuel=Núñez: mn@sip.ucm.es; Robert M.=Hierons: r.hierons@sheffield.ac.uk,"Abstract
Context:
Mutual Information
 is an information 
theoretic measure
 designed to quantify the amount of similarity between two 
random variables
 ranging over two sets. In this paper, we adapt this concept and show how it can be used to select a 
good
 test suite to test from a 
Finite State Machine
 (
FSM
) based on a 
maximise diversity
 approach.
Objective:
The main goal of this paper is to use 
Mutual Information
 in order to select test suites to test from 
FSM
s and evaluate whether we obtain better results, concerning the quality of the selected test suite, than current state-of-the-art measures.
Method:
First, we defined our scenario. We considered the case where we receive two (or more) test suites and we have to choose between them. We were interested in this scenario because it is a 
recurrent
 case in regression testing. Second, we defined our notion based on 
Mutual Information
: Biased 
Mutual Information
. Finally, we carried out experiments in order to evaluate the measure.
Results:
We obtained experimental evidence that demonstrates the potential value of the measure. We also showed that the time needed to compute the measure is negligible when compare to the time needed to apply extra testing. We compared our measure with a state-of-the-art test selection measure and showed that our proposal outperforms it. Finally, we have compared our measure with a notion of transition coverage. Our experiments showed that our measure is slightly worse than transition coverage, as expected, but its computation is 10 times faster.
Conclusion:
Our experiments showed that Biased Mutual Information is a good measure for selecting test suites, outperforming the current state-of-the-art measure, and having a (negative) correlation to fault coverage. Therefore, we can conclude that our new measure can be used to select the test suite that is likely to find more faults. As a result, it has the potential to be used to automate test generation.",21 Mar 2025,5,"The abstract presents a method using Mutual Information to select test suites, which may have some relevance to early-stage ventures in software testing, but may not have as immediate impact as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920302470,Validating class integration test order generation systems with Metamorphic Testing,April 2021,Information and Software Technology,Not Found,Miao=Zhang: miazhang9-c@my.cityu.edu.hk; Jacky Wai=Keung: jacky.Keung@cityu.edu.hk; Tsong Yueh=Chen: tychen@swin.edu.au; Yan=Xiao: dcsxan@nus.edu.sg,"Abstract
Context:
Previous studies proposed different kinds of approaches for class integration test order generation, and corresponding systems can be implemented based on these approaches. Such class integration test order generation systems can facilitate the process of software integration testing if they are implemented correctly.
Objective:
However, a test oracle problem exists in the class integration test order generation systems. Since these approaches for class integration test order generation normally deliver a local optimum rather than a global optimum, there are no practically feasible ways to validate their generated class integration test orders, that is, these implementation systems are untestable.
Method:
To address the test oracle problem, we apply Metamorphic Testing (MT) to validate class integration test order generation systems. Metamorphic Relations (MRs), which are the key components of MT, reason about relations between test outputs of a system. Five effective MRs are developed to ensure the quality of the class integration test order generation systems. In these proposed MRs, follow-up test inputs are generated by modifying classes or class dependencies in the source test inputs while some characteristics of the source test outputs are preserved, for example, the same class integration test order or the equal stubbing cost. Faults can be detected in systems if an individual MR is violated for certain tests.
Results:
Failure detection of MT has been successfully demonstrated in empirical experiments on three systems implementing different typical class integration test order generation approaches. More than 84% of faulty programs can be detected by all MRs, for three class integration test order generation systems under investigation.
Conclusion:
The experimental results show that the proposed MRs are able to systematically and effectively detect faults in class integration test order generation systems. This study explores a new application domain in MT and further extends its applications in 
Software Engineering
.",21 Mar 2025,9,"The study addresses a significant problem in software integration testing and proposes a practical solution using Metamorphic Testing. The results demonstrate high effectiveness in detecting faults in class integration test order generation systems, which can have a direct impact on improving software quality for European startups."
https://www.sciencedirect.com/science/article/pii/S0950584920302044,Fast and curious: A model for building efficient monitoring- and decision-making frameworks based on quantitative data,April 2021,Information and Software Technology,Not Found,Iris=Figalist: iris.figalist@siemens.com; Christoph=Elsner: Not Found; Jan=Bosch: Not Found; Helena Holmström=Olsson: Not Found,"Abstract
Context:
Nowadays, the hype around 
artificial intelligence
 is at its absolute peak. Large amounts of data are collected every second of the day and a variety of tools exists to enable easy 
analysis of data
. In practice, however, making meaningful use of it is way more challenging. For instance, affected stakeholders often struggle to specify their information needs and to interpret the results of such analyses.
Objective:
In this study we investigate how to enable continuous monitoring of information needs, and the generation of knowledge and insights for various stakeholders involved in the lifecycle of software-intensive products. The overarching goal is to support their decision making by providing relevant insights related to their area of responsibility.
Methods:
We implement multiple monitoring- and decision-making frameworks for six individual, real-world cases selected from three different platforms and covering four types of stakeholders. We compare the individual procedures to derive a 
generic process
 for instantiating such frameworks as well as a model to scale it up for multiple stakeholders.
Results:
For one, we discovered that information needs of stakeholders are often related to a limited subset of 
data sources
 and should be specified in stages. For another, stakeholders often benefit from sharing and reusing existing components among themselves in later phases. Specifically, we identify three types of reuse: (1) Data and knowledge, (2) tools and methods, and (3) concepts. As a result, key aspects of our model are iterative feedback and specification cycles as well as the reuse of appropriate components to speed up the 
instantiation
 process and maximize the efficiency of the model.
Conclusion:
Our results indicate that knowledge and insights can be generated much faster and stakeholders feel the benefits of the analysis very early on by iteratively specifying information needs and by systematically sharing and reusing knowledge, tools and concepts.",21 Mar 2025,8,"The study focuses on enabling continuous monitoring of information needs for stakeholders involved in software-intensive products. The results suggest a model that can generate knowledge and insights faster, which could be valuable for decision-making processes of early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584920302184,Industry-Academia research collaboration in software engineering: The Certus model,April 2021,Information and Software Technology,"Software engineering, Industry-academia collaboration, Research collaboration, Research knowledge co-creation, Collaboration model, Technology transfer, Knowledge transfer, Research exploitation, Research-based innovation",Dusica=Marijan: dusica@simula.no; Arnaud=Gotlieb: Not Found,"Abstract
Context
Research collaborations between 
software engineering
 industry and academia can provide significant benefits to both sides, including improved innovation capacity for industry, and real-world environment for motivating and validating research ideas. However, building scalable and effective research collaborations in software engineering is known to be challenging. While such challenges can be varied and many, in this paper we focus on the challenges of achieving participative knowledge creation supported by active dialog between industry and academia and continuous commitment to 
joint
 problem solving.
Objective
This paper aims to understand what are the elements of a successful industry-academia collaboration that enable the culture of participative knowledge creation.
Method
We conducted participant observation collecting qualitative data spanning 8 years of collaborative research between a software engineering research group on software V&V and the Norwegian IT sector. The 
collected data
 was analyzed and synthesized into a practical collaboration model, named the Certus Model.
Results
The model is structured in seven phases, describing activities from setting up research projects to the exploitation of 
research results
. As such, the Certus model advances other collaborations models from literature by delineating different phases covering the complete life cycle of participative research knowledge creation.
Conclusion
The Certus model describes the elements of a research collaboration process between researchers and practitioners in software engineering, grounded on the principles of research knowledge co-creation and continuous commitment to joint problem solving. The model can be applied and tested in other contexts where it may be adapted to the local context through experimentation.",21 Mar 2025,7,"The research emphasizes successful industry-academia collaborations in software engineering, providing a practical model for participative knowledge creation. This could potentially benefit European startups by enhancing innovation capacity and problem-solving capabilities."
https://www.sciencedirect.com/science/article/pii/S0950584920302160,Do users care about ad’s performance costs? Exploring the effects of the performance costs of in-app ads on user experience,April 2021,Information and Software Technology,Not Found,Cuiyun=Gao: cygao@cse.cuhk.edu.hk; Jichuan=Zeng: jczeng@cse.cuhk.edu.hk; Federica=Sarro: f.sarro@ucl.ac.uk; David=Lo: davidlo@smu.edu.sg; Irwin=King: king@cse.cuhk.edu.hk; Michael R.=Lyu: lyu@cse.cuhk.edu.hk,"Abstract
Context:
 In-app advertising is the primary source of revenue for many mobile apps. The cost of advertising (ad cost) is non-negligible for app developers to ensure a good 
user experience
 and continuous profits. Previous studies mainly focus on addressing the hidden performance costs generated by ads, including consumption of memory, CPU, data traffic, and 
battery
. However, there is no research on analyzing users’ perceptions of ads’ performance costs to our knowledge.
Objective:
 To fill this gap and better understand the effects of performance costs of in-app ads on 
user experience
, we conduct a study on analyzing user concerns about ads’ performance costs.
Method:
 First, we propose RankMiner, an approach to quantify user concerns about specific app issues, including performance costs. Then, based on the usage traces of 20 subject apps, we measure the performance costs of ads. Finally, we conduct correlation analysis on the performance costs and quantified user concerns to explore whether users complain more for higher performance costs.
Results:
 Our findings include the following: (1) RankMiner can quantify users’ concerns better than baselines by an improvement of 214% and 2.5% in terms of 
Pearson
 
correlation coefficient
 (a metric for computing correlations between two variables) and NDCG score (a metric for computing accuracy in prioritizing issues), respectively. (2) The performance costs of the with-ads versions are statistically significantly larger than those of no-ads versions with 
negligible effect
 size; (3) Users are more concerned about the battery costs of ads, and tend to be insensitive to ads’ data traffic costs.
Conclusion:
 Our study is complementary to previous work on in-app ads, and can encourage developers to pay more attention to alleviating the most user-concerned performance costs, such as battery cost.",21 Mar 2025,6,"The study analyzes user concerns about performance costs of in-app ads and proposes a method to quantify these concerns. While it may not directly impact European early-stage ventures, it offers insights that could help developers improve user experience and ad performance."
https://www.sciencedirect.com/science/article/pii/S0950584920302299,"A methodical framework for service oriented architecture adoption: Guidelines, building blocks, and method fragments",April 2021,Information and Software Technology,Not Found,Supriya=Pulparambil: s.pulparambil@squ.edu.com; Youcef=Baghdadi: ybaghdadi@squ.edu.om; Camille=Salinesi: camille.salinesi@univ-paris1.fr,"Abstract
Context
Rapidly-changing business requirements expect high business process flexibility that can be achieved using 
service oriented architecture
 (SOA). This requires enterprises to adopt SOA and assess their SOA adoption maturity to achieve continuous improvement. SOA realization demands service development with varying levels of 
granularity
.
Objectives
The research aims to develop a methodical framework for SOA realization based on Welke's SOA maturity model, a model that assumes a methodology dimension. The framework is concerned with formalizing knowledge on how to identify and shape the main 
building blocks
 of a method at each maturity level.
Methods
The research applies the principles of 
design science research
 and method engineering to develop a methodical framework for SOA realization.
Results
The research identifies the gaps in SOA realization methods and illustrates how a methodical framework based on a maturity model facilitates the SOA adoption process. The evaluation results revealed that the framework would help enterprises to select method fragments required at each maturity level to accomplish business excellence.
Conclusion
The implications of this research are twofold: from a theoretical perspective, the researchers or practitioners can use the results for further study. From a practical standpoint, enterprises can use the methodical guidelines to assess their current maturity level and select and implement the required method fragments from the method base provided in the proposed framework.",21 Mar 2025,5,"The research develops a methodical framework for SOA realization based on a maturity model. While the framework could assist enterprises in adopting SOA and improving business processes, its direct impact on European startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920302482,On the prediction of long-lived bugs: An analysis and comparative study using FLOSS projects,April 2021,Information and Software Technology,Not Found,Luiz Alberto Ferreira=Gomes: luizgomes@pucpcaldas.br; Ricardo=da Silva Torres: ricardo.torres@ntnu.no; Mario Lúcio=Côrtes: cortes@ic.unicamp.br,"Abstract
Context:
Software evolution and maintenance activities in today’s Free/Libre 
Open Source Software
 (FLOSS) rely primarily on information extracted from 
bug reports
 registered in 
bug tracking systems
. Many studies point out that most bugs that adversely affect the user’s experience across versions of 
FLOSS projects
 are long-lived bugs. However, proposed approaches that support bug fixing procedures do not consider the real-world lifecycle of a bug, in which bugs are often fixed very fast. This may lead to useless efforts to automate the bug management process.
Objective:
This study aims to confirm whether the number of long-lived bugs is significantly high in popular open-source projects and to characterize the population of long-lived bugs by considering the attributes of 
bug reports
. We also aim to conduct a comparative study evaluating the prediction accuracy of five well-known 
machine learning algorithms
 and text mining techniques in the task of predicting long-lived bugs.
Methods:
We collected bug reports from six popular open-source projects repositories (Eclipse, Freedesktop, Gnome, GCC, Mozilla, and WineHQ) and used the following 
machine learning algorithms
 to predict long-lived bugs: K-Nearest Neighbor, Naïve Bayes, 
Neural Networks
, 
Random Forest
, and 
Support Vector Machines
.
Results:
Our results show that long-lived bugs are relatively frequent (varying from 7.2% to 40.7%) and have unique characteristics, confirming the need to study solutions to support bug fixing management. We found that the Neural Network classifier yielded the best results in comparison to the other algorithms evaluated.
Conclusion:
Research efforts regarding long-lived bugs are needed and our results demonstrate that it is possible to predict long-lived bugs with a high accuracy (around 70.7%) despite the use of simple prediction algorithms and text mining methods.",21 Mar 2025,8,"This study addresses a practical issue in software development by predicting long-lived bugs in open-source projects, with potential implications for early-stage ventures relying on bug tracking systems."
https://www.sciencedirect.com/science/article/pii/S0950584920302469,Multifaceted infrastructure for self-adaptive IoT systems,April 2021,Information and Software Technology,Not Found,Rossana M.C.=Andrade: rossana@great.ufc.br; Belmondo R.=Aragão: belmondorodrigues@great.ufc.br; Pedro Almir M.=Oliveira: pedromartins@great.ufc.br; Marcio E.F.=Maia: marcio@great.ufc.br; Windson=Viana: windson@great.ufc.br; Tales P.=Nogueira: tales@great.ufc.br,"Abstract
Background:
Internet of Things
 (IoT) enables the interaction among objects to provide services to their users. Areas such as eHealth, smart energy, and smart buildings have been benefiting from the IoT potential. However, the development of IoT systems is still complex because it deals with a highly dynamic, volatile, and heterogeneous environment. These characteristics require discovering devices, managing these devices’ context, and self-adapt their behavior.
Goal
: In this work, we propose a self-adaptive IoT infrastructure to support multiple facets, 
i.e.
, the contextual discovery of smart objects, the context management, and the self-adaptation process of the development of these systems.
Methods
: We evaluated the proposed infrastructure by developing a smart building application with and without it. The evaluation focused on four issues: the feasibility of integrating the context management through middleware platforms with adaptation based on workflows in a request/response communication model, the impact of our infrastructure on the development of self-adaptive IoT systems considering 
cyclomatic complexity
 and coupling 
code metrics
; the impact of using contextual filters on the orchestrator of self-adaptation; and the impact on the quality of the self-adaptation.
Results
: The results suggest that: (i) it is feasible to use the proposed infrastructure in the development of self-adaptive IoT systems; (ii) there is a reduction in the 
cyclomatic complexity
 and the coupling with our approach, (iii) there is a considerable decrease in the number of rules evaluated at runtime, (iv) our infrastructure reduces the execution time of the adaptations when using contextual filters, and (v) the self-adaptation process was effective when using the orchestrator of self-adaptations.
Conclusion
: With these results, we observed that the proposed multifaceted infrastructure could reduce the complexity related to the development of IoT systems, in addition to optimizing their self-adaptation process.",21 Mar 2025,7,The proposed self-adaptive IoT infrastructure can benefit startups in various sectors by simplifying the development of IoT systems and optimizing their self-adaptation process.
https://www.sciencedirect.com/science/article/pii/S0950584920301993,Understanding and addressing quality attributes of microservices architecture: A Systematic literature review,March 2021,Information and Software Technology,Not Found,Shanshan=Li: Not Found; He=Zhang: dr.hezhang@gmail.com; Zijia=Jia: Not Found; Chenxing=Zhong: Not Found; Cheng=Zhang: Not Found; Zhihao=Shan: Not Found; Jinfeng=Shen: Not Found; Muhammad Ali=Babar: Not Found,"Abstract
Context
: As a rapidly adopted 
architectural style
 in 
software engineering
, 
Microservices Architecture
 (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA.
Objective
: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems.
Method
: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA.
Results
: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, 
scalability, performance, availability, monitorability, security
, and 
testability
. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for 
scalability
, four for 
performance
, four for 
availability
, four for 
monitorability
, three for 
security
, and two for 
testability
.
Conclusion
: This SLR concludes that for MSA-based systems: 1) Although 
scalability
 is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., 
performance
. Apart from the six identified QAs in this study, other QAs for MSA like 
maintainability
 need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the 
return on investment
, since this 
architectural style
 additionally cause some pains in practice.",21 Mar 2025,9,"The investigation of Quality Attributes in Microservices Architecture provides valuable insights for early-stage ventures adopting MSA, helping them make informed decisions about system design and optimization."
https://www.sciencedirect.com/science/article/pii/S0950584920302019,Feature selection and embedding based cross project framework for identifying crashing fault residence,March 2021,Information and Software Technology,Not Found,Zhou=Xu: Not Found; Tao=Zhang: Not Found; Jacky=Keung: Not Found; Meng=Yan: mengy@cqu.edu.cn; Xiapu=Luo: csxluo@comp.polyu.edu.hk; Xiaohong=Zhang: Not Found; Ling=Xu: Not Found; Yutian=Tang: Not Found,"Abstract
Context: The automatically produced crash reports are able to analyze the root of fault causing the crash (crashing fault for short) which is a critical activity for 
software quality assurance
.
Objective: Correctly predicting the existence of crashing fault residence in stack traces of crash report can speed up 
program debugging
 process and optimize debugging efforts. Existing work focused on the collected label information from bug-fixing logs, and the extracted features of crash instances from stack traces and 
source code
 for 
I
dentification of 
C
rashing 
F
ault 
R
esidence (
ICFR
) of newly-submitted crashes. This work develops a novel cross project ICFR framework to address the data scarcity problem by using labeled crash data of other project for the ICFR task of the project at hand. This framework removes irrelevant features, reduces distribution differences, and eases the 
class imbalance
 issue of cross project data since these factors may negatively impact the ICFR performance.
Method: The proposed framework, called 
FSE
, combines 
F
eature 
S
election and feature 
E
mbedding techniques. The FSE framework first uses an 
information gain
 ratio based feature ranking method to select a relevant feature subset for cross project data, and then employs a state-of-the-art 
W
eighted 
B
alanced 
D
istribution 
A
daptation (
WBDA
) method to map features of cross project data into a common space. WBDA considers both marginal and conditional distributions as well as their weights to reduce 
data distribution
 discrepancies. Besides, WBDA balances the class proportion of each project data to alleviate the 
class imbalance
 issue.
Results: We conduct experiments on 7 projects to evaluate the performance of our FSE framework. The results show that FSE outperforms 25 methods under comparison.
Conclusion: This work proposes a cross project learning framework for ICFR, which uses feature selection and embedding to remove irrelevant features and reduce distribution differences, respectively. The results illustrate the performance superiority of our FSE framework.",21 Mar 2025,8,"The cross project learning framework for predicting crashing faults can significantly speed up program debugging, benefiting startups in optimizing their software quality assurance processes."
https://www.sciencedirect.com/science/article/pii/S0950584920302202,A decentralized approach for discovering runtime software architectural models of distributed software systems,March 2021,Information and Software Technology,Not Found,Jason=Porter: jason.porter@ung.edu; Daniel A.=Menascé: menasce@gmu.edu,"Abstract
Context:
Runtime software 
architectural models
 of self-adaptive systems are needed for making adaptation decisions in architecture-based self-adaptive systems. However, when these systems are distributed and highly dynamic, there is an added need to discover the system’s software architecture model at runtime. Current methods of runtime architecture discovery use a 
centralized approach
, in which the process is carried out from a single location. These methods are inadequate for large distributed software systems because they do not scale up well and have a 
single point of failure
.
Objective and Method:
This paper describes DeSARM (Decentralized Software Architecture discoveRy Mechanism), a completely decentralized and automated approach, based on gossiping and message tracing, for runtime discovery of software architecture models of distributed software systems. DeSARM is able to identify at runtime important architectural characteristics such as components and connectors, in addition to 
synchronous and asynchronous communication
 patterns. Furthermore, through its use of gossiping, DeSARM exhibits the properties of scalability, global consistency among participating nodes, and resiliency to failures. This paper demonstrates DeSARM’s properties and answers key research questions through experimentation with software architectures of varying sizes and complexities executing within a computer cluster.
Results:
DeSARM enables the decentralized discovery of runtime software 
architectural models
 of distributed software systems while exhibiting the properties of scalability, global consistency among participating nodes and resiliency to failures. Scalability is achieved through DeSARM’s ability to successfully discover software architectures of increasing sizes and complexities executing across node counts of increasing sizes. Global consistency among participating nodes is achieved by each node within the system discovering the complete software architecture independently but in coordination with each other. Finally, resiliency to failures is achieved by DeSARM successfully discovering the software architecture of the system in the presence of component failures.",21 Mar 2025,7,"The DeSARM approach for decentralized software architecture discovery has the potential to simplify architecture modeling for distributed systems, offering scalability and resiliency benefits for startups adopting such systems."
https://www.sciencedirect.com/science/article/pii/S0950584920302275,Mining the Technical Roles of GitHub Users,March 2021,Information and Software Technology,Not Found,João Eduardo=Montandon: joao.montandon@dcc.ufmg.br; Marco Tulio=Valente: mtov@dcc.ufmg.br; Luciana L.=Silva: luciana.lourdes.silva@ifmg.edu.br,"Abstract
Context:
Modern software development demands high levels of technical specialization. These conditions make IT companies focus on creating cross-functional teams, such as frontend, backend, and mobile developers. In this context, the success of software projects is highly influenced by the expertise of these teams in each field.
Objective:
In this paper, we investigate machine-learning based approaches to automatically identify the technical roles of open source developers.
Method:
For this, we first build a ground truth with 2284 developers labeled in six different roles: backend, frontend, full-stack, mobile, devops, and data science. Then, we build three different machine-learning models used to identify these roles.
Results:
These models presented competitive results for precision (0.88) and AUC (0.89) when identifying all six roles. Moreover, our results show that programming-languages are the most relevant features to predict the investigated roles.
Conclusion:
The approach proposed in this paper can assist companies during their hiring process, such as by recommending developers with the expertise required by job positions.",21 Mar 2025,8,"The paper investigates machine-learning based approaches to automatically identify technical roles of open source developers, which can greatly assist companies in their hiring process for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302317,Exploring the software repositories of embedded systems: An industrial experience,March 2021,Information and Software Technology,Not Found,Jakub=Polaczek: Not Found; Janusz=Sosnowski: j.sosnowski@ii.pw.edu.pl,"Abstract
Context
Tracing reports for software repositories have attracted many researchers. Most of them have focused on defect analysis and development processes in relation to open source programs. There exists a gap between open source and industrial software projects, which, in particular, relates to different schemes for creating software repositories and development schemes. This is especially true for embedded systems that gain large markets and become more complex.
Objective
The aim is to explore the software repositories of industrial embedded systems and derive characteristic features in order to evaluate quality and identify problems to do with development processes.
Method
In this paper we have proposed a novel approach to software repository analysis based on the fine grained exploration of issue tracking and code control repositories. In particular, we distinguish the various activities of project actors (e.g. creating new functions, correcting defects, improving performance, modifying tests) and analyse them in a context, not only of a single project, but also a set of correlated projects that have been developed in the company. These issues have been neglected in the literature. These analyses needed new holistic schemes for repository exploration, including various statistical metrics, text mining, and machine learning techniques.
Results
In exploring selected industrial projects we have identified that only 40–75% of issues relate to defects; the issue reports and commit descriptions included here comprise a lot of data that has been disregarded in the literature. These data allow us to trace diverse types of code changes and identify imperfections in software repositories.
Conclusion
We show that fine grained repository analysis gives a broader and more complete view of project development, which may lead to its improvement.",21 Mar 2025,6,"The novel approach to software repository analysis provides a broader view of project development, which can potentially impact the quality and problem identification in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302305,Security in agile software development: A practitioner survey,March 2021,Information and Software Technology,"Survey, Security engineering, Agile software development, Software security, Security standards, Security assurance",Kalle=Rindell: kakrind@utu.fi; Jukka=Ruohonen: Not Found; Johannes=Holvitie: Not Found; Sami=Hyrynsalmi: Not Found; Ville=Leppänen: Not Found,"Abstract
Context:
 Software 
security engineering
 provides the means to define, implement and verify security in software products. Software security engineering is performed by following a software security 
development life cycle
 model or a security 
capability maturity model
. However, agile 
software development methods
 and processes, dominant in the software industry, are viewed to be in conflict with these security practices and the security requirements.
Objective:
 Empirically verify the use and impact of software security engineering activities in the context of 
agile software development
, as practiced by software developer professionals.
Method:
 A survey (
N
=
61
) was performed among software practitioners in Finland regarding their use of 40 common security engineering practices and their perceived security impact, in conjunction with the use of 16 agile software development items and activities.
Results:
 The use of agile items and activities had a measurable effect on the selection of security engineering practices. Perceived impact of the security practices was lower than the rate of use would imply: This was taken to indicate a selection bias, caused by e.g. developers’ awareness of only certain security engineering practices, or by difficulties in applying the security engineering practices into an iterative software development workflow. Security practices deemed to have most impact were proactive and took place in the early phases of software development.
Conclusion:
 Systematic use of agile practices conformed, and was observed to take place in conjunction with the use of security practices. Security activities were most common in the requirement and implementation phases. In general, the activities taking place early in the life cycle were also considered most impactful. A discrepancy between the level of use and the perceived security impact of many security activities was observed. This prompts research and methodological development for better integration of security engineering activities into software development processes, methods, and tools.",21 Mar 2025,7,"The study empirically verifies the impact of software security engineering activities in agile software development, which is crucial for startups adopting agile methods and facing security challenges."
https://www.sciencedirect.com/science/article/pii/S0950584920302433,SRPTackle: A semi-automated requirements prioritisation technique for scalable requirements of software system projects,March 2021,Information and Software Technology,Not Found,Fadhl=Hujainah: fadelhogina@gmail.com; Rohani=Binti Abu Bakar: Not Found; Abdullah B.=Nasser: Not Found; Basheer=Al-haimi: Not Found; Kamal Z.=Zamli: Not Found,"Abstract
Context
Requirement prioritisation (RP) is often used to select the most important 
system requirements
 as perceived by system stakeholders. RP plays a vital role in ensuring the development of a quality system with defined constraints. However, a closer look at existing RP techniques reveals that these techniques suffer from some key challenges, such as scalability, lack of quantification, insufficient prioritisation of participating stakeholders, overreliance on the participation of professional expertise, lack of automation and excessive time consumption. These key challenges serve as the motivation for the present research.
Objective
This study aims to propose a new semiautomated scalable prioritisation technique called ‘SRPTackle’ to address the key challenges.
Method
SRPTackle provides a semiautomated process based on a combination of a constructed requirement priority value formulation function using a multi-criteria decision-making method (i.e. weighted sum model), 
clustering algorithms
 (K-means and K-means++) and a 
binary search tree
 to minimise the need for expert involvement and increase efficiency. The effectiveness of SRPTackle is assessed by conducting seven experiments using a benchmark dataset from a large actual software project.
Results
Experiment results reveal that SRPTackle can obtain 93.0% and 94.65% as minimum and maximum accuracy percentages, respectively. These values are better than those of alternative techniques. The findings also demonstrate the capability of SRPTackle to prioritise large-scale requirements with reduced time consumption and its effectiveness in addressing the key challenges in comparison with other techniques.
Conclusion
With the time effectiveness, ability to scale well with numerous requirements, automation and clear implementation guidelines of SRPTackle, project managers can perform RP for large-scale requirements in a proper manner, without necessitating an extensive amount of effort (e.g. tedious manual processes, need for the involvement of experts and time workload).",21 Mar 2025,9,The proposed semiautomated scalable prioritization technique addresses key challenges in requirement prioritization and can greatly benefit early-stage ventures by improving efficiency and effectiveness in selecting important system requirements.
https://www.sciencedirect.com/science/article/pii/S0950584920302159,Dynamic random testing with test case clustering and distance-based parameter adjustment,March 2021,Information and Software Technology,Not Found,Hanyu=Pei: peihanyu@buaa.edu.cn; Beibei=Yin: yinbeibei@buaa.edu.cn; Kai-Yuan=Cai: kycai@buaa.edu.cn,"Abstract
Context
Software testing is essential in 
software engineering
 to improve 
software reliability
. One goal of software testing strategies is to detect faults faster. Dynamic Random Testing (DRT) strategy uses the testing results to guide the selection of test cases, which has shown to be effective in the 
fault detection
 process.
Objective
Previous studies have demonstrated that DRT is greatly affected by the test case classification and the process of adjusting the testing profile. In this paper, we propose Distance-based DRT (D-DRT) strategies, aiming at enhancing the 
fault detection
 effectiveness of DRT.
Method
D-DRT strategies utilize distance information of inputs into the test case classification and the testing profile adjustment process. The test cases are vectorized based on the input parameters and classified into disjoint 
subdomains
 through certain 
clustering methods
. And the distance information of 
subdomains
, along with testing results, are used to adjust the testing profile, such that test cases that are closer to failure-causing subdomains are more likely to be selected.
Results
We conduct empirical studies to evaluate the performance of the proposed algorithms using 12 versions of 4 open-source programs. The experimental results show that, compared with Random Testing (RT), Random Partition Testing (RPT), DRT and Adaptive Testing (AT), our strategies achieve greater fault detection effectiveness with a low computational cost. Moreover, the distance-based testing profile adjustment method is the dominant factor in the improvement of the D-DRT strategy.
Conclusion
D-DRT strategies are effective testing strategies, and the distance-based testing profile adjustment method plays a crucial role.",21 Mar 2025,7,"The proposed D-DRT strategies aim at enhancing fault detection effectiveness in software testing, which is essential for startups to ensure software reliability and quality."
https://www.sciencedirect.com/science/article/pii/S0950584920302287,A study of effectiveness of deep learning in locating real faults,March 2021,Information and Software Technology,Not Found,Zhuo=Zhang: zz8477@126.com; Yan=Lei: yanlei@cqu.edu.cn; Xiaoguang=Mao: xgmao@nudt.edu.cn; Meng=Yan: mengy@cqu.edu.cn; Ling=Xu: xuling@cqu.edu.cn; Xiaohong=Zhang: xhongz@cqu.edu.cn,"Abstract
Context:
 The recent progress of 
deep learning
 has shown its promising learning ability in making sense of data, and many fields have utilized this learning ability to learn an effective model, successfully solving their problems. 
Fault localization
 has explored and used 
deep learning
 to server an aid in debugging, showing the promising results on fault localization. However, as far as we know, there is no detailed studies on evaluating the benefits of using 
deep learning
 for locating real faults present in programs. 
Objective:
 To understand the benefits of 
deep learning
 in locating real faults, this paper explores more about 
deep learning
 by studying the effectiveness of fault localization using 
deep learning
 for a set of real bugs reported in the widely used programs. 
Method:
 We use three representative deep learning architectures (
i.e.
 
convolutional neural network
, 
recurrent neural network
 and multi-layer perceptron) for fault localization, and conduct large-scale experiments on 8 real-world programs equipped with all real faults to evaluate their effectiveness on fault localization. 
Results:
 We observe that the localization effectiveness varies considerably among three 
neural networks
 in the context of real faults. Specifically, convolutional 
neural network
 performs the best in locating real faults, showing an average of 38.97% and 26.22% saving over multi-layer 
perceptron
 and 
recurrent neural network
 respectively; 
recurrent neural network
 and multi-layer 
perceptron
 yield comparable effectiveness even if the effectiveness of 
recurrent neural network
 is marginally higher than multi-layer 
perceptron
. 
Conclusion:
 In context of real faults, 
convolutional neural network
 is the most effective for fault localization among the investigated architectures, and we suggest potential factors of deep learning for improving fault localization.",21 Mar 2025,6,"The study on fault localization using deep learning is relevant for startups dealing with software development, but the focus on deep learning architectures may limit the practical application for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302196,Method-level bug localization using hybrid multi-objective search,March 2021,Information and Software Technology,Not Found,Rafi=Almhana: ralmhana@umich.edu; Marouane=Kessentini: marouane@umich.edu; Wiem=Mkaouer: mwmvse@rit.edu,"Abstract
Context:
 One of the time-consuming maintenance tasks is the localization of bugs especially in large software systems. Developers have to follow a tedious process to reproduce the abnormal behavior then inspect a large number of files. While several studies have been proposed for bugs localization, the majority of them are recommending classes/files as outputs which may still require high inspection effort. Furthermore, there is a significant difference between the natural language used in 
bug reports
 and the programming language which limits the efficiency of existing approaches since most of them are mainly based on lexical similarity.
Objective:
 In this paper, we propose an automated approach to find and rank the potential methods in order to localize the source of a bug based on a bug report description.
Method:
 Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a hybrid multi-objective 
optimization algorithm
 combining local and global search. The relevance of the recommended code fragments is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. Our approach operates on two main steps. The first step is to find the best set of classes satisfying the two conflicting criteria of relevance and the number of classes to recommend using a global search based on NSGA-II. The second step is to locate the most appropriate methods to inspect, using a local multi-objective search based on Simulated Annealing (MOSA) from the list of classes recommended by the first step.
Results:
 We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many 
bug reports
. Our hybrid multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 78% of the bug reports leading to a significant reduction of developers’ effort comparing to class-level bug localization techniques.
Conclusion:
 The experimental results show that the search-based approach significantly outperforms four state-of-the-art methods in recommending relevant files for bug reports.",21 Mar 2025,8,"The automated approach proposed for bug localization in large software systems can significantly reduce developers' effort, making it highly practical and impactful for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302391,A risk prediction model for software project management based on similarity analysis of context histories,March 2021,Information and Software Technology,Not Found,Alexsandro Souza=Filippetto: alexsandrofilippetto@gmail.com; Robson=Lima: robsonklima@gmail.com; Jorge Luis Victória=Barbosa: jbarbosa@unisinos.br,"Abstract
Context
Risk event management has become strategic in Project Management, where uncertainties are inevitable. In this sense, the use of concepts of ubiquitous computing, such as contexts, context histories, and mobile computing can assist in proactive project management.
Objective
This paper proposes a computational model for the reduction of the probability of project failure through the prediction of risks. The purpose of the study is to show a model to assist teams to identify and monitor risks at different points in the life cycle of projects. The work presents as scientific contribution to the use of context histories to infer the recommendation of risks to new projects.
Method
The research conducted a case study in a software development company. The study was applied in two scenarios. The first involved two teams that assessed the use of the prototype during the implementation of 5 projects. The second scenario considered 17 completed projects to assess the recommendations made by the Átropos model comparing the recommendations with the risks in the original projects. In this scenario, Átropos used 70% of each project's execution as learning for the recommendations of risks generated to the same projects. Thus, the scenario aimed to assess whether the recommended risks are contained in the remaining 30% of the executed projects. We used as context histories, a database with 153 software projects from a financial company.
Results
A project team with 18 professionals assessed the recommendations, obtaining a result of 73% acceptance and 83% accuracy when compared to projects already being executed. The results demonstrated a high percentage of acceptance of the recommendation of risks compared to the other models that do not use the characteristics and similarities of projects.
Conclusion
The results show the applicability of the risk recommendation to new projects, based on the similarity analysis of context histories. This study applies inferences on context histories in the development and planning of projects, focusing on risk recommendation. Thus, with recommendations considering the characteristics of each new project, the manager starts with a larger set of information to make more assertive project planning.",21 Mar 2025,7,"The computational model for risk prediction in project management can provide valuable insights for startups in managing uncertainties, but the specific context of project management may limit its immediate applicability."
https://www.sciencedirect.com/science/article/pii/S0950584920302263,On deriving conceptual models from user requirements: An empirical study,March 2021,Information and Software Technology,"Requirements engineering, Conceptual modeling, Use cases, User stories, Derivation process",Fabiano=Dalpiaz: f.dalpiaz@uu.nl; Patrizia=Gieske: Not Found; Arnon=Sturm: Not Found,"Abstract
Context:
 There are numerous textual notations and techniques that can be used in requirements engineering. Currently, practitioners make a choice without having scientific evidence regarding their suitability for given tasks. This uninformed choice may affect task performance. 
Objective:
 In this research, we investigate the adequacy of two well-known notations: use cases and user stories, as a starting point for the manual derivation of a structural conceptual model that represents the domain of the system. We also examine other factors that may affect the performance of this task. 
Methods:
 This work relies on two experiments. The first is a controlled classroom experiment. The second one is a quasi-experiment, conducted over multiple weeks, that aims at evaluating the quality of the derived conceptual model in light of the notation used, the adopted derivation process, and the complexity of the system to be. We 
measure quality
 in terms of validity and completeness of the conceptual model. 
Results:
 The results of the controlled experiment indicate that, for deriving conceptual models, user stories fit better than use cases. Yet, the second experiment indicates that the quality of the derived conceptual models is affected mainly by the derivation process and by the complexity of the case rather than the notation used. 
Contribution:
 We present evidence that the task of deriving a conceptual model is affected significantly by additional factors other than requirements notations. Furthermore, we propose implications and hypotheses that pave the way for further studies that compare alternative notations for the same task as well as for other tasks. Practitioners may use our findings to analyze the factors that affect the quality of the conceptual model when choosing a requirements notation and an 
elicitation
 technique that best fit their needs.",21 Mar 2025,5,"The investigation on the adequacy of use cases and user stories in requirements engineering is valuable, but the focus on manual derivation of conceptual models may not have a direct impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920302251,Improving requirements specification use by transferring attention with eye tracking data,March 2021,Information and Software Technology,Not Found,Maike=Ahrens: maike.ahrens@inf.uni-hannover.de; Kurt=Schneider: kurt.schneider@inf.uni-hannover.de,"Abstract
Context
Software requirements specifications are the main point of reference in traditional software projects. Especially in large projects, these documents get read by multiple people, multiple times. Several guidelines and templates already exist to support writing a 
good specification
. However, not much research has been done in investigating how to support the use of specifications and help readers to find relevant information and navigate in the document more efficiently.
Objective
We aim to ease the reading process of requirements specifications by making use of previously recorded attention data. Therefore, we created three different attention transfer features based on eye tracking data obtained from observing readers when using specifications.
Method
In a student experiment, we evaluated if these attention visualizations positively affect the roles software architect, UI-designer and tester when reading a specification for the first time.
Results
The results show that the attention visualizations did not decrease navigation effort, but helped to draw the readers’ attention towards highlighted parts and decreased the average time spent on pages. They were mostly perceived as valuable by the readers.
Conclusions
We explored and evaluated the approach of visualizing other readers’ 
attention focus
 to help support new readers. Our results include interesting findings on what works well, what does not and what could be enhanced. We present several suggestions on how attention data could be used to fasten document navigation, direct reading and facilitate user-specific reading.",21 Mar 2025,7,"The use of attention visualizations based on eye tracking data to support reading software requirements specifications can enhance document navigation and reader attention, providing practical value for startups in the software development space."
https://www.sciencedirect.com/science/article/pii/S0950584920301002,Runtime testing of context-aware variability in adaptive systems,March 2021,Information and Software Technology,Not Found,Erick Barros dos=Santos: erickbarros@great.ufc.br; Rossana M.C.=Andrade: rossana@great.ufc.br; Ismayle de Sousa=Santos: ismaylesantos@great.ufc.br,"Abstract
Context:
 A Dynamically Adaptive System (DAS) supports runtime adaptations to handle changes in the 
operational environment
. These adaptations can change the system’s structure or behavior and even the logic of its adaptation mechanism. However, these adaptations may insert defects, leading the system to fail at runtime.
Objective:
 Aiming to identify these failures, testing can be executed to verify the system at runtime. Studies in the literature mostly focus on testing to verify the adaptations at design-time or functionalities at runtime, rather than exercising the adaptation mechanism at runtime. So, we propose RETAkE (RuntimE Testing of dynamically Adaptive systEms).
Method:
 RETAkE is an approach to perform the runtime testing based on the system’s context variability and feature modeling. RETAkE tests the adaptation mechanism, enabling the verification of its adaptation rules with the system’s variability model. The runtime testing is supported by the verification of behavioral properties. For the evaluation, we used the mutation testing technique with two DAS. We also conducted an evaluation to measure the overhead introduced when RETAkE is integrated to the DAS.
Results:
 RETAkE identified the mutants in the two mobile DAS, but the results vary due to the probabilistic nature of the approach to generate test sequences. Regarding the overhead, test sequences of size 30 had a low impact. However, bigger test sequences increase the overhead.
Conclusion:
 The integration of RETAkE to the DAS adaptation mechanism can support the discovery of adaptation failures that occur at runtime. Furthermore, the results of the evaluation suggest its feasibility to perform runtime testing.",21 Mar 2025,7,"The proposed RETAkE approach for runtime testing of dynamically adaptive systems can help in identifying adaptation failures, which is crucial for system reliability. The evaluation results suggest its feasibility, making it a valuable contribution to early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920301610,Quality Assessment in Systematic Literature Reviews: A Software Engineering Perspective,February 2021,Information and Software Technology,Not Found,Lanxin=Yang: yang931001@outlook.com; He=Zhang: hezhang@nju.edu.cn; Haifeng=Shen: Haifeng.Shen@acu.edu.au; Xin=Huang: njuhuangx@outlook.com; Xin=Zhou: job@wetist.com; Guoping=Rong: ronggp@nju.edu.cn; Dong=Shao: dongshao@nju.edu.cn,"Abstract
Context
: Quality Assessment (QA) of reviewed literature is paramount to a 
Systematic Literature Review
 (SLR) as the quality of conclusions completely depends on the quality of selected literature. A number of researchers in 
Software Engineering
 (SE) have developed a variety of QA instruments and also reported their challenges. We previously conducted a tertiary study on SLRs with QA from 2004 to 2013, and reported the findings in 2015.
Objective
: With the widespread use of SLRs in SE and the increasing adoption of QA in these SLRs in recent years, it is necessary to empirically investigate whether the previous conclusions are still valid and whether there are new insights to the subject in question using a larger and a more up-to-date SLR set. More importantly, we aim to depict a clear picture of QA used in SLRs in SE by aggregating and distilling good practices, including the commonly used QA instruments as well as the major roles and aspects of QA in research.
Method
: An extended tertiary study was conducted with the newly collected SLRs from 2014 to 2018 and the original SLRs from 2004 to 2013 to systematically review the QA used by SLRs in SE during the 15-year period from 2004 to 2018. In addition, this extended study also compared and contrasted the findings of the previous study conducted in 2015.
Results
: A total of 241 SLRs between 2004 and 2018 were included, from which we identified a number of QA instruments. These instruments are generally designed to focus on the rationality of study design, the rigor of study execution and analysis, and the credibility and contribution of study findings and conclusions, with the emphasis largely placed on its rigor. The quality data is mainly used for literature selection or as evidence to support conclusions.
Conclusions
: QA has received much attention in SE in more recent years and the improvement is evident since the last study in 2015. New findings show that the aims are more concise, the instruments are more diverse and rigorous, and the criteria are more thoughtful.",21 Mar 2025,6,"The extended study on Quality Assessment in Systematic Literature Reviews shows an improvement in QA practices in Software Engineering. While valuable for researchers, the direct practical impact on early-stage ventures may not be significant."
https://www.sciencedirect.com/science/article/pii/S0950584920301968,An empirical study of performance using Clone & Own and Software Product Lines in an industrial context,February 2021,Information and Software Technology,Not Found,Jorge=Echeverría: jecheverria@usj.es; Francisca=Pérez: mfperez@usj.es; José Ignacio=Panach: joigpana@uv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Clone and Own (CaO) is a widespread approach to generate new software products from existing software products by adding small changes. The Software Product Line (SPL) approach addresses the development of families of products with similar features, moving away from the production of isolated products. Despite the popularity of both approaches, no experiment has yet compared them directly.
Objective:
The goal of this paper is to know the different performances of software engineers in the software 
products development process
 using two different approaches (SPL and CaO).
Method:
We conducted an experiment in the induction hobs software environment with software engineers. This experiment is a single factor experiment where the factor is the approach that is used to develop software products, with two treatments: (SPL or CaO). We compared the results obtained by the software engineers when they develop software products related to effectiveness, efficiency, and satisfaction.
Results:
The findings show that: (1) the SPL approach is more efficient even though the number of checking actions required by this approach is greater than the number required by the CaO approach; (2) the SPL approach offers more possibilities than software engineers need to perform their daily tasks; and (3) software engineers require better search capabilities in the CaO approach. The possible explanations for these results are presented in the paper.
Conclusions:
The results show that there are significant differences in effectiveness, efficiency, and satisfaction, with the SPL approach yielding the best results.",21 Mar 2025,8,"The comparison between the Software Product Line and Clone and Own approaches for software development shows significant differences in effectiveness, efficiency, and satisfaction. This study provides practical insights that can benefit early-stage ventures in making informed decisions."
https://www.sciencedirect.com/science/article/pii/S0950584920301737,Boundary sampling to boost mutation testing for deep learning models,February 2021,Information and Software Technology,Not Found,Weijun=Shen: Not Found; Yanhui=Li: yanhuili@nju.edu.cn; Yuanlei=Han: Not Found; Lin=Chen: Not Found; Di=Wu: Not Found; Yuming=Zhou: Not Found; Baowen=Xu: Not Found,"Abstract
Context: The prevalent application of 
Deep Learning
 (DL) models has raised concerns about their reliability. Due to the data-driven programming paradigm, the quality of 
test datasets
 is extremely important to gain accurate assessment of 
DL
 models. Recently, researchers have introduced mutation testing into 
DL
 testing, which applies 
mutation operators
 to generate mutants from DL models, and observes whether the 
test data
 can identify mutants to check the quality of test dataset. However, there still exist many factors (e.g., huge labeling efforts and high running cost) hindering the implementation of mutation testing for DL models.
Objective: We desire for an approach to selecting a smaller, sensitive, representative and efficient subset of the whole test dataset to promote the current mutation testing (e.g., reduce labeling and running cost) for DL Models.
Method: We propose boundary sample selection (BSS), which employs the distance of samples to decision boundary of DL models as the indicator to construct the appropriate subset. To evaluate the performance of BSS, we conduct an extensive empirical study with two widely-used datasets, three popular DL models, and 14 up-to-date DL 
mutation operators
. Results
: We observe that (1) The sizes of our subsets generated by BSS are much smaller (about 3%-20% of the whole test set). (2) Under most 
mutation operators
, our subsets are superior (about 9.94-21.63) than the whole test sets in observing mutation effects. (3) Our subsets could replace the whole test sets to a very high degree (higher than 97%) when considering mutation score. (4) The MRR values of our proposed subsets are clearly better (about 2.28-13.19 times higher) than that of the whole test sets.
Conclusions: The result shows that BSS can help testers save labelling cost, run mutation testing quickly and identify killed mutants early.",21 Mar 2025,9,"The BSS approach for selecting a smaller, sensitive, and efficient subset of test datasets for mutation testing of DL models is highly impactful. It addresses key challenges and provides practical benefits that can be crucial for the success of early-stage ventures utilizing DL models."
https://www.sciencedirect.com/science/article/pii/S0950584920301713,Laprob: A Label propagation-Based software bug localization method,February 2021,Information and Software Technology,Not Found,Zhengliang=Li: lzl@smail.nju.edu.cn; Zhiwei=Jiang: jiangzhiwei@outlook.com; Xiang=Chen: xchencs@ntu.edu.cn; Kaibo=Cao: imkbcao@gmail.com; Qing=Gu: guq@nju.edu.cn,"Abstract
Context
Bug localization, which locates suspicious snippets related to the bugs mentioned in the 
bug reports
, is time-consuming and laborious. Many automatic bug 
localization methods
 have been proposed to speed up the process of bug fixing and reduce the burden on developers. However, these methods have not fully utilized the intra-relations and inter-relations among the 
bug reports
 and the source files (i.e., call relationships between the source files).
Objective
In this paper, we propose a novel method LaProb (a label propagation-based software bug localization method) that makes full use of the intra-relations and inter-relations among the bug reports and the source files.
Method
LaProb transforms the problem of bug localization into a multi-label distribution learning problem. LaProb first constructs a BHG (Biparty Hybrid Graph) by analyzing the structures and contents of bug reports and source files, and calculates the intra-relations between pairs of bug reports and source files, as well as the inter-relations between bug reports and source files. Based on BHG, LaProb then predicts the label distribution on source files by using the label 
propagation algorithm
 for the target bug report. Finally, LaProb finishes the bug localization task by sorting the results of label propagation.
Results
The experimental results on nine open-source software projects (i.e., SWT, AspectJ, Eclipse, ZXing, SEC, HIVE, HBASE, WFLY and ROO) show that compared with several state-of-the-art methods (including BugLocator, BRTracer, BLUiR, AmaLgam, Locus and BLIZZARD), LaProb performs the best in terms of all five metrics on average. For 
MAP
 
performance measure
, LaProb achieves an improvement of 30.9%, 36.6%, 28.0%, 22.2%, 20.1% and 53.5%, respectively.
Conclusion
LaProb is capable of making full use of the intra-relations and inter-relations among the bug reports and the source files and achieves better performance than seven state-of-the-art methods.",21 Mar 2025,8,The LaProb method for bug localization utilizing intra and inter-relations among bug reports and source files outperforms state-of-the-art methods. This can greatly benefit early-stage ventures by speeding up bug fixing processes and reducing developer burden.
https://www.sciencedirect.com/science/article/pii/S0950584920302007,Community detection in software ecosystem by comprehensively evaluating developer cooperation intensity,February 2021,Information and Software Technology,Not Found,Tingting=Hou: Not Found; Xiangjuan=Yao: yaoxj@cumt.edu.cn; Dunwei=Gong: Not Found,"Abstract
Context
: As soon as the concept of software ecosystem was proposed, it has aroused great interest in both academia and industry. Software ecosystem can be described as a special complex network. Community structures are critical towards understanding not only the 
network topology
 but also how the network functions. Traditional community 
detection algorithms
 in complex networks mainly utilize the 
network topology
 to measure the similarities between nodes. Because of the complexity of information interaction in software ecosystem, only considering the topology structure will lead to unreasonable division of communities.
Objective
: For solving community detection in software ecosystem more reasonably, we present a method of community detection by comprehensively evaluating developer cooperation intensity in software ecosystems.
Method
: First, we combine network topology information and developer interaction information to calculate the developer cooperation intensity, so as to deeply explore the relationship between developers from both topological and semantic properties. Then a community detection algorithm ABDCI is proposed based on the cooperation intensity of developers by referring to the 
hierarchical clustering
 idea of Louvain algorithm. Finally, this method is applied to many different types of developer networks in the software ecosystem through GitHub hosting platform.
Results
: Comparing with three classical community 
detection algorithms
, we find that the proposed method can identify a clearer community structure for the developer collaboration network in the software ecosystem.
Conclusion
: Our approach provides an effective and extensible technique for solving the community detection problem of real developer collaboration network in software ecosystem. According to our findings, we conclude that community detection algorithms based on comprehensive topological properties and semantic properties are more suitable for real communities in software ecosystems than traditional single-property algorithms.",21 Mar 2025,8,"The method presented offers a more reasonable approach to community detection in software ecosystems, which can be beneficial for startups looking to understand developer collaboration networks."
https://www.sciencedirect.com/science/article/pii/S095058492030197X,Test data generation using genetic programming,February 2021,Information and Software Technology,Not Found,M.=Nosrati: mohammad.nosrati@gmail.com; H.=Haghighi: h_haghighi@sbu.ac.ir; M.=Vahidi Asl: m.vahidi.asl@gmail.com,"Abstract
Context:
Typically, search-based test data generation methods search on a population of program input values. Program input values can be regarded as solutions to underlying path constraints over program input parameters. One way to discover these path constraints is to use the symbolic execution method. Search-based methods attempt to find input values which are solutions to these path constraints, without knowing the actual constraints.
Objective:
In this paper, we show that we can search for the underlying path constraints using search-based methods, without resorting to symbolic execution. Trying to discover the exact or a good enough 
approximation
 of the underlying constraints may lead to a more targeted search, compared to directly searching for program input values. Besides, the construction of approximate constraints by searching may help to avoid some problems of symbolic execution.
Method:
The proposed method uses 
genetic programming
 for 
learning constraints
 on program input parameters.
Results:
To evaluate the performance of the proposed approach, a series of experiments have been conducted on a number of different 
benchmark programs
. For 91.8% of 
benchmark programs
, the proposed method achieved the best efficiency among the competitive algorithms.
Conclusion:
The results show that, if constraint solving can be provided for some or all parameter types of the methods of programs under test, our approach can improve the efficiency and effectiveness of search-based test data generation.",21 Mar 2025,7,"The proposed method can improve the efficiency of search-based test data generation in software programs, which can be valuable for early-stage ventures in optimizing their testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584920301956,Social network analysis of open source software: A review and categorisation,February 2021,Information and Software Technology,Not Found,Kelvin=McClean: Not Found; Des=Greer: des.greer@qub.ac.uk; Anna=Jurek-Loughrey: Not Found,"Abstract
Context:
 As companies have become large users of 
Open Source Software
, it is important that they feel comfortable in their Open Source strategies. One of the critical differences between Open Source and Proprietary Software is the communication networks.
Objective:
 This paper tries to set a base for understanding how open source teams are structured and how they change. This is vital to understanding 
Open Source Software
 Communities.
Method:
 The paper looks into previous research on 
Social Network Analysis
 of Open Source Software, using a systematic literature review. Papers were gathered from Scopus, IEEEXplore and ACM Digital Library, and used or discarded based on predetermined inclusion and exclusion criteria. Research which focuses on the 
success factors
 of Open Source Software through Network Analysis is also examined.
Results:
 A subjective categorisation is established for the papers: Structure, Lifecycle and Communication. It was found that the structure of a project has a large bearing on project success, with developers having previously worked together being indicative of project success. Other structure indicators of success are having a small but structured hierarchy, a diverse user and developer base, and project prominence. However, it was found that information on how these structures appear and evolve over time is lacking, and future research into temporal data models to determine project success information is suggested.
Conclusions:
 A categorisation of existing research on 
Social Network Analysis
 is provided as a basis for further research. Further work into the lifecycle of 
OSS projects
 through Social Network Analysis of temporal project information is suggested.",21 Mar 2025,6,"Understanding the structure and evolution of open-source software teams can provide valuable insights for startups leveraging OSS strategies, though the impact on early-stage ventures may be more indirect."
https://www.sciencedirect.com/science/article/pii/S0950584920302020,Towards a unified criteria model for usability evaluation in the context of open source software based on a fuzzy Delphi method,February 2021,Information and Software Technology,Not Found,Kareem A.=Dawood: Not Found; Khaironi Y.=Sharif: Not Found; Abdul A.=Ghani: Not Found; H.=Zulzalil: Not Found; A.A.=Zaidan: aws.alaa@fskik.upsi.edu.my; B.B.=Zaidan: Not Found,"Abstract
Context
A plethora of models are available for open-source software (OSS) 
usability evaluation
. However, these models lack consensus between scholars as well as standard bodies on a specific set of 
usability evaluation
 criteria. Retaining irrelevant criteria and omitting essential ones will mislead the direction of the usability evaluation.
Objective
This study introduces a three-step method to develop a usability evaluation model in the context of OSS.
Method
The fuzzy Delphi method has been employed to unify the usability evaluation criteria in the context of OSS. The first step in the method is the usability criteria analysis, which involves redefining and restructuring all collected usability criteria reported in the literature. The second step is fuzzy Delphi analysis, which includes the design and validates the fuzzy Delphi instrument and the utilisation of the fuzzy Delphi method to analyse the fuzziness consensus of experts' opinions on the usability evaluation criteria. The third step is the proposal of the OSS usability evaluation model.
Results
A total of 124 usability criteria were identified, redefined, and restructured by creating groups of related meaning criteria. The result of the groupings generated 11 main criteria; the findings of the fuzzy Delphi narrowed down the criteria to only seven. The final set of criteria was sent back to the panellists for 
reconsideration
 of their responses. The panellists verified that these criteria are suitable in the evaluation of the usability of OSS.
Discussion
The empirical analysis confirmed that the proposed evaluation model is acceptable in assessing the usability of OSS. Therefore, this model can be used as a reference metric for OSS usability evaluation which will have a practical benefit for the community in public and private organisations in helping the decision-maker to select the best OSS software package amongst the alternatives.",21 Mar 2025,9,"The usability evaluation model proposed can be a practical benefit for startups in selecting the best OSS software package, improving decision-making processes related to software usability."
https://www.sciencedirect.com/science/article/pii/S0950584920302032,Two-level clustering of UML class diagrams based on semantics and structure,February 2021,Information and Software Technology,Not Found,Zongmin=Ma: Not Found; Zhongchen=Yuan: yuanzhongchen@163.com; Li=Yan: Not Found,"Abstract
Context
The reuse of 
software design
 has been an important issue of 
software reuse
. 
UML
 
class diagrams
 are widely applied in 
software design
 and has become DE factor standard. As a result, the reuse of 
UML
 
class diagrams
 has received more attention. With the increasing number of class diagrams stored in reuse repository, their retrieval becomes a time-consuming job. The clustering can narrow down retrieval range and improve the retrieval efficiency. But few efforts have been done in clustering 
UML
 class diagrams. This paper tries to propose a 
clustering approach
 for 
UML
 class diagrams.
Objective
This paper proposes a two-level clustering of UML class diagrams, namely, semantic clustering and structural clustering. The UML class diagrams stored in reuse repository are clustered into a few domains based on semantics in the first level and a few categories based on structure in the second level.
Method
We propose a 
clustering algorithm
 named 
CUFS
, in which the idea of partitioning and 
hierarchical clustering
 is combined and feature similarity is proposed for the similarity measure between two clusters in order to merge clusters. A better feature representation of a cluster, namely, feature class diagram, is proposed in this paper. In order to form each sub-cluster, the semantic and 
structural similarities
 between UML class diagrams are defined, respectively.
Results
A series of experimental results show that, the proposed feature similarity measure not only speeds up the 
clustering process
, but also expresses the closeness degree between clusters for merging clusters. The proposed algorithm shows a good 
clustering quality
 and efficiency under the condition of different size and distribution of UML class diagrams.
Conclusion
It is concluded that the proposed two-level 
clustering method
 considers both semantics and structure contained in a class diagram, which can flexibly adapt to different clustering requirements. Also, the proposed 
clustering algorithm
 performs better than other related algorithms, regardless of in semantic, structural and hybrid clustering.",21 Mar 2025,7,"The proposed clustering approach for UML class diagrams can help startups in efficiently retrieving and reusing software designs, which can streamline development processes."
https://www.sciencedirect.com/science/article/pii/S0950584920301944,Revisiting heterogeneous defect prediction methods: How far are we?,February 2021,Information and Software Technology,Not Found,Xiang=Chen: xchencs@ntu.edu.cn; Yanzhou=Mu: myz_2019218009@tju.edu.cn; Ke=Liu: l51415370@163.com; Zhanqi=Cui: czq@bistu.edu.cn; Chao=Ni: jacknichao920209@gmail.com,"Abstract
Context:
 Cross-project 
defect prediction
 applies to the scenarios that the target projects are new projects. Most of the previous studies tried to utilize the 
training data
 from other projects (i.e., the source projects). However, metrics used by practitioners to measure the extracted program modules from different projects may not be the same, and performing heterogeneous 
defect prediction
 (HDP) is challenging.
Objective:
 Researchers have proposed many novel HDP methods with promising performance until now. Recently, unsupervised defect prediction (UDP) methods have received more attention and show competitive performance. However, to our best knowledge, whether HDP methods can perform significantly better than UDP methods has not yet been thoroughly investigated.
Method:
 In this article, we perform a comparative study to have a holistic look at this issue. Specifically, we compare five HDP methods with four UDP methods on 34 projects in five groups under the same experimental setup from three different perspectives: non-effort-aware performance indicators (NPIs), effort-aware performance indicators (EPIs) and diversity analysis on identifying defective modules.
Result:
 We have the following findings: (1) HDP methods do not perform significantly better than some of UDP methods in terms of two NPIs and four EPIs. (2) According to two satisfactory criteria recommended by previous studies, the satisfactory ratio of existing HDP methods is pessimistic. (3) The diversity of prediction for defective modules across HDP 
vs
. UDP methods is more than that within HDP methods or UDP methods.
Conclusion:
 The above findings implicate there is still a long way for the HDP issue to go. Given this, we present some observations about the road ahead for HDP.",21 Mar 2025,6,"The study on defect prediction methods has practical implications for software development, especially for early-stage ventures looking to improve their product quality."
https://www.sciencedirect.com/science/article/pii/S0950584920300033,Large-scale intent analysis for identifying large-review-effort code changes,February 2021,Information and Software Technology,Not Found,Song=Wang: wangsong@eecs.yorku.ca; Chetan=Bansal: chetanb@microsoft.com; Nachiappan=Nagappan: nachin@microsoft.com,"Abstract
Context
: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and 
code refactoring
. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis.
Objective
: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (
LRE
) changes—changes with large review effort.
Method
: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying 
LRE
 changes. We conduct our study on four large-scale projects, one from 
Microsoft
 and three are 
open source projects
, i.e., Qt, 
Android
, and OpenStack.
Results
: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) 
machine learning
 based prediction models are applicable for identifying 
LRE
 changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average.
Conclusion
: The change intent analysis and its application on LRE identification proposed in this study has already been used in 
Microsoft
 to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a 
case study
 of developing and deploying the intent analysis system in 
Microsoft
. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value.",21 Mar 2025,8,The study on change intents and review effort identification has direct practical value for startups to streamline their code review processes and improve efficiency.
https://www.sciencedirect.com/science/article/pii/S0950584920300021,RSTrace+: Reviewer suggestion using software artifact traceability graphs,February 2021,Information and Software Technology,Not Found,Emre=Sülün: emre.sulun@bilkent.edu.tr; Eray=Tüzün: eraytuzun@cs.bilkent.edu.tr; Uğur=Doğrusöz: ugur@cs.bilkent.edu.tr,"Abstract
Context:
Various types of artifacts (requirements, 
source code
, test cases, documents, etc.) are produced throughout the lifecycle of a software. These artifacts are connected with each other via 
traceability links
 that are stored in modern 
application lifecycle management
 repositories. Throughout the lifecycle of a software, various types of changes can arise in any one of these artifacts. It is important to review such changes to minimize their potential 
negative impacts
. To make sure the review is conducted properly, the reviewer(s) should be chosen appropriately.
Objective:
We previously introduced a novel approach, named RSTrace, to automatically recommend reviewers that are best suited based on their familiarity with a given artifact. In this study, we introduce an advanced version of RSTrace, named RSTrace+ that accounts for recency information of 
traceability links
 including practical tool support for GitHub.
Methods:
In this study, we conducted a series of experiments on finding the appropriate code reviewer(s) using RSTrace+ and provided a comparison with the other code reviewer recommendation approaches.
Results:
We had initially tested RSTrace+ on an 
open source project
 (Qt 3D Studio) and achieved a top-3 accuracy of 0.89 with an MRR (mean reciprocal ranking) of 0.81. In a further empirical evaluation of 40 open source projects, we compared RSTrace+ with Naive-Bayes, RevFinder and Profile based approach, and observed higher accuracies on the average.
Conclusion:
We confirmed that the proposed reviewer recommendation approach yields promising top-k and MRR scores on the average compared to the existing reviewer recommendation approaches. Unlike other code reviewer recommendation approaches, RSTrace+ is not limited to recommending reviewers for 
source code
 artifacts and can potentially be used for recommending reviewers for other types of artifacts. Our approach can also visualize the affected artifacts and help the developer to make assessments of the potential impacts of change to the reviewed artifact.",21 Mar 2025,9,The RSTrace+ approach for recommending reviewers based on familiarity with artifacts can greatly benefit startups in improving their code review processes and overall software quality.
https://www.sciencedirect.com/science/article/pii/S0950584919302204,Performance analysis of out-of-distribution detection on trained neural networks,February 2021,Information and Software Technology,Not Found,Jens=Henriksson: jens.henriksson@semcon.com; Christian=Berger: christian.berger@gu.se; Markus=Borg: Not Found; Lars=Tornberg: Not Found; Sankar Raman=Sathyamoorthy: Not Found; Cristofer=Englund: Not Found,"Abstract
Context
Deep Neural Networks
 (DNN) have shown great promise in various domains, for example to support pattern recognition in medical imagery. However, DNNs need to be tested for robustness before being deployed in safety critical applications. One common challenge occurs when the model is exposed to data samples outside of the 
training data
 domain, which can yield to outputs with high confidence despite no prior knowledge of the given input.
Objective
The aim of this paper is to investigate how the performance of detecting out-of-distribution (OOD) samples changes for 
outlier detection
 methods (e.g., supervisors) when DNNs become better on 
training samples
.
Method
Supervisors are components aiming at detecting out-of-distribution samples for a DNN. The experimental setup in this work compares the performance of supervisors using metrics and datasets that reflect the most common setups in related works. Four different DNNs with three different supervisors are compared during different stages of training, to detect at what point during training the performance of the supervisors begins to deteriorate.
Results
Found that the 
outlier detection
 performance of the supervisors increased as the accuracy of the underlying DNN improved. However, all supervisors showed a large variation in performance, even for variations of network parameters that marginally changed the model accuracy. The results showed that understanding the relationship between training results and supervisor performance is crucial to improve a model’s robustness.
Conclusion
Analyzing DNNs for robustness is a challenging task. Results showed that variations in model parameters that have small variations on model predictions can have a large impact on the out-of-distribution detection performance. This kind of behavior needs to be addressed when DNNs are part of a safety critical application and hence, the necessary safety argumentation for such systems need be structured accordingly.",21 Mar 2025,7,"Analyzing DNNs for robustness is crucial for startups using AI technologies, as it impacts the reliability and safety of their applications while providing insights that can guide decision-making."
https://www.sciencedirect.com/science/article/pii/S0950584920301920,Software engineering and advanced applications conference 2019 – selected papers,February 2021,Information and Software Technology,Not Found,Rafael=Capilla: Not Found; Miroslaw=Staron: special_issue@staron.nu,"Abstract
Software Engineering and Advanced Applications (SEAA) is a long-standing international forum for researchers, practitioners, and students to present and discuss the latest innovations, trends, experiences, and concerns in the field of Software Engineering and Advanced Applications in information technology for software-intensive systems. In this special issue, we present a selection of papers which show the current trends in software engineering – improved systematic reviews, deep learning and cloud computing.",21 Mar 2025,5,"Although the SEAA forum is valuable for researchers and practitioners, the abstract does not provide direct, actionable insights for early-stage ventures or startups."
https://www.sciencedirect.com/science/article/pii/S0950584920300070,A comprehensive empirical evaluation of generating test suites for mobile applications with diversity,February 2021,Information and Software Technology,Not Found,Thomas=Vogel: thomas.vogel@informatik.hu-berlin.de; Chinh=Tran: mail@chinhtran.de; Lars=Grunske: grunske@informatik.hu-berlin.de,"Abstract
Context:
 In search-based 
software engineering
 we often use popular heuristics with 
default configurations
, which typically lead to suboptimal results, or we perform experiments to identify configurations on a trial-and-error basis, which may lead to better results for a specific problem. We consider the problem of generating test suites for mobile applications (apps) and rely on 
Sapienz
, a state-of-the-art approach to this problem that uses a popular heuristic (NSGA-II) with a default configuration. 
Objective:
 We want to achieve better results in generating test suites with 
Sapienz
 while avoiding trial-and-error experiments to identify a more suitable configuration of 
Sapienz
. 
Method:
 We conducted a fitness landscape analysis of 
Sapienz
 to analytically understand the search problem, which allowed us to make informed decisions about the heuristic and configuration of 
Sapienz
 when developing 
Sapienz
div
. We comprehensively evaluated 
Sapienz
div
 in a head-to-head comparison with 
Sapienz
 on 34 apps. 
Results:
 Analyzing the fitness landscape of 
Sapienz
, we observed a lack of diversity of the evolved test suites and a stagnation of the search after 25 generations. 
Sapienz
div
 realizes mechanisms that preserve the diversity of the test suites being evolved. The evaluation showed that 
Sapienz
div
 achieves better or at least similar test results than 
Sapienz
 concerning coverage and the number of revealed faults. However, 
Sapienz
div
 typically produces longer test sequences and requires more 
execution time
 than 
Sapienz
. 
Conclusions:
 The understanding of the search problem obtained by the fitness landscape analysis helped us to find a more suitable configuration of 
Sapienz
 without trial-and-error experiments. By promoting diversity of test suites during the search, improved or at least similar test results in terms of faults and coverage can be achieved.",21 Mar 2025,6,"The study provides insights into improving test suites generation for mobile applications, which can be valuable for early-stage ventures working on app development. However, the method may require more execution time."
https://www.sciencedirect.com/science/article/pii/S0950584920300069,A Systematic Comparison of search-Based approaches for LDA hyperparameter tuning,February 2021,Information and Software Technology,"Topic modeling, Latent dirichlet allocation, Search-based software engineering, Metaheuristic search, Duplicate bug report, Hyperparameter optimization",Annibale=Panichella: a.panichella@tudelft.nl,"Abstract
Context:
Latent Dirichlet Allocation
 (LDA) has been successfully used in the literature to extract topics from software documents and support developers in various 
software engineering
 tasks. While LDA has been mostly used with default settings, previous studies showed that default hyperparameter values generate sub-optimal topics from software documents.
Objective:
 Recent studies applied meta-heuristic search (mostly evolutionary algorithms) to configure LDA in an unsupervised and automated fashion. However, previous work advocated for different meta-heuristics and surrogate metrics to optimize. The objective of this paper is to shed light on the influence of these two factors when tuning LDA for SE tasks.
Method:
We empirically evaluated and compared seven state-of-the-art meta-heuristics and three alternative surrogate metrics (i.e., fitness functions) to solve the problem of identifying duplicate 
bug reports
 with LDA. The benchmark consists of ten real-world and open-source projects from the 
Bench4BL
 dataset.
Results:
Our results indicate that (1) meta-heuristics are mostly comparable to one another (except for random search and CMA-ES), and (2) the choice of the surrogate metric impacts the quality of the generated topics and the tuning overhead. Furthermore, calibrating LDA helps identify twice as many duplicates than untuned LDA when inspecting the top five past similar reports.
Conclusion:
No meta-heuristic and/or fitness function outperforms all the others, as advocated in prior studies. However, we can make recommendations for some combinations of meta-heuristics and fitness functions over others for practical use. Future work should focus on improving the surrogate metrics used to calibrate/tune LDA in an unsupervised fashion.",21 Mar 2025,5,"The research sheds light on tuning LDA for software engineering tasks, but the results indicate that no meta-heuristic or fitness function outperforms others significantly, which may limit practical value for startups."
https://www.sciencedirect.com/science/article/pii/S0950584920301580,Recommending tags for pull requests in GitHub,January 2021,Information and Software Technology,Not Found,Jing=Jiang: jiangjing@buaa.edu.cn; Qiudi=Wu: 1040814720@qq.com; Jin=Cao: 13277061183@163.com; Xin=Xia: xin.xia@monash.edu; Li=Zhang: lily@buaa.edu.cn,"Abstract
Context
In GitHub, contributors make code changes, then create and 
submit
 pull requests to projects. Tags are a simple and effective way to attach additional information to pull requests and facilitate their organization. However, little effort has been devoted to study pull requests’ tags in GitHub.
Objective
Our objective in this paper is to propose an approach which automatically recommends tags for pull requests in GitHub.
Method
We make a survey on the usage of tags in pull requests. Survey results show that tags are useful for developers to track, search or classify pull requests. But some respondents think that it is difficult to choose right tags and keep consistency of tags. 60.61% of respondents think that a 
tag recommendation
 tool is useful. In order to help developers choose tags, we propose a method FNNRec which uses feed-forward neural network to analyze titles, description, file paths and contributors.
Results
We evaluate the effectiveness of FNNRec on 10 projects containing 68,497 tagged pull requests. The experimental results show that on average, FNNRec outperforms approach TagDeepRec and TagMulRec by 62.985% and 24.953% in terms of 
F
1
−
s
c
o
r
e
@
3
,
 respectively.
Conclusion
FNNRec is useful to find appropriate tags and improve tag setting process in GitHub.",21 Mar 2025,8,The approach of automatically recommending tags for pull requests in GitHub can provide practical utility for startups to improve workflow efficiency. The results show significant improvement over existing approaches.
https://www.sciencedirect.com/science/article/pii/S0950584920301701,Integrated framework for incorporating sustainability design in software engineering life-cycle: An empirical study,January 2021,Information and Software Technology,Not Found,Theresia Ratih Dewi=Saputri: trdsaputri@ajou.ac.kr; Seok-Won=Lee: leesw@ajou.ac.kr,"Abstract
Context:
Owing to the critical role of software-intensive systems in society, software engineers have the accountability to consider sustainability as a goal while structuring a software system. However, there are no 
practical guidelines
 providing a tangible decomposition of the sustainability aspect. Moreover, there are limited quantifiable methods to support 
sustainable design
 and analysis.
Objectives:
The purpose of this study is to help software practitioners to take sustainability into account by providing systematic guidelines for the software 
engineering process
. We propose a framework that presents a meta model to decompose sustainability requirements and an assessment approach to evaluate sustainability achievements.
Method:
This work presents an integrated framework that combines a goal-based approach, scenario-based approach, and feature modeling to gather sustainability 
related requirements
 and corresponding features. For sustainability assessment, software analysis and 
machine learning techniques
 are utilized to analyze software products based on sustainability metrics and criteria.
Results and Conclusions:
The empirical study conducted with participants from academia and industry revealed that the proposed framework improves participant’s ability to consider sustainability aspect in their 
software engineering
 tasks through focusing on requirements, design, and evaluation. With the provided sustainability meta-model, the participants could extract more stakeholders, requirements, and features in shorter time. Moreover, the empirical study result also demonstrated that this study is capable to indicate specific scenarios that should be redesigned to improve the sustainability achievements level.",21 Mar 2025,7,Providing systematic guidelines for integrating sustainability into software engineering processes can be beneficial for startups looking to incorporate sustainability practices. The empirical study demonstrates improvement in participants' ability in sustainability considerations.
https://www.sciencedirect.com/science/article/pii/S0950584920301725,Architectural decision-making as a financial investment: An industrial case study,January 2021,Information and Software Technology,Not Found,Areti=Ampatzoglou: areti.ampatzoglou@rug.nl; Elvira-Maria=Arvanitou: e.arvanitou@uom.edu.gr; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Paris=Avgeriou: paris@cs.rug.nl; Angeliki-Agathi=Tsintzira: Not Found; Alexander=Chatzigeorgiou: achat@uom.edu.gr,"Abstract
Context
Making 
architectural decisions
 is a crucial task but also very difficult, considering the scope of the decisions and their impact on 
quality attributes
. To make matters worse, 
architectural decisions
 need to combine both technical and business factors, which are very dissimilar by nature.
Objectives
We provide a cost-benefit approach and supporting tooling that treats architectural decisions as financial investments by: (a) combining both technical and business factors; and (b) transforming the involved factors into currency, allowing their uniform aggregation. Apart from illustrating the method, we validate both the proposed approach and the tool, in terms of fitness for purpose, usability, and potential limitations.
Method
To validate the approach, we have performed a 
case study
 in a software development company, in the domain of low-energy embedded systems. We employed triangulation in the 
data collection phase
 of the 
case study
, by performing interviews, focus groups, an observational session, and questionnaires.
Results
The results of the study suggested that the proposed approach: (a) provides a structured process for systematizing decision-making; (b) enables the involvement of multiple stakeholders, distributing the decision-making responsibility to more knowledgeable people; (c) uses monetized representations that are important for assessing decisions in a unified manner; and (d) enables decision reuse and documentation.
Conclusions
The results of the study suggest that architectural decision-making can benefit from treating this activity as a financial investment. The various benefits that have been identified from mixing financial and technological aspects are well-accepted from 
industrial stakeholders
.",21 Mar 2025,7,"The cost-benefit approach to architectural decision-making provides a structured process and involvement of multiple stakeholders, which can be valuable for startups. The results suggest benefits from treating architectural decisions as financial investments."
https://www.sciencedirect.com/science/article/pii/S0950584920301749,Governance and Management of Green IT: A Multi-Case Study,January 2021,Information and Software Technology,Not Found,J. David=Patón-Romero: JoseDavid.Paton@gmail.com; Maria Teresa=Baldassarre: mariateresa.baldassarre@uniba.it; Moisés=Rodríguez: mrodriguez@aqclab.es; Per=Runeson: per.runeson@cs.lth.se; Martin=Höst: martin.host@cs.lth.se; Mario=Piattini: Mario.Piattini@uclm.es,"Abstract
Context
The changes that are taking place with respect to environmental sensitivity are forcing organizations to adopt a new approach to this problem. Implementing 
sustainability
 initiatives has become a priority for the social and environmental awareness of organizations that want to stay ahead of the curve. One of the business areas that has, more than others, proven to be a vital asset and a potential ally of the environment, is the area of Information Technology (IT). Through this area, Green IT practices advocate 
sustainability
 in and by IT. However, organizations have a significant handicap in this regard, due to the lack of specific Green IT standards and frameworks that help them carry out this type of sustainability practices.
Objective
We have developed the 
“Governance and Management Framework for Green IT”
 (GMGIT), which establishes the necessary characteristics to implement Green IT in organizations, from the point of view of the governance and management of this area. After developing and validating a first version of this framework, we have performed a set of improvements, obtaining the GMGIT 2.0, which we want to validate.
Method
We have conducted a series of empirical validations at international level based on 
case studies
, whose characteristics and results are presented in this study.
Results
The results of this multi-case study show an example of the current situation of organizations in Green IT, as well as the resolution of problems encountered during the validations conducted with the GMGIT 1.0.
Conclusion
The findings obtained demonstrate the usefulness, applicability, and validity of the framework when implementing, auditing, and improving Green IT in organizations in a systematic and progressive manner.",21 Mar 2025,5,"The development of a Governance and Management Framework for Green IT can have a positive impact on organizations' sustainability practices, but the practical value for early-stage ventures might be limited as it is more geared towards established organizations."
https://www.sciencedirect.com/science/article/pii/S0950584920301841,Statement frequency coverage: A code coverage criterion for assessing test suite effectiveness,January 2021,Information and Software Technology,Not Found,Alireza=Aghamohammadi: aaghamohammadi@ce.sharif.edu; Seyed-Hassan=Mirian-Hosseinabadi: hmirian@sharif.edu; Sajad=Jalali: sajalali@ce.sharif.edu,"Abstract
Context:
Software testing is a pivotal activity in the development of high-quality software. As software is evolving through its life cycle, the need for a fault-revealing criterion assessing the effectiveness of the test suite grows. Over the years, researchers have proposed coverage-based criteria, including statement and branch coverage, to solve this issue. In literature, the effectiveness of such criteria is attested in terms of their correlations with the mutation score.
Objective:
In this paper, we aim at proposing a coverage-based criterion named statement frequency coverage, which outperforms statement and branch coverage in terms of correlation with mutation score.
Method:
To this end, we incorporated the frequency of executed statements into 
statement coverage
 and created a coverage-based criterion for assessing test suite effectiveness. Statement frequency coverage assigns a continuous value to a statement whose value is proportional to the number of times executed during test execution. We evaluated our approach on 22 real-world Python projects with more than 118 000 source lines of code (without blank lines, comments, and test cases) and 21 000 test cases through measuring the correlation between statement frequency coverage and corresponding mutation score.
Results:
The results show that statement frequency coverage outperforms statement and branch coverage criteria. The correlation between it and the corresponding mutation score is higher than the correlation of statement and branch coverage with their mutation score. The results also show that unlike statement and branch coverage, there is no statistical difference between statement frequency coverage and mutation score.
Conclusion:
Statement frequency coverage is a better choice compared to statement and branch coverage in assessing test suite effectiveness in the real-world setting. Furthermore, we demonstrate that although statement frequency coverage subsumes 
statement coverage
, it is incomparable to branch coverage under the adequate test suite condition.",21 Mar 2025,9,"The proposal of a new coverage-based criterion for assessing test suite effectiveness in software testing can greatly benefit early-stage ventures, especially startups, by improving the quality of their software development process."
https://www.sciencedirect.com/science/article/pii/S0950584920301853,A unified framework for declarative debugging and testing,January 2021,Information and Software Technology,Not Found,Rafael=Caballero: rafacr@ucm.es; Enrique=Martin-Martin: emartinm@ucm.es; Salvador=Tamarit: stamarit@dsic.upv.es,"Abstract
Context:
Debugging is the most challenging and time consuming task in software development. However, it is not properly integrated in the software 
development cycle
, because the result of so much effort is not available in further iterations of the cycle, and the debugging process itself does not benefit from the outcome of other phases such as testing.
Objective:
We propose to integrate debugging and testing within a single unified framework where each phase generates useful information for the other and the outcomes of each phase are reused.
Method:
We consider a declarative debugging setting that employs tests to automatically entail the validity of some subcomputations, thus decreasing the time and effort needed to find a bug. Additionally, the 
debugger
 stores as new tests the information collected from the user during the debugging phase. This information becomes part of the program test suite, and can be used in future 
debugging sessions
, and also as 
regression tests
.
Results:
We define a general framework where declarative debugging establishes a bidirectional collaboration with testing. The new setting preserves the properties of the underlying declarative debugging framework (weak completeness and soundness) while generating test cases that can be used later in other 
debugging sessions
 or even in other cycles of the software development. The proposed framework is general enough to be instantiated to very different programming languages: Erlang (functional), Java (imperative, object-oriented), and 
SQL
 (data query); and the experimental results obtained for Erlang programs validate the effectiveness of the framework.
Conclusion:
We propose a general unified framework for debugging and testing that simplifies each phase and maximizes the 
reusability
 of the outcomes in the different phases of the software 
development cycle
, therefore reducing the overall effort.",21 Mar 2025,8,"The integration of debugging and testing within a unified framework can streamline the software development cycle, benefiting early-stage ventures in terms of time and effort savings, making it a valuable tool for startups."
https://www.sciencedirect.com/science/article/pii/S0950584920301464,The effectiveness of data augmentation in code readability classification,January 2021,Information and Software Technology,Not Found,Qing=Mi: miqing@bjut.edu.cn; Yan=Xiao: dcsxan@nus.edu.sg; Zhi=Cai: caiz@bjut.edu.cn; Xibin=Jia: jiaxibin@bjut.edu.cn,"Abstract
Context:
 Training 
deep learning
 models for code readability classification requires large datasets of quality pre-labeled data. However, it is almost always time-consuming and expensive to acquire readability data with manual labels.
Objective:
 We thus propose to introduce 
data augmentation
 approaches to artificially increase the size of training set, this is to reduce the risk of overfitting caused by the lack of readability data and further improve the 
classification accuracy
 as the ultimate goal.
Method:
 We create transformed versions of code snippets by manipulating original data from aspects such as comments, indentations, and names of classes/methods/variables based on domain-specific knowledge. In addition to basic transformations, we also explore the use of Auxiliary Classifier 
GANs
 to produce 
synthetic data
.
Results:
 To evaluate the proposed approach, we conduct a set of experiments. The results show that the classification performance of 
deep neural networks
 can be significantly improved when they are trained on the augmented corpus, achieving a state-of-the-art accuracy of 87.38%.
Conclusion:
We consider the findings of this study as primary evidence of the effectiveness of 
data augmentation
 in the field of code readability classification.",21 Mar 2025,7,The introduction of data augmentation approaches for deep learning models in code readability classification can be beneficial for startups looking to improve classification accuracy without requiring large amounts of manually labeled data.
https://www.sciencedirect.com/science/article/pii/S0950584920301877,What skills do IT companies look for in new developers? A study with Stack Overflow jobs,January 2021,Information and Software Technology,Not Found,João Eduardo=Montandon: joao.montandon@dcc.ufmg.br; Cristiano=Politowski: Not Found; Luciana Lourdes=Silva: Not Found; Marco Tulio=Valente: Not Found; Fabio=Petrillo: Not Found; Yann-Gaël=Guéhéneuc: Not Found,"Abstract
Context:
 There is a growing demand for information on how IT companies look for candidates to their open positions. 
Objective:
 This paper investigates which hard and soft skills are more required in IT companies by analyzing the description of 20,000 job opportunities. 
Method:
 We applied open card sorting to perform a high-level analysis on which types of hard skills are more requested. Further, we manually analyzed the most mentioned soft skills. 
Results:
 Programming languages are the most demanded hard skills. Communication, collaboration, and problem-solving are the most demanded soft skills. 
Conclusion:
 We recommend developers to organize their resumé according to the positions they are applying. We also highlight the importance of soft skills, as they appear in many job opportunities.",21 Mar 2025,6,"The investigation on the required hard and soft skills in IT companies can provide valuable insights for job seekers and developers, but the direct impact on early-stage ventures might be more indirect."
https://www.sciencedirect.com/science/article/pii/S0950584920301865,On the influence of model fragment properties on a machine learning-based approach for feature location,January 2021,Information and Software Technology,Not Found,Manuel=Ballarín: mballarin@usj.es; Ana C.=Marcén: acmarcen@usj.es; Vicente=Pelechano: pele@dsic.upv.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context:
Leveraging 
machine learning techniques
 to address feature location on models has been gaining attention. Machine learning techniques empower software product companies to take advantage of the knowledge and the experience to improve the performance of the feature location process. Most of the machine learning-based works for feature location on models report the 
machine learning techniques
 and the tuning parameters in detail. However, these works focus on the size and the distribution of the data sets, neglecting the properties of their contents.
Objective:
In this paper, we analyze the influence of three model fragment properties (density, multiplicity, and dispersion) on a machine learning-based approach for feature location.
Method:
The analysis of these properties is based on an industrial case provided by CAF, a worldwide provider of railway solutions. The test cases were evaluated through a 
machine learning
 technique that uses different subsets of a 
knowledge base
 to learn how to locate unknown features.
Results:
Results show that the density and dispersion properties have a direct impact on the results. In our 
case study
, the model fragments with extra-small density values achieve results with up to 43% more precision, 41% more recall, 42% more F-measure, and 0.53 more Matthews 
Correlation Coefficient
 (MCC) than the model fragments with other density values. On the other hand, the model fragments with extra-small and small dispersion values achieve results with up to 53% more precision, 52% more recall, 52% more F-measure, and 0.57 more MCC than the model fragments with other dispersion values.
Conclusions:
The analysis of the results shows that both density and dispersion properties significantly influence the results. These results can serve not only to improve the reports by means of the model fragment properties, but also to be able to compare machine learning-based feature location approaches fairly improving the feature location results.",21 Mar 2025,8,"The study provides valuable insights into how model fragment properties influence machine learning-based feature location, which can significantly improve feature location results for software product companies."
https://www.sciencedirect.com/science/article/pii/S0950584920301889,COSTE: Complexity-based OverSampling TEchnique to alleviate the class imbalance problem in software defect prediction,January 2021,Information and Software Technology,Not Found,Shuo=Feng: shuofeng5-c@my.cityu.edu.hk; Jacky=Keung: jacky.keung@cityu.edu.hk; Xiao=Yu: xyu224-c@my.cityu.edu.hk; Yan=Xiao: xiaoyan.hhu@gmail.com; Kwabena Ebo=Bennin: kwabena.bennin@wur.nl; Md Alamgir=Kabir: makabir4-c@my.cityu.edu.hk; Miao=Zhang: miazhang9-c@my.cityu.edu.hk,"Abstract
Context:
Generally, there are more non-defective instances than defective instances in the datasets used for 
software defect
 prediction (SDP), which is referred to as the 
class imbalance problem
. Oversampling techniques are frequently adopted to alleviate the problem by generating new synthetic defective instances. Existing techniques generate either near-duplicated instances which result in overgeneralization (high probability of false alarm, 
p
f
) or overly diverse instances which hurt the prediction model’s ability to find defects (resulting in low probability of detection, 
p
d
). Furthermore, when existing oversampling techniques are applied in SDP, the effort needed to inspect the instances with different complexity is not taken into consideration.
Objective:
In this study, we introduce Complexity-based OverSampling TEchnique (COSTE), a novel oversampling technique that can achieve low 
p
f
 and high 
p
d
 simultaneously. Meanwhile, COSTE also performs better in terms of 
N
o
r
m
(
p
o
p
t
)
 and 
A
C
C
, two effort-aware measures that consider the testing effort.
Method:
COSTE combines pairs of defective instances with similar complexity to generate synthetic instances, which improves the diversity within the data, maintains the ability of prediction models to find defects, and takes the different testing effort needed for different instances into consideration. We conduct experiments to compare COSTE with Synthetic Minority Oversampling TEchnique, Borderline-SMOTE, Majority Weighted Minority Oversampling TEchnique and MAHAKIL.
Results:
The experimental results on 23 releases of 10 projects show that COSTE greatly improves the diversity of the synthetic instances without compromising the ability of prediction models to find defects. In addition, COSTE outperforms the other oversampling techniques under the same testing effort. The statistical analysis indicates that COSTE’s ability to outperform the other oversampling techniques is significant under the statistical Wilcoxon rank sum test and Cliff’s effect size.
Conclusion:
COSTE is recommended as an efficient alternative to address the 
class imbalance problem
 in SDP.",21 Mar 2025,9,"The novel oversampling technique COSTE addresses the class imbalance problem in software defect prediction effectively, outperforming existing techniques and considering effort-aware measures, making it highly valuable for improving prediction models in SDP."
https://www.sciencedirect.com/science/article/pii/S0950584920301919,Evaluating the effects of similar-class combination on class integration test order generation,January 2021,Information and Software Technology,Not Found,Miao=Zhang: miazhang9-c@my.cityu.edu.hk; Jacky Wai=Keung: jacky.Keung@cityu.edu.hk; Yan=Xiao: dcsxan@nus.edu.sg; Md Alamgir=Kabir: makabir4-c@my.cityu.edu.hk,"Abstract
Context:
In integration testing, the order in which classes are integrated and tested is significant for the construction of test stubs. With the existing approaches, it is usually difficult to generate the sub-optimal test orders for real applications, which have large numbers of classes.
Objective:
There exist moderately large numbers of classes in software systems, which is one of the main factors that complicate the generation of class integration test order (CITO). The main objectives of this study are reducing the problem space for CITO generation, and minimizing the stubbing cost of the generated test orders.
Method:
The approach proposed in this study is based on the hypothesis that similar-class combination can remove class dependencies and yield a smaller problem space. Identical class dependence and symmetric classes are the two main properties that are used to identify similar classes. In addition, a new cycle-breaking algorithm is introduced to minimize the stubbing cost of the generated test orders, which fully considers the two factors (number of test stubs and the corresponding stubbing complexity) that affect the overall stubbing cost. Empirical experiments are conducted on nine open-source Java programs to evaluate the performance of the proposed approach.
Results:
With similar-class combination, the proposed approach reduced the numbers of classes and class dependencies by over 10% and 6%, respectively, for six programs. Moreover, for four programs, the proposed approach reduced the number of cycles among class dependencies by more than 20%. The cycle-breaking algorithm achieved reduction of more than 13% in the stubbing cost, thus outperforming other competing techniques.
Conclusions:
The proposed method relies on the two aforementioned important properties to identify similar classes, and these properties are known to significantly improve the performance of CITO generation. The results obtained in this study confirmed the capability of the proposed approach in terms of minimizing the number of classes and class dependencies in programs. It outperformed other competing methods in minimizing the stubbing costs of the generated test orders.",21 Mar 2025,7,"The proposed approach for class integration test order generation shows promising results in reducing class dependencies and stubbing costs in software systems, providing practical benefits for software testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584920301907,Archetypes of delay: An analysis of online developer conversations on delayed work items in IBM Jazz,January 2021,Information and Software Technology,Not Found,Abdoul-Djawadou=Salaou: adsalaou@unistra.fr; Daniela=Damian: Daniela.damian@uvic.ca; Casper=Lassenius: casper.lassenius@aalto.fi; Dragoş=Voda: dragos.voda@aalto.fi; Pierre=Gançarski: gancarski@unistra.fr,"Abstract
Context.
A widely adopted methodology, 
agile software development
 provides enhanced flexibility to actively adjust a project scope. In agile teams, particularly in distributed environment, developers interact, manage requirements knowledge, and coordinate primarily in online collaboration tools. Developer conversations become invaluable sources to track and understand developers’ interactions around implementation of requirements, as well as the progress of implementation relative to the project scope and the planned iterations in agile projects. Although extensive research around iteration planning exists, there is a lack of research that leverages developer conversation data to understand delays in implementing planned requirements in agile projects.
Objective.
By using developer conversations in a large agile project at IBM, this work aims to analyze conversation in work items (WIs) that are delayed and derive patterns that suggest reasons for delay in the project.
Method.
We conducted a 
case study
 of the IBM Jazz project, and used thematic analysis to code the developer conversations as time-series, and cluster analysis to identify patterns that differentiated the evolution of discussions in WIs that were late vs. not late in the project.
Results.
We identified six main patterns of WI delay. Through semantic analysis of developer conversations within particular clusters we were able to explain the reasons for delays in each pattern. In comparison to non-late WIs, we find that the major reason for delay is a lack of frequent communication associated with a poor project management of WIs. Similarly, non-late tasks more often delegate to children tasks to accelerate the implementation of requirements, in addition to processing requests quickly to resolve bottlenecks in implementation.
Conclusion.
Our study complements existing research in bringing evidence that developer conversations are a useful resource that can highlight delays in requirement implementation, as well as recommend patterns in the dynamics of developers interactions relevant to such delays.",21 Mar 2025,6,"The analysis of developer conversations in agile projects to understand delays in requirement implementation is insightful, but may have limited immediate application for early-stage ventures compared to the other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920301890,A Method to Estimate Software Strategic Indicators in Software Development: An Industrial Application,January 2021,Information and Software Technology,Not Found,Martí=Manzano: mmanzano@essi.upc.edu; Claudia=Ayala: Not Found; Cristina=Gómez: Not Found; Antonin=Abherve: Not Found; Xavier=Franch: Not Found; Emilia=Mendes: Not Found,"Abstract
Context
Exploiting software development related data from software-development intensive organizations to support tactical and strategic decision making is a challenge. Combining data-driven approaches with expert knowledge has been highlighted as a sensible approach for leading software-development intensive organizations to rightful decision-making improvements. However, most of the existing proposals lack of important aspects that hinders their industrial uptake such as: 
customization
 guidelines to fit the proposals to other contexts and/or automatic or semi-automatic data collection support for putting them forward in a real organization. As a result, existing proposals are rarely used in the industrial context.
Objective
Support software-development intensive organizations with guidance and tools for exploiting software development related data and expert knowledge to improve their decision making.
Method
We have developed a novel method called SESSI (Specification and Estimation of Software Strategic Indicators) that was articulated from industrial experiences with Nokia, Bittium, Softeam and iTTi in the context of Q-Rapids European project following a design science approach. As part of the industrial 
summative evaluation
, we performed the first 
case study
 focused on the application of the method.
Results
We detail the phases and steps of the SESSI method and illustrate its application in the development of ModelioNG, a software product of Modeliosoft development firm.
Conclusion
The application of the SESSI method in the context of ModelioNG 
case study
 has provided us with useful feedback to improve the method and has evidenced that applying the method was feasible in this context.",21 Mar 2025,6,"The SESSI method aims to support software-development intensive organizations with data and expert knowledge for decision-making, but lacks specific details on its impact on early-stage ventures or startups."
https://www.sciencedirect.com/science/article/pii/S0950584920301476,Exploring software bug-proneness based on evolutionary clique modeling and analysis,December 2020,Information and Software Technology,Not Found,Ran=Mo: moran@mail.ccnu.edu.cn; Zhen=Yin: yinzhen0906@126.com,"Abstract
Context:
Even if evolutionary coupling between files has been widely used for various studies, such as change impact analysis, 
defect prediction
, and 
software design
 analysis etc., there has little work focusing on studying the linkage among evolutionary coupled files.
Objective:
In this paper, we propose a novel model, 
evolutionary clique (EClique)
, to characterize evolutionary coupled files as maintainable groups for bug fixes, analyze their bug-proneness and examine the possible causes of the bug-proneness.
Methods:
To identify ECliques from a project, we propose two history measures to reason about the evolutionary coupling between files, and create a novel 
clustering algorithm
. Given the evolutionary coupling information, our 
clustering algorithm
 will automatically identify ECliques in a project.
Results:
We conduct analyses on 33,099 commits of ten 
open source projects
 to evaluate the usefulness of our 
EClique
 modeling and analysis approach: (1) The results show that files involved in an 
EClique
 are more likely to share similar design characteristics and change together for resolving bugs; (2) The results also show that the identified ECliques significantly contribute to a project’s bug-proneness. Meanwhile, the majority of a project’s bug-proneness can be captured by just a few ECliques which only contain a small portion of files; (3) Finally, we qualitatively demonstrate that bug-prone ECliques often exhibit design problems that propagate changes among files and can potentially be the causes of bug-proneness.
Conclusion:
To reduce the bug-proneness of a software project, practitioners should pay attention to the identified ECliques, and resolve design problems embedded in these ECliques.",21 Mar 2025,7,"The proposed model and analysis approach can help in reducing bug-proneness in software projects, which can be valuable for early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S0950584920301567,Exploring the Relation between Technical Debt Principal and Interest: An Empirical Approach,December 2020,Information and Software Technology,Not Found,Areti=Ampatzoglou: areti.ampatzoglou@rug.nl; Nikolaos=Mittas: nmittas@chem.ihu.gr; Angeliki-Agathi=Tsintzira: angeliki.agathi.tsintzira@gmail.com; Apostolos=Ampatzoglou: a.ampatzoglou@uom.edu.gr; Elvira-Maria=Arvanitou: earvanitoy@gmail.com; Alexander=Chatzigeorgiou: achat@uom.edu.gr; Paris=Avgeriou: paris@cs.rug.nl; Lefteris=Angelis: lef@csd.auth.gr,"Abstract
Context
The cornerstones of technical debt (TD) are two concepts borrowed from economics: principal and interest. Although in economics the two terms are related, in TD there is no study on this direction so as to validate the strength of the metaphor.
Objective
We study the relation between Principal and Interest, and subsequently dig further into the ‘ingredients’ of each concept (since they are multi-faceted). In particular, we investigate if artifacts with similar levels of TD Principal exhibit a similar amount of TD Interest, and vice-versa.
Method
To achieve this goal, we performed an empirical study, analyzing the dataset using the Mantel test. Through the Mantel test, we examined the relation between TD Principal and Interest, and identified aspects that are able to denote proximity of artifacts, with respect to TD. Next, through Linear Mixed Effects (LME) modelling we studied the 
generalizability
 of the results.
Results
The results of the study suggest that TD Principal and Interest are related, in the sense that classes with similar levels of TD Principal tend to have similar levels of Interest. Additionally, we have reached the conclusion that aggregated measures of TD Principal or Interest are more capable of identifying proximate artifacts, compared to isolated metrics. Finally, we have provided empirical evidence on the fact that improving certain quality properties (e.g., size and coupling) should be prioritized while ranking refactoring opportunities in the sense that high values of these properties are in most of the cases related to artifacts with higher levels of TD Principal.
Conclusions
The findings shed light on the relations between the two concepts, and can be useful for both researchers and practitioners: the former can get a 
deeper understanding
 of the concepts, whereas the latter can use our findings to guide their TD management processes such as prioritization and repayment.",21 Mar 2025,8,"The study sheds light on the relation between Principal and Interest in technical debt, providing insights that can be valuable for startups navigating technical debt management."
https://www.sciencedirect.com/science/article/pii/S0950584920301579,Predicting continuous integration build failures using evolutionary search,December 2020,Information and Software Technology,Not Found,Islem=Saidani: islem.saidani.1@ens.etsmlt.ca; Moataz=Chouchen: moataz.chouchen.1@ens.etsmtl.ca; Mohamed Wiem=Mkaouer: mwmvse@rit.edu,"Abstract
Context:
 Continuous Integration (CI) is a 
common practice
 in modern software development and it is increasingly adopted in the open-source as well as the software industry markets. CI aims at supporting developers in integrating code changes constantly and quickly through an automated build process. However, in such context, the build process is typically time and resource-consuming which requires a high maintenance effort to avoid build failure.
Objective:
 The goal of this study is to introduce an automated approach to cut the expenses of CI build time and provide support tools to developers by predicting the CI build outcome.
Method:
 In this paper, we address problem of CI build failure by introducing a novel search-based approach based on Multi-Objective 
Genetic Programming
 (MOGP) to build a CI build failure prediction model. Our approach aims at finding the best combination of CI built features and their appropriate threshold values, based on two conflicting objective functions to deal with both failed and passed builds.
Results:
 We evaluated our approach on a benchmark of 56,019 builds from 10 large-scale and long-lived software projects that use the Travis CI build system. The statistical results reveal that our approach outperforms the state-of-the-art techniques based on 
machine learning
 by providing a better balance between both failed and passed builds. Furthermore, we use the generated prediction rules to investigate which factors impact the CI build results, and found that features related to (1) specific statistics about the project such as team size, (2) last build information in the current build and (3) the types of changed files are the most influential to indicate the potential failure of a given build.
Conclusion:
 This paper proposes a multi-objective search-based approach for the problem of CI build failure prediction. The performances of the models developed using our MOGP approach were statistically better than models developed using 
machine learning techniques
. The experimental results show that our approach can effectively reduce both 
false negative
 rate and false positive rate of CI build failures in highly imbalanced datasets.",21 Mar 2025,9,"The automated approach for CI build failure prediction can significantly benefit early-stage ventures by reducing false negative and false positive rates, improving efficiency in software development."
https://www.sciencedirect.com/science/article/pii/S0950584920301592,Reducing efforts of software engineering systematic literature reviews updates using text classification,December 2020,Information and Software Technology,Not Found,Willian Massami=Watanabe: http://www.wwatana.be/; Katia Romero=Felizardo: katiascannavino@utfpr.edu.br; Arnaldo=Candido: arnaldoc@utfpr.edu.br; Érica Ferreira=de Souza: ericasouza@utfpr.edu.br; José Ede de Campos=Neto: Not Found; Nandamudi Lankalapalli=Vijaykumar: vijay.nl@inpe.br,"Abstract
Context
Systematic Literature Reviews (SLRs) are frequently used to synthesize evidence in 
Software Engineering
 (SE), however replicating and keeping SLRs up-to-date is a major challenge. The activity of studies selection in SLR is labor intensive due to the large number of studies that must be analyzed. Different approaches have been investigated to support SLR processes, such as: Visual Text Mining or 
Text Classification
. But acquiring the initial dataset is time-consuming and labor intensive.
Objective
In this work, we proposed and evaluated the use of 
Text Classification
 to support the studies selection activity of new evidences to update SLRs in SE.
Method
We applied Text 
Classification techniques
 to investigate how effective and how much effort could be spared during the studies selection phase of an SLR update. Considering the SLRs update scenario, the studies analyzed in the primary SLR could be used as a 
classified dataset
 to train Supervised 
Machine Learning algorithms
. We conducted an experiment with 8 
Software Engineering
 SLRs. In the experiments, we investigated the use of multiple preprocessing and feature extraction tasks such as tokenization, stop words removal, word lemmatization, TF-IDF (Term-Frequency/Inverse-Document-Frequency) with 
Decision Tree
 and 
Support Vector Machines
 as 
classification algorithms
. Furthermore, we configured the classifier activation threshold for maximizing Recall, hence reducing the number of Missed selected studies.
Results
The techniques accuracies were measured and the results achieved on average a F-Score of 0.92 and 62% of exclusion rate when varying the activation threshold of the classifiers, with a 4% average number of Missed selected studies. Both the Exclusion rate and number of Missed selected studies were significantly different when compared to classifier which did not use the configuration of the activation threshold.
Conclusion
The results showed the potential of the techniques in reducing the effort required of SLRs updates.",21 Mar 2025,6,"The use of Text Classification to support SLR processes can save effort and reduce the labor-intensive nature of studies selection, which can be a valuable tool for startups conducting systematic literature reviews in SE."
https://www.sciencedirect.com/science/article/pii/S0950584920301555,Empirical software product line engineering: A systematic literature review,December 2020,Information and Software Technology,Not Found,Ana Eva=Chacón-Luna: achaconl1@unemi.edu.ec; Antonio Manuel=Gutiérrez: antonio.gutierrez@isis-papyrus.com; José A.=Galindo: jagalindo@us.es; David=Benavides: benavides@us.es,"Abstract
Context:
The adoption of 
Software Product Line Engineering
 (SPLE) is usually only based on its theoretical benefits instead of empirical evidences. In fact, there is no work that synthesizes the empirical studies on SPLE. This makes it difficult for researchers to base their contributions on previous works validated with an empirical strategy.
Objective:
The objective of this work is to discover and summarize the studies that have used empirical evidences in SPLE limited to those ones with the intervention of humans. This will allow evaluating the quality and to know the scope of these studies over time. Doing so, research opportunities can arise
Methods:
A 
systematic literature review
 was conducted. The scope of the work focuses on those studies in which there is human intervention and were published between 2000 and 2018. We considered peer-reviewed papers from journals and top 
software engineering
 conferences.
Results:
Out of a total of 1880 studies in the initial set, a total of 62 primary studies were selected after applying a series of inclusion and exclusion criteria. We found that, approximately 56% of the studies used the empirical 
case study
 strategy while the rest used experimental strategies. Around 86% of the case studies were performed in an industrial environment showing the penetration of SPLE in 
industry
.
Conclusion:
The interest of empirical studies has been growing since 2008. Around 95.16% of the studies address aspects related to domain engineering while application engineering received less attention. Most of the experiments and 
case study
 evaluated showed an 
acceptable level
 of quality. The first study found dates from 2005 and since then, the interest in the empirical SPLE has increased.",21 Mar 2025,5,"The synthesis of empirical studies on SPLE can provide valuable insights for researchers, but the direct practical impact on early-stage ventures might be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920301609,A revised open source usability defect classification taxonomy,December 2020,Information and Software Technology,Not Found,Nor Shahida Mohamad=Yusop: nor_shahida@uitm.edu.my; John=Grundy: john.grundy@monash.edu; Jean-Guy=Schneider: Not Found; Rajesh=Vasa: rajesh.vasa@deakin.edu.au,"Abstract
Context
: Reporting usability defects is a critical part of improving software. Accurately classifying these reported usability defects is critical for reporting, understanding, triaging, prioritizing and ultimately fixing such defects. However, existing usability defect 
classification taxonomies
 have several limitations when used for open source software (OSS) development. This includes incomplete coverage of usability defect problems, unclear 
criticality
 of defects, lack of formal usability training of most 
OSS
 defect reporters and developers, and inconsistent terminology and descriptions.
Objective
: To address this gap, as part of our wider usability defect reporting research, we have developed a new usability defect taxonomy specifically designed for use on 
OSS projects
.
Method
: We used 
Usability Problem
 Taxonomy (UPT) to classify 377 usability 
defect reports
 from 
Mozilla Thunderbird
, Firefox for 
Android
, and the 
Eclipse Platform
. At the same time, we also used the card-sorting technique to group defects that could not be classified using UPT. We looked for commonalities and similarities to further group the defects within each category as well as across categories.
Results
: We constructed a new taxonomy for classifying 
OSS
 usability defects, called Open Source Usability Defect Classification (OSUDC). OSUDC was developed by incorporating 
software engineering
 and 
usability engineering
 needs to make it feasible to be used in 
open source software development
. The use of the taxonomy has been validated on five real cases of usability defects. However, evaluation results using the OSUDC were only moderately successful.
Conclusion
: The OSUDC serves as a common vocabulary to describe and classify usability defects with respect to graphical user interface issues. It may help software developers to better understand usability defects and prioritize them accordingly. For researchers, the OSUDC will be helpful when investigating both trends of usability defect types and understanding the root cause of usability defect problems.",21 Mar 2025,8,"The development of a new taxonomy for classifying OSS usability defects can have a significant impact on software development processes, helping developers better prioritize and understand defects."
https://www.sciencedirect.com/science/article/pii/S0950584920301695,The impact of personality traits and knowledge collection behavior on programmer creativity,December 2020,Information and Software Technology,Not Found,Aamir=Amin: aamir@utar.edu.my; Shuib=Basri: Not Found; Mobashar=Rehman: Not Found; Luiz Fernando=Capretz: Not Found; Rehan=Akbar: Not Found; Abdul Rehman=Gilal: Not Found; Muhammad Farooq=Shabbir: Not Found,"Abstract
Context: Creativity is one of the essential ingredients in successful software engineering. However, majority of the work related to creativity in software engineering has focused on creativity in requirement engineering. Furthermore, there are very few studies that examine programmer creativity and the impact of individual and contextual factors on it.
Objective: The objective of the study is to analyze the impact of the 
big five personality traits
 including extraversion, agreeableness, conscientiousness, 
neuroticism
 and openness to experience, as well as knowledge collection behavior on a programmer's creativity intention.
Method: A quantitative survey was conducted and data from 294 programmers, working in offshore software development projects, was collected. The data was later analyzed using Smart-PLS (3.0).
Results and Conclusions: The results indicated that openness to experience, extraversion, conscientiousness and knowledge collection behavior positively predicted a programmer's creativity intention. On the other hand, 
neuroticism
 negatively predicts creativity intention of the programmer. The study also concluded that all of the independent variables, except the agreeableness trait, significantly predict creativity intention which in turn significantly predicts creativity. As a result, our conclusions indicate that programmer's 
personality traits
 and knowledge collection behavior play a key role in shaping their intention to be creative. Hence, 
personality traits
 and knowledge collection behavior should be given due attention during the hiring process of creativity-oriented software companies.",21 Mar 2025,7,Analyzing the impact of personality traits on programmer creativity intention can provide valuable insights for creativity-oriented software companies during the hiring process.
https://www.sciencedirect.com/science/article/pii/S0950584919302113,On the diffuseness of technical debt items and accuracy of remediation time when using SonarQube,December 2020,Information and Software Technology,Not Found,Maria Teresa=Baldassarre: mariateresa.baldassarre@uniba.it; Valentina=Lenarduzzi: valentina.lenarduzzi@lut.fi; Simone=Romano: simone.romano@uniba.it; Nyyti=Saarimäki: nyyti.saarimaki@tuni.fi,"Abstract
Context
. Among the 
static analysis
 tools available, SonarQube is one of the most used. SonarQube detects Technical Debt (TD) items—i.e., violations of coding rules—and then estimates TD as the time needed to remedy TD items. However, practitioners are still skeptical about the accuracy of remediation time estimated by the tool. 
Objective
. In this paper, we analyze both diffuseness of TD items and accuracy of remediation time, estimated by SonarQube, to fix TD items on a set of 21 open-source Java projects. 
Method
. We designed and conducted a 
case study
 where we asked 81 junior developers to fix TD items and reduce the TD of 21 projects. 
Results
. We observed that TD items are diffused in the analyzed projects and most items are code smells. Moreover, the results point out that the remediation time estimated by SonarQube is inaccurate and, as compared to the actual time spent to fix TD items, is in most cases overestimated. 
Conclusions
. The results of our study are promising for practitioners and researchers. The former can make more aware decisions during project execution and resource management, the latter can use this study as a starting point for improving TD estimation models.",21 Mar 2025,5,"The analysis of technical debt estimation accuracy in SonarQube provides useful insights for practitioners and researchers, but the impact may not be as significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302356,Assessing safety-critical systems from operational testing: A study on autonomous vehicles,December 2020,Information and Software Technology,"Autonomous systems, Safety assurance, Statistical testing, Safety-critical systems, Ultra-high reliability, Conservative Bayesian inference, AI safety, Proven in use, Globally at least equivalent, Software reliability growth models",Xingyu=Zhao: xingyu.zhao@hw.ac.uk; Kizito=Salako: k.o.salako@city.ac.uk; Lorenzo=Strigini: l.strigini@city.ac.uk; Valentin=Robu: v.robu@hw.ac.uk; David=Flynn: d.flynn@hw.ac.uk,"Abstract
Context
Demonstrating high reliability and safety for safety-critical systems (SCSs) remains a hard problem. Diverse evidence needs to be combined in a rigorous way: in particular, results of operational testing with other evidence from design and verification. Growing use of 
machine learning
 in SCSs, by precluding most established methods for gaining assurance, makes evidence from operational testing even more important for supporting safety and reliability claims.
Objective
We revisit the problem of using operational testing to demonstrate high reliability. We use Autonomous Vehicles (AVs) as a current example. AVs are making their debut on public roads: methods for assessing whether an AV is safe enough are urgently needed. We demonstrate how to answer 5 questions that would arise in assessing an AV type, starting with those proposed by a highly-cited study.
Method
We apply new theorems extending our Conservative 
Bayesian
 Inference (CBI) approach, which exploit the rigour of 
Bayesian methods
 while reducing the risk of involuntary misuse associated (we argue) with now-common applications of Bayesian inference; we define additional conditions needed for applying these methods to AVs.
Results
Prior knowledge
 can bring substantial advantages if the AV design allows strong expectations of safety before road testing. We also show how naive attempts at conservative assessment may lead to over-optimism instead; why extrapolating the trend of disengagements (take-overs by human drivers) is not suitable for safety claims; use of knowledge that an AV has moved to a “less stressful” environment.
Conclusion
While some reliability targets will remain too high to be practically verifiable, our CBI approach removes a major source of doubt: it allows use of prior knowledge without inducing dangerously optimistic biases. For certain ranges of required reliability and prior beliefs, CBI thus supports feasible, sound arguments. Useful conservative claims can be derived from limited prior knowledge.",21 Mar 2025,6,"Revisiting the problem of using operational testing for safety-critical systems like Autonomous Vehicles can provide valuable methods for ensuring safety and reliability, but the practical implementation may still present challenges."
https://www.sciencedirect.com/science/article/pii/S0950584919302101,A dynamic evolutionary multi-objective virtual machine placement heuristic for cloud data centers,December 2020,Information and Software Technology,"VM placement, Multi-objective optimisation, Resource overcommitment, Resource wastage, Live migration, Energy consumption, Pareto optimal set, Genetic algorithm, Data center simulation",Ennio=Torre: Not Found; Juan J.=Durillo: Not Found; Vincenzo=de Maio: Not Found; Prateek=Agrawal: Not Found; Shajulin=Benedict: Not Found; Nishant=Saurabh: Not Found; Radu=Prodan: radu@itec.aau.at,"Abstract
Minimizing the resource wastage reduces the energy cost of operating a 
data center
, but may also lead to a considerably high resource overcommitment affecting the 
Quality of Service
 (QoS) of the running applications. The effective tradeoff between resource wastage and overcommitment is a challenging task in virtualized Clouds and depends on the allocation of virtual machines (VMs) to physical resources. We propose in this paper a multi-objective method for dynamic 
VM placement
, which exploits live migration mechanisms to simultaneously optimize the resource wastage, overcommitment ratio and migration energy. Our 
optimization algorithm
 uses a novel evolutionary meta-heuristic based on an island population model to approximate the 
Pareto optimal set
 of VM placements with good accuracy and diversity. Simulation results using traces collected from a real Google cluster demonstrate that our method outperforms related approaches by reducing the migration energy by up to 57% with a QoS increase below 6%.",21 Mar 2025,4,"The proposal of a multi-objective method for dynamic VM placement in virtualized Clouds presents a solution to a challenging task, but the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920301312,Crowdsourced software testing: A systematic literature review,November 2020,Information and Software Technology,Not Found,Sultan=Alyahya: sualyahya@ksu.edu.sa,"Abstract
Context
Crowdsourced software testing (CST) refers to the use of 
crowdsourcing techniques
 in the domain of software testing. CST is an emerging area with its applications rapidly increasing in the last decade.
Objective
A comprehensive review on CST has been conducted to determine the current studies aiming to improve and assess the value of using CST as well as the challenges identified by these evaluation studies.
Method
We conducted a systematic literature review on CST by searching six popular databases. We identified 50 primary studies that passed our quality assessment criteria and defined two research questions covering the aim of the study.
Results
There are three main process activities that the current literature aims to improve, namely selection of suitable testers, reporting of defects, and validation of submitted defects. In addition, there are 23 CST evaluation studies and most of them involve a large group and real crowd. These studies have identified 27 different challenges encountered during the application of crowdsourcing in software testing.
Conclusions
The improvements achieved for the specific process activities in CST help explore other unexplored process activities. Similarly, knowing the characteristics of the evaluation studies can direct us on what other studies are worth investigating. Additionally, many of the challenges identified by the evaluation studies represent research problems that need better understanding and alternative solutions. This research also offers opportunities for practitioners to understand and apply new solutions proposed in the literature and the variations between them. Moreover, it provides awareness to the related parties regarding the challenges reported in the literature, which they may encounter during CST tasks.",21 Mar 2025,7,The research on crowdsourced software testing provides valuable insights for startups looking to improve their testing processes and understand the challenges in this area.
https://www.sciencedirect.com/science/article/pii/S0950584920301129,BITA*: Business-IT alignment framework of multiple collaborating organisations,November 2020,Information and Software Technology,Not Found,Ayalew=Kassahun: ayalew.kassahun@wur.nl; Bedir=Tekinerdogan: Not Found,"Abstract
Context
Businesses today must collaborate in a coordinated fashion. To collaborate, they must align their business processes and IT by complying to a common reference architecture. The common reference architecture that addresses their specific collaboration requirements is generally an adaptation of an existing generic reference architecture. However, a design framework for adapting reference architectures is lacking.
Objective
In this paper we propose a design framework for aligning business processes and IT across diverse collaborating organisations in order to derive a more specific reference architecture from a generic one.
Method
We developed the design framework using the guidelines of ISO/IEC/IEEE standard for modelling design viewpoints and validated it in a real-life business 
case study
.
Results
We developed an architectural design framework which we call BITA* that is composed of three coherent architectural design viewpoints. The BP2BP alignment viewpoint provides alignment modelling abstractions for business analysts to be used to align business collaboration processes. The IT2IT alignment viewpoint provides alignment modelling abstractions for software architects to be used to align distributed IT systems. The BP2IT alignment viewpoint provides alignment modelling abstractions for interdisciplinary teams of business and IT specialists for aligning the mapping of business collaboration processes and the underlying distributed IT. The modelling abstractions are applied in a case study to derive a reference architecture for meat supply chain transparency systems.
Conclusion
A key challenge in developing the design framework is the difficulty of comparing models of business processes and IT that come from diverse organisations. Our main contribution is the set of modelling abstractions, which enabled us to represent business processes and IT in a uniform and comparable manner, and the systematic approach for applying the modelling abstractions. The framework is applied in the agri-food sector and needs to be evaluated further in multiple case studies from various application domains.",21 Mar 2025,8,"The design framework proposed for aligning business processes and IT has practical implications for startups collaborating with diverse organisations, offering a systematic approach to derive specific reference architectures."
https://www.sciencedirect.com/science/article/pii/S0950584920301300,From software architecture to analysis models and back: Model-driven refactoring aimed at availability improvement,November 2020,Information and Software Technology,"Software architecture, Availability, Bidirectional model transformation, Refactoring",Vittorio=Cortellessa: vittorio.cortellessa@univaq.it; Romina=Eramo: romina.eramo@univaq.it; Michele=Tucci: michele.tucci@univaq.it,"Abstract
Context
With the ever-increasing evolution of software systems, their architecture is subject to frequent changes due to multiple reasons, such as new requirements. Appropriate architectural changes driven by non-functional requirements are particularly challenging to identify because they concern quantitative analyses that are usually carried out with specific languages and tools. A considerable number of approaches have been proposed in the last decades to derive non-functional analysis models from architectural ones. However, there is an evident lack of automation in the backward path that brings the analysis results back to the software architecture.
Objective
In this paper, we propose a model-driven approach to support designers in improving the availability of their software systems through refactoring actions.
Method
The proposed framework makes use of bidirectional model transformations to map UML models onto Generalized 
Stochastic Petri Nets
 (GSPN) analysis models and vice versa. In particular, after availability analysis, our approach enables the application of model refactoring, possibly based on well-known fault tolerance patterns, aimed at improving the availability of the 
architectural model
.
Results
We validated the effectiveness of our approach on an 
Environmental Control System
. Our results show that the approach can generate: (i) an analyzable availability model from a software architecture description, and (ii) valid software architecture models back from availability models. Finally, our results highlight that the application of fault tolerance patterns significantly improves the availability in each considered scenario.
Conclusion
The approach integrates bidirectional model transformation and fault 
tolerance techniques
 to support the availability-driven refactoring of architectural models. The results of our experiment showed the effectiveness of the approach in improving the software availability of the system.",21 Mar 2025,9,The model-driven approach for improving software system availability through refactoring actions is highly impactful for startups dealing with evolving software architecture and non-functional requirements.
https://www.sciencedirect.com/science/article/pii/S0950584920301397,A microservice composition approach based on the choreography of BPMN fragments,November 2020,Information and Software Technology,Not Found,Pedro=Valderas: pvalderas@pros.upv.es; Victoria=Torres: vtorres@pros.upv.es; Vicente=Pelechano: pele@pros.upv.es,"Abstract
Context
Microservices
 must be composed to provide users with complex and elaborated functionalities. It seems that the decentralized nature of microservices makes a choreography style more appropriate to achieve such cooperation, where lighter solutions based on asynchronous events are generally used. However, a microservice composition based on choreography distributes the flow logic of the composition among microservices making further analysis and updating difficult, i.e. there is not a big picture of the composition that facilitates these tasks. 
Business Process Model and Notation
 (BPMN) is the OMG standard developed to represent Business Processes (BPs), being widely used to define the big picture of such compositions. However, BPMN is usually considered in orchestration-based solutions, and orchestration can be a drawback to achieve the decoupling pursued by a 
microservice architecture
.
Objective
Defining a microservice composition approach that allows us to create a composition in a BPMN model, which facilitates further analysis for taking engineering decisions, and execute them through an event-based choreography to have a high degree of decoupling and independence among microservices.
Method
We followed a research methodology for 
information systems
 that consists of a 5-step process: awareness of the problem, suggestion, development, evaluation, and conclusion.
Results
We presented a microservice composition approach based on the choreography of BPMN fragments. On the one hand, we propose to describe the big picture of the composition with a BPMN model, providing a valuable mechanism to analyse it when engineering decisions need to be taken. On the other hand, this model is split into fragments in order to be executed through an event-based choreography form, providing the high degree of decoupling among microservices demanded in this type of architecture. This composition approach is supported by a 
microservice architecture
 defined to achieve that both descriptions of a composition (big picture and split one) coexist. A realization of this architecture in Java/Spring technology is also presented.
Conclusions
The evaluation that is done to our work allows us to conclude that the proposed approach for composing microservices is more efficient than solutions based on ad-hoc development.",21 Mar 2025,8,The microservice composition approach based on BPMN and choreography provides a valuable method for startups to analyze and execute microservice compositions with a high degree of decoupling.
https://www.sciencedirect.com/science/article/pii/S0950584920301385,Type slicing: An accurate object oriented slicing based on sub-statement level dependence graph,November 2020,Information and Software Technology,Not Found,Wang=Lulu: Not Found; Li=Bixin: bx.li@seu.edu.cn; Kong=Xianglong: Not Found,"Abstract
Context
Program slicing is very useful in program analysis and software engineering. It computes the slice, which is a part of program and contains all the statements related to the given slicing criterion. The more accurate a slicing technique could be, the smaller the slice is.
Objective
This paper aims to improve the current slicing accuracy for object-oriented programs. The slicing accuracy is mainly related to three factors, the dependency graph (which extracts the inner relationships of source code), the slicing criterion (which determines the slicing requirement), and the slicing algorithm (which computes the slice for the criterion from the dependency graph).
Method
Our method consists of three parts. First, we present a 
Sub-Statement Level Dependence Graph
 (SSLDG), which computes finer-grained dependences for object-oriented programs than mostly-used statement level graph. Second, we present a new type slicing criterion called 
(Sub-statement) Type Slicing Criterion
 (STSC), which supports the user to specify not only the statement and object variable, but also the type of object among its polymorphic types. At last, a corresponding slicing algorithm called 
(Sub-statement) Type Slicing
 (STS) is designed to perform the slicing process.
Results
We implement STS on Java programs as MyJavaSlicer, and run it with ten open source projects and random slicing criteria. The results show that STS slicing algorithm as well as SSLDG would make slices 35.90% smaller than traditional two-phase slicing; additionally using STSC would make the slices 48.4% further smaller; STSC also helps traditional two-phase slices reduced by 56.90%.
Conclusions
STS could provide more accurate slices than traditional two-phase slicing, and it also runs faster on most cases; STSC helps specify the slicing requirement, and roughly halves the size of slices for both slicing algorithms.",21 Mar 2025,7,"The improvement in slicing accuracy for object-oriented programs presents a practical advancement for startups working with such programs, potentially leading to more efficient program analysis and development."
https://www.sciencedirect.com/science/article/pii/S0950584920301403,Automated model-based performance analysis of software product lines under uncertainty,November 2020,Information and Software Technology,Not Found,Paolo=Arcaini: arcaini@nii.ac.jp; Omar=Inverso: Not Found; Catia=Trubiani: Not Found,"Abstract
Context:
 A Software Product Line (SPL) can express the variability of a system through the specification of configuration options. Evaluating performance characteristics, such as the 
system response time
 and 
resource utilization
, of a software product is challenging, even more so in the presence of uncertain values of the attributes.
Objective:
 The goal of this paper is to automate the generation of performance models for software products derived from the feature model by selection heuristics. We aim at obtaining model-based predictive results to quantify the correlation between the features, along with their uncertainties, and the 
system performance
. This way, software engineers can be informed on the performance characteristics before implementing the system.
Method:
 We propose a tool-supported framework that, starting from a feature model annotated with performance-related characteristics, derives 
Queueing Network
 (QN) performance models for all the products of the SPL. Model-based performance analysis is carried out on the models obtained by selecting the products that show the maximum and minimum performance-based costs.
Results:
 We applied our approach to almost seven thousand feature models including more than one hundred and seventy features. The generation of QN models is automatically performed in much less than one second, whereas their model-based performance analysis embeds simulation delays and requires about six minutes on average.
Conclusion:
 The experimental results confirm that our approach can be effective on a variety of systems for which software engineers may be provided with early insights on the 
system performance
 in reasonably short times. Software engineers are supported in the task of understanding the performance bounds that may encounter when (de)selecting different configuration options, along with their uncertainties.",21 Mar 2025,7,"The automation of generating performance models for software products can provide valuable insights to software engineers before system implementation, helping them understand performance characteristics and uncertainties."
https://www.sciencedirect.com/science/article/pii/S0950584920301361,PostFinder: Mining Stack Overflow posts to support software developers,November 2020,Information and Software Technology,Not Found,Riccardo=Rubei: riccardo.rubei@univaq.it; Claudio=Di Sipio: claudio.disipio@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Juri=Di Rocco: juri.dirocco@univaq.it; Davide=Di Ruscio: davide.diruscio@univaq.it,"Abstract
Context –
 During the development of complex software systems, programmers look for external resources to understand better how to use specific APIs and to get advice related to their current tasks. Stack Overflow provides developers with a broader insight into API usage as well as useful code examples. Given the circumstances, tools and techniques for mining Stack Overflow are highly desirable. 
Objective –
 In this paper, we introduce PostFinder, an approach that analyzes the project under development to extract suitable context, and allows developers to retrieve messages from Stack Overflow being relevant to the API function calls that have already been invoked. 
Method –
 PostFinder augments posts with additional data to make them more exposed to queries. On the client side, it boosts the context code with various factors to construct a query containing information needed for matching against the stored indexes. Multiple facets of the data available are used to optimize the search process, with the ultimate aim of recommending highly relevant SO posts. 
Results –
 The approach has been validated utilizing a user study involving a group of 12 developers to evaluate 500 posts for 50 contexts. Experimental results indicate the suitability of PostFinder to recommend relevant Stack Overflow posts and concurrently show that the tool outperforms a well-established baseline. 
Conclusions –
 We conclude that PostFinder can be deployed to assist developers in selecting relevant Stack Overflow posts while they are programming as well as to replace the module for searching posts in a code-to-code search engine.",21 Mar 2025,6,"PostFinder offers a useful tool for developers to retrieve relevant Stack Overflow posts related to their programming tasks, improving efficiency and providing more insights into API usage."
https://www.sciencedirect.com/science/article/pii/S0950584920301452,Cloud applications monitoring: An industrial study,November 2020,Information and Software Technology,"Cloud monitoring, Applications monitoring, Incident handling, Rapid response organizational structures, Online survey, Industrial study",Damian A.=Tamburri: d.a.tamburri@tue.nl; Marco=Miglierina: marco.miglierina@contentwise.com; Elisabetta Di=Nitto: elisabetta.dinitto@polimi.it,"Abstract
Context
Modern software systems employ large IT infrastructures hosted in on-premise clouds or using “rented” cloud resources from specific vendors. The unifying force across any cloud strategy is incremental product and application improvement against conservation of those resources. This is where monitoring of cloud applications becomes a key asset
Objective
To shed light over the status of monitoring practices in industry, we study: (a) monitoring practices and tools adoption in industry; (b) size and complexity of industrial monitoring problems; (c) the role of software architecture and software process with respect to monitoring strategies.
Method
We conduct mixed-methods empirical research featuring interviews and a web survey featuring 140+ practitioners from over 70 different organizations.
Results
Even if the market makes available a significant set of monitoring tools, our results show a rather unappealing picture of industrial monitoring: (a) industrial decision-makers do not perceive monitoring as a key asset even though the downtime of their applications correlates heavily with the level of automation and responsiveness enabled by monitoring; (b) monitoring is done with crude technology, mostly MySQL querying or similar (e.g., Nagios); finally, (c) incidents are discovered by clients rather than application owners.
Conclusion
We conclude that the road toward the industrial adoption of cutting-edge monitoring technology is still one of the less travelled, presumably in connection to the considerable investment required. Furthermore, the lack of industrial cloud monitoring standards does not help in addressing the proliferation of multiple tool combinations, with varying effectiveness. Further research should be invested in looking into and addressing these major concerns.",21 Mar 2025,5,"The study on monitoring practices in industry highlights the challenges and shortcomings in industrial monitoring, indicating the need for improvement and investment in cutting-edge technology."
https://www.sciencedirect.com/science/article/pii/S0950584920301427,Towards automatically generating block comments for code snippets,November 2020,Information and Software Technology,Not Found,Yuan=Huang: huangyjn@gmail.com; Shaohao=Huang: huangshh29@mail2.sysu.edu.cn; Huanchao=Chen: fchenhch@mail2.sysu.edu.cn; Xiangping=Chen: chenxp8@mail.sysu.edu.cn; Xiapu=Luo: csxluo@comp.polyu.edu.hk; Nan=Jia: jianan_0101@163.com; Xinyu=Hu: husense@foxmail.com; Xiaocong=Zhou: isszxc@mail.sysu.edu.cn,"Abstract
Code commenting is a common programming practice of practical importance to help developers review and comprehend 
source code
. There are two main types of code comments for a method: header comments that summarize the method functionality located before a method, and block comments that describe the functionality of the code snippets within a method. Inspired by the effectiveness of 
deep learning
 techniques in the NLP field, many studies focus on using the machine 
translation model
 to automatically generate comment for the 
source code
. Because the data set of block comments is difficult to collect, current studies focus more on the automatic generation of header comments than that of block comments. However, block comments are important for 
program comprehension
 due to their explanation role for the code snippets in a method. To fill the gap, we have proposed an approach that combines heuristic rules and learning-based method to collect a large number of comment-code pairs from 1,032 
open source projects
 in our previous study. In this paper, we propose a reinforcement learning-based method, 
RL-BlockCom
, to automatically generate block comments for code snippets based on the collected comment-code pairs. Specifically, we utilize the 
abstract syntax tree
 (i.e., AST) of a code snippet to generate a token sequence with a statement-based traversal way. Then we propose a composite learning model, which combines the actor-critic algorithm of 
reinforcement learning
 with the encoder-decoder algorithm, to generate block comments. On the data set of the comment-code pairs, the BLEU-4 score of our method is 24.28, which outperforms the baselines and state-of-the-art in comment generation.",21 Mar 2025,8,"The RL-BlockCom approach using reinforcement learning to automatically generate block comments for code snippets shows promising results, addressing the gap in automatic generation of block comments and improving program comprehension."
https://www.sciencedirect.com/science/article/pii/S0950584920301440,The impact of using a domain language for an agile requirements management,November 2020,Information and Software Technology,Not Found,Matias=Urbieta: murbieta@lifia.info.unlp.edu.ar; Leandro=Antonelli: lanto@lifia.info.unlp.edu.ar; Gustavo=Rossi: gustavo@lifia.info.unlp.edu.ar; Julio Cesar Sampaio=do Prado Leite: http://www.inf.puc-rio.br/julio,"Abstract
Context
: The development of software systems is a complex activity because of its nature and the management of its construction. It is challenging to create and follow a plan. Moreover, budget overrun is a common consequence of this situation. Agile methods, like Scrum, help to mitigate this problem using incremental and 
iterative development
. Agile methods jump start new developments, but it is difficult to be agile after several months when the software has to deal with many requirements that are scattered and tangled across several User Stories written in different Sprints. 
Objective
: In this paper, we propose a traceability approach anchored on an index structure to access specific User Stories from a large set. Our proposed strategy has the goal to consolidate the information dispersed in different User Stories into a particular lexicon: The Language Extended Lexicon (LEL). 
Method
: The proposed approach consists of a set of rules which extract the information dispersed in the User Stories and organize it in symbols of the Lexicon. Thus, the Lexicon supplies a consolidated and organized structure to mitigate the problem of tangled information that generates lack of traceability among different sprints. 
Results
: We assessed how the Lexicon built by our approach improves everyday activities related to requirement management. The assessment is based on a 
quantitative evaluation
 with 36 subjects. 
Conclusion
: The approach presents benefits for requirement tracing in 
agile methodologies
 supported by the preliminary results of the evaluation. We have developed an application (a prototype) that automates the LEL derivation rules from a set of User Stories.",21 Mar 2025,6,"The proposed traceability approach anchored on an index structure can benefit agile methodologies by consolidating and organizing information from different User Stories into a Language Extended Lexicon, aiding in requirement tracing."
https://www.sciencedirect.com/science/article/pii/S0950584920301439,Effectiveness of Kotlin vs. Java in android app development tasks,November 2020,Information and Software Technology,Not Found,Luca=Ardito: luca.ardito@polito.it; Riccardo=Coppola: riccardo.coppola@polito.it; Giovanni=Malnati: giovanni.malnati@polito.it; Marco=Torchiano: marco.torchiano@polito.it,"Abstract
Context:
Kotlin is a new programming language representing an alternative to Java; they both target the same 
JVM
 and can safely coexist in the same application. Kotlin is advertised as capable to solve several known limitations of Java. Recent surveys show that Kotlin achieved a relevant diffusion among Java developers. 
Goal:
We planned to empirically assess a few typical promises of Kotlin w.r.t. known Java’s limitations, in terms of development effectiveness, 
maintainability
, and ease of development. 
Method:
Our experiment involved 27 teams of 4 people each that completed a set of maintenance tasks (both defect correction and feature addition) on 
Android
 apps written in either Java or Kotlin. In addition to the number of fixed defects, effort, and code size, we collected, though a questionnaire, the participants’ perceptions about the avoidance of known pitfalls. 
Results:
We did not observe any significant difference in terms of 
maintainability
 between the two languages.We found a significant difference regarding the amount of code written, which constitutes evidence of better 
conciseness
 of Kotlin. Concerning ease of development, the frequency of NullPointerExceptions reported by the subjects was significantly lower when developing in Kotlin. On the other hand, no significant difference was found in the occurrence of other common Java pitfalls. Finally, the IDE support was deemed better for Java than Kotlin. 
Conclusions:
Some of the promises of Kotlin to be a ”better Java” have been confirmed by our empirical assessment. Evidence suggests that the effort in transitioning to Kotlin can provide some advantages to Java developers, especially regarding code 
conciseness
.Our results may serve as the basis for further investigations on the properties of the language.",21 Mar 2025,8,The evaluation of Kotlin against Java in terms of development effectiveness and maintainability provides practical insights for European early-stage ventures looking to choose a programming language. The results offer advantages and considerations for startups transitioning to Kotlin.
https://www.sciencedirect.com/science/article/pii/S0950584920301373,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,November 2020,Information and Software Technology,Not Found,Lucy Ellen=Lwakatare: llucy@chalmers.se; Aiswarya=Raj: Not Found; Ivica=Crnkovic: Not Found; Jan=Bosch: Not Found; Helena Holmström=Olsson: Not Found,"Abstract
Background
: Developing and maintaining large scale 
machine learning
 (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems.
Objective
: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges.
Method
: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four 
quality attributes
: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment.
Results
: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions.
Conclusion
: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in 
ML system
 development practice and the lack of solutions point to directions for future research.",21 Mar 2025,7,The challenges and solutions identified in the development and maintenance of large scale ML-based software systems provide valuable insights for European startups working in industries with ML applications. The identified challenges highlight practical concerns for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S095058491930223X,Guidelines for the search strategy to update systematic literature reviews in software engineering,November 2020,Information and Software Technology,"Systematic literature review update, Systematic literature reviews, Software engineering, Snowballing, Searching for evidence",Claes=Wohlin: claes.wohlin@bth.se; Emilia=Mendes: emilia.mendes@bth.se; Katia Romero=Felizardo: katiascannavino@utfpr.edu.br; Marcos=Kalinowski: kalinowski@inf.puc-rio.br,"Abstract
Context
Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.
Objective
The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.
Method
To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.
Results
The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results.
Conclusions
Our proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE.",21 Mar 2025,6,"The proposed guidelines for updating Systematic Literature Reviews in Software Engineering offer practical advice for researchers and practitioners. While relevant, the impact on European early-stage ventures may be more indirect compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301740,"The Symposium on Search-Based Software Engineering: Past, Present and Future",November 2020,Information and Software Technology,Not Found,Thelma Elita=Colanzi: thelma@din.uem.br; Wesley K.G.=Assunção: wesleyk@utfpr.edu.br; Silvia R.=Vergilio: silvia@inf.ufpr.br; Paulo Roberto=Farah: paulo.farah@udesc.br; Giovani=Guizzo: g.guizzo@ucl.ac.uk,"Abstract
Context
Search-Based 
Software Engineering
 (SBSE) is the research field where 
Software Engineering
 (SE) problems are modelled as search problems to be solved by search-based techniques. The Symposium on Search Based Software Engineering (SSBSE) is the premier event on SBSE, which had its 11
th
 edition in 2019.
Objective
In order to better understand the characteristics and evolution of papers published at SSBSE, this work reports results from a mapping study targeting the proceedings of all SSBSE editions. Despite the existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.
Method
A 
systematic mapping study
 was conducted with a set of four research questions, in which 134 studies published in all editions of SSBSE, dated from 2009 to 2019, were evaluated. In a fifth question, 32 papers published in the challenge track were summarised.
Results
Throughout the years, 290 authors from 25 countries have contributed to the main track of the symposium, with the collaboration of at least two institutions in 46.3% of the papers. SSBSE papers have got substantial external visibility, as most citations are from different venues. The SE tasks addressed by SSBSE are mostly related to software testing, 
software debugging
, 
software design
, and maintenance. 
Evolutionary algorithms
 are present in 75% of the papers, being the most common search technique. The evaluation of the SBSE approaches usually includes industrial systems.
Conclusions
SSBSE has helped increase the popularity of SBSE in the SE research community and has played an important role on making SBSE mature. There are still problems and challenges to be addressed in the SBSE field, which can be tackled by SSBSE authors in further studies.",21 Mar 2025,7,The mapping study on the characteristics and evolution of papers published at SSBSE can provide relevant information to European startups considering entering the field of Search-Based Software Engineering. The insights can guide new studies and collaborations among research groups.
https://www.sciencedirect.com/science/article/pii/S0950584920300744,NLP-assisted software testing: A systematic mapping of the literature,October 2020,Information and Software Technology,Not Found,Vahid=Garousi: v.garousi@qub.ac.uk; Sara=Bauer: sara.bauer@uibk.ac.at,"Abstract
Context
To reduce manual effort of extracting test cases from natural-language requirements, many approaches based on 
Natural Language Processing
 (NLP) have been proposed in the literature. Given the large amount of approaches in this area, and since many practitioners are eager to utilize such techniques, it is important to synthesize and provide an overview of the state-of-the-art in this area.
Objective
Our objective is to summarize the state-of-the-art in NLP-assisted software testing which could benefit practitioners to potentially utilize those NLP-based techniques. Moreover, this can benefit researchers in providing an overview of the research landscape.
Method
To address the above need, we conducted a survey in the form of a systematic literature mapping (classification). After compiling an initial pool of 95 papers, we conducted a systematic voting, and our final pool included 67 technical papers.
Results
This review paper provides an overview of the contribution types presented in the papers, types of NLP approaches used to assist software testing, types of required input requirements, and a review of tool support in this area. Some key results we have detected are: (1) only four of the 38 tools (11%) presented in the papers are available for download; (2) a larger ratio of the papers (30 of 67) provided a shallow exposure to the NLP aspects (almost no details).
Conclusion
This paper would benefit both practitioners and researchers by serving as an “index” to the body of knowledge in this area. The results could help practitioners utilizing the existing NLP-based techniques; this in turn reduces the cost of test-case design and decreases the amount of human resources spent on test activities. After sharing this review with some of our industrial collaborators, initial insights show that this review can indeed be useful and beneficial to practitioners.",21 Mar 2025,6,"The overview of NLP-assisted software testing techniques serves as a valuable resource for practitioners and researchers. While the benefits of utilizing these techniques are highlighted, the direct impact on European early-stage ventures may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920301166,Engineering human-in-the-loop interactions in cyber-physical systems,October 2020,Information and Software Technology,Not Found,Miriam=Gil: mgil@pros.upv.es; Manoli=Albert: malbert@pros.upv.es; Joan=Fons: jjfons@pros.upv.es; Vicente=Pelechano: pele@pros.upv.es,"Abstract
Context:
 Cyber-Physical Systems (CPSs) are gradually and widely introducing autonomous capabilities into everything. However, human participation is required to accomplish tasks that are better performed with humans (often called human-in-the-loop). In this way, human-in-the-loop solutions have the potential to handle complex tasks in unstructured environments, by combining the cognitive skills of humans with 
autonomous systems
 behaviors.
Objective:
 The objective of this paper is to provide appropriate techniques and methods to help designers analyze and design human-in-the-loop solutions. These solutions require interactions that engage the human, provide natural and understandable collaboration, and avoid disturbing the human in order to improve human experience.
Method:
 We have analyzed several works that identified different requirements and critical factors that are relevant to the design of human-in-the-loop solutions. Based on these works, we have defined a set of design principles that are used to build our proposal. Fast-prototyping techniques have been applied to simulate the designed human-in-the-loop solutions and validate them.
Results:
 We have identified the technological challenges of designing human-in-the-loop CPSs and have provided a method that helps designers to identify and specify how the human and the system should work together, focusing on the 
control strategies
 and interactions required.
Conclusions:
 The use of our approach facilitates the design of human-in-the-loop solutions. Our method is practical at earlier stages of the 
software life cycle
 since it allows domain experts to focus on the problem and not on the solution.",21 Mar 2025,6,"The paper provides practical techniques for designers to analyze and design human-in-the-loop solutions, which can be beneficial for early-stage ventures integrating autonomous systems with human cognition."
https://www.sciencedirect.com/science/article/pii/S0950584920301142,Refactoring effect on internal quality attributes: What haven’t they told you yet?,October 2020,Information and Software Technology,Not Found,Eduardo=Fernandes: emfernandes@inf.puc-rio.br; Alexander=Chávez: achavez@tecgraf.puc-rio.br; Alessandro=Garcia: afgarcia@inf.puc-rio.br; Isabella=Ferreira: isabella.ferreira@polymtl.ca; Diego=Cedrim: dccedrim@amazon.com; Leonardo=Sousa: leo.sousa@sv.cmu.edu; Willian=Oizumi: woizumi@inf.puc-rio.br,"Abstract
Context
Code refactoring
 was conceived for enhancing code structures, often in terms of internal quality attributes such as cohesion and coupling. Developers may have to apply multiple 
refactoring operations
 to achieve the expected enhancement. Re-refactoring occurs whenever one or more refactoring operations are performed on a previously refactored code element. The literature often assumes each single refactoring improves rather than worsens internal quality attributes, while re-refactoring implies further improvements. Unfortunately, quantitative evidence on this matter is scarce if not nonexistent.
Objective
This paper extends a large quantitative study about the refactoring effect on internal quality attributes with new insights, plus an unprecedented re-refactoring effect analysis. We particularly investigate if re-refactoring operations are more effective in improving attributes when compared to single operations.
Method
We analyzed 23 
open software
 projects with 29,303 refactoring operations, from which nearly 50% constitute re-refactorings. We assessed five attributes: cohesion, complexity, coupling, inheritance, and size. We combined descriptive analysis and statistical tests to deeply understand the effect of both refactoring and re-refactoring on each attribute.
Results
Contrary to current knowledge, our study revealed that 90% of refactoring operations, and 100% of re-refactoring operations, were applied to code elements with at least one critical attribute. Critical attribute is an attribute whose metrics used for computing it have anomalous values, e.g. high coupling. Most operations (65%) improve attributes presumably associated with the refactoring type applied; the other operations (35%) keep those attributes unaffected. Whenever refactoring and re-refactoring operations are applied without additional changes, i.e., root-canal refactoring, attributes tend to improve or at least not worsen. Surprisingly, if these operations occur with additional changes such as feature additions, i.e., floss refactoring, they mostly improve rather than worsen attributes.
Conclusions
Besides revealing the effect of refactoring and re-refactoring on each attribute, we derived insights on leveraging the current refactoring practices.",21 Mar 2025,8,"The study offers quantitative evidence and insights on the effect of refactoring operations on code quality attributes, which can be crucial for startups focusing on enhancing their code structures."
https://www.sciencedirect.com/science/article/pii/S0950584920301130,Using simulated annealing for locating array construction,October 2020,Information and Software Technology,"Locating arrays, Combinatorial interaction testing, Software testing, Simulated annealing",Tatsuya=Konishi: Not Found; Hideharu=Kojima: Not Found; Hiroyuki=Nakagawa: Not Found; Tatsuhiro=Tsuchiya: t-tutiya@ist.osaka-u.ac.jp,"Abstract
Context
Combinatorial interaction testing is known to be an efficient testing strategy for computing and 
information systems
. Locating arrays are mathematical objects that are useful for this testing strategy, as they can be used as a test suite that permits 
fault localization
 as well as fault detection. In this application, each row of an array is used as an individual test.
Objective
This paper proposes an algorithm for constructing locating arrays with a small number of rows. Testing cost increases as the number of tests increases; thus the problem of finding locating arrays of small sizes is of practical importance.
Method
The proposed algorithm uses simulated annealing, a meta-heuristic algorithm, to find locating array of a given size. The whole algorithm repeatedly executes the 
simulated annealing algorithm
 with the input array size being dynamically varied.
Results
Experimental results show (1) that the proposed algorithm is able to construct locating arrays for problem instances of large sizes and (2) that, for problem instances for which nontrivial locating arrays are known, the algorithm is often able to generate locating arrays that are smaller than or at least equal to the known arrays.
Conclusion
Based on the results, we conclude that the proposed algorithm can produce small locating arrays and scale to practical problems.",21 Mar 2025,7,The algorithm proposed for constructing locating arrays with a small number of rows can be valuable for startups seeking efficient testing strategies for their computing systems.
https://www.sciencedirect.com/science/article/pii/S0950584920300653,"Early prediction of quality of service using interface-level metrics, code-level metrics, and antipatterns",October 2020,Information and Software Technology,Not Found,Chaima=Abid: cabid@umich.edu; Marouane=Kessentini: marouane@umich.edu; Hanzhang=Wang: hanzwang@ebay.com,"Abstract
Context:
 With the current high trends of deploying and using web services in practice, effective techniques for maintaining high quality of Service are becoming critical for both service providers and subscribers/users. Service providers want to predict the quality of service during early stages of development before releasing them to customers. Service clients consider the quality of service when selecting the best one satisfying their preferences in terms of price/budget and quality between the services offering the same features. The majority of existing studies for the prediction of quality of service are based on 
clustering algorithms
 to classify a set of services based on their collected quality attributes. Then, the user can select the best service based on his expectations both in terms of quality and features. However, this assumption requires the deployment of the services before being able to make the prediction and it can be time-consuming to collect the required data of running web services during a period of time. Furthermore, the clustering is only based on well-known quality attributes related to the services performance after deployment. 
Objective:
 In this paper, we start from the hypothesis that the quality of the source code and 
interface design
 can be used as indicators to predict the quality of service attributes without the need to deploy or run the services by the subscribers. 
Method:
 We collected 
training data
 of 707 web services and we used 
machine learning
 to generate 
association rules
 that predict the quality of service based on the interface and code quality metrics, and antipatterns. 
Results:
 The empirical validation of our prediction techniques shows that the generated 
association rules
 have strong support and high confidence which confirms our hypothesis that source code and interface quality metrics/antipatterns are correlated with web service quality attributes which are response time, availability, throughput, successability, reliability, compliance, best practices, latency, and documentation. 
Conclusion:
 To the best of our knowledge, this paper represents the first study to validate the correlation between interface metrics, source 
code metrics
, antipatterns and quality of service. Another contribution of our work consists of generating association rules between the code/interface metrics and quality of service that can be used for prediction purposes before deploying new releases.",21 Mar 2025,9,"The paper introduces machine learning techniques to predict quality of service attributes based on code and interface quality metrics, offering a potential method for startups to assess and improve their service quality early in development."
https://www.sciencedirect.com/science/article/pii/S0950584920300902,Understanding the relationship of conflict and success in software development projects,October 2020,Information and Software Technology,Not Found,Mohammad R.=Basirati: Not Found; Marko=Otasevic: Not Found; Koushyar=Rajavi: Not Found; Markus=Böhm: Not Found; Helmut=Krcmar: Not Found,"Abstract
Context
Software development incorporates numerous people with diverse expertise and expectations. This makes conflict a common phenomenon in software development. Besides human causes, many conflicts in software development root in the tools and processes. Moreover, the growing role of software in any type of system is increasing the heterogeneity in software projects. The number and variety of tools and processes are increasing. Nevertheless, the relationship between conflicts, particularly rooted in non-human elements, and software project success is still unclear.
Objective
We aim to understand the impact of conflict on the success of software development projects for different types of conflict and different environments. Particularly, we distinguish between human-rooted conflict (HRC) and non-human-rooted conflict (NHRC). Moreover, we investigate whether organization size and team size moderate the impact of conflict on software project success.
Methods
First, we conduct a survey and analyze it using 
structural equation modeling
 (SEM) to investigate any correlation between conflict and software project success. Second, we explore the reasons behind the relationship between conflict and software project success by conducting 13 semi-structured expert interviews.
Results
HRC is always a threat to software project success for any organization or team size. Based on the interviews, resolving an HRC is regularly problematic. On the other hand, NHRC is negatively correlated with software project success only in corporate organizations and small teams. High coordination overhead and dependency on tools and processes make NHRC more influential in corporate organizations. In contrast, overlooking non-human elements and lack of experienced individuals in smaller teams make them more vulnerable to NHRC.
Conclusion
While the detrimental impact of HRC is constant for software project success, NHRC can be controlled efficiently. Corporate organizations need to frequently improve the non-human elements in the development. Smaller teams should expect tools and processes to be significantly influential in their success.",21 Mar 2025,6,"The study highlights the impact of conflict on software project success, providing insights that can be useful for startups managing conflicts in their development processes and tools."
https://www.sciencedirect.com/science/article/pii/S0950584920301026,Investigating the relationship between personalities and agile team climate of software professionals in a telecom company,October 2020,Information and Software Technology,Not Found,Sai Datta=Vishnubhotla: sai-datta.vishnubhotla@bth.se; Emilia=Mendes: Not Found; Lars=Lundberg: Not Found,"Abstract
Context
Previous research found that the performance of a team not only depends on the team personality composition, but also on the interactive effects of team climate. Although investigation on personalities associated with software development has been an active research area over the past decades, there has been very limited research in relation to team climate.
Objective
Our study investigates the association between the 
five factor model
 
personality traits
 (openness to experience, 
conscientiousness
, extraversion, agreeableness and neuroticism) and the factors related to team climate (team vision, participative safety, support for innovation and task orientation) within the context of agile teams working in a 
telecom company
.
Method
A survey was used to gather data on personality characteristics and team climate perceptions of 43 members from eight agile teams. The data was initially used for correlation analysis; then, regression models were developed for predicting the 
personality traits
 related to team climate perception.
Results
We observed a statistically significant 
positive correlation
 between openness to experience and support for innovation (r = 0.31). Additionally, agreeableness was observed to be positively correlated with overall team climate (r = 0.35). Further, from regression models, we observed that 
personality traits
 accounted to less than 15% of the variance in team climate.
Conclusion
A person's ability to easily get along with team members (agreeableness) has a significant positive influence on the perceived level of team climate. Results from our 
regression analysis
 suggest that further data may be needed, and/or there are other human factors, in addition to 
personality traits
, that should also be investigated with regard to their relationship with team climate. Overall, the relationships identified in our study are likely to be applicable to organizations within the telecommunications domain that use scrum methodology for software development.",21 Mar 2025,9,"The study provides valuable insights into the relationship between personality traits and team climate in agile teams, which can be highly beneficial for startups working in the telecommunications sector."
https://www.sciencedirect.com/science/article/pii/S0950584920300914,Recommending refactorings via commit message analysis,October 2020,Information and Software Technology,Not Found,Soumaya=Rebai: srebal@umich.edu; Marouane=Kessentini: marouane@umich.edu; Vahid=Alizadeh: alizadeh@umich.edu; Oussama Ben=Sghaier: oussama@umich.edu; Rick=Kazman: kazman@hawaii.edu,"Abstract
Context
The purpose of software restructuring, or refactoring, is to improve software quality and developer productivity.
Objective
Prior studies have relied mainly on static and dynamic analysis of code to detect and recommend refactoring opportunities, such as code smells. Once identified, these smells are fixed by applying refactorings which then improve a set of quality metrics. While this approach has value and has shown promising results, many detected refactoring opportunities may not be related to a developer’s current context and intention. Recent studies have shown that while developers document their refactoring intentions, they may miss relevant refactorings aligned with their rationale.
Method
In this paper, we first identify refactoring opportunities by analyzing developer commit messages and check the quality improvements in the changed files, then we distill this knowledge into usable context-driven refactoring recommendations to complement static and dynamic analysis of code.
Results
The evaluation of our approach, based on six 
open source projects
, shows that we outperform prior studies that apply refactorings based on static and dynamic analysis of code alone.
Conclusion
This study provides compelling evidence of the value of using the information contained in existing commit messages to recommend future refactorings.",21 Mar 2025,8,"The approach of analyzing developer commit messages to recommend context-driven refactorings shows promise in improving software quality and developer productivity, which can be useful for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920301178,Neural networks learn to detect and emulate sorting algorithms from images of their execution traces,October 2020,Information and Software Technology,Not Found,Cătălin F.=Perticas: perticascatalin@gmail.com; Bipin=Indurkhya: bipin.indurkhya@uj.edu.pl,"Abstract
Context
Recent advancements in the applicability of 
neural networks
 across a variety of fields, such as 
computer vision
, 
natural language processing
 and others, have re-sparked an interest in program induction methods. (Kitzelman 
[1]
, Gulwani et al. 
[2]
 or Kant [3].)
Problem
When performing a program induction task, it is not feasible to search across all possible programs that map an input to an output because the number of possible combinations or sequences of instructions is too high: at least an 
exponential growth
 based on the generated program length. Moreover, there does not exist a general framework to formulate such program induction tasks and current computational limitations do not allow a very wide range of 
machine learning
 applications in the field of computer programs generation.
Objective
In this study, we analyze the effectiveness of execution traces as 
learning representations
 for 
neural network models
 in a program induction set-up. Our goal is to generate visualizations of program execution dynamics, specifically of sorting algorithms, and to apply 
machine learning techniques
 on them to capture their semantics and emulate their behavior using 
neural networks
.
Method
We begin by classifying images of execution traces for algorithms working on a finite array of numbers, such as various sorting and 
data structures
 algorithms. Next we experiment with detecting sub-program patterns inside the trace sequence of a larger program. The last step is to predict future steps in the execution of various sorting algorithms. More specifically, we try to emulate their behavior by observing their execution traces. We also discuss generalizations to other classes of programs, such as 1-D 
cellular automata
.
Results
Our experiments show that 
neural networks
 are capable of modeling the mechanisms underlying simple algorithms if enough execution traces are provided as data. We compare the performance of our program induction model with other similar experimental results from Graves et al. [4] and Vinyals et al. [5]. We were also able to demonstrate that sorting algorithms can be treated both as images displaying spatial patterns, as well as sequential instructions in a 
domain specific language
, such as swapping two elements. We tested our approach on three types of increasingly harder tasks: detection, recognition and emulation.
Conclusions
We demonstrate that simple algorithms can be modelled using 
neural networks
 and provide a method for representing specific classes of programs as either images or sequences of instructions in a domain-specific language, such that a neural network can learn their behavior. We consider the complexity of various set-ups to arrive at some improvements based on the 
data representation
 type. The insights from our experiments can be applied for designing better models of program induction.",21 Mar 2025,7,"The study on using neural networks to model algorithms and program induction techniques can have implications for startups in the fields of computer vision and natural language processing, although may require further exploration for practical applications."
https://www.sciencedirect.com/science/article/pii/S095058492030118X,An empirical evaluation of the use of models to improve the understanding of safety compliance needs,October 2020,Information and Software Technology,Not Found,Jose Luis=de la Vara: joseluis.delavara@uclm.es; Beatriz=Marín: beatriz.marin@mail.udp.cl; Clara=Ayora: claraayora@gmail.com; Giovanni=Giachetti: ggiachetti@inacap.cl,"Abstract
Context
Critical systems in application domains such as automotive, railway, aerospace, and healthcare are required to comply with safety standards. The understanding of the safety compliance needs specified in these standards can be difficult from their text. A possible solution is to use models.
Objective
We aim to evaluate the use of models to understand safety compliance needs.
Method
We have studied the effectiveness, efficiency, and perceived benefits in understanding these needs, with models and with the text of safety standards, by means of an experiment. The standards considered are DO-178C and EN 50128. We use SPEM-like diagrams to graphically represent the models.
Results
The mean effectiveness of 20 undergraduate students in understanding the needs and the mean efficiency were higher with models (22% and 38%, respectively), and the difference is statistically significant (p-value ≤ 0.02). Most of the students agreed upon the ease of understanding the structure of safety compliance needs with models when compared to the text, but on average, the students were undecided about whether the models are easy to understand or easier to understand than the text.
Conclusions
The results allow us to claim that the use of models can improve the understanding of safety compliance needs. Nonetheless, there seems to be room for improvement in relation to the perceived benefits. It must be noted that our conclusions may differ if the subjects were experienced practitioners.",21 Mar 2025,5,"While the use of models to understand safety compliance needs is important, the practical impact on early-stage ventures may be limited, as the study focuses more on compliance with safety standards in critical systems."
https://www.sciencedirect.com/science/article/pii/S0950584920301282,Requirements elicitation methods based on interviews in comparison: A family of experiments,October 2020,Information and Software Technology,Not Found,Silvia=Rueda: silvia.rueda@uv.es; Jose Ignacio=Panach: joigpana@uv.es; Damiano=Distante: damiano.distante@unitelmasapienza.it,"Abstract
Context
There are several methods to elicit requirements through interviews between an end-user and a team of software developers. The choice of the best method in this context is usually on subjective developers’ preferences instead of objective reasons. There is a lack of empirical evaluations of methods to elicit requirements that help developers to choose the most suitable one.
Objective
This paper designs and conducts a family of experiments to compare three methods to elicit requirements: Unstructured Interviews, where there is no specific protocol or artifacts; 
Joint
 Application Design (JAD), where each member of the development team has a specific role; Paper Prototyping, where developers contrast the requirements with the end-user through prototypes.
Method
The experiment is a between-subjects design with next response variables: number of requirements, time, diversity, completeness, quality and performance. The experiment consists of a maximum of 4 rounds of interviews between students that play the role of developers and an instructor that plays the role of client. Subjects had to elaborate a requirements specification document as results of the interviews. We recruited 167 subjects in 4 replications in 3 years. Subjects were gathered in development teams of 6 developers at most, and each team was an experimental unit.
Results
We found some significant differences. Paper Prototyping yields the best results to elicit as many requirements as possible, JAD requires the highest time to report the requirements and the least overlapping, and Unstructured Interviews yields the highest overlapping and the lowest time to report the requirements.
Conclusions
Paper Prototyping is the most suitable for eliciting functional requirements, JAD is the most suitable for non-functional requirements and to avoid overlapping, Unstructured Interviews is the fastest but with poor quality in the results.",21 Mar 2025,6,"Comparing methods to elicit requirements provides valuable insights, but the applicability to early-stage ventures may be limited as the study focuses on experimental settings with students rather than practical implications for startups."
https://www.sciencedirect.com/science/article/pii/S0950584920301154,A statistical pattern based feature extraction method on system call traces for anomaly detection,October 2020,Information and Software Technology,Not Found,Zhen=Liu: Not Found; Nathalie=Japkowicz: Not Found; Ruoyu=Wang: rywang@scut.edu.cn; Yongming=Cai: Not Found; Deyu=Tang: Not Found; Xianfa=Cai: Not Found,"Abstract
Context
In host-based 
anomaly detection
, feature extraction on the system call traces is important to build an effective 
anomaly detection
 model. Different kinds of feature extraction methods are recently proposed and most of them aim at preserving the positional information of the system calls within a trace. These extracted features are generally named from system calls, therefore, cannot be used directly in the case of cross platform applications. In addition, some of these feature extraction methods are very costly to implement.
Objective
This paper presents a new feature extraction method. It aims at extracting features that are irrelevant to the names of system calls. The samples represented by the extracted features can be directly used in the case of cross platform applications. In addition, this method is lightweight in that the feature values are not expensive to compute.
Method
The proposed method firstly transforms the system calls in a trace into frequency sequences of n-grams and then explores a fixed number of statistical features on the frequency sequences. The extracted features are irrelevant to the names/indexes of system calls on a platform. The calculation of feature values works on the frequency sequences rather than on system call sequences. These feature vectors built on the training set with only normal data are then used to train a one class 
classification model
 for 
anomaly detection
.
Results
We compared our method with four previously proposed feature extraction methods on system call traces. When used on the same platform, even though our method does not always obtain the highest AUC, overall, it performs better than all the compared methods. When testing on cross platform, it performs the best among all compared methods.
Conclusion
The features extracted by our method are platform-independent and are suitable for 
anomaly detection
 across platforms.",21 Mar 2025,8,"The proposed feature extraction method for anomaly detection across platforms presents a lightweight and effective solution, demonstrating superior performance compared to existing methods in both single and cross-platform scenarios."
https://www.sciencedirect.com/science/article/pii/S0950584920301324,Effort-Aware semi-Supervised just-in-Time defect prediction,October 2020,Information and Software Technology,Not Found,Weiwei=Li: liweiwei@nuaa.edu.cn; Wenzhou=Zhang: wenzhou2671@163.com; Xiuyi=Jia: jiaxy@njust.edu.cn; Zhiqiu=Huang: zqhuang@nuaa.edu.cn,"Abstract
Context
Software defect
 prediction is an important technique that can help practitioners allocate their quality assurance efforts. In recent years, just-in-time (JIT) 
defect prediction
 has attracted considerable interest, as it enables developers to identify risky changes at check-in time.
Objective
Many studies have conducted research from supervised and unsupervised perspectives. A model that does not rely on label information would be preferred. However, the performance of unsupervised models proposed by previous studies in the classification scenario was unsatisfactory due to the lack of 
supervised information
. Furthermore, most supervised models fail to outperform simple unsupervised models in the ranking scenario. To overcome this weakness, we conduct research from the semi-supervised perspective that only requires a small quantity of labeled data for training.
Method
In this paper, we propose a semi-supervised model for JIT defect prediction named Effort-Aware Tri-Training (EATT), which is an effort-aware method using a 
greedy strategy
 to rank changes. We compare EATT with the state-of-the-art supervised and unsupervised models with respect to different labeled rate.
Results
The experimental results on six open-source projects demonstrate that EATT outperforms existing supervised and unsupervised models for effort-aware JIT defect prediction, and has similar or superior performance in classifying defect-inducing changes.
Conclusion
The results show that EATT can not only achieve high 
classification accuracy
 as supervised models, but also offer more practical value than other compared models from the perspective of the effort needed to review changes.",21 Mar 2025,9,"The Effort-Aware Tri-Training model for JIT defect prediction offers a novel semi-supervised approach that outperforms existing supervised and unsupervised models, providing practical value and efficient ranking of defect-inducing changes."
https://www.sciencedirect.com/science/article/pii/S0950584920301117,Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems,October 2020,Information and Software Technology,Not Found,Behzad=Soleimani Neysiani: Not Found; Seyed Morteza=Babamir: babamir@kashanu.ac.ir; Masayoshi=Aritsugi: Not Found,"Abstract
Context
There are many duplicate 
bug reports
 in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems.
Objective
The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between 
bug reports
 accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model.
Method
This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and 
inverse document frequency
 of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones.
Results
The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the 
Android
, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for 
accuracy
 and 
precision
 and more than 4.5% and 5.9% improvement for 
recall
 and 
F1-measure
, respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier.
Conclusion
Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based 
machine learning algorithms
 are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers.",21 Mar 2025,10,"The proposed feature extraction model for duplicate bug report detection significantly improves validation performance, achieving notable improvements in accuracy, precision, recall, and F1-measure using advanced techniques and classifiers, offering practical value for software triage systems."
https://www.sciencedirect.com/science/article/pii/S0950584920301336,Simplifying the Search of npm Packages,October 2020,Information and Software Technology,Not Found,Ahmad=Abdellatif: a_bdella@encs.concordia.ca; Yi=Zeng: ze_yi@encs.concordia.ca; Mohamed=Elshafei: m_lshafe@encs.concordia.ca; Emad=Shihab: eshihab@encs.concordia.ca; Weiyi=Shang: shang@encs.concordia.ca,"Abstract
Context
Code reuse, generally done through software packages, allows developers to reduce time-to-market and improve code quality. The npm ecosystem is a Node.js package 
management system
 which contains more than 700 K Node.js packages and to help developers find high-quality packages that meet their needs, npms developed a search engine to rank Node.js packages in terms of quality, popularity, and maintenance. However, the current ranking mechanism for npms tends to be arbitrary and contains many different equations, which increases complexity and computation.
Objective
The goal of this paper is to empirically improve the efficiency of npms by simplifying the used components without impacting the current npms package ranks.
Method
We use feature selection methods with the aim of simplifying npms’ equations. We remove the features that do not have a significant effect on the package’s rank. Then, we study the impact of the simplified npms on the packages’ rank, the amount of resources saved compared to the original npms, and the performance of the simplified npms as npm evolves.
Results
Our findings indicate that (1) 31% of the unique variables of npms’ equation can be removed without breaking the original packages’ ranks; (2) The simplified npms, on average, preserves the overlapping of the packages by 98% and the ranking of those packages by 97%; (3) Using the simplified npms saves 10% of packages scoring time and more than 1.47 million network requests on each scoring run; (4) As the npm evolve through a period of 12 months, the simplified-npms was able to achieve results similar to the original npms.
Conclusion
Our results show that the simplified npms preserves the original ranks of packages and is more efficient than the original npms. We believe that using our approach, helps the npms community speed up the scoring process by saving 
computational resources
 and time.",21 Mar 2025,7,"The effort to simplify the npms ranking mechanism shows promising results by preserving package ranks and achieving efficiency gains, however, the impact on practical value for early-stage ventures may not be as substantial compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300689,Model composition in Model Driven Engineering: A systematic literature review,September 2020,Information and Software Technology,Not Found,Anas=Abouzahra: anas.abouzahra@edu.uiz.ac.ma; Ayoub=Sabraoui: Not Found; Karim=Afdel: Not Found,"Abstract
Context
Model Driven Engineering
 (MDE) aims to alleviate complexity and improve 
reusability
 in software development. The development of complex software implies to divide it into independent parts before then assembled. This is how the problem of model composition has become an interesting and stills an emerging topic in MDE.
Objective
Our goal is to analyze the current state of the art in model composition in the context of 
Model Driven Engineering
.
Method
We use the 
systematic literature review
 based on the guidelines proposed by Biolchini et al., Brereton et al., and Kitchenham and Charters. We propose five research questions and six quality assessments.
Results
Of the 9270 search results, 56 have been considered relevant studies. These studies have resulted in 36 primary studies.
Conclusion
The evaluation shows that most of approaches allow more than two models as inputs of the composition, allow composing heterogeneous models and enable the tuning of the composition schema, while the important limitations are about the maturity of implementations and the lack on the management of future evolutions or 
backwards compatibility
.",21 Mar 2025,6,"The analysis on model composition in Model Driven Engineering provides valuable insights, but the practical impact on European early-stage ventures, especially startups, may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300719,Automated isolation for white-box test generation,September 2020,Information and Software Technology,"Testing, Test generation, White-box, Isolation, Mocking, Code transformation, Empirical evaluation",Dávid=Honfi: honfi@mit.bme.hu; Zoltán=Micskei: micskeiz@mit.bme.hu,"Abstract
Context:
 White-box test generation is a technique used for automatically selecting test inputs using only the code under test. However, such techniques encounter challenges when applying them to complex programs. One of the challenges is handling invocations to external modules or dependencies in the code under test.
Objective:
 Without using proper isolation, like mocks, generated tests cannot cover all parts of the source code. Moreover, invoking 
external dependencies
 may cause unexpected side effects (e.g., accessing the file system or network). Our goal was to tackle this issue while maintaining the advantages of white-box test generation.
Method:
 In this paper, we present an automated approach addressing the external dependency challenge for white-box test generation. This technique isolates the test generation and execution by transforming the code under test and creating a parameterized sandbox with generated mocks. We implemented the approach in a ready-to-use tool using Microsoft Pex as a test generator, and evaluated it on 10 open-source projects from GitHub having more than 38.000 lines of code in total.
Results:
 The results from the evaluation indicate that if the lack of isolation hinders white-box test generation, then our approach is able to help: it increases the code coverage reached by the automatically generated tests, while it prevents invoking any external module or dependency. Also, our results act as a unique baseline for the test generation performance of Microsoft Pex on open-source projects.
Conclusion:
 Based on the results, our technique might serve well for handling external dependencies in white-box test generation as it increases the coverage reached in such situations, while maintaining the practical applicability of the tests generated on the isolated code.",21 Mar 2025,8,"The automated approach addressing external dependency challenges for white-box test generation can significantly impact the effectiveness and coverage of generated tests, making it practical and valuable for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300616,CodeGRU: Context-aware deep learning with gated recurrent unit for source code modeling,September 2020,Information and Software Technology,Not Found,Yasir=Hussain: yaxirhuxxain@nuaa.edu.cn; Zhiqiu=Huang: zqhuang@nuaa.edu.cn; Yu=Zhou: zhouyu@nuaa.edu.cn; Senzhang=Wang: szwang@nuaa.edu.cn,"Abstract
Context: Recently 
deep learning
 based 
Natural Language Processing
 (NLP) models have shown great potential in the modeling of 
source code
. However, a major limitation of these approaches is that they take 
source code
 as simple tokens of text and ignore its contextual, syntactical and structural dependencies.
Objective: In this work, we present CodeGRU, a 
gated recurrent unit
 based 
source code
 
language model
 that is capable of capturing source code’s contextual, syntactical and structural dependencies.
Method: We introduce a novel approach which can capture the 
source code
 context by leveraging the source code token types. Further, we adopt a novel approach which can learn variable size context by taking into account source code’s syntax, and structural information.
Results: We evaluate CodeGRU with real-world data set and it shows that CodeGRU outperforms the state-of-the-art language models and help reduce the vocabulary size up to 24.93%. Unlike previous works, we tested CodeGRU with an independent test set which suggests that our methodology does not requisite the source code comes from the same domain as 
training data
 while providing suggestions. We further evaluate CodeGRU with two 
software engineering
 applications: source code suggestion, and source code completion.
Conclusion: Our experiment confirms that the source code’s contextual information can be vital and can help improve the software language models. The extensive evaluation of CodeGRU shows that it outperforms the state-of-the-art models. The results further suggest that the proposed approach can help reduce the vocabulary size and is of practical use for software developers.",21 Mar 2025,9,"CodeGRU presents a novel approach to capturing contextual information in source code, outperforming state-of-the-art models and reducing vocabulary size, providing valuable tools for startups in software development."
https://www.sciencedirect.com/science/article/pii/S0950584920300926,A large scale study on how developers discuss code smells and anti-pattern in Stack Exchange sites,September 2020,Information and Software Technology,Not Found,Amjed=Tahir: a.tahir@massey.ac.nz; Jens=Dietrich: Not Found; Steve=Counsell: Not Found; Sherlock=Licorish: Not Found; Aiko=Yamashita: Not Found,"Abstract
Context:
 In this paper, we investigate how developers discuss 
code smells
 and 
anti-patterns
 across three technical Stack Exchange sites. Understanding developers perceptions of these issues is important to inform and align future research efforts and direct tools vendors to design tailored tools that best suit developers. 
Method:
 we mined three Stack Exchange sites and used quantitative and qualitative methods to analyse more than 4000 posts that discuss code smells and anti-patterns.
Results:
 results showed that developers often asked their peers to smell their code, thus utilising those sites as an 
informal, crowd-based
 code smell/anti-pattern detector. The majority of questions (556) asked were focused on smells like Duplicated Code, 
Spaghetti Code
, God and Data Classes. In terms of languages, most of discussions centred around popular languages such as C
#
 (772 posts), JavaScript (720) and Java (699), however greater support is available for Java compared to other languages (especially 
modern languages
 such as Swift and Kotlin). We also found that developers often discuss the downsides of implementing specific 
design patterns
 and ‘flag’ them as potential anti-patterns to be avoided. Some well-defined smells and anti-patterns are discussed as potentially being acceptable practice in certain scenarios. In general, developers actively seek to consider 
trade-offs
 to decide whether to use a design pattern, an anti-pattern or not.
Conclusion:
 our results suggest that there is a need for: 1) more 
context and 
domain sensitive
 evaluations of code smells and anti-patterns, 2) better guidelines for making 
trade-offs
 when applying 
design patterns
 or eliminating smells/anti-patterns in industry, and 3) a unified, constantly updated, catalog of smells and anti-patterns. We conjecture that the 
crowd-based
 detection approach considers contextual factors and thus tend to be more trusted by developers than automated detection tools.",21 Mar 2025,7,"The study on developers' discussions of code smells and anti-patterns provides insights and implications for future tool development and research alignment, offering practical guidance for early-stage ventures in software development."
https://www.sciencedirect.com/science/article/pii/S0950584920300732,Semantically find similar binary codes with mixed key instruction sequence,September 2020,Information and Software Technology,Not Found,Yuancheng=Li: yuancheng@ncepu.cn; Boyan=Wang: Not Found; Baiji=Hu: Not Found,"Abstract
Context
Software similarity comparison has always been a common technique for 
software reuse
 detection, 
plagiarism detection
, and defect detection.
Objective
Considering the role of API calls and 
arithmetic operations
 in software execution, a semantic-based dynamic software analysis method–mixed key instruction sequence (MKIS) is proposed.
Method
MKIS embeds key value sets into a vector and constructs a novel software execution sequence that contains API calls and 
arithmetic operations
 during software execution. To determine the location of key values, a key-value equivalent 
matching algorithm
 is proposed, combined with the longest common subsequence algorithm to optimize the software execution sequence.
Results
Experiments show that MKIS can accurately compare the similarity of binary programs without obtaining the software source code, and has better resiliency and credibility.
Conclusion
Moreover, in the case when the software source code is changed with some main function-independent modification and code obfuscator, 
software reuse
 can be successfully detected.",21 Mar 2025,6,"The proposed semantic-based dynamic software analysis method has the potential to impact software reuse and plagiarism detection, though the practical application for startups may require further validation and adaptation."
https://www.sciencedirect.com/science/article/pii/S095058492030104X,Comparing manual and automated feature location in conceptual models: A Controlled experiment,September 2020,Information and Software Technology,Not Found,Francisca=Pérez: mfperez@usj.es; Jorge=Echeverría: jecheverria@usj.es; Raúl=Lapeña: rlapena@usj.es; Carlos=Cetina: ccetina@usj.es,"Abstract
Context
Maintenance activities cannot be completed without locating the set of software artifacts that realize a particular feature of a software system. Manual Feature Location (FL) is widely used in industry, but it becomes challenging (time-consuming and error prone) in large software repositories. To reduce manual efforts, automated FL techniques have been proposed. Research efforts in FL tend to make comparisons between automated FL techniques, ignoring manual FL techniques. Moreover, existing research puts the focus on code, neglecting other artifacts such as models.
Objective
This paper aims to compare manual FL against automated FL in models to answer important questions about performance, productivity, and satisfaction of both treatments.
Method
We run an experiment for comparing manual and automated FL on a set of 18 subjects (5 experts and 13 non-experts) in the domain of our industrial partner, BSH, manufacturer of induction hobs for more than 15 years. We measure performance (recall, precision, and F-measure), productivity (ratio between F-measure and spent time), and satisfaction (perceived ease of use, perceived usefulness, and intention to use) of both treatments, and perform statistical tests to assess whether the obtained differences are significant.
Results
Regarding performance, manual FL significantly outperforms automated FL in precision and F-measure (up to 27.79% and 19.05%, respectively), whereas automated FL significantly outperforms manual FL in recall (up to 32.18%). Regarding productivity, manual FL obtains 3.43%/min, which improves automated FL significantly. Finally, there are no significant differences in satisfaction for both treatments.
Conclusions
The findings of our work can be leveraged to advance research to improve the results of manual and automated FL techniques. For instance, automated FL in industry faces issues such as low discrimination capacity. In addition, the obtained satisfaction results have implications for the usage and possible combination of manual, automated, and guided FL techniques.",21 Mar 2025,5,"While comparing manual and automated Feature Location techniques, the study offers insights into performance and productivity, but the direct impact on early-stage ventures in the European startup ecosystem may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584920300872,On an optimal analogy-based software effort estimation,September 2020,Information and Software Technology,Not Found,Passakorn=Phannachitta: passakorn.p@cmu.ac.th,"Abstract
Context:
 An analogy-based software effort estimation technique estimates the required effort for a new software project based on the total effort used in completing past similar projects. In practice, offering high accuracy can be difficult for the technique when the new software project is not similar to any completed projects. In this case, the accuracy will rely heavily on a process called effort adaptation, where the level of difference between the new project and its most similar past projects is quantified and transformed to the difference in the effort. In the past, attempts to adapt to the effort used 
machine learning algorithms
; however, no algorithm was able to offer a significantly higher performance. On the contrary, only a simple heuristic such as scaling the effort by consulting the difference in software size was adopted.
Objective:
More recently, million-dollar prize data-science competitions have fostered the rapid development of more powerful 
machine learning
 algorithms, such as the 
Gradient boosting
 machine and 
Deep learning algorithm
. Therefore, this study revisits the comparison of software effort adaptors that are based on heuristics and 
machine learning algorithms
.
Method:
A systematic comparison of software effort estimators, which they all were fully optimized by Bayesian optimization technique, was carried out on 13 standard benchmark datasets. The comparison was supported by robust performance metrics and robust statistical test methods.
Conclusion:
The results suggest a novel strategy to construct a more accurate analogy-based estimator by adopting a combined effort adaptor. In particular, the analogy-based model that adapts to the effort by integrating the 
Gradient boosting
 machine algorithm and a traditional adaptation technique based on productivity adjustment has performed the best in the study. Particularly, this model significantly outperformed various state-of-the-art effort estimation techniques, including a current standard benchmark algorithmic-based technique, analogy-based techniques, and machine learning-based techniques.",21 Mar 2025,8,"This study offers a novel strategy for software effort estimation by combining traditional adaptation techniques with machine learning algorithms, showing significant performance improvements, which can directly impact the accuracy of effort estimations for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584920300938,ManQ: Many-objective optimization-based automatic query reduction for IR-based bug localization,September 2020,Information and Software Technology,Not Found,Misoo=Kim: misoo12@skku.edu; Eunseok=Lee: leees@skku.edu,"Abstract
Context
An information retrieval-based bug localization (IRBL) method is proposed to localize buggy files using a 
bug report
 as a query. The performance of this method strongly depends on the quality of the query. However, these queries contain noise terms that hinder their use for IRBL. To improve the quality of a query, an automatic query reduction (AQR) technique that removes noise words from the query is needed.
Objective
Our objective is to develop an AQR method for IRBL. Most existing AQR techniques are based on single 
objective optimization
, which presents issues in terms of biased and limited performance. To solve these issues, it is necessary to find a subquery that comprehensively satisfies all of their objectives.
Method
We propose an AQR technique called ManQ, which is a many-objective optimization-based AQR method for IRBL. We design 15 objective functions to (1) maintain the query quality properties, (2) maintain the important terms, (3) maintain the initial information, and (4) minimize the query length. ManQ finds a final subquery that maximize the return values of these objective functions.
Results
The experimental results show that ManQ improves the quality of poor queries. We also show that if we select the best query among the candidates generated by ManQ, we can increase the number of improved queries by more than 53.4% of all queries.
Conclusion
ManQ improves the performance of IRBL by improving the quality of queries through a many-objective optimization approach.",21 Mar 2025,6,"The proposed AQR method for IRBL improves the quality of queries and performance of bug localization, which could benefit startups dealing with software bugs. However, the impact may be less direct compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920301038,Developer portraying: A quick approach to understanding developers on OSS platforms,September 2020,Information and Software Technology,Not Found,Wenhua=Yang: ywh@nuaa.edu.cn; Minxue=Pan: mxp@nju.edu.cn; Yu=Zhou: zhouyu@nju.edu.cn; Zhiqiu=Huang: zqhuang@nuaa.edu.cn,"Abstract
Context
Millions of software developers are using open-source software (OSS) platforms to host their code and collaborate with each other. They possess different programming skills, styles, and preferences, etc., and it is important to understand them for making collaborative decisions such as programming task assignment. Existing OSS platforms do not provide sufficient information about developers, and we need to spend significant effort in searching the OSS platforms for such information.
Objective
Different than the basic developer information displayed on OSS platforms, we propose portraying developers as a quick approach for characterizing and understanding them. We discuss how to build developer portraits to make them concise yet informative.
Method
We propose a multi-dimensional developer portrait model to specify the attributes of various aspects concerning software development about developers. Then, a method that leverages text analysis, web data analysis, and code analysis techniques is presented to analyze a developer’s various sources of data on OSS platforms for constructing the portrait.
Results
The constructed portraits can be vividly displayed on the web to help people quickly understand developers and make better decisions during 
collaborative software development
. 
Case studies
 on two representative problems in the 
software engineering
 area—code recommendation and programming task assignment—are conducted, and the results show the improvement in recommendation and the potential for proper assignments when using our portraits.
Conclusion
The developer portrait is an effective form to characterize developers. It can help people quickly understand the developers and can be applied to various applications in the software development process.",21 Mar 2025,9,The developer portrait model proposed in this study can significantly aid collaborative decision-making in software development by providing concise yet informative representations of developers. This can be highly valuable for startups looking to optimize team dynamics.
https://www.sciencedirect.com/science/article/pii/S0950584920300641,Multiple fault localization of software programs: A systematic literature review,August 2020,Information and Software Technology,Not Found,Abubakar=Zakari: abubakar.zakari@yahoo.com; Sai Peck=Lee: Not Found; Rui=Abreu: Not Found; Babiker Hussien=Ahmed: Not Found; Rasheed Abubakar=Rasheed: Not Found,"Abstract
Context
Multiple 
fault localization
 (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed.
Objective
The aim of this study is to systematically review existing research on MFL in the 
software fault
 localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain.
Method
Consequently, using an evidence-based 
systematic methodology
, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process.
Result
The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most.
Conclusion
The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community.",21 Mar 2025,5,"While the systematic review on multiple fault localization provides valuable insights into existing research trends, the practical impact on early-stage ventures may be limited due to the focus on academic challenges and less mature solutions."
https://www.sciencedirect.com/science/article/pii/S0950584920300434,Software product line applied to the internet of things: A systematic literature review,August 2020,Information and Software Technology,Not Found,Ricardo Theis=Geraldi: ricardo.geraldi@ppgia.pucpr.br; Sheila=Reinehr: sheila.reinehr@pucpr.br; Andreia=Malucelli: malu@ppgia.pucpr.br,"Abstract
Context
Internet of Things
 (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. 
SPL engineering
 consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities.
Objective
This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out.
Method
A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria.
Results
The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.",21 Mar 2025,7,"Investigating the application of SPL engineering and VM in IoT systems has the potential to offer insights into managing IoT devices efficiently. The findings could be beneficial for startups in the IoT sector, although the direct impact may vary."
https://www.sciencedirect.com/science/article/pii/S0950584920300665,LTRWES: A new framework for security bug report detection,August 2020,Information and Software Technology,Not Found,Yuan=Jiang: jiangyuan@hit.edu.cn; Pengcheng=Lu: pclu57@gmail.com; Xiaohong=Su: sxh@hit.edu.cn; Tiantian=Wang: sweetwtt@126.com,"Abstract
Context
: Security 
bug reports
 (SBRs) usually contain security-related vulnerabilities in software products, which could be exploited by malicious attackers. Hence, it is important to identify SBRs quickly and accurately among 
bug reports
 (BRs) that have been disclosed in 
bug tracking systems
. Although a few methods have been already proposed for the detection of SBRs, challenging issues still remain due to noisy samples, 
class imbalance
 and data scarcity.
Object
: This motivates us to reveal the potential challenges faced by the state-of-the-art SBRs prediction methods from the viewpoint of data filtering and representation. Furthermore, the purpose of this paper is also to provide a general framework and new solutions to solve these problems.
Method
: In this study, we propose a novel approach LTRWES that incorporates learning to rank and 
word embedding
 into the identification of SBRs. Unlike previous keyword-based approaches, LTRWES is a content-based data filtering and representation framework that has several 
desirable properties
 not shared in other methods. Firstly, it exploits ranking model to efficiently filter non-security bug reports (NSBRs) that have higher content similarity with respect to SBRs. Secondly, it applies 
word embedding
 technology to transform the rest of NSBRs, together with SBRs, into low-dimensional real-value vectors.
Result
: Experiment results on benchmark and large real-world datasets show that our proposed method outperforms the state-of-the-art method.
Conclusion
: Overall, the LTRWES is valid with high performance. It will help security engineers to identify SBRs from thousands of NSBRs more accurately than existing algorithms. Therefore, this will positively encourage the research and development of the content-based methods for security bug report detection.",21 Mar 2025,8,"The proposed method addresses important challenges in security bug report detection, outperforming existing methods and enabling more accurate identification of SBRs. This can significantly impact early-stage ventures by improving the security of their software products."
https://www.sciencedirect.com/science/article/pii/S0950584920300707,A feedback-directed method of evolutionary test data generation for parallel programs,August 2020,Information and Software Technology,Not Found,Dunwei=Gong: Not Found; Feng=Pan: Not Found; Tian=Tian: tian_tiantian@126.com; Su=Yang: Not Found; Fanlin=Meng: Not Found,"Abstract
Context:
 
Genetic algorithms
 can be utilized for automatic test data generation. Test data are encoded as individuals which are evolved for a number of generations using genetic operators. Test data of a parallel program include not only the program input, but also the communication information between each pair of processes. Traditional genetic algorithms, however, do not make full use of information provided by a population’s evolution, resulting in a low efficiency in generating test data. 
Objective:
 This paper emphasizes the problem of test data generation for parallel programs, and presents a feedback-directed genetic algorithm for generating test data of path coverage. 
Method:
 Information related to a schedule sequence is exploited to improve genetic operators. Specifically, a scheduling sequence is evaluated according to how well an individual covers the target path. The probability of the crossover and 
mutation points
 being located in the region is determined based on the evaluation result, which prevents a good schedule sequence from being destroyed. If crossover and mutation are performed in the scheduling sequence, the location of crossover and 
mutation points
 is further determined according to the relationship between nodes to be covered and the scheduling sequence. In this way, the population can be evolved in a narrowed 
search space
. 
Results:
 The proposed algorithm is applied to test 11 parallel programs. The experimental results show that, compared with the genetic algorithm without utilizing information during the population evolution, the proposed algorithm significantly reduces the number of generations and the time consumption. 
Conclusion:
 The proposed algorithm can greatly improve the efficiency in evolutionary test data generation.",21 Mar 2025,7,"The algorithm presented for test data generation for parallel programs can greatly improve efficiency, reducing the number of generations and time consumption. This can benefit startups working on parallel programs by enhancing their testing processes."
https://www.sciencedirect.com/science/article/pii/S0950584920300690,An experimental and practical study on the equivalent mutant connection: An evolutionary approach,August 2020,Information and Software Technology,Not Found,Pedro=Delgado-Pérez: pedro.delgado@uca.es; Francisco=Chicano: Not Found,"Abstract
Context
Mutation testing is considered to be a powerful approach to assess and improve the quality of test suites. However, this technique is expensive mainly because some mutants are semantically equivalent to the original program; in general, equivalent mutants require manual revision to differentiate them from useful ones, which is known as the Equivalent Mutant Problem (EMP).
Objective
In the past, several authors have proposed different techniques to individually identify certain equivalent mutants, with notable advances in the last years. In our work, by contrast, we address the EMP from a global perspective. Namely, we wonder the extent to which equivalent mutants are connected (i.e., whether they share 
mutation operators
 and code areas) as well as the extent to which the knowledge of that connection can benefit the mutant selection process. Such a study could allow going beyond the implicit limit in the traditional individual detection of equivalent mutants.
Method
We use an 
evolutionary algorithm
 to select the mutants, an approach called Evolutionary Mutation Testing (EMT). We propose a new derived version, 
Equivalence-Aware EMT
 (EA-EMT), which penalizes the fitness of known equivalent mutants so that they do not transfer their features to the next generations of mutants.
Results
In our experiments applying EMT to well-known C++ programs, we found that (i) equivalent mutants often originate from other equivalent mutants (over 60% on average); (ii) EA-EMT’s approach of penalizing known equivalent mutants provides better results than the original EMT in most of the cases (notably, the more equivalent mutants are detected, the better); and (iii) we can combine EA-EMT with Trivial Compiler Equivalence as a way to automatically identify equivalent mutants in a real situation, reaching a more stable version of EMT.
Conclusions
This novel approach opens the way for improvement in other related areas that deal with equivalent versions.",21 Mar 2025,9,The approach of addressing the Equivalent Mutant Problem from a global perspective and proposing a new Evolutionary Mutation Testing method can greatly impact the quality of test suites. Startups can benefit from improved testing processes and more efficient identification of mutants.
https://www.sciencedirect.com/science/article/pii/S0950584920300501,Automatic block dimensioning on GPU-accelerated programs through particle swarm optimization,July 2020,Information and Software Technology,Not Found,Claudio M.N.A.=Pereira: cmnap@ien.gov.br; Andre L.S.=Pinheiro: Not Found; Roberto=Schirru: Not Found,"Abstract
Context
Nowadays, the use of GPU to improve performance of computationally expensive systems are widely explored. On GPU-accelerated programs, performance is related to the partition of the problem into blocks of threads in such a way that the parallel tasks to be executed better fit the GPU architecture. Although there exists some general guidelines to help defining 
block dimensions
, finding the optimum partition is still a complex and problem dependent task. In this work, it has been investigated the use of 
particle swarm optimization
 (PSO) to optimize blocks dimensions aiming to minimize 
programs execution time
. The approach was evaluated on a GPU-accelerated wind field calculation program, in which block dimensioning was based on literature guidelines and empirical adjusts. Before PSO optimization, the program was about 25 times faster than the sequential program. After applying PSO, speedup increased to about 60 times. Unexpected optimized configurations were observed, ratifying that finding optimum dimensioning is a complex task. So the use of a 
robust optimization
 tool, such as PSO, demonstrated to be very profitable, allowing automatic optimization of blocks dimensions without necessity of a 
priori knowledge
 about problem, programs peculiarities and GPU architecture.
Objective
Improve speedup of GPU-accelerated programs by automatic defining optimized 
block dimensions
 using PSO.
Method
A GPU-accelerated wind field calculation problem has been focused. A PSO was interfaced to the program in order to find the block dimensions that leads to a minimum 
execution time
. Results were compared to literature results.
Results
The speedup obtained with the proposed approach is more than 2 times the original speedup.
Conclusion
PSO, demonstrated to be very profitable, allowing automatic optimization of blocks dimensions without necessity of a 
priori knowledge
 about problem/programs peculiarities and/or GPU architecture.",21 Mar 2025,8,"Using particle swarm optimization to optimize block dimensions for GPU-accelerated programs can significantly improve performance, providing a speedup of more than 2 times the original speedup. This can have a positive impact on startups utilizing GPU acceleration for their systems."
https://www.sciencedirect.com/science/article/pii/S0950584920300458,Search-based fault localisation: A systematic mapping study,July 2020,Information and Software Technology,Not Found,Plinio S.=Leitao-Junior: plinio@inf.ufg.br; Diogo M.=Freitas: diogom42@gmail.com; Silvia R.=Vergilio: silvia@inf.ufpr.br; Celso G.=Camilo-Junior: celso@inf.ufg.br; Rachel=Harrison: rachel.harrison@brookes.ac.uk,"Abstract
Context
Software 
Fault Localisation
 (FL) refers to finding faulty software elements related to failures produced as a result of test case execution. This is a laborious and time consuming task. To allow FL automation search-based algorithms have been successfully applied in the field of Search-Based Fault Localisation (SBFL). However, there is no study mapping the SBFL field to the best of our knowledge and we believe that such a map is important to promote new advances in this field.
Objective
To present the results of a mapping study on SBFL, by characterising the proposed methods, identifying sources of used information, adopted evaluation functions, applied algorithms and elements regarding reported experiments.
Method
Our mapping followed a defined process and a search protocol. The conducted analysis considers different dimensions and categories related to the main characteristics of 
SBFL methods
.
Results
All methods are grounded on the coverage spectra category. Overall the methods search for solutions related to suspiciousness formulae to identify possible faulty code elements. Most studies use 
evolutionary algorithms
, mainly 
Genetic Programming
, by using a single-objective function. There is little investigation of real-and-multiple-fault scenarios, and the subjects are mostly written in C and Java. No consensus was observed on how to apply the 
evaluation metrics
.
Conclusions
Search-based fault localisation has seen a rise in interest in the past few years and the number of studies has been growing. We identified some research opportunities such as exploring new sources of fault data, exploring multi-objective algorithms, analysing benchmarks according to some classes of faults, as well as, the use of a unique definition for evaluation measures.",21 Mar 2025,7,The mapping study on Search-Based Fault Localization provides insights into the current state of the field and identifies research opportunities. This knowledge can be valuable for startups looking to improve their fault localization processes and enhance the quality of their software products.
https://www.sciencedirect.com/science/article/pii/S095058491930240X,Management of quality requirements in agile and rapid software development: A systematic mapping study,July 2020,Information and Software Technology,Not Found,Woubshet=Behutiye: woubshet.behutiye@oulu.fi; Pertti=Karhapää: Not Found; Lidia=López: Not Found; Xavier=Burgués: Not Found; Silverio=Martínez-Fernández: Not Found; Anna Maria=Vollmer: Not Found; Pilar=Rodríguez: Not Found; Xavier=Franch: Not Found; Markku=Oivo: Not Found,"Abstract
Context
Quality requirements (QRs) describe the desired 
quality of software
, and they play an important role in the success of software projects. In 
agile software development
 (ASD), QRs are often ill-defined and not well addressed due to the focus on quickly delivering functionality. Rapid software development (RSD) approaches (e.g., continuous delivery and continuous deployment), which shorten delivery times, are more prone to neglect QRs. Despite the significance of QRs in both ASD and RSD, there is limited synthesized knowledge on their management in those approaches.
Objective
This study aims to synthesize state-of-the-art knowledge about QR management in ASD and RSD, focusing on three aspects: bibliometric, strategies, and challenges.
Research method
Using a 
systematic mapping study
 with a snowballing search strategy, we identified and structured the literature on QR management in ASD and RSD.
Results
We found 156 primary studies: 106 are empirical studies, 16 are experience reports, and 34 are theoretical studies. Security and performance were the most commonly reported QR types. We identified various QR management strategies: 74 practices, 43 methods, 13 models, 12 frameworks, 11 advices, 10 tools, and 7 guidelines. Additionally, we identified 18 categories and 4 non-recurring challenges of managing QRs. The limited ability of ASD to handle QRs, time constraints due to short iteration cycles, limitations regarding the testing of QRs and neglect of QRs were the top categories of challenges.
Conclusion
Management of QRs is significant in ASD and is becoming important in RSD. This study identified research gaps, such as the need for more tools and guidelines, lightweight QR management strategies that fit short iteration cycles, investigations of the link between QRs challenges and technical debt, and extension of empirical validation of existing strategies to a wider context. It also synthesizes QR management strategies and challenges, which may be useful for practitioners.",21 Mar 2025,7,"The study provides valuable insights into quality requirements management in agile and rapid software development approaches, offering strategies and identifying challenges that can benefit early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584920300422,Orientation-based Ant colony algorithm for synthesizing the test scenarios in UML activity diagram,July 2020,Information and Software Technology,Not Found,Vinay=Arora: vinay.arora@thapar.edu; Maninder=Singh: Not Found; Rajesh=Bhatia: Not Found,"Abstract
Context
The model-based analysis is preferred over the code-based analysis as it speeds up the development process and directs the guiding effort. In the software industry, the Unified Modeling Language (UML) is a set standard followed by the developers as well as system analysts to extract all attainable paths of controls, usually known as scenarios under an activity diagram.
Objective
In this manuscript, a bio-inspired methodology has been applied on concurrent sub-part of a UML activity diagram to fetch various feasible test scenarios.
Method
The food search pattern of an ant has been taken as a base heuristic. An orientation factor has been introduced in the existing ant colony optimization algorithm. Experiments have been performed using three student projects, five synthetic models and an openly available model repository named LINDHOLMEN data-set at Github.
Results
The statistical analysis has validated the results obtained through various existing approaches and the proposed approach. Experimentation shows that the orientation-based ant colony algorithm has produced better results as compared to the existing Genetic Algorithm (GA) and Ant Colony Optimization (ACO) on the basis of feasible test scenarios generated.",21 Mar 2025,5,"While the bio-inspired methodology applied in the study is innovative, its practical application in early-stage European ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300392,BVDetector: A program slice-based binary code vulnerability intelligent detection system,July 2020,Information and Software Technology,Not Found,Junfeng=Tian: Not Found; Wenjing=Xing: Not Found; Zhen=Li: lizhen@hbu.edu.cn,"Abstract
Context
Software 
vulnerability detection
 is essential to ensure cybersecurity. Currently, most software is published in binary form, thus researchers can only detect vulnerabilities in these software by analysing binary programs. Although existing research approaches have made a substantial contribution to binary 
vulnerability detection
, there are still many deficiencies, such as high false positive rate, detection with 
coarse granularity
, and dependence on expert experience.
Objective
The goal of this study is to perform fine-grained intelligent detection on the vulnerabilities in binary programs. This leads us to propose a fine-grained representation of binary programs and introduce 
deep learning techniques
 to intelligently detect the vulnerabilities.
Method
We use program slices of library/API function calls to represent binary programs. Additionally, we design and construct a Binary 
Gated Recurrent Unit
 (BGRU) network model to intelligently learn 
vulnerability patterns
 and automatically detect vulnerabilities in binary programs.
Results
This approach yields the design and implementation of a program slice-based binary code vulnerability intelligent detection system called BVDetector. We show that BVDetector can effectively detect vulnerabilities related to library/API function calls in binary programs, which reduces the false positive rate and 
false negative
 rate of vulnerability detection.
Conclusion
This paper proposes a program slice-based binary code vulnerability intelligent detection system called BVDetector. The experimental results show that BVDetector can effectively reduce the 
false negative
 rate and false positive rate of binary vulnerability detection.",21 Mar 2025,9,The use of deep learning techniques for intelligent detection of vulnerabilities in binary programs has significant practical value and impact on European startups by improving detection accuracy and reducing false positives.
https://www.sciencedirect.com/science/article/pii/S0950584920300628,Deep learning model for end-to-end approximation of COSMIC functional size based on use-case names,July 2020,Information and Software Technology,Not Found,Mirosław=Ochodek: mochodek@cs.put.poznan.pl; Sylwia=Kopczyńska: sylwia.kopczynska@cs.put.poznan.pl; Miroslaw=Staron: miroslaw.staron@cse.gu.se,"Abstract
Context
COSMIC is a widely used functional size measurement (FSM) method that supports software development effort estimation. The FSM methods measure functional product size based on functional requirements. Unfortunately, when the description of the product’s functionality is often abstract or incomplete, the size of the product can only be approximated since the object to be measured is not yet fully described. Also, the measurement performed by human-experts can be time-consuming, therefore, it is worth considering automating it.
Objective
Our objective is to design a new prediction model capable of approximating COSMIC-size of use cases based only on their names that is easier to train and more accurate than existing techniques.
Method
Several neural-network architectures are investigated to build a COSMIC size 
approximation
 model. The accuracy of models is evaluated in a simulation study on the dataset of 437 use cases from 27 software development projects in the 
Management Information Systems
 (MIS) domain. The accuracy of the models is compared with the Average Use-Case 
approximation
 (AUC), and two recently proposed two-step models—Average Use-Case Goal-aware Approximation (AUCG) and 
Bayesian Network
 Use-Case Goal AproxImatioN (BN-UCGAIN).
Results
The best prediction accuracy was obtained for a 
convolutional neural network
 using a word-embedding model trained on Wikipedia+Gigaworld. The accuracy of the model outperformed the baseline AUC model by ca. 20%, and the two-step models by ca. 5–7%. In the worst case, the improvement in the prediction accuracy is visible after estimating 10 use cases.
Conclusions
The proposed 
deep learning
 model can be used to automatically approximate COSMIC size of 
software applications
 for which the requirements are documented in the form of use cases (or at least in the form of use-case names). The advantage of the model is that it does not require collecting 
historical data
 other than COSMIC size and names of use cases.",21 Mar 2025,6,"Automating the approximating of COSMIC size based on use case names can be beneficial for startups in Europe, although the impact may not be as high compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300604,Assessing the effectiveness of approximate functional sizing approaches for effort estimation,July 2020,Information and Software Technology,Not Found,Sergio=Di Martino: sergio.dimartino@unina.it; Filomena=Ferrucci: fferrucci@unisa.it; Carmine=Gravino: gravino@unisa.it; Federica=Sarro: f.sarro@ucl.ac.uk,"Abstract
Context
: Functional Size Measurement (FSM) methods, like Function Points Analysis (FPA) or 
COSMIC
, are well-established approaches to estimate software size. Several 
approximations
 of these methods have been recently proposed as they require less time/information to be applied, however their effectiveness for effort prediction is not known.
Objective
: The effectiveness of approximated functional size measures for estimating the development effort is a key open question, since an approximate sizing approach may miss to capture factors affecting the effort. Therefore, we empirically investigated the use of approximate FPA and 
COSMIC
 sizing approaches, also compared with their standard versions, for effort estimation.
Method
: We measured 25 industrial software projects realised by a single company by using FPA, COSMIC, two approximate sizing approaches proposed by 
IFPUG
 for FPA (i.e. High Level and Indicative FPA), and three approximate sizing approaches proposed by the 
COSMIC organisation
 for COSMIC (i.e. Average Functional Process, Fixed Size Classification, and Equal Size Band). Then we investigated the quality of the regression models built using the obtained measures to estimate the development effort.
Results
: Models based on High Level FPA are effective, providing a prediction accuracy comparable to the one of the original FPA, while those based on the Indicative FPA method show poor estimation accuracy. Models based on COSMIC approximate 
sizing methods
 are also quite effective, in particular those based on the Equal Size Band approximation provided an accuracy similar to the one of standard COSMIC.
Conclusion
: Project managers should be aware that predictions based on High Level FPA and standard FPA can be similar, making this approximation very interesting and effective, while Indicative FPA should be avoided. COSMIC 
approximations
 can also provide accurate effort estimates, nevertheless, the Fixed Size Classification and Equal Size Band approaches introduce subjectivity in the measurement.",21 Mar 2025,8,"The empirical investigation on the effectiveness of approximated functional size measures for effort estimation provides practical implications for European early-stage ventures, allowing them to make informed decisions on sizing approaches."
https://www.sciencedirect.com/science/article/pii/S0950584920300495,Analyzing and documenting the systematic review results of software testing ontologies,July 2020,Information and Software Technology,Not Found,Guido=Tebes: guido.tebes92@gmail.com; Denis=Peppino: denispeppino92@gmail.com; Pablo=Becker: beckerp@ing.unlpam.edu.ar; Gerardo=Matturro: matturro@ort.edu.uy; Martin=Solari: martin.solari@ort.edu.uy; Luis=Olsina: olsinal@ing.unlpam.edu.ar,"Abstract
Context
Software testing is a complex area since it has a large number of specific methods, processes and strategies, involving a lot of domain concepts. Therefore, it would be valuable to have a conceptualized software testing ontology that explicitly and unambiguously defines the concepts. Consequently, it is important to find out the available evidence in the literature on primary studies for software testing ontologies. In particular, we are looking for research that has a rich ontological coverage that includes Non-Functional Requirements (NFRs) and Functional Requirements (FRs) concepts in conjunction with static and dynamic testing concepts, which can be used in method and process specifications for a family of testing strategies.
Objective
The main goal for this secondary study is to identify, evaluate and synthesize the available primary studies on conceptualized software testing ontologies.
Method
To conduct this study, we use the Systematic Literature Review (SLR) approach, which follows our enhanced SLR process. We set three research questions. Additionally, to quantitatively evaluate the quality of the selected conceptualized ontologies, we designed a NFRs tree and its associated metrics and indicators.
Results
We obtained 12 primary studies documenting conceptualized testing ontologies by using three different 
retrieval methods
. In general, we noted that most of them have a lack of NFRs and static testing terminological coverage. Finally, we observe that none of them is directly linked with FRs and NFRs conceptual components.
Conclusion
A general benefit of having the suitable software testing ontology is to minimize the current heterogeneity, ambiguity and incompleteness problems in terms, properties and relationships. We have confirmed that exists heterogeneity, ambiguity, and incompleteness for concepts dealing with testing artifacts, roles, activities, and methods. Moreover, we did not find the suitable ontology for our aim since none of the conceptualized ontologies are directly linked with NFRs and FRs components.",21 Mar 2025,7,"The study focuses on the development of a conceptualized software testing ontology, aiming to minimize current problems in software testing. The findings could have a positive impact on early-stage ventures by providing a structured approach to software testing."
https://www.sciencedirect.com/science/article/pii/S095058492030063X,Demystifying the adoption of behavior-driven development in open source projects,July 2020,Information and Software Technology,Not Found,Fiorella=Zampetti: Not Found; Andrea=Di Sorbo: Not Found; Corrado Aaron=Visaggio: Not Found; Gerardo=Canfora: Not Found; Massimiliano=Di Penta: dipenta@unisannio.it,"Abstract
Context:
Behavior-Driven Development (BDD) features the capability, through appropriate domain-specific languages, of specifying acceptance test cases and making them executable. The availability of frameworks such as Cucumber or RSpec makes the application of BDD possible in practice. However, it is unclear to what extent developers use such frameworks, and whether they use them for actually performing BDD, or, instead, for other purposes such as unit testing. 
Objective:
In this paper, we conduct an empirical investigation about the use of BDD tools in open source, and how, when a BDD tool is in place, BDD specifications co-evolve with source code. 
Method:
Our investigation includes three different phases: (i) a large-scale analysis to understand the extent to which BDD frameworks are used in 50,000 popular open-source projects written in five programming languages; (ii) a study on the co-evolution of scenarios, fixtures and production code in a sample of 20 Ruby projects, through the Granger’s causality test, and (iii) a survey with 31 developers to understand how they use BDD frameworks. 
Results:
Results of the study indicate that  ≃  27% of the sampled projects use BDD frameworks, with a prevalence in Ruby projects (68%). In about 37% of the cases, we found a co-evolution between scenarios/fixtures and production code. Specifically, changes to scenarios and fixtures often happen together or after changes to source code. Moreover, survey respondents indicate that, while they understand the intended purpose of BDD frameworks, most of them write tests while/after coding rather than strictly applying BDD. 
Conclusions:
Even if the BDD frameworks usage is widespread among 
open source projects
, in many cases they are used for different purposes such as unit testing activities. This mainly happens because developers felt BDD remains quite effort-prone, and its application goes beyond the simple adoption of a BDD framework.",21 Mar 2025,5,"The investigation on the usage of BDD frameworks in open source projects provides insights on developer practices. While the findings are interesting, the practical impact on early-stage ventures may be limited as BDD frameworks are more widely used for unit testing."
https://www.sciencedirect.com/science/article/pii/S0950584920300598,An empirical comparison of predictive models for web page performance,July 2020,Information and Software Technology,Not Found,Raghu=Ramakrishnan: raghuramakrishnan71@gmail.com; Arvinder=Kaur: Not Found,"Abstract
Context
The quality of 
user experience
 is the cornerstone of any organization’s successful digital transformation journey. Web pages are the main touchpoint for users to access services in a digital mode. Web page performance is a key determinant of the quality of 
user experience
. The 
negative impact
 of poor web page performance on the productivity, profits, and brand value of an organization is well-recognized. The use of realistic prediction models for predicting page load time at the early stages of development can help minimize the effort and cost arising out of fixing performance defects late in the lifecycle.
Objective
We present a comprehensive evaluation of models based on 18 widely used 
machine learning techniques
 on their capability to predict page load times. The models use only those metrics which relate to the form and structure of a page because such metrics are easy to ascertain during the early stages with minimal effort.
Method
The 
machine learning techniques
 are trained on more than 8,700 pages from HTTP Archive data, a database of web performance information widely used to conduct web performance research. The trained models are then validated using the 10-fold cross-validation method and accuracy measures like the 
Pearson
 correlation coefficient (r), 
Root Mean Square Error
 (RMSE), and Normalized 
Root Mean Square Error
 (NRMSE) are reported.
Results
Radial Basis Function
 regression and 
Random Forest
 outperform all other techniques. The value of r ranges from 0.69-0.92, indicating a high correlation between the observed and 
predicted values
. The NRMSE varies between 0.11-0.16, implying that RMSE is less than 16% of the range of actual value. The RMSE improves by 41%-54% compared to the best baseline prediction model.
Conclusion
It is possible to build realistic prediction models using 
machine learning techniques
 that can be used by practitioners during the early stages of development with minimal effort.",21 Mar 2025,9,The evaluation of machine learning models for predicting web page load times has significant practical value for early-stage ventures. Improving page performance early in the development process can positively impact user experience and overall success of startups.
https://www.sciencedirect.com/science/article/pii/S0950584920300446,On the performance of hybrid search strategies for systematic literature reviews in software engineering,July 2020,Information and Software Technology,Not Found,Erica=Mourão: ericamourao@id.uff.br; João Felipe=Pimentel: Not Found; Leonardo=Murta: Not Found; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Emilia=Mendes: Not Found; Claes=Wohlin: Not Found,"Abstract
Context
When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort.
Objective
The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing.
Method
We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and 
hybrid strategies
 using precision, recall, and F-measure to investigate the performance of each strategy.
Results
Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall.
Conclusion
We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.",21 Mar 2025,8,The proposal and evaluation of hybrid search strategies for conducting Systematic Literature Reviews (SLRs) offer a practical approach to balancing review quality and effort. This can benefit early-stage ventures by providing efficient ways to gather evidence and information.
https://www.sciencedirect.com/science/article/pii/S0950584920300483,State identification sequences from the splitting tree,July 2020,Information and Software Technology,Not Found,Michal=Soucha: michal.soucha@gmail.com; Kirill=Bogdanov: k.bogdanov@sheffield.ac.uk,"Abstract
Context:
 Software testing based on finite-state machines.
Objective:
Improving the performance of existing 
testing methods
 by construction of more efficient separating sequences, so that states entered by a system under test can be identified in a much shorter span of time.
Method:
 This paper proposes an efficient way to construct separating sequences for subsets of states for any deterministic finite-state machine. It extends an existing algorithm that builds an adaptive distinguishing sequence (ADS) from a splitting tree to machines that do not possess an ADS. Our extension to this 
construction algorithm
 allows one not only to construct a separating sequence for any subset of states but also form sets of separating sequences, such as harmonized state identifiers (HSI) and incomplete adaptive distinguishing sequences, that are used by efficient testing and learning algorithms.
Results:
 The experiments confirm that the length and number of test sequences produced by 
testing methods
 that use HSIs constructed by our extension is significantly improved.
Conclusion:
By constructing more efficient separating sequences the performance of existing test methods significantly improves.",21 Mar 2025,6,"The focus on improving testing methods through the construction of efficient separating sequences has potential benefits for software testing based on finite-state machines. While the results are promising, the direct impact on European early-stage ventures may be more specific to certain industries."
https://www.sciencedirect.com/science/article/pii/S0950584919302137,Automatic prediction of the severity of bugs using stack traces and categorical features,July 2020,Information and Software Technology,Not Found,Korosh Koochekian=Sabor: k_kooche@ece.concordia.ca; Mohammad=Hamdaqa: mhamdaqa@ru.is; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca,"Abstract
Context
The severity of a bug is often used as an indicator of how a bug negatively affects system functionality. It is used by developers to prioritize bugs which need to be fixed. The problem is that, for various reasons, bug submitters often enter the incorrect 
severity level
, delaying the bug resolution process. Techniques that can automatically predict the severity of a bug can significantly reduce the bug triaging overhead. In our previous work, we showed that the accuracy of description-based severity prediction techniques could be significantly improved by using stack traces as a source of information.
Objective
In this study, we expand our previous work by exploring the effect of using categorical features, in addition to stack traces, to predict the severity of bugs. These categorical features include faulty product, faulty component, and operating system. We experimented with other features and observed that they do not improve the severity prediction accuracy. A Software system is composed of many products; each has a set of components. Components interact with each to provide the functionality of the product. The operating 
system field
 refers to the operating system on which the software was running on during the crash.
Method
The proposed approach uses a 
linear combination
 of stack trace and categorical features similarity to predict the severity. We adopted a cost sensitive K Nearest Neighbor approach to overcome the unbalance label distribution problem and improve the classifier accuracy.
Results
Our experiments on 
bug reports
 of Eclipse submitted between 2001 and 2015 and Gnome submitted between 1999 and 2015 show that the accuracy of our severity prediction approach can be improved from 5% to 20% by considering categorical features, in addition to stack traces.
Conclusion
The accuracy of predicting the severity of bugs is higher when combining stack traces and three categorical features, product, component, and operating system.",21 Mar 2025,7,The study on predicting bug severity using categorical features and stack traces could significantly reduce bug triaging overhead for startups needing efficient bug resolution processes.
https://www.sciencedirect.com/science/article/pii/S0950584919302332,MAESTRO: Automated test generation framework for high test coverage and reduced human effort in automotive industry,July 2020,Information and Software Technology,Not Found,Yunho=Kim: Not Found; Dongju=Lee: dongju.lee@mobis.co.kr; Junki=Baek: jk.baek@mobis.co.kr; Moonzoo=Kim: moonzoo@cs.kaist.ac.kr,"Abstract
Context
The importance of automotive software has been rapidly increasing because software controls many components of motor vehicles such as smart-key system, tire pressure 
monitoring system
, and 
advanced driver assistance system
. Consequently, the automotive industry spends a large amount of human effort to test automotive software and is interested in automated testing techniques to ensure high-quality automotive software with reduced human effort.
Objective
Applying automated test generation techniques to automotive software is technically challenging because of 
false alarms
 caused by imprecise test drivers/stubs and lack of tool supports for symbolic analysis of bit-fields and 
function pointers
 in C. To address such challenges, we have developed an automated testing framework MAESTRO.
Method
MAESTRO automatically builds a test driver and stubs for a target task (i.e., a software unit consisting of target functions). Then, it generates test inputs to a target task with the test driver and stubs by applying concolic testing and fuzzing together in an adaptive way. In addition, MAESTRO transforms a target program that uses bit-fields into a semantically equivalent one that does not use bit-fields. Also, MAESTRO supports symbolic 
function pointers
 by identifying the candidate functions of a symbolic function pointer through 
static analysis
.
Results
MAESTRO achieved 94.2% branch coverage and 82.3% MC/DC coverage on the four target modules (238 KLOC) developed by Hyundai Mobis. Furthermore, it significantly reduced the cost of coverage testing by reducing the manual effort for coverage testing by 58.8%.
Conclusion
By applying automated testing techniques, MAESTRO can achieve high test coverage for automotive software with significantly reduced manual testing effort.",21 Mar 2025,9,"The automated testing framework MAESTRO can help early-stage automotive software startups achieve high test coverage with reduced manual effort, ensuring high-quality software."
https://www.sciencedirect.com/science/article/pii/S0950584920300227,Test coverage criteria for software product line testing: Systematic literature review,June 2020,Information and Software Technology,Not Found,Jihyun=Lee: jihyun30@jbnu.ac.kr; Sungwon=Kang: sungwon.kang@kaist.ac.kr; Pilsu=Jung: psjung@kaist.ac.kr,"Abstract
Context
In software product line testing (SPLT), test coverage criterion is an important concept, as it provides a means of measuring the extent to which domain testing has been performed and redundant application testing can be avoided based on the test coverage level achieved in domain testing. However, no previous literature reviews on SPLT have addressed test coverage criterion in SPLT.
Objective
The objectives of this paper are as follows: (1) to clarify the notions of test basis and test coverage criterion for SPLT; (2) to identify the test coverage criteria currently used for SPLT; (3) to investigate how various SPLT aspects, such as the 
SPLT method
, variability implementation mechanism, and variability management approach, affect the choice of test coverage criterion for SPLT; and (4) to analyze the limitations of test coverage criteria currently used for SPLT.
Method
This paper conducts a systematic review of test coverage criteria in SPLT with 78 selected studies.
Results
We have several findings that can guide the future research on SPLT. One important finding is that choice of test coverage criterion in SPLT is independent from variability implementation mechanism, variability management, SPL approach, and binding time but is dependent on the variability representation used in development artifacts. Another that is easily overlooked is that SPL test coverage criteria with the same test coverage criterion names of single system testing neither adequately convey what should be covered by the test methods applying them, nor can they be more generally regarded as extensions or generalizations for SPLT of their corresponding test coverage criteria of single system testing.
Conclusion
This study showed that SPL test coverage criteria should be defined or redefined so that they can clearly deliver the target properties to be satisfied by SPLT.",21 Mar 2025,5,The paper on test coverage criteria in software product line testing provides theoretical insights but may have limited practical impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584920300379,A systematic review of unsupervised learning techniques for software defect prediction,June 2020,Information and Software Technology,Not Found,Ning=Li: Not Found; Martin=Shepperd: martin.shepperd@brunel.ac.uk; Yuchen=Guo: Not Found,"Abstract
Background
Unsupervised 
machine learners
 have been increasingly applied to software 
defect prediction
. It is an approach that may be valuable for software practitioners because it reduces the need for labeled 
training data
.
Objective
Investigate the use and performance of 
unsupervised learning
 techniques in 
software defect
 prediction.
Method
We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our 
inclusion criteria
 published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the 
confusion matrices
 and employed the Matthews 
Correlation Coefficient
 (MCC) as our main performance measure.
Results
Our meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check.
Conclusion
Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.",21 Mar 2025,8,The investigation on unsupervised learning techniques in software defect prediction offers valuable insights for startups looking to reduce the need for labeled training data and improve prediction performance.
https://www.sciencedirect.com/science/article/pii/S0950584920300215,Software architectures of the convergence of cloud computing and the Internet of Things: A systematic literature review,June 2020,Information and Software Technology,Not Found,Ahmad=Banijamali: ahmad.banijamali@oulu.fi; Olli-Pekka=Pakanen: Not Found; Pasi=Kuvaja: Not Found; Markku=Oivo: Not Found,"Abstract
Context
Over the last few years, there has been an increasing interest in the convergence of 
cloud computing
 and the 
Internet of Things
 (IoT). Although software systems in this domain have attracted researchers to develop a large body of knowledge on 
software architecture designs
, there is no 
systematic analysis
 of this knowledge.
Objective
This study aims to identify and synthesise state-of-the-art 
architectural elements
 including the 
design patterns
, styles, views, 
quality attributes
, and evaluation methodologies in the convergence of 
cloud computing
 and IoT.
Method
We used systematic literature review (SLR) methodology for a detailed analysis of 82 primary studies of a total of 1618 studies.
Results
We extracted six 
architectural design
 patterns in this domain; among them, edge connectivity patterns stand out as the most popular choice. The service-oriented architecture is the most frequently applied style in this context. Among all applicable 
quality attributes
, scalability, timeliness, and security were the most investigated quality attributes. In addition, we included nine cross analyses to address the relationship between 
architectural patterns
, styles, views, and evaluation methodologies with respect to different quality attributes and 
application areas
.
Conclusions
Our findings indicate that research on software architectures in this domain is increasing. Although few studies were found in which industrial evaluations were presented, industry requires more scientific and empirically validated design frameworks to guide 
software engineering
 in this domain. This work provides an overview of the field while identifying areas for future research.",21 Mar 2025,6,"The synthesis of architectural elements in cloud computing and IoT provides useful knowledge, but the practical impact on early-stage ventures may be limited due to the complexity of software architecture designs."
https://www.sciencedirect.com/science/article/pii/S0950584920300276,Mining API usage scenarios from stack overflow,June 2020,Information and Software Technology,Not Found,Gias=Uddin: gias.uddin@mail.mcgill.ca; Foutse=Khomh: Not Found; Chanchal K=Roy: Not Found,"Abstract
Context
APIs
 play a central role in software development. The seminal research of Carroll et al. [15] on minimal manual and subsequent studies by Shull et al. [79] showed that developers prefer task-based API documentation instead of traditional hierarchical official documentation (e.g., Javadoc). The Q&A format in Stack Overflow offers developers an interface to ask and answer questions related to their development tasks.
Objective
With a view to produce API documentation, we study automated techniques to mine API 
usage scenarios
 from Stack Overflow.
Method
We propose a framework to mine API 
usage scenarios
 from Stack Overflow. Each task consists of a code example, the task description, and the reactions of developers towards the code example. First, we present an algorithm to automatically link a code example in a forum post to an API mentioned in the textual contents of the forum post. Second, we generate a natural language description of the task by summarizing the discussions around the code example. Third, we automatically associate developers reactions (i.e., positive and negative opinions) towards the code example to offer information about code quality.
Results
We evaluate the algorithms using three benchmarks. We compared the algorithms against seven baselines. Our algorithms outperformed each baseline. We developed an online tool by automatically mining API usage scenarios from Stack Overflow. A user study of 31 software developers shows that the participants preferred the mined usage scenarios in Opiner over API official documentation. The tool is available online at: 
http://opiner.polymtl.ca/
.
Conclusion
With a view to produce API documentation, we propose a framework to automatically mine API usage scenarios from Stack Overflow, supported by three novel algorithms. We evaluated the algorithms against a total of eight state of the art baselines. We implement and deploy the framework in our proof-of-concept online tool, Opiner.",21 Mar 2025,7,"The framework proposed in this abstract offers a practical solution to automatically mine API usage scenarios from Stack Overflow, improving the documentation process. This has a positive impact on early-stage ventures by providing developers with easier access to relevant information."
https://www.sciencedirect.com/science/article/pii/S0950584920300240,Is this GitHub project maintained? Measuring the level of maintenance activity of open-source projects,June 2020,Information and Software Technology,Not Found,Jailton=Coelho: jailtoncoelho@dcc.ufmg.br; Marco Tulio=Valente: Not Found; Luciano=Milen: Not Found; Luciana L.=Silva: Not Found,"Abstract
Context
GitHub hosts an impressive number of high-quality OSS projects. However, selecting “the right tool for the job” is a challenging task, because we do not have precise information about those high-quality projects.
Objective
In this paper, we propose a data-driven approach to measure the level of maintenance activity of GitHub projects. Our goal is to alert users about the risks of using unmaintained projects and possibly motivate other developers to assume the maintenance of such projects.
Method
We train 
machine learning
 models to define a metric to express the level of maintenance activity of GitHub projects. Next, we analyze the historical evolution of 2927 active projects in the time frame of one year.
Results
From 2927 active projects, 16% become unmaintained in the interval of one year. We also found that Objective-C projects tend to have lower maintenance activity than projects implemented in other languages. Finally, software tools—such as compilers and editors—have the highest maintenance activity over time.
Conclusions
A metric about the level of maintenance activity of GitHub projects can help developers to select 
open source projects
.",21 Mar 2025,6,The data-driven approach to measure maintenance activity of GitHub projects is valuable in helping developers select high-quality open source projects. This abstract provides useful insights but may not have a direct immediate impact on early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584920300264,A systematic literature review on automated log abstraction techniques,June 2020,Information and Software Technology,Not Found,Diana=El-Masri: diana.el-masri@polymtl.ca; Fabio=Petrillo: fabio@petrillo.com; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@concordia.ca; Abdelwahab=Hamou-Lhadj: wahab.hamou-lhadj@concordia.ca; Anas=Bouziane: anas.bouziane@polymtl.ca,"Abstract
Context:
 Logs are often the first and only information available to software engineers to understand and debug their systems. Automated log-analysis techniques help software engineers gain insights into large log data. These techniques have several steps, among which log abstraction is the most important because it transforms raw log-data into high-level information. Thus, log abstraction allows software engineers to perform further analyses. Existing log-abstraction techniques vary significantly in their designs and performances. To the best of our knowledge, there is no study that examines the performances of these techniques with respect to the following seven 
quality aspects
 concurrently: mode, coverage, delimiter independence, efficiency,scalability, system knowledge independence, and parameter tuning effort.
Objectives:
 We want (1) to build a 
quality model
 for evaluating automated log-abstraction techniques and (2) to evaluate and recommend existing automated log-abstraction techniques using this 
quality model
.
Method:
 We perform a systematic literature review (SLR) of automated log-abstraction techniques. We review 89 research papers out of 2,864 initial papers.
Results:
 Through this SLR, we (1) identify 17 automated log-abstraction techniques, (2) build a quality model composed of seven desirable aspects: mode, coverage, delimiter independence, efficiency, scalability, system knowledge independence, and parameter tuning effort, and (3) make recommendations for researchers on future research directions.
Conclusion:
 Our quality model and recommendations help researchers learn about the state-of-the-art automated log-abstraction techniques, identify research gaps to enhance existing techniques, and develop new ones. We also support software engineers in understanding the advantages and limitations of existing techniques and in choosing the suitable technique to their unique use cases.",21 Mar 2025,8,The evaluation of log-abstraction techniques and the quality model proposed in this abstract provide valuable guidance for software engineers and researchers. This can benefit early-stage ventures by assisting them in choosing suitable log analysis techniques for their systems.
https://www.sciencedirect.com/science/article/pii/S0950584920300288,A large scale empirical study of the impact of Spaghetti Code and Blob anti-patterns on program comprehension,June 2020,Information and Software Technology,Not Found,Cristiano=Politowski: c_polito@encs.concordia.ca; Foutse=Khomh: foutse.khomh@polymtl.ca; Simone=Romano: simone.romano@uniba.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Fabio=Petrillo: fabio@petrillo.com; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@concordia.ca; Abdou=Maiga: ma_karim@yahoo.fr,"Abstract
Context
Several studies investigated the impact of anti-patterns (
i.e.,
 “poor” solutions to recurring design problems) during maintenance activities and reported that anti-patterns significantly affect the developers’ effort required to edit files. However, before developers edit files, they must understand the 
source code
 of the systems. This 
source code
 must be easy to understand by developers.
Objective
In this work, we provide a complete assessment of the impact of two instances of two anti-patterns, Blob or 
Spaghetti Code
, on 
program comprehension
.
Method
We analyze the impact of these two anti-patterns through three empirical studies conducted at Polytechnique Montré al (Canada) with 24 participants; at Carlton University (Canada) with 30 participants; and at University Basilicata (Italy) with 79 participants.
Results
We collect data from 372 tasks obtained thanks to 133 different participants from the three universities. We use three metrics to assess the developers’ comprehension of the source code: (1) the duration to complete each task; (2) their percentage of correct answers; and, (3) the NASA task load index for their effort.
Conclusions
We report that, although single occurrences of Blob or Spaghetti code anti-patterns have little effect on code comprehension, two occurrences of either Blob or 
Spaghetti Code
 significantly increases the developers’ time spent in their tasks, reduce their percentage of correct answers, and increase their effort. Hence, we recommend that developers act on both anti-patterns, which should be refactored out of the source code whenever possible. We also recommend further studies on combinations of anti-patterns rather than on single anti-patterns one at a time.",21 Mar 2025,6,"The assessment of the impact of anti-patterns on program comprehension in this abstract is important for developers. However, the immediate practical value for early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300380,WCA: A weighting local search for constrained combinatorial test optimization,June 2020,Information and Software Technology,Not Found,Yingjie=Fu: fuyj@ios.ac.cn; Zhendong=Lei: leizd@ios.ac.cn; Shaowei=Cai: caisw@ios.ac.cn; Jinkun=Lin: jkunlin@gmail.com; Haoran=Wang: wanghr@ios.ac.cn,"Abstract
Context
Covering array generation (CAG) is the core task of Combinatorial interaction testing (CIT), which is widely used to discover interaction faults in real-world systems. Considering the universality, constrained covering array generation (CCAG) is more in line with the characteristics of applications, and has attracted a lot of researches in the past few years.
Objective
In CIT, a covering array (CA) with smaller size means lower cost of testing, particularly for the systems where the execution of a test suite is time consuming. As constraints between parameters are ubiquitous in real systems, this work is dedicated to more efficient algorithms for CCAG. Specifically, we aim to develop a 
heuristic search algorithm
 for CCAG, which allows generating CAs with smaller size in a limited time when compared with existing algorithms.
Method
We propose a weighting 
local search algorithm
 named 
WCA
, which makes use of weights associated with the tuples and dynamically adjusts them during the search, helping the algorithm to avoid search stagnation. As far as we know, this is the first weighting local search for solving CCAG.
Results
We apply 
WCA
 to a wide range of benchmarks, including real-world ones and synthetic ones. The results show that 
WCA
 achieves a significant improvement over three state-of-the-art competitors in 2-way and 3-way CCAG, in terms of both effectiveness and efficiency. The importance of weighting is also reflected by the experimental comparison between 
WCA
 and its alternative algorithm without the weighting mechanism.
Conclusion
WCA
 is an effective 
heuristic algorithm
 for CCAG to obtain smaller CAs efficiently, and the weighting mechanism plays a crucial role.",21 Mar 2025,9,The development of a heuristic search algorithm for constrained covering array generation with significant improvements over existing algorithms is highly valuable for software testing. This directly impacts early-stage ventures by reducing testing costs and time.
https://www.sciencedirect.com/science/article/pii/S0950584920300409,Identifying security issues for mobile applications based on user review summarization,June 2020,Information and Software Technology,Not Found,Chuanqi=Tao: taochuanqi@nuaa.edu.cn; Hongjing=Guo: Not Found; Zhiqiu=Huang: Not Found,"Abstract
Context
With the development of mobile apps, public concerns about security issues are continually rising. From the user’s perspective, it is crucial to be aware of the security issues of apps. Reviews serve as an important channel for users to discover the diverse issues of apps. However, previous works rarely rely on existing reviews to provide a detailed summarization of the app’s security issues.
Objective
To provide a detailed overview of apps’ security issues for users, this paper introduces SRR-Miner, a novel review summarization approach that automatically summarizes security issues and users’ sentiments.
Method
SRR-Miner follows a keyword-based approach to extracting security-related review sentences. It summarizes security issues and users’ sentiments with  <misbehavior-aspect-opinion>  triples, which makes full use of the deep analysis of sentence structures. SRR-Miner also provides visualized review summarization through a radar chart.
Results
The evaluation on 17 mobile apps shows that SRR-Miner achieves higher F1-score and 
MCC
 than Machine Learning-based 
classification approaches
 in extracting security-related review sentences. It also accurately identifies misbehaviors, aspects and opinions from review sentences. A qualitative study shows that SRR-Miner outperforms two state-of-the-art approaches (AR-Miner and SUR-Miner) in terms of summarizing security issues and users’ sentiments. A further user survey indicates the usefulness of the summarization of SRR-Miner.
Conclusion
SRR-Miner is capable of automatically extracting security-related review sentences based on keywords, and summarizing misbehaviors, aspects and opinions of review sentences with a deep analysis of the sentence structures.",21 Mar 2025,7,"The development of SRR-Miner provides a significant improvement in summarizing security issues and users' sentiments in mobile apps, thus enhancing security awareness and potentially benefitting early-stage ventures in ensuring app security."
https://www.sciencedirect.com/science/article/pii/S095058492030029X,Detecting Java software similarities by using different clustering techniques,June 2020,Information and Software Technology,Not Found,Andrea=Capiluppi: andrea.capiluppi@brunel.ac.uk; Davide=Di Ruscio: davide.diruscio@univaq.it; Juri=Di Rocco: juri.dirocco@univaq.it; Phuong T.=Nguyen: phuong.nguyen@univaq.it; Nemitari=Ajienka: nemitari.ajienka@edgehill.ac.uk,"Abstract
Background
Research on empirical 
software engineering
 has increasingly been conducted by analysing and measuring vast amounts of software systems. Hundreds, thousands and even millions of systems have been (and are) considered by researchers, and often within the same study, in order to test theories, demonstrate approaches or run prediction models. A much less investigated aspect is whether the collected metrics might be context-specific, or whether systems should be better analysed in clusters.
Objective
The objectives of this study are (i) to define a set of 
clustering techniques
 that might be used to group similar software systems, and (ii) to evaluate whether a suite of well-known object-oriented metrics is context-specific, and its values differ along the defined clusters.
Method
We group software systems based on three different 
clustering techniques
, and we collect the values of the metrics suite in each cluster. We then test whether clusters are statistically different between each other, using the Kolgomorov-Smirnov (KS) hypothesis testing.
Results
Our results show that, for two of the used techniques, the KS null hypothesis (e.g., the clusters come from the same population) is rejected for most of the metrics chosen: the clusters that we extracted, based on application domains, show statistically different 
structural properties
.
Conclusions
The implications for researchers can be profound: metrics and their interpretation might be more sensitive to context than acknowledged so far, and application domains represent a promising filter to cluster similar systems.",21 Mar 2025,5,"The study on clustering techniques and context-specific metrics in software systems offers insights for researchers, but the impact on early-stage ventures may not be as direct compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584920300410,A study of run-time behavioral evolution of benign versus malicious apps in android,June 2020,Information and Software Technology,Not Found,Haipeng=Cai: haipeng.cai@wsu.edu; Xiaoqin=Fu: xiaoqin.fu@wsu.edu; Abdelwahab=Hamou-Lhadj: wahab.hamou-lhadj@concordia.ca,"Abstract
Context
The constant evolution of the Android platform and its applications have imposed significant challenges both to understanding and securing the Android ecosystem. Yet, despite the growing body of relevant research, it remains unclear how Android apps evolve in terms of their run-time behaviors in ways that impede our gaining consistent empirical knowledge about the workings of the ecosystem and developing effective 
technical solutions
 to defending it against security threats. Intuitively, an essential step towards addressing these challenges is to first understand the evolution itself. Among others, one avenue to examining a program’s run-time behavior is to dissect the program’s execution in terms of its 
syntactic
 and semantic structure.
Objective
In this paper, we study how benign Android apps execute differently from 
malware
 over time, in terms of their execution structures measured by the distribution and interaction among functionality scopes, app components, and callbacks. In doing so, we attempt to reveal how relevant app execution structure is to app security orientation (i.e., benign or malicious).
Method
By tracing the method calls and inter-component communications (ICCs) of 15,451 benign apps and 15,183 
malware
 developed during eight years (2010–2017), we systematically characterized the execution structure of malware versus benign apps and revealed similarities and disparities between them that are not previously known.
Results
Our results show, among other findings, that (1) despite their similarity in execution distribution over functionality scopes, malware accessed framework functionalities mainly through third-party libraries, while benign apps were dominated by calls within the framework; (2) use of 
Activity
 component had been rising in malware while benign apps saw continuous drop in such uses; (3) malware invoked significantly more 
Services
 but less 
Content Providers
 than benign apps during the evolution of both groups; (4) malware carried ICC data significantly less often via standard data fields than benign apps, albeit both groups did not carry any data in most ICCs; and (5) newer malware tended to have more even distribution of callbacks among event-handler categories, while the distribution remained constant in benign apps over time.
Conclusion
We discussed how these findings inform understanding app behaviors, optimizing static and dynamic code analysis of Android apps, and developing sustainable app security defense solutions.",21 Mar 2025,9,The analysis of how Android apps evolve in terms of security and the identification of differences between benign and malware apps can greatly benefit early-stage ventures in developing effective defense solutions for their apps.
https://www.sciencedirect.com/science/article/pii/S0950584920300185,Test Case Prioritization in Continuous Integration environments: A systematic mapping study,May 2020,Information and Software Technology,Not Found,Jackson A.=Prado Lima: japlima@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br,"Abstract
Context:
 Continuous Integration (CI) environments allow frequent integration of software changes, making software evolution more rapid and cost-effective. In such environments, the 
regression test
 plays an important role, as well as the use of Test Case Prioritization (TCP) techniques. Such techniques attempt to identify the test case order that maximizes certain goals, such as early fault detection. This research subject has been raising interest because some new challenges are faced in the CI context, as TCP techniques need to consider time constraints of the CI environments.
Objective:
 This work presents the results of a 
systematic mapping study
 on Test Case Prioritization in Continuous Integration environments (TCPCI) that reports the main characteristics of TCPCI approaches and their evaluation aspects.
Method:
 The mapping was conducted following a plan that includes the definition of research questions, selection criteria and search string, and the selection of search engines. The search returned 35 primary studies classified based on the goal and kind of used TCP technique, addressed CI particularities and 
testing problems
, and adopted evaluation measures.
Results:
 The results show a growing interest in this research subject. Most studies have been published in the last four years. 80% of the approaches are history-based, that is, are based on the failure and test execution history. The great majority of studies report evaluation results by comparing prioritization techniques. The preferred measures are Time and number/percentage of Faults Detected. Few studies address CI 
testing problems
 and characteristics, such as parallel execution and test case volatility.
Conclusions:
 We observed a growing number of studies in the field. Future work should explore other 
information sources
 such as models and requirements, as well as CI particularities and testing problems, such as test case volatility, time constraint, and flaky tests, to solve existing challenges and offer cost-effective approaches to the software industry.",21 Mar 2025,6,"The systematic mapping study on Test Case Prioritization in CI environments provides valuable insights for optimizing software testing processes, which can indirectly benefit early-stage ventures by improving testing efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584920300045,Time pressure in software engineering: A systematic review,May 2020,Information and Software Technology,Not Found,Miikka=Kuutila: miikka.kuutila@oulu.fi; Mika=Mäntylä: Not Found; Umar=Farooq: Not Found; Maëlick=Claes: Not Found,"Abstract
Context
Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. Experiments and 
case studies
 have investigated the relationship between time pressure and software quality and productivity.
Objective
The present work aims to provide an overview of studies related to time pressure in 
software engineering
; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence.
Method
Our search strategy examined 5414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in 
software engineering
.
Results
The majority of high quality studies report increased productivity and decreased quality under time pressure. The most frequent categories of studies focus on quality assurance, 
cost estimation
, and process simulation. It appears that time pressure is usually caused by errors in 
cost estimation
. The effect of time pressure is most often identified during 
software quality assurance
.
Conclusions
The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.",21 Mar 2025,8,The overview of studies related to time pressure in software engineering and the synthesis of empirical evidence can help early-stage ventures in managing time constraints effectively and improving productivity and software quality.
https://www.sciencedirect.com/science/article/pii/S0950584919302733,Web service design defects detection: A bi-level multi-objective approach,May 2020,Information and Software Technology,Not Found,Soumaya=Rebai: srebal@umich.edu; Marouane=Kessentini: marouane@umich.edu; Hanzhang=Wang: hanzwang@ebay.com; Bruce=Maxim: bmaxim@umich.edu,"Abstract
Context:
 Web services frequently evolve to integrate new features, update existing operations and fix errors to meet the new requirements of subscribers. While this evolution is critical, it may have a 
negative impact
 on the quality of services (QoS) such as reduced cohesion, increased coupling, poor response time and availability, etc. Thus, the design of services could become hard to maintain and extend in future releases. Recent studies addressed the problem of web service design antipatterns detection, also called design defects, by either manually defining detection rules, as combination of quality metrics, or generating them automatically from a set of defect examples. The manual definition of these rules is time-consuming and difficult due to the subjective nature of design issues, especially to find the right thresholds value. The efficiency of the generated rules, using automated approaches, will depend on the quality of the training set since examples of web services antipatterns are limited. Furthermore, the majority of existing studies for design defects detection for web services are limited to structural information (interface/code static metrics) and they ignore the use of quality of services (QoS) or performance metrics, such as response time and availability, for this detection process or understanding the impact of antipatterns on these QoS attributes.
Objective:
 To address these challenges, we designed a bi-level multi-objective optimization approach to enable the generation of antipattern examples that can improve the efficiency of detection rules.
Method:
 The upper-level generates a set of detection rules as a combination of quality metrics with their threshold values maximizing the coverage of defect examples extracted from several existing web services and artificial ones generated by a lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules of the upper level and minimizes the similarity to well-designed web service examples. The generated detection rules, by our approach, are based on a combination of dynamic QoS attributes and structural information of web service (static interface/code metrics).
Results:
 The statistical analysis of our results, based on a data-set of 662 web services, confirms the efficiency of our approach in detecting web service antipatterns comparing to the current state of the art in terms of precision and recall.
Conclusion:
 The multi-objective search formulation at both levels helped to diversify the generated artificial web service defects which produced better quality of detection rules. Furthermore, the combination of dynamic QoS attributes and structural information of web services improved the efficiency of the generated detection rules.",21 Mar 2025,7,"The research addresses a critical issue in web service design, impacting the quality of services. The proposed optimization approach shows significant improvement in detecting design defects, which can benefit early-stage ventures in terms of maintaining and extending services."
https://www.sciencedirect.com/science/article/pii/S095058492030001X,Understanding predictive factors for merge conflicts,May 2020,Information and Software Technology,Not Found,Klissiomara=Dias: kld2@cin.ufpe.br; Paulo=Borba: phmb@cin.ufpe.br; Marcos=Barreto: msb5@cin.ufpe.br,"Abstract
Context:
 Merge conflicts often occur when developers change the same code artifacts. Such conflicts might be frequent in practice, and resolving them might be costly and is an error-prone activity.
Objective:
 To minimize these problems by reducing merge conflicts, it is important to better understand how conflict occurrence is affected by technical and organizational factors.
Method:
 With that aim, we investigate seven factors related to modularity, size, and timing of developers contributions. To do so, we reproduce and analyze 73504 merge scenarios in GitHub repositories of Ruby and Python MVC projects.
Results:
 We find evidence that the likelihood of merge conflict occurrence significantly increases when contributions to be merged are not modular in the sense that they involve files from the same MVC slice (related model, view, and controller files). We also find bigger contributions involving more developers, commits, and changed files are more likely associated with merge conflicts. Regarding the timing factors, we observe contributions developed over longer periods of time are more likely associated with conflicts. No evaluated factor shows 
predictive power
 concerning both the number of merge conflicts and the number of files with conflicts.
Conclusion:
 Our results could be used to derive recommendations for development teams and merge conflict prediction models. Project management and assistive tools could benefit from these models.",21 Mar 2025,6,"The study provides valuable insights into factors affecting merge conflicts in software development, offering potential recommendations for development teams. While the findings are relevant, the practical impact on early-stage ventures may not be as immediate as other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058491930271X,"Views on quality requirements in academia and practice: commonalities, differences, and context-dependent grey areas",May 2020,Information and Software Technology,Not Found,Andreas=Vogelsang: andreas.vogelsang@tu-berlin.de; Jonas=Eckhardt: jonas@eckhardt.tv; Daniel=Mendez: daniel.mendez@bth.se; Moritz=Berger: moritz.berger@imbie.uni-bonn.de,"Abstract
Context:
 Quality requirements (QRs) are a topic of constant discussions both in industry and academia. Debates entwine around the definition of quality requirements, the way how to handle them, or their importance for project success. While many academic endeavors contribute to the body of knowledge about QRs, practitioners may have different views. In fact, we still lack a consistent body of knowledge on QRs since much of the discussion around this topic is still dominated by observations that are strongly context-dependent. This holds for both academic and practitioners’ views. Our assumption is that, in consequence, those views may differ.
Objective:
 We report on a study to better understand the extent to which available research statements on quality requirements, as found in exemplary peer-reviewed and frequently cited publications, are reflected in the perception of practitioners. Our goal is to analyze differences, commonalities, and context-dependent grey areas in the views of academics and practitioners to allow a discussion on potential misconceptions (on either sides) and opportunities for future research.
Method:
 We conducted a survey with 109 practitioners to assess whether they agree with research statements about QRs reflected in the literature. Based on a statistical model, we evaluate the impact of a set of context factors to the perception of research statements.
Results:
 Our results show that a majority of the statements is well respected by practitioners; however, not all of them. When examining the different groups and backgrounds of respondents, we noticed interesting deviations of perceptions within different groups that may lead to new research questions.
Conclusions:
Our results help identifying prevalent context-dependent differences about how academics and practitioners view QRs and pinpointing statements where further research might be useful.",21 Mar 2025,5,"The research on quality requirements perception between academics and practitioners is interesting, but the direct impact on early-stage ventures may be limited. The findings may lead to future research opportunities but may not provide immediate value to startups."
https://www.sciencedirect.com/science/article/pii/S0950584920300203,Identifying self-admitted technical debt through code comment analysis with a contextualized vocabulary,May 2020,Information and Software Technology,Not Found,Mário André de Freitas=Farias: mario.andre@ifs.edu.br; Manoel Gomes de Mendonça=Neto: manoel.mendonca@ufba.br; Marcos=Kalinowski: kalinowski@inf.puc-rio.br; Rodrigo Oliveira=Spínola: rodrigo.spinola@unifacs.br,"Abstract
Context
Previous work has shown that one can explore code comments to detect Self-Admitted Technical Debt (SATD) using a contextualized vocabulary. However, current detection strategies still return a large number of 
false positives
 items. Moreover, those strategies do not allow the automatic identification of the type of debt of the identified items.
Objective
This work applies, evaluates, and improves a set of contextualized patterns we built to detect self-admitted technical debt using code comment analysis. We refer to this set of patterns as the self-admitted technical debt identification vocabulary.
Method
We carry out three empirical studies. Firstly, 23 participants analyze the patterns of a previously defined contextualized vocabulary and register their level of importance in identifying SATD items. Secondly, we perform a qualitative analysis to investigate the relation between each pattern and types of debt. Finally, we perform a feasibility study using a new vocabulary, improved based on the results of the previous empirical studies, to automatically identify self-admitted technical debt items, and types of debt, that exist in three 
open source projects
.
Results
More than half of the new patterns were considered decisive or very decisive to detect technical debt items. The new vocabulary was able to find items associated to code, design, defect, documentation, and requirement debt. Thus, the result of the work is an improved vocabulary that considers the level of importance of each pattern and the relationship between patterns and debt types to support the identification and classification of SATD items.
Conclusion
The studies allowed us to improve a vocabulary to identify self-admitted technical debt items through code comments analysis. The results show that the use of pattern-based code comment analysis can contribute to improve existing methods, or create new ones, for automatically identifying and classifying technical debt items.",21 Mar 2025,8,The work on identifying self-admitted technical debt through code comment analysis is practical and beneficial for software maintenance and improvement. The improved vocabulary and results of the study can help startups in managing technical debt effectively.
https://www.sciencedirect.com/science/article/pii/S0950584920300197,Incorporating fault-proneness estimations into coverage-based test case prioritization methods,May 2020,Information and Software Technology,Not Found,Mostafa=Mahdieh: Not Found; Seyed-Hassan=Mirian-Hosseinabadi: hmirian@sharif.edu; Khashayar=Etemadi: Not Found; Ali=Nosrati: Not Found; Sajad=Jalali: Not Found,"Abstract
Context:
 During the 
development process
 of a software program, regression testing is used to ensure that the correct behavior of the software is retained after updates to the 
source code
. This regression testing becomes costly over time as the number of test cases increases and it makes sense to prioritize test cases in order to execute fault-detecting test cases as soon as possible. There are many coverage-based test case prioritization (TCP) methods that only use the code coverage data to prioritize test cases. By incorporating the fault-proneness estimations of code units into the coverage-based TCP methods, we can improve such techniques.
Objective:
 In this paper, we aim to propose an approach which improves coverage-based TCP methods by considering the fault-proneness distribution over code units. Further, we present the results of an empirical study that shows using our proposed approach significantly improves the additional strategy, which is a widely used coverage-based TCP method.
Method:
 The approach presented in this study uses the bug history of the software in order to introduce a 
defect prediction
 method to learn a 
neural network model
. This model is then used to estimate fault-proneness of each area of the 
source code
 and then the estimations are incorporated into coverage-based TCP methods. Our proposed approach is a general idea that can be applied to many coverage-based methods, such as the additional and total TCP methods.
Results:
 The proposed methods are evaluated on datasets collected from the development history of five real-world projects including 357 versions in total. The experiments show that using an appropriate bug history can improve coverage-based TCP methods.
Conclusion:
 The proposed approach can be applied to various coverage-based TCP methods and the experiments show that it can improve these methods by incorporating estimations of code units fault-proneness.",21 Mar 2025,6,"The approach to improve coverage-based test case prioritization methods shows promise in enhancing regression testing. While the findings can benefit software development projects, the direct impact on startups may require further implementation and study."
https://www.sciencedirect.com/science/article/pii/S0950584920300239,Detection of malicious software by analyzing the behavioral artifacts using machine learning algorithms,May 2020,Information and Software Technology,Not Found,Jagsir=Singh: erjagsirsingh18@gmail.com; Jaswinder=Singh: dr.jaswinder@pbi.ac.in,"Abstract
Malicious software
 deliberately affects the 
computer systems
. Malware are analyzed using static or dynamic analysis techniques. Using these techniques, unique patterns are extracted to 
detect malware
 correctly. In this paper, a behavior-based 
malware detection
 technique is proposed. Various runtime features are extracted by setting up a dynamic analysis environment using the Cuckoo sandbox. Three primary features are processed for developing malware classifier. Firstly, printable strings are processed word by word using text mining techniques which produced a very high dimension matrix of the string features. Then we apply the 
singular value
 decomposition technique for reducing dimensions of string features. Secondly, 
Shannon entropy
 is computed over the printable strings and API calls to consider the randomness of API and PSI features. In addition to these features, behavioral features regarding file operations, registry key modification and network activities are used in 
malware detection
. Finally, all features are integrated in the training feature set to develop the malware classifiers using the 
machine learning algorithms
. The proposed technique is validated with 16489 malware and 8422 benign files. Our experimental results show the accuracy of 99.54% in malware detection using ensemble 
machine learning algorithms
. Moreover, it aims to develop a behavior-based malware detection technique of high accuracy by processing the runtime features in a new way.",21 Mar 2025,8,"The proposed behavior-based malware detection technique using machine learning algorithms achieves a high accuracy rate of 99.54%, which is beneficial for early-stage ventures to protect their systems from cyber threats."
https://www.sciencedirect.com/science/article/pii/S0950584920300252,A survey on the practical use of UML for different software architecture viewpoints,May 2020,Information and Software Technology,Not Found,Mert=Ozkaya: mozkaya@cse.yeditepe.edu.tr; Ferhat=Erata: Not Found,"Abstract
Context
Software 
architecture viewpoints
 modularize the software architectures in terms of different viewpoints that each address a different concern. 
Unified Modeling Language
 (UML) is so popular among practitioners for modeling software architectures from different viewpoints.
Objective
In this paper, we aimed at understanding the practitioners’ UML usage for the modeling of software architectures from different viewpoints.
Method
To this end, 109 practitioners with diverse profiles have been surveyed to understand practitioners’ UML usage for six different viewpoints: functional, information, concurrency, development, deployment, and operational. Each viewpoint has been considered in terms of a set of software models that can be created in that viewpoint.
Results
The survey includes 35 questions for different viewpoint models, and the results lead to interesting findings. While the top popular viewpoints for the UML-based software architecture modeling are the functional (96%) and information (99%) viewpoints, the least popular one is the operational viewpoint that is considered by 26% of the practitioners. The top popular 
UML modeling
 tool is Enterprise Architect regardless of the viewpoints considered. Concerning the software models that can be created in each viewpoint, UML’s 
class diagram
 is practitioners’ top choice for the functional structure (71%), 
data structure
 (85%), concurrency structure (75%), software code structure (34%), and system installation (39%), and system support (16%) models; UML’s 
sequence diagram
 is the top choice for the 
data lifecycle
 models (47%); UML’s deployment diagram for the physical structure (71%), mapping between the functional and physical components (53%), and system migration (21%) models; UML’s activity diagram for the data flow (65%), software build and release processes (20–22%), and system administration (36%) models; UML’s component diagram for the mapping between the functional and concurrent components (35%), software module structure (47%), and system configuration (21%) models; and UML’s 
package diagram
 for the software module structure (47%) models.",21 Mar 2025,5,The survey on UML usage for software architecture modeling provides insights but lacks a direct practical impact on European early-stage ventures or startups.
https://www.sciencedirect.com/science/article/pii/S0950584919302563,Generative software module development for domain-driven design with annotation-based domain specific language,April 2020,Information and Software Technology,Not Found,Duc Minh=Le: duclm@hanu.edu.vn; Duc-Hanh=Dang: hanhdd@vnu.edu.vn; Viet-Ha=Nguyen: hanv@vnu.edu.vn,"Abstract
Context
Object-oriented domain-driven design (DDD) aims to iteratively develop software around a realistic model of the application domain, which both thoroughly captures the domain requirements and is technically feasible for implementation. The main focus of recent work in DDD has been on using a form of annotation-based 
domain specific language
 (aDSL), internal to an object-oriented programming language, to build the domain model. However, these work do not consider software modules as first-class objects and thus lack a method for their development.
Objective
In this paper, we tackle software module development with the DDD method by adopting a 
generative approach
 that uses aDSL. To achieve this, we first extend a previous work on module-based software architecture with three enhancements that make it amenable to generative development. We then treat module configurations as first-class objects and define an aDSL, named 
MCCL
, to express module configuration classes. To improve productivity, we define function 
MCCGen
 to automatically generate each configuration class from the module’s domain class.
Method
We define our method as a refinement of an aDSL-based 
software development method
 from a previous work. We apply meta-modelling with UML/OCL to define 
MCCL
 and implement 
MCCL
 in a Java software framework. We evaluate the applicability of our method using a 
case study
 and formally define an evaluation framework for module generativity. We also analyse the correctness and performance of function 
MCCGen
.
Results
MCCL
 is an aDSL for module configurations. Our evaluation shows 
MCCL
 is applicable to complex problem domains. Further, the MCCs and software modules can be generated with a high and quantifiable degree of automation.
Conclusion
Our method bridges an important gap in DDD with a software module development method that uses a novel aDSL with a module-based software architecture and a 
generative technique
 for module configuration.",21 Mar 2025,7,"The paper addresses software module development with the DDD method using a generative approach, filling a gap in the field, which can benefit startups in creating technically feasible domain models."
https://www.sciencedirect.com/science/article/pii/S0950584919302630,Simulation environment for the choice of the decision making algorithm in multi-version real-time system,April 2020,Information and Software Technology,Not Found,Igor V.=Kovalev: Not Found; Mikhail V.=Saramud: msaramud@sfu-kras.ru; Vasiliy V.=Losev: Not Found,"Abstract
Context
Nowadays the most effective way to improve the reliability of software is an approach with the introduction of software redundancy - multi-version programming. The reliability of a multi-version system is determined not only by the reliability of the versions that make it up, but to a greater degree by the decision making algorithm.
Objective
Our objective is evaluation and selection of the most reliable voting algorithms in multi-version environments. In order to get this objective there is a need to check all the algorithms in the execution environment, simulating characteristic of the developed system. Thus, we obtain the characteristics of the quality of the algorithm operation in precisely those conditions in which it will work in the system that is developed.
Method
The article suggests weighted voting algorithms with a forgetting element, as well as modifications of existing voting algorithms. To be able to check the quality of their work, the simulation environment has been implemented that simulates the operation of the software multi-version execution environment.
Results
The article substantiates the use of the most reliable decision making algorithms in the decision block of the real-time operating system. A comparative analysis of decision making algorithms for the operation of the decision making block of the multi-version real-time execution environment has been carried out.
Conclusions
The software implementation of the simulation environment that implements the simulations of versions with given characteristics is considered, not only classical decision making algorithms, but also the author's modifications are investigated. The environment allows to obtain the 
quality characteristics
 of all implemented decision making algorithms with given system characteristics. The modeling results are considered, the dependence of the system 
reliability indicators
 on its input parameters is shown, a comparative analysis of various decision making algorithms based on the modeling results is made.",21 Mar 2025,6,"The evaluation and selection of reliable voting algorithms in multi-version environments is important for software reliability, but the direct practical impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919302629,Patterns of user involvement in experiment-driven software development,April 2020,Information and Software Technology,Not Found,Sezin=Yaman: sezin.yaman@helsinki.fi; Fabian=Fagerholm: fabian.fagerholm@helsinki.fi; Myriam=Munezero: myriam.munezero@helsinki.fi; Tomi=Männistö: tomi.mannisto@helsinki.fi; Tommi=Mikkonen: tommi.mikkonen@helsinki.fi,"Abstract
Background
Experiments are often used as a means to continuously validate user needs and to aid in making software development decisions. Involving users in the development of software products benefits both the users and companies. How software companies efficiently involve users in both general development and in experiments remains unclear; however, it is especially important to determine the perceptions and attitudes held by practitioners in different roles in these companies.
Objective
We seek to: 1) explore how software companies involve users in software development and experimentation; 2) understand how developer, manager and UX designer roles perceive and involve users in experimentation; and 3) uncover systematic patterns in practitioners’ views on user involvement in experimentation. The study aims to reveal behaviors and perceptions that could support or undermine experiment-driven development, point out what skills could enhance experiment-driven development, and raise awareness of such issues for companies that wish to adopt experiment-driven development.
Methods
We conducted a survey within four Nordic software companies, inviting practitioners in three major roles: developers, managers, and UX designers. We asked the respondents to indicate how they involve users in their job function, as well as their perspectives regarding software experiments and ethics.
Results and Conclusion
We identified six patterns describing experimentation and user involvement. For instance, managers were associated with a cautious user notification policy, that is, to always let users know of an experiment they are subject to, and they also believe that users have to be convinced before taking part in experiments. We discovered that, due to lack of clear processes for involving users and the lack of a common understanding of ethics in experimentation, practitioners tend to rationalize their perceptions based on their own experiences. Our patterns were based on empirical evidence and they can be evaluated in different populations and contexts.",21 Mar 2025,4,"The study on user involvement in software development and experiments provides insights, but the direct impact on European early-stage ventures or startups is not explicitly stated."
https://www.sciencedirect.com/science/article/pii/S0950584919302605,Collaborative or individual identification of code smells? On the effectiveness of novice and professional developers,April 2020,Information and Software Technology,Not Found,Roberto=Oliveira: roberto.oliveira@ueg.br; Rafael=de Mello: rmaiani@inf.puc-rio.br; Eduardo=Fernandes: emfernandes@inf.puc-rio.br; Alessandro=Garcia: afgarcia@inf.puc-rio.br; Carlos=Lucena: lucena@inf.puc-rio.br,"Abstract
Context
The code smell identification aims to reveal code structures that harm the software 
maintainability
. Such identification usually requires a 
deep understanding
 of multiple parts of a system. Unfortunately, developers in charge of identifying code smells individually can struggle to identify, confirm, and refute code smell suspects. Developers may reduce their struggle by identifying code smells in pairs through the collaborative smell identification.
Objective
The current knowledge on the effectiveness of collaborative smell identification remains limited. Some scenarios were not explored by previous work on effectiveness of collaborative versus individual smell identification. In this paper, we address a particular scenario that reflects various organizations worldwide. We also compare our study results with recent studies.
Method
We have carefully designed and conducted a controlled experiment with 34 developers. We exploited a particular scenario that reflects various organizations: novices and professionals inspecting systems they are unfamiliar with. We expect to minimize some critical threats to validity of previous work. Additionally, we interviewed 5 project leaders aimed to understand the potential adoption of the collaborative smell identification in practice.
Results
Statistical testing suggests 27% more precision and 36% more recall through the collaborative smell identification for both novices and professionals. These results partially confirm previous work in a not previously exploited scenario. Additionally, the interviews showed that leaders would strongly adopt the collaborative smell identification. However, some organization and tool constraints may limit such adoption. We derived recommendations to organizations concerned about adopting the collaborative smell identification in practice.
Conclusion
We recommend that organizations allocate novice developers for identifying code smells in collaboration. Thus, these organizations can promote the 
knowledge sharing
 and the correct smell identification. We also recommend the allocation of developers that are unfamiliar with the system for identifying smells. Thus, organizations can allocate more experience developers in more critical tasks.",21 Mar 2025,8,"The study addresses a practical scenario for organizations worldwide and provides concrete recommendations based on the results, which can have a significant impact on improving software maintainability."
https://www.sciencedirect.com/science/article/pii/S0950584919302617,On the value of quality attributes for refactoring ATL model transformations: A multi-objective approach,April 2020,Information and Software Technology,Not Found,Bader=Alkhazi: balkhazi@umich.edu; Chaima=Abid: cabid@umich.edu; Marouane=Kessentini: marouane@umich.edu; Manuel=Wimmer: manuel.wimmer@jku.at,"Abstract
Context
Model transformations play a fundamental role in Model-Driven Engineering (MDE) as they are used to manipulate models and to transform them between source and target metamodels. However, model transformation programs lack significant support to maintain good quality which is in contrast to established programming paradigms such as object-oriented programming. In order to improve the quality of model transformations, the majority of existing studies suggest manual support for the developers to execute a number of refactoring types on model transformation programs. Other recent studies aimed to automate the refactoring of model transformation programs, mostly focusing on the ATLAS Transformation Language (ATL), by improving mainly few quality metrics using a number of refactoring types.
Objective
In this paper, we propose a novel set of quality attributes to evaluate refactored 
ATL programs
 based on the hierarchical 
quality model
 QMOOD.
Method
We used the proposed quality attributes to guide the selection of the best refactorings to improve 
ATL programs
 using multi-objective search.
Results
We validate our approach on a comprehensive dataset of model transformations. The statistical analysis of our experiments on 30 runs shows that our automated approach recommended useful refactorings based on a benchmark of ATL transformations and compared to random search, mono-objective search formulation, a previous work based on a different formulation of multi-objective search with few quality metrics, and a semi-automated refactoring approach not based on 
heuristic search
.
Conclusion
All these existing studies did not use our QMOOD adaptation for ATL which confirms the relevance of our quality attributes to guide the search for good refactoring suggestions.",21 Mar 2025,7,"The proposed quality attributes and automated approach have the potential to enhance the quality of model transformations, although the impact on practical application may require further validation."
https://www.sciencedirect.com/science/article/pii/S0950584919302721,Regression testing for large-scale embedded software development – Exploring the state of practice,April 2020,Information and Software Technology,Not Found,Nasir Mehmood=Minhas: nasir.mehmood.minhas@bth.se; Kai=Petersen: kai.petersen@bth.se; Jürgen=Börstler: jurgen.borstler@bth.se; Krzysztof=Wnuk: krzysztof.wnuk@bth.se,"Abstract
Context
A majority of the regression testing techniques proposed by academics have not been adopted in 
industry
. To increase adoption rates, we need to improve our understanding of the practitioners’ perspectives on regression testing.
Objective
This study aims at exploring the regression testing state of practice in the large-scale 
embedded software
 development. The study has two objectives: 1) to highlight the potential challenges in practice, and 2) to identify the industry-relevant research areas regarding regression testing.
Method
We conducted a qualitative study in two large-scale 
embedded software
 development companies, where we carried out semi-structured interviews with representatives from five software testing teams.
Results
The practitioners run regression testing mostly with limited scope based on the size, complexity, and location of the change. Test cases are prioritized on the basis of risk and critical functionality. The practitioners rely on their knowledge and experience for the decision making regarding selection and prioritization of test cases. The companies are using both automated and manual regression testing, and mainly rely on in-house developed tools for test automation. The challenges identified in the companies are: time to test, information management, test suite maintenance, lack of communication, test selection/prioritization, lack of assessment, etc. Regression testing goals identified in this study are customer satisfaction, critical defect detection, confidence, effectiveness, efficiency, and controlled slip through of faults.
Conclusions
Considering the current state of practice and the identified challenges we conclude that there is a need to reconsider the 
regression test
 strategy in the companies. Researchers need to analyze the 
industry
 perspective when proposing new regression testing techniques.",21 Mar 2025,9,"The study sheds light on the regression testing state of practice in embedded software development, highlighting challenges and suggesting a reconsideration of regression test strategy, which could greatly benefit the industry."
https://www.sciencedirect.com/science/article/pii/S0950584919302551,Comparison of development methodologies in web applications,March 2020,Information and Software Technology,Not Found,Jimmy=Molina-Ríos: jmolina@utmachala.edu.ec; Nieves=Pedreira-Souto: nieves.pedreira@udc.es,"Abstract
Context
Web applications development is at its peak due to the advance of technological trends and the constant dependence of the Internet. As a result of the needs of developers, new development methodologies have emerged. However, that does not mean that companies always implement an optimal 
development process
; instead, there are several disadvantages presented by an inadequate and not versatile methodologies.
Objective
The aim is to compare web development methodologies based on dynamic features presented during the life cycle to identify their use, relevance, and characteristics. The process employing is an SLR and field research to Ecuadorian development companies.
Method
The method used 
is
 a systematic literature review (SLR) for the identification of characteristics and processes of development methodologies. Additionally, a survey of Ecuadorian web application developers was implemented to assess the importance of using a method during the project.
Results
The literature review exhibited as a result that UWE and OOHDM have greater flexibility than other methodologies before dynamic environments during the web 
development process
. On the other hand, within field research was obtained that companies use different 
software development methods
 than those assessed in the study (hybrid methodologies). However, within the range of companies using the compared methodologies, UWE is the most selected.
Conclusions
Each methodology holds particular features and employment environment, which makes them useful in specific conditions. Through the field research, it is possible to conclude that most of the companies use different methodologies than the evaluated ones; thus, the process is guided by hybrids methods or models based on experience. On the other hand, through the SLR, we identified UWE as the most suitable methodology for web development under dynamic environments, such as the size of the company, the need to modify the requirements, or the knowledge that the development team has about the process.",21 Mar 2025,6,"While the comparison of web development methodologies is valuable, the focus on Ecuadorian companies may limit the generalizability of the findings to a broader European context."
https://www.sciencedirect.com/science/article/pii/S0950584919302484,Software trustworthiness evaluation model based on a behaviour trajectory matrix,March 2020,Information and Software Technology,Not Found,Junfeng=Tian: Not Found; Yuhui=Guo: tjf@hbu.edu.cn,"Abstract
Context
Software trustworthiness is a highly important 
research topic
. 
Trustworthiness evaluation
 based on factors that affect software behaviour is conducted mainly according to the influence degrees of these factors on the software behaviour to evaluate trustworthiness. As a result, minimization of the interference of human factors is considered.
Objective
In this study, to ensure the objectivity of evaluating the trustworthiness of software behaviour, a software trustworthiness evaluation model based on a behaviour trajectory matrix, namely, BTBM-TM was proposed.
Method
Checkpoints were set up in the trajectory of the software behaviour, and binary code was introduced to express the software behaviour trajectory tree. The scenario information of the checkpoints was acquired, and used to construct behaviour trajectory matrices, which were used to represent the behaviour trajectory. The behaviour trajectory matrices were transformed into grayscale images, which were used to train the 
deep residual network
 (ResNet) to classify the software behaviour. The trained 
deep residual network
 was used to categorize the current software behaviour, and the 
cosine similarity
 algorithm was used to calculate the deviation degree of the software behaviour trajectory; to perform a dual evaluation of the trustworthiness of software behaviour.
Results
The behaviour trajectory information of the Model class of 300 cycles was used to evaluate the trustworthiness of the mine-sweeping game. The trustworthiness evaluation results of the software behaviour of the scheme proposed in this paper (BTBM-TM) were compared with those of the schemes from references [6] and [10]. The accuracies of the schemes from [6] and [10] are lower than that of the BTBM-TM scheme.
Conclusions
The trajectory of software behaviour is represented by a matrix and converted into a grayscale image, whose 
processing method
 is used to evaluate the trustworthiness of software behaviour more objectively and accurately.",21 Mar 2025,5,"The proposed trustworthiness evaluation model based on software behaviour trajectory matrix is innovative, but the practical implications and potential industry impact require further exploration."
https://www.sciencedirect.com/science/article/pii/S0950584919302125,Better together: Comparing vulnerability prediction models,March 2020,Information and Software Technology,Not Found,Christopher=Theisen: http://www.theisencr.github.io/; Laurie=Williams: lawilli3@ncsu.edu,"Abstract
Context
Vulnerability Prediction Models (VPMs) are an approach for prioritizing security inspection and testing to find and fix vulnerabilities. VPMs have been created based on a variety of metrics and approaches, yet widespread adoption of VPM usage in practice has not occurred. Knowing which VPMs have strong prediction and which VPMs have low data requirements and resources usage would be useful for practitioners to match VPMs to their project’s needs. The low density of vulnerabilities compared to defects is also an obstacle for practical VPMs.
Objective
The goal of the paper is to help security practitioners and researchers choose appropriate features for vulnerability prediction through a comparison of Vulnerability Prediction Models.
Method
We performed replications of VPMs on Mozilla Firefox with 28,750 
source code files
 featuring 271 vulnerabilities using software metrics, text mining, and crash data. We then combined features from each VPM and reran our classifiers.
Results
We improved the F-score of the best VPM (.20 to 0.28) by combining features from three types of VPMs and using Naive Bayes as the classifier. The strongest features in the combined model were the number of times a file was involved in a crash, the number of outgoing calls from a file, and the string “nullptr”.
Conclusion
Our results indicate that further work is needed to develop new features for input into classifiers. In addition, new analytic approaches for VPMs are needed for VPMs to be useful in practical situations, due to the low density of vulnerabilities in software (less than 1% for our dataset).",21 Mar 2025,7,The paper addresses the need for better Vulnerability Prediction Models (VPMs) in software security which can have a significant impact on early-stage ventures to improve security measures.
https://www.sciencedirect.com/science/article/pii/S0950584919302496,How are distributed bugs diagnosed and fixed through system logs?,March 2020,Information and Software Technology,Not Found,Wei=Yuan: cindyyuanwei@buaa.edu.cn; Shan=Lu: shanlu@uchicago.edu; Hailong=Sun: sunhl@buaa.edu.cn; Xudong=Liu: liuxd@act.buaa.edu.cn,"Abstract
Context
Distributed systems are the backbone of today’s computing ecosystems. Debugging distributed bugs is crucial and challenging. There are still many unknowns about debugging real-world distributed bugs, especially through system logs.
Objective
This paper aims to provide a comprehensive study of how system logs can help diagnose and fix distributed bugs in practice.
Method
The study was carried out with three core research questions (RQs): How to identify failures in distributed bugs through logs? How to find and utilize bug-related log entries to figure out the root causes? How are distributed bugs fixed and how are logs and patches related? To answer these questions, we studied 106 real-world distributed bugs randomly sampled from five widely used distributed systems, and manually checked the 
bug report
, the log, the patch, the source code and other related information for each of these bugs.
Results
Seven findings are observed and the main findings include: (1) For only about half of the distributed bugs, the failures are indicated by FATAL or ERROR log entries. FATAL are not always fatal, and INFO could be fatal. (2) For more than half of the studied bugs, root-cause diagnosis relies on log entries that are not part of the failure symptoms. (3) One third of the studied bugs are fixed by eliminating end symptoms instead of root causes. Finally, a distributed bug dataset with the in-depth analysis has been released to the research community.
Conclusion
The findings in our study reveal the characteristics of distributed bugs, the differences from debugging single-machine system bugs, and the usages and limitations of existing logs. Our study also provides guidance and opportunities for future research on distributed bug diagnosis, fixing, and log analysis and enhancement.",21 Mar 2025,8,The study on debugging real-world distributed bugs through system logs provides valuable insights that can benefit European startups working on distributed systems and software development.
https://www.sciencedirect.com/science/article/pii/S0950584919302502,"A fine-grained requirement traceability evolutionary algorithm: Kromaia, a commercial video game case study",March 2020,Information and Software Technology,Not Found,Daniel=Blasco: dblasco@usj.es; Carlos=Cetina: ccetina@usj.es; Óscar=Pastor: opastor@dsic.upv.es,"Abstract
Context:
Commercial 
video games
 usually feature an extensive 
source code
 and requirements that are related to code lines from multiple methods. Traceability is vital in terms of maintenance and content update, so it is necessary to explore such 
search spaces
 properly.
Objective:
This work presents and evaluates CODFREL (Code Fragment-based Requirement Location), our approach to fine-grained 
requirement traceability
, which lies in an 
evolutionary algorithm
 and includes encoding and genetic operators to manipulate code fragments that are built from 
source code
 lines. We compare it with a baseline approach (Regular-LSI) by configuring both approaches with different 
granularities
 (code lines / complete methods).
Method:
We evaluated our approach and Regular-LSI in the Kromaia video game 
case study
, which is a commercial video game released on PC and PlayStation 4. The approaches are configured with method and code line granularity and work on 20 requirements that are provided by the development company. Our approach and Regular-LSI calculate similarities between requirements and code fragments or methods to propose possible solutions and, in the case of CODFREL, to guide the 
evolutionary algorithm
.
Results:
The results, which compare code line and method granularity configurations of CODFREL with different granularity configurations of Regular-LSI, show that our approach outperforms Regular-LSI in precision and recall, with values that are 26 and 8 times better, respectively, even though it does not achieve the optimal solutions. We make an open-source implementation of CODFREL available.
Conclusions:
Since our approach takes into consideration key issues like the source code size in commercial 
video games
 and the requirement dispersion, it provides better starting points than Regular-LSI in the search for solution candidates for the requirements. However, the results and the influence of domain-specific language on them show that more explicit knowledge is required to improve such results.",21 Mar 2025,9,"The CODFREL approach for fine-grained requirement traceability in commercial video games outperforms the baseline approach, which can be beneficial for startups in the gaming industry."
https://www.sciencedirect.com/science/article/pii/S0950584919302393,Adequate vs. inadequate test suite reduction approaches,March 2020,Information and Software Technology,Not Found,Carmen=Coviello: carmen.coviello@unibas.it; Simone=Romano: simone.romano@uniba.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Alessandro=Marchetto: alex.marchetto@gmail.com; Anna=Corazza: anna.corazza@unina.it; Giuliano=Antoniol: antoniol@ieee.org,"Abstract
Context:
 Regression testing is an important activity that allows ensuring the correct behavior of a system after changes. As the system grows, the time and resources to perform regression testing increase. Test Suite Reduction (TSR) approaches aim to speed up regression testing by removing obsolete or redundant test cases. These approaches can be classified as adequate or inadequate. Adequate TSR approaches reduce test suites and completely preserve test requirements (
e.g.,
 covered statements) of the original test suites. Inadequate TSR approaches do not preserve test requirements. The percentage of satisfied test requirements indicates the inadequacy level.
Objective:
 We compare some state-of-the-art adequate and inadequate TSR approaches with respect to the size of reduced test suites and their fault-detection capability. We aim to increase our body of knowledge on TSR approaches by comparing: 
(i)
 well-known traditional adequate TSR approaches; 
(ii)
 their inadequate variants; and 
(iii)
 several variants of a novel Clustering-Based (CB) approach for (adequate and inadequate) TSR.
Method:
 We conducted an experiment to compare adequate and inadequate TSR approaches. This comparison is founded on a public dataset containing information on real faults.
Results:
 The most important findings from our experiment can be summarized as follows: 
(i)
 there is not an inadequate TSR approach that outperforms the others;
(ii)
 some inadequate variants of the CB approach, and few traditional inadequate approaches, outperform the adequate ones in terms of reduction in test suite size with a 
negligible effect
 on fault-detection capability; and 
(iii)
 the CB approach is less sensitive than the other inadequate approaches, that is, variations in the inadequacy level have small effect on reduction in test suite size and on loss in fault-detection capability.
Conclusions:
 These findings imply that inadequate TSR approaches and especially the CB approach might be appealing because they lead to a greater reduction in test suite size (with respect to the adequate ones) at the expense of a small loss in fault-detection capability.",21 Mar 2025,6,"The comparison of Test Suite Reduction (TSR) approaches may provide some insights for startups looking to optimize regression testing processes, but the practical impact is less immediate compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919302228,A systematic literature review of machine learning techniques for software maintainability prediction,March 2020,Information and Software Technology,Not Found,Hadeel=Alsolai: hadeel.alsolai@strath.ac.uk; Marc=Roper: marc.roper@strath.ac.uk,"Abstract
Context
Software 
maintainability
 is one of the fundamental 
quality attributes
 of 
software engineering
. The accurate prediction of software 
maintainability
 is a significant challenge for the effective management of the software maintenance process.
Objective
The major aim of this paper is to present a systematic review of studies related to the prediction of 
maintainability
 of object-oriented software systems using 
machine learning techniques
. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software 
maintainability
 measurements, metrics, datasets, evaluation measures, individual models and ensemble models.
Method
The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.
Results
We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other 
software quality attributes
. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level 
product metrics
 as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on 
regression problems
 and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.
Conclusion
Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.",21 Mar 2025,8,The systematic review on machine learning techniques for predicting software maintainability offers valuable knowledge that can aid startups in improving the quality and maintainability of their software systems.
https://www.sciencedirect.com/science/article/pii/S0950584919302587,Intelligent software engineering in the context of agile software development: A systematic literature review,March 2020,Information and Software Technology,Not Found,Mirko=Perkusich: mirko@embedded.ufcg.edu.br; Lenardo=Chaves e Silva: lenardo.silva@embedded.ufcg.edu.br; Alexandre=Costa: alexandre.costa@embedded.ufcg.edu.br; Felipe=Ramos: felipe.ramos@embedded.ufcg.edu.br; Renata=Saraiva: renata.saraiva@embedded.ufcg.edu.br; Arthur=Freire: arthur.freire@embedded.ufcg.edu.br; Ednaldo=Dilorenzo: ednaldo.dilorenzo@virtus.ufcg.edu.br; Emanuel=Dantas: emanuel.dantas@embedded.ufcg.edu.br; Danilo=Santos: danilo.santos@embedded.ufcg.edu.br; Kyller=Gorgônio: kyller@embedded.ufcg.edu.br; Hyggo=Almeida: hyggo@embedded.ufcg.edu.br; Angelo=Perkusich: perkusic@virtus.ufcg.edu.br,"Abstract
CONTEXT
: Intelligent 
Software Engineering
 (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, 
natural language processing
, perception or supporting decision-making.
OBJECTIVE
: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to 
Agile Software Development
 (ASD). Furthermore, we assess its maturity and identify adoption risks.
METHOD
: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. 
RESULTS
: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and 
machine learning
 are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, 
resource allocation
, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques.
CONCLUSION
: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.",21 Mar 2025,7,"This study explores the application of intelligent techniques in Agile Software Development, which can have practical value for European early-stage ventures by enhancing decision-making processes and reducing adoption risks. However, the field is still in its infancy, requiring more empirical studies."
https://www.sciencedirect.com/science/article/pii/S0950584919302575,An empirically evaluated checklist for surveys in software engineering,March 2020,Information and Software Technology,Not Found,Jefferson Seide=Molléri: jefferson@simula.no; Kai=Petersen: Not Found; Emilia=Mendes: Not Found,"Abstract
Context:
 Over the past decade 
Software Engineering
 research has seen a steady increase in survey-based studies, and there are several guidelines providing support for those willing to carry out surveys. The need for auditing survey research has been raised in the literature. Checklists have been used both to conduct and to assess different types of empirical studies, such as experiments and 
case studies
.
Objective:
 To operationalize the assessment of survey studies by means of a checklist. To fulfill such goal, we aim to derive a checklist from standards for survey research and further evaluate the appropriateness of the checklist in the context of software engineering research.
Method:
 We systematically aggregated knowledge from 12 methodological studies supporting survey-based research in software engineering. We identified the key stages of the survey process and its recommended practices through thematic analysis and vote counting. We evaluated the checklist by applying it to existing surveys and analyzed the results. Thereafter, we gathered the feedback of experts (the surveys’ authors) on our analysis and used the feedback to improve the survey checklist.
Results:
 The evaluation provided insights regarding limitations of the checklist in relation to its understanding and objectivity. In particular, 19 of the 38 checklist items were improved according to the feedback received from experts.
Conclusion:
 The proposed checklist is appropriate for auditing survey reports as well as a support tool to guide ongoing research with regard to the survey design process. A discussion on how to use the checklist and what its implications are for research practice is also provided.",21 Mar 2025,5,"The checklist proposed for auditing survey reports can be a useful tool for software engineering research. While it provides support for survey design processes, its impact on early-stage ventures may not be as significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918301642,Towards assisting developers in API usage by automated recovery of complex temporal patterns,March 2020,Information and Software Technology,Not Found,Mohamed Aymen=Saied: m_saied@encs.concordia.ca; Erick=Raelijohn: erick.raelijohn@umontreal.ca; Edouard=Batot: batotedo@iro.umontreal.ca; Michalis=Famelis: famelis@iro.umontreal.ca; Houari=Sahraoui: sahraouh@iro.umontreal.ca,"Abstract
Context
Despite the many advantages, the use of external libraries through their APIs remains difficult because of the usage patterns and constraints that are hidden or not properly documented. Existing work provides different techniques to recover API usage patterns from client programs in order to help developers use those libraries. However, most of these techniques produce patterns that generally do not involve 
temporal properties
.
Objective
In this paper, we discuss the problem of temporal usage patterns recovery and propose an algorithm to solve it. We also discuss how the obtained patterns can be used at different stages of client development.
Method
We address the recovery of temporal API usage patterns as an 
optimization problem
 and solve it using a genetic-programming algorithm.
Results
Our evaluation on different APIs shows that the proposed algorithm allows to derive non-trivial temporal usage that are useful and generalizable to new API clients.
Conclusion
Recovering API usage temporal patterns helps client developers to use APIs in an appropriate way. In addition to potentially improve productivity, such patterns also helps preventing errors that result from an incorrect use of the APIs.",21 Mar 2025,8,"The proposed algorithm for recovering temporal API usage patterns can be beneficial for European startups using external libraries. By helping developers use APIs appropriately and preventing errors, it has the potential to improve productivity and software quality."
https://www.sciencedirect.com/science/article/pii/S0950584919302083,Architecting systems of systems: A tertiary study,February 2020,Information and Software Technology,Not Found,Héctor=Cadavid: h.f.cadavid.rengifo@rug.nl; Vasilios=Andrikopoulos: v.andrikopoulos@rug.nl; Paris=Avgeriou: p.avgeriou@rug.nl,"Abstract
Context:
 The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent 
constituent systems
 that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the 
Internet of Things
.
Objective:
 This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and 
software quality attributes
 by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. 
Method:
 We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. 
Results:
 The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and 
quality attributes
. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. 
Conclusions:
 Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.",21 Mar 2025,6,"The investigation into System of Systems (SoS) architecting provides valuable insights, but the impact on early-stage ventures may be limited. While the research highlights research gaps and issues within the field, the practical application for startups may not be immediate."
https://www.sciencedirect.com/science/article/pii/S0950584919302034,Assisting engineers extracting requirements on components from domain documents,February 2020,Information and Software Technology,Not Found,Xiaoli=Lian: lianxiaoli@buaa.edu.cn; Wenchuang=Liu: liuwenchuang@bytedance.com; Li=Zhang: lily@buaa.edu.cn,"Abstract
Context
: When entering an unfamiliar domain, organizations usually have to invest significant time and effort performing domain analysis with the purpose of acquiring 
system requirements
. This process usually involves collecting domain documents extensively, retrieving and reviewing the related ones carefully, searching for the requirements knowledge, then extracting and specifying 
system requirements
. Furthermore, the task must often be performed repeatedly throughout the early phases of projects. Depending on the nature of the domain and the availability of documentation, this process is extremely time-consuming and may require non-trivial human effort.
Objective
: In order to assist engineers identifying requirements knowledge from a collect of domain documents, previously we proposed an approach MaRK in the Conference RE’16 which ranks the domain documents by their relevance to components and highlights the content that are likely to contain component-related information. Experiments showed MaRK can almost identify the top and bottom documents in the reference list. However, it tends to underestimate the relevance of the domain documents that have a number of sections with medium knowledge density.
Method
: We improve the ranking algorithm in MaRK and propose MaRK-II. In addition, to assist engineers locating the relevant information in lengthy documents, we preserve the highlighting work in MaRK and strengthen MaRK-II by extracting the summary of component-related text. MaRK-II is evaluated with the documents in three domains.
Results
: We found that MaRK-II significantly outperforms MaRK and 
VSM
 on ranking the documents by their relevance. And a user study showed that MaRK-II is indeed helpful for engineers to extract requirements on components.
Conclusions
: Our approach provides three mechanisms including documents ranking, pertinent content highlighting and summarizing to help engineers obtaining requirements from a collection of domain documents.",21 Mar 2025,9,"The MaRK-II approach, which assists engineers in identifying requirements knowledge from domain documents, can have a high practical value for European early-stage ventures. By significantly outperforming previous methods and aiding in requirements extraction, it can save time and effort for startup projects."
https://www.sciencedirect.com/science/article/pii/S0950584919302095,Toward recursion aware complexity metrics,February 2020,Information and Software Technology,Not Found,Gordana=Rakić: gordana.rakic@dmi.uns.ac.rs; Melinda=Tóth: tothmelinda@elte.hu; Zoran=Budimac: zoran.budimac@dmi.uns.ac.rs,"Abstract
Context
: Software developers spend a significant amount of time on reading, comprehending, and debugging of 
source code
. Numerous software metrics can give us awareness of incomprehensible functions or of flaws in their collaboration. Invocation chains, especially recursive ones, affect solution complexity, readability, and 
understandability
. Even though decomposed and recursive solutions are characterized as short and clear in comparison with iterative ones, they hide the complexity of the observed problem and solution. As the collaboration between functions can strongly depend on context, difficulties are usually detected in debugging, testing or by 
static analysis
, while metrics support is still very weak.
Objective
: We introduce a new complexity metric, called Overall Path Complexity (OPC), which is aware of (recursive) call chains in the observed 
source code
. As invocations are basic collaboration mechanism and recursions are broadly accepted, the OPC metric is intended to be applicable independently on programming language and paradigm.
Method
: We propose four different versions of the OPC calculation algorithm and explore and discuss their suitability. We have validated proposed metrics based on a Framework specially designed for evaluation and validation of software complexity metrics and accordingly performed theoretical, empirical and practical validation. Practical validation was performed on toy examples and industrial cases (47012 LOCs, 2899 functions, and 758 recursive paths) written in Erlang.
Result
: Based on our analysis we selected the most suitable (of 4 proposed) OPC calculation formula, and showed that the new metric expresses advanced properties of the software in comparison with other available metrics that was confirmed by 
low correlation
.
Conclusion
: We introduced the OPC metric calculated on the Overall Control 
Flow Graph
 as an extension of 
Cyclomatic Complexity
 by adding awareness of (recursive) invocations. The values of the new metric can lead us to find the problematic fragments of the code or of the execution paths.",21 Mar 2025,8,"The introduction of a new complexity metric, Overall Path Complexity (OPC), has practical implications for software developers, enhancing the understanding of source code and identifying problematic fragments or execution paths."
https://www.sciencedirect.com/science/article/pii/S0950584919302289,A generic methodology for early identification of non-maintainable source code components through analysis of software releases,February 2020,Information and Software Technology,Not Found,Michail D.=Papamichail: mpapamic@issel.ee.auth.gr; Andreas L.=Symeonidis: Not Found,"Abstract
Context
Contemporary development approaches consider that time-to-market is of 
utmost importance
 and assume that software projects are constantly evolving, driven by the continuously changing requirements of end-users. This practically requires an iterative process where software is changing by introducing new or updating existing software/user features, while at the same time continuing to support the stable ones. In order to ensure efficient software evolution, the need to produce maintainable software is evident.
Objective
In this work, we argue that non-maintainable software is not the outcome of a single change, but the consequence of a series of changes throughout the 
development lifecycle
. To that end, we define a 
maintainability
 evaluation methodology across releases and employ various information residing in software repositories, so as to decide on the maintainability of software.
Method
Upon using the dropping of packages as a non-maintainability indicator (accompanied by a series of quality-related criteria), the proposed methodology involves using one-class-classification techniques for evaluating maintainability at a package level, on four different axes each targeting a primary 
source code
 property: complexity, cohesion, coupling, and inheritance.
Results
Given the qualitative and 
quantitative evaluation
 of our methodology, we argue that apart from providing accurate and interpretable maintainability evaluation at package level, we can also identify non-maintainable components at an early stage. This early stage is in many cases around 50% of the software package lifecycle.
Conclusion
Based on our findings, we conclude that modeling the trending behavior of certain 
static analysis
 metrics enables the effective identification of non-maintainable software components and thus can be a valuable tool for the software engineers.",21 Mar 2025,7,The methodology for evaluating maintainability across software releases using quality-related criteria at a package level provides valuable insights for ensuring efficient software evolution and early identification of non-maintainable components.
https://www.sciencedirect.com/science/article/pii/S0950584919302216,Practical detection of CMS plugin conflicts in large plugin sets,February 2020,Information and Software Technology,Not Found,Igor=Lima: isol2@cin.ufpe.br; Jeanderson=Cândido: j.barroscandido@tudelft.nl; Marcelo=d’Amorim: damorim@cin.ufpe.br,"Abstract
Context
Content Management Systems
 (CMS), such as WordPress, are a very popular category of software for creating web sites and blogs. These systems typically build on top of plugin architectures. Unfortunately, it is not uncommon that the combined activation of multiple plugins in a CMS web site will produce unexpected behavior. Conflict-detection techniques exist but they do not scale.
Objective
This paper proposes 
Pena
, a technique to detect conflicts in large sets of plugins as those present in plugin market places.
Method
Pena
 takes on input a configuration, consisting of a potentially large set of plugins, and reports on output the offending plugin combinations. 
Pena
 uses an iterative divide-and-conquer search to explore the large space of plugin combinations and a staged filtering process to eliminate 
false alarms
.
Results
We evaluated 
Pena
 with plugins selected from the WordPress official repository and compared its efficiency and accuracy against the technique that checks conflicts in all pairs of plugins. Results show that 
Pena
 is 12.4x to 19.6x more efficient than the comparison baseline and can find as many conflicts as it.",21 Mar 2025,6,"The technique Pena presented for conflict detection in plugin architectures addresses a common issue in Content Management Systems, providing a more efficient alternative to existing conflict-detection techniques."
https://www.sciencedirect.com/science/article/pii/S0950584919302307,Energy efficient adaptation engines for android applications,February 2020,Information and Software Technology,Not Found,Angel=Cañete: angelcv@lcc.uma.es; Jose-Miguel=Horcas: Not Found; Inmaculada=Ayala: Not Found; Lidia=Fuentes: Not Found,"Abstract
Context
 The energy consumption of mobile devices is increasing due to the improvement in their components (e.g., better processors, larger screens). Although the hardware consumes the energy, the software is responsible for managing hardware resources such as the camera software and its functionality, and therefore, affects the energy consumption. Energy consumption not only depends on the installed code, but also on the execution context (environment, devices status) and how the user interacts with the application.
Objective
 In order to reduce the energy consumption based on 
user behavior
, it is necessary to dynamically adapt the application. However, the adaptation mechanism also consumes a certain amount of energy in itself, which may lead to an important increase in the energy expenditure of the application in comparison with the benefits of the adaptation. Therefore, this footprint must be measured and compared with the benefit obtained.
Method
 In this paper, we (1) determine the benefits, in terms of energy consumption, of dynamically adapting mobile applications, based on 
user behavior
; and (2) advocate the most energy-efficient adaptation mechanism. We provide four different implementations of a proposed adaptation model and measure their energy consumption.
Results
 The proposed adaptation engines do not increase the energy consumption when compared to the benefits of the adaptation, which can reduce the energy consumption by up to 20%.
Conclusion
 The adaptation engines proposed in this paper can decrease the energy consumption of the mobile devices based on user behavior. The overhead introduced by the adaptation engines is negligible in comparison with the benefits obtained by the adaptation.",21 Mar 2025,5,"The research on energy-efficient adaptation mechanisms for mobile applications based on user behavior has practical implications, although the impact may be limited by the marginal benefits obtained compared to the energy expenditure on adaptation."
https://www.sciencedirect.com/science/article/pii/S0950584919302290,A focus area maturity model for software ecosystem governance,February 2020,Information and Software Technology,Not Found,Slinger=Jansen: slinger.jansen@uu.nl,"Abstract
Context
Increasingly, software companies are realizing that they can no longer compete through product excellence alone. The ecosystems that surround platforms, such as operating systems, enterprise applications, and even social networks are undeniably responsible for a large part of a platform’s success. With this realization, software producing organizations need to devise tools and strategies to improve their ecosystems and reinvent tools that others have invented many times before.
Objective
In this article, the software ecosystem governance maturity model (SEG-
M
2
) is presented, which has been designed along the principles of a focus area maturity model. The SEG-
M
2
 has been designed for software producing organizations to assess their ecosystem governance practices, set a goal for improvement, and execute an improvement plan.
Method
The model has been created following an established focus area maturity model design method. The model has been evaluated in six evaluating 
case studies
 with practitioners, first by applying the model to their organizations and secondly by evaluating with the practitioners whether the evaluation and improvement advice from the model is valid, useful, and effective.
Result
The model is extensively described and illustrated using six desk studies and six 
case studies
.
Conclusions
The model is evaluated by both researchers and practitioners as a useful collection of practices that enable decision making about software ecosystem governance. We find that maturity models are an effective tool in disseminating a large collection of knowledge, but that research and creation tooling for maturity models is limited.",21 Mar 2025,4,"The software ecosystem governance maturity model (SEG-M2) provides a framework for software producing organizations to assess their ecosystem governance practices, but the overall impact on early-stage ventures may be less direct compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918302258,Energy aware simulation and testing of smart-spaces,February 2020,Information and Software Technology,Not Found,Khaled=El-Fakih: kelfakih@aus.edu; Teruhiro=Mizumoto: mizumoto@ist.osaka-u.ac.jp; Keiichi=Yasumoto: yasumoto@is.naist.jp; Teruo=Higashino: higashino@ist.osaka-u.ac.jp,"Abstract
Context
A smart-space SS typically consists of many rooms, with temperature and humidity environment attributes, devices, and software components that communicate with each other to satisfy certain test purposes that need to be checked over various realistic exterior environment weather conditions.
Objective
We present a novel energy-aware approach for the validation of smart-spaces while minimizing the energy consumption encountered during testing.
Method
A framework for deriving minimal (energy) cost tests is provided. It includes SS, a controlled environment 
Env
 that depicts the exterior conditions, and a 
Tester
 that can control SS and 
Env
, derive and runs tests, and observe relevant SS attributes in order to release a success verdict whenever a test purpose is met. A simulator is proposed for deriving tests by appropriately exploring part of the SS behavior employing several 
cost functions
 for computing the estimated cost and duration of test events.
Results
The framework is deployed in a real SS environment which is used to assess the actual energy consumption of derived tests in practice. Experiments show that the actual 
power consumption
 of the derived tests is close to the ones estimated by the simulator. A 
case study
 that assesses the gains in using energy aware tests in comparison to non energy-aware alternatives is also provided.
Conclusions
The obtained results highlight the importance of considering 
power consumption
 in the development and testing of smart-spaces.",21 Mar 2025,8,"This abstract presents a novel energy-aware approach for the validation of smart-spaces, which can have a significant impact on reducing energy consumption in early-stage ventures working with smart technologies."
https://www.sciencedirect.com/science/article/pii/S0950584919302022,"Impact of usability mechanisms: An experiment on efficiency, effectiveness and user satisfaction",January 2020,Information and Software Technology,Not Found,Juan M.=Ferreira: jmferreira1978@fpuna.edu.py; Silvia T.=Acuña: silvia.acunna@uam.es; Oscar=Dieste: odieste@fi.upm.es; Sira=Vegas: svegas@fi.upm.es; Adrián=Santos: adrian.santos.parrilla@oulu.fi; Francy=Rodríguez: francy.rodriguez@uam.es; Natalia=Juristo: natalia@fi.upm.es,"Abstract
Context
As a software quality characteristic, usability includes the attributes of efficiency, effectiveness and user satisfaction. There are several recommendations in the literature on how to build usable software systems, but there are not very many empirical studies that provide evidence about their impact.
Objective
We report an experiment carried out with users to understand the effect of three usability mechanisms —Abort Operation, Progress Feedback and Preferences— on efficiency, effectiveness and user satisfaction. Usability mechanisms are functionalities that should, according to the HCI community, be implemented within a software system to increase its usability.
Method
The experiment was conducted with 168 users divided into 24 experimental groups. Each group performs three online shopping tasks. We measure efficiency variables (number of clicks and time taken), effectiveness (percentage of task completion) and user satisfaction gathered from a questionnaire.
Results
The adoption of Abort Operation has a significantly positive effect on efficiency (time taken), effectiveness and user satisfaction. The adoption of Progress Feedback does not appear to have any impact on any of the variables. The adoption of Preferences has a significantly positive effect on effectiveness and user satisfaction but no influence on efficiency.
Conclusions
We provide relevant evidence of the impact of the three usability mechanisms on efficiency, effectiveness and user satisfaction. In no case do the usability mechanisms degrade user performance. The effort to adopt Abort Operation and Preferences appears to be justified by the benefits in terms of effectiveness and user satisfaction. Also Abort Operation enables the user to be more productive. We believe that the effects on efficiency, effectiveness and satisfaction depend not only on mechanism functionality but also on the problem domain. The impact of a mechanism in other contexts could differ. Therefore, we need to conduct further experiments to gather more evidence and confirm these results.",21 Mar 2025,7,"The experiment on usability mechanisms provides valuable insights for software systems development, which can benefit European startups aiming to improve user satisfaction and efficiency in their products."
https://www.sciencedirect.com/science/article/pii/S0950584919302046,The missing link – A semantic web based approach for integrating screencasts with security advisories,January 2020,Information and Software Technology,Not Found,Ellis E.=Eghan: e_eghan@encs.concordia.ca; Parisa=Moslehi: p_mosleh@encs.concordia.ca; Juergen=Rilling: juergen.rilling@concordia.ca; Bram=Adams: bram.adams@polymtl.ca,"Abstract
Context
Collaborative tools and repositories have been introduced to facilitate 
open source software development
, allowing projects, developers, and users to share their knowledge and expertise through formal and informal channels such as repositories, Q&A websites, blogs and screencasts. While significant progress has been made in mining and cross-linking traditional software repositories, limited work exists in making 
multimedia content
 in the form of screencasts or 
audio recordings
 an integrated part of 
software engineering
 processes.
Objective
The objective of this research is to provide a standardized ontological representation that allows for a seamless knowledge integration of screencasts with other software artifacts across knowledge resource boundaries.
Method
In this paper, we propose a modeling approach that takes advantage of the Semantic Web and its inference services to capture and establish 
traceability links
 between knowledge extracted from different resources such as 
vulnerability information
 in 
NVD
, project dependency information from Maven Central, and YouTube screencasts.
Results
We performed a 
case study
 on 48 videos that illustrate attacks on vulnerable systems and show that our approach can successfully link relevant vulnerabilities and screencasts with an average precision of 98% and an average recall of 54% when vulnerability identifiers (CVE ID) are explicitly mentioned in the metadata (title and description) of videos. When no CVE ID is present, our initial results show that for a reduced search space (for one vulnerability), using only the textual content of the image frames, our approach is still able to link video-vulnerability pairs and rank the correct result within the top two positions of the result set.
Conclusion
Our approach not only establishes bi-directional, direct, and indirect 
traceability links
 from screencasts to these other software artifacts; these links can also be used to guide practitioners in comprehending the potential security impact of vulnerable components in their projects.",21 Mar 2025,7,"The standardized ontological representation for integrating screencasts with software artifacts can enhance knowledge sharing in open source software development, benefiting startups collaborating on software projects."
https://www.sciencedirect.com/science/article/pii/S0950584919302010,Requirements specification for developers in agile projects: Evaluation by two industrial case studies,January 2020,Information and Software Technology,Not Found,Juliana=Medeiros: juliana.medeiros@ifpb.edu.br; Alexandre=Vasconcelos: amlv@cin.ufpe.br; Carla=Silva: ctlls@cin.ufpe.br; Miguel=Goulão: mgoul@fct.unl.pt,"Abstract
Context
An inadequate requirements specification activity acts as a catalyst to other problems, such as low team productivity and difficulty in maintaining software. Although 
Agile Software Development
 (ASD) has grown in recent years, research pointed out several limitations concerning its 
requirements engineering
 activities, such as Software Requirements Specification (SRS) provided in high level and targeted to the customer, lack of information required to perform design activities and low availability of the customer. To overcome these issues, the RSD (Requirements Specification for Developers) approach was proposed to create an SRS that provides information closer to development needs. In addition, existing literature reviews identify a demand for more empirical studies on the requirements specification activity in ASD.
Objective
Face to this, this work presents the evaluation of the RSD approach with respect to how it affects the teamwork and to identify its strengths and limitations.
Methods
This evaluation was performed by means of two industrial 
case studies
 conducted using a multiple-case design, focusing on software engineers as the analysis unit. Data were collected during 15 months from documents, observations, and interviews. They were triangulated, analyzed, and synthesized using techniques of grounded theory.
Results
The findings pointed out that the readability of SRS was compromised when several requirements are specified in the same RSD artifact. Evaluation also indicated the need of prioritization and categorization of the 
acceptance criteria
, a tool for creating, searching and tracing the artifacts, and obtaining 
acceptance tests
 from 
acceptance criteria
. On the other hand, the findings showed that the practices used to specify requirements using the RSD approach have the potential to produce a more objective SRS, tailored for the development team.
Conclusion
As a consequence, the structure of the RSD artifact was considered as a factor that improved the team performance in the two 
case studies
.",21 Mar 2025,6,"The evaluation of the RSD approach can provide valuable lessons for improving teamwork and requirements specification in Agile Software Development, which can be beneficial for early-stage ventures adopting Agile methodologies."
https://www.sciencedirect.com/science/article/pii/S0950584919302058,Automatic extraction of product line architecture and feature models from UML class diagram variants,January 2020,Information and Software Technology,Not Found,Wesley K.G.=Assunção: wesleyk@inf.ufpr.br; Silvia R.=Vergilio: silvia@inf.ufpr.br; Roberto E.=Lopez-Herrejon: roberto.lopez@etsmtl.ca,"Abstract
Context
Software Product Lines (SPLs) are families of 
related products
 developed for specific domains. SPLs commonly emerge from existing variants when their individual maintenance and/or evolution become complex. Even though there exists a vast research literature on SPL extraction, the majority of the approaches have only focused on 
source code
, are partially automated, or do not reflect domain constraints. Such limitations can make more difficult the extraction, management, documentation and generation of some important SPL artifacts such as the 
product line architecture
, a fact that can impact negatively the evolution and maintenance of SPLs.
Objective
To tackle these limitations, this work presents ModelVars2SPL (
Model Variants to SPL Core Assets
), an automated approach to aid the development of SPLs from existing system variants.
Method
The input for ModelVars2SPL is a set of 
Unified Modeling Language
 (UML) 
class diagrams
 and the list of features they implement. The approach extracts two main assets: (i) Feature Model (FM), which represents the combinations of features, and (ii) a Product Line Architecture (PLA), which represents a global structure of the variants. ModelVars2SPL is composed of four automated steps. We conducted a thorough evaluation of ModelVars2SPL to analyze the artefacts it generates and its performance.
Results
The results show that the FMs well-represent the features organization, providing useful information to define and manage commonalities and variabilities. The PLAs show a global structure of current variants, facilitating the understanding of existing implementations of all variants.
Conclusions
An advantage of ModelVars2SPL is to exploit the use of UML design models, that is, it is independent of the programming language, and supports the re-engineering process in the design level, allowing practitioners to have a broader view of the SPL.",21 Mar 2025,6,"The ModelVars2SPL automated approach can assist in developing SPLs from existing system variants, potentially simplifying the management and evolution of software product lines for European startups dealing with complex software systems."
https://www.sciencedirect.com/science/article/pii/S095058491930206X,Utilising CI environment for efficient and effective testing of NFRs,January 2020,Information and Software Technology,Not Found,Liang=Yu: liang.yu@bth.se; Emil=Alégroth: emil.alegroth@bth.se; Panagiota=Chatzipetrou: panagiota.chatzipetrou@oru.se; Tony=Gorschek: tony.gorschek@bth.se,"Abstract
Context
Continuous integration (CI) is a practice that aims to continuously verify 
quality aspects
 of a software intensive system both for functional and non-functional requirements (NFRs). Functional requirements are the inputs of development and can be tested in isolation, utilising either manual or automated tests. In contrast, some NFRs are difficult to test without functionality, for NFRs are often aspects of functionality and express 
quality aspects
. Lacking this 
testability
 attribute makes NFR testing complicated and, therefore, underrepresented in industrial practice. However, the emergence of CI has radically affected software development and created new avenues for software quality evaluation and quality 
information acquisition
. Research has, consequently, been devoted to the utilisation of this additional information for more efficient and effective NFR verification.
Objective
We aim to identify the state-of-the-art of utilising the CI environment for NFR testing, hereinafter referred to as CI-NFR testing.
Method
Through rigorous selection, from an initial set of 747 papers, we identified 47 papers that describe how NFRs are tested in a CI environment. Evidence-based analysis, through coding, is performed on the identified papers in this SLR.
Results
Firstly, ten CI approaches are described by the papers selected, each describing different tools and nine different NFRs where reported to be tested. Secondly, although possible, CI-NFR testing is associated with eight challenges that adversely affect its adoption. Thirdly, the identified CI-NFR testing processes are tool-driven, but there is a lack of NFR testing tools that can be used in the CI environment. Finally, we proposed a CI framework for NFRs testing.
Conclusion
A synthesised CI framework is proposed for testing various NFRs, and associated CI tools are also mapped. This contribution is valuable as results of the study also show that CI-NFR testing can help improve the quality of NFR testing in practices.",21 Mar 2025,8,"The abstract proposes a new framework for NFR testing in the CI environment, which can significantly impact software quality evaluation and improvement."
https://www.sciencedirect.com/science/article/pii/S0950584918302271,Ontology-based test generation for automated and autonomous driving functions,January 2020,Information and Software Technology,Not Found,Yihao=Li: yihao.li@ist.tugraz.at; Jianbo=Tao: Not Found; Franz=Wotawa: wotawa@ist.tugraz.at,"Abstract
Context:
 Ontologies are known as a formal and explicit conceptualization of entities, their interfaces, behaviors, and relationships. They have been applied in various application domains such as 
autonomous driving
 where ontologies are used for decision making, traffic description, auto-pilot etc. It has always been a challenge to test the corresponding safety-critical software systems in 
autonomous driving
 that have been playing an increasingly important role in our daily routines.
Objective:
 Failures in these systems potentially not only cause great financial loss but also the loss of lives. Therefore, it is vital to obtain and cover as many as critical driving scenarios during auto drive testing to ensure that the system can always reach a fail-safe state under different circumstances.
Method:
 We outline a general framework for testing, verification, and validation for automated and autonomous driving functions. The introduced method makes use of ontologies for describing the environment of autonomous vehicles and convert them to input models for combinatorial testing. The combinatorial test suite comprises abstract test cases that are mapped to concrete test cases that can be executed using simulation environments.
Results:
 We discuss in detail on how to automatically convert ontologies to the corresponding combinatorial testing input models. Specifically, we present two conversion algorithms and compare their applicability using ontologies with different sizes. We also carried out a 
case study
 to further demonstrate the practical value of applying ontology-based test generation in industrial settings.
Conclusion:
 The proposed approach for testing autonomous driving takes ontologies describing the environment of autonomous vehicles, and automatically converts it to test cases that are used in a simulation environment to verify automated driving functions. The conversion relies on combinatorial testing. The first experimental results relying on an example from the automotive industry indicates that the approach can be used in practice.",21 Mar 2025,9,"The abstract introduces a method for testing autonomous driving functions using ontologies and combinatorial testing, addressing critical safety concerns and demonstrating practical value in an industrial setting."
https://www.sciencedirect.com/science/article/pii/S0950584919301843,Improving feature location accuracy via paragraph vector tuning,December 2019,Information and Software Technology,Not Found,Allysson Costa e=Silva: allcostaes@ufu.br; Marcelo de Almeida=Maia: marcelo.maia@ufu.br,"Abstract
Context
Feature location techniques are still not highly accurate despite advances in the field.
Objective
This paper aims at investigating the impact of applying different tunings to paragraph vector to the feature location problem. It evaluates the influence of different 
artificial neural network
 (ANN) configurations for 
learning rate
 and negative sampling loss function in paragraph 
vectors training
.
Method
The suggested weight configuration relies on the search for an adequate ANN 
learning rate
 and an adequate calibration of negative sampling skip-gram mode of the Doc2vec (DV) algorithm. A dataset with 633 feature descriptions, extracted from six open-source Java projects, organized within method 
granularity
, is used for the empirical assessment.
Results
Our results suggest that feature location techniques benefit from the use of paragraph vector with systematic tuning. We show that an adequate update policy for 
ANN weights
 can increase feature location accuracy. An adequate calibration for negative sampling also improved accuracy. We got it with no default values of negative sampling pointed by literature. Moreover, an ensemble of learning rate policies and the use of a tuned DV negative sampling option had overcome state-of-the-art approaches.
Conclusions
We show evidence of a relationship between hyper-parameter settings and accuracy gain. Modern paragraph vector approaches require adequate calibration to produce better results, and we have improved the accuracy of feature location process with proper tuning.",21 Mar 2025,7,"The abstract investigates the impact of different tunings to paragraph vectors for feature location techniques, offering insights into hyper-parameter settings for accuracy gain."
https://www.sciencedirect.com/science/article/pii/S0950584919301715,Software process line as an approach to support software process reuse: A systematic literature review,December 2019,Information and Software Technology,Not Found,Eldânae=Nogueira Teixeira: danny@cos.ufrj.br; Fellipe Araújo=Aleixo: Not Found; Francisco Dione de Sousa=Amâncio: Not Found; Edson=OliveiraJr: Not Found; Uirá=Kulesza: Not Found; Cláudia=Werner: Not Found,"Abstract
Context
Software 
Process Line
 (SPrL) aims at providing a systematic reuse technique to support reuse experiences and knowledge in the definition of software processes for new projects thus contributing to reduce effort and costs and to achieve improvements in quality. Although the research body in SPrL is expanding, it is still an immature area with results offering an overall view scattered with no consensus.
Objective
The goal of this work is to identify existing approaches for developing, using, managing and visualizing the evolution of SPrLs and to characterize their support, especially during the development of reusable process family artefacts, including an overview of existing SPrL supporting tools in their multiple stages; to 
analyse variability
 management and component-based aspects in SPrL; and, finally, to list practical examples and conducted evaluations. This research aims at reaching a broader and more consistent view of the research area and to provide perspectives and gaps for future research.
Method
We performed a systematic literature review according to well-established guidelines set. We used tools to partially support the process, which relies on a six-member research team.
Results
We report on 49 primary studies that deal mostly with conceptual or theoretical proposals and the domain engineering stage. Years 2014, 2015, and 2018 yielded the largest number of articles. This can indicate SPrL as a recent research theme and one that attracts ever-increasing interest.
Conclusion
Although this research area is growing, there is still a lack of practical experiences and approaches for actual applications or project-specific process derivations and decision-making support. The concept of an integrated reuse infrastructure is less discussed and explored; and the development of integrated tools to support all reuse stages is not fully addressed. Other topics for future research are discussed throughout the paper with gaps pointed as opportunities for improvements in the area.",21 Mar 2025,6,"The abstract identifies existing approaches for developing and managing Software Process Lines, contributing to reduce effort and costs, but lacks strong practical applications and integrated tools."
https://www.sciencedirect.com/science/article/pii/S095058491930165X,Impact of the conceptual model's representation format on identifying and understanding user stories,December 2019,Information and Software Technology,Not Found,Marina=Trkman: marina.trkman@gmail.com; Jan=Mendling: Not Found; Peter=Trkman: Not Found; Marjan=Krisper: Not Found,"Abstract
Context
Eliciting user stories is a major challenge for 
agile development
 approaches. Conceptual models are used to support the identification of user stories and increase their understanding. In many companies, existing model documentation stored as either use cases or 
BPMN
 models is available. However, these two types of business process models might not be equally effective for 
elicitation
 tasks due to their formats.
Objective
We address the effectiveness of different 
elicitation
 tasks when supported either with visual or textual conceptual model. Since the agile literature shows little attention to reusing existing 
BPMN
 documentation, we propose several hypotheses to compare it to the use of textual use case models.
Method
We conducted an experiment to compare the effectiveness of the two business process formats: textual use cases and visual BPMN models. We studied their effects on three elicitation tasks: identifying user stories and understanding their execution-order and integration dependencies.
Results
The subjects better understood execution-order dependencies when visual input in the form of BPMN models was provided. The performance of the other two tasks showed no statistical differences.
Conclusion
We addressed an important problem of user story elicitation: which informationally equivalent model (visual BPMN or textual use case) is more effective when identifying and understanding user stories.",21 Mar 2025,7,"The abstract compares the effectiveness of visual BPMN models and textual use cases in user story elicitation, providing insights into which model format is more effective for specific tasks."
https://www.sciencedirect.com/science/article/pii/S0950584919301673,Assessing the effectiveness of goal-oriented modeling languages: A family of experiments,December 2019,Information and Software Technology,Not Found,Silvia=Abrahão: sabrahao@dsic.upv.es; Emilio=Insfran: einsfran@dsic.upv.es; Fernando=González-Ladrón-de-Guevara: fgonzal@omp.upv.es; Marta=Fernández-Diego: marferdi@omp.upv.es; Carlos=Cano-Genoves: carcage1@dsic.upv.es; Raphael=Pereira de Oliveira: raphael.oliveira@ifs.edu.br,"Abstract
Context
Several goal-oriented languages focus on modeling stakeholders’ objectives, interests or wishes. However, these languages can be used for various purposes (e.g., exploring system solutions or evaluating alternatives), and there are few guidelines on how to use these models downstream to the software requirements and design artifacts. Moreover, little attention has been paid to the empirical evaluation of this kind of languages. In a previous work, we proposed value@GRL as a specialization of the Goal Requirements Language (GRL) to specify stakeholders’ goals when dealing with early requirements in the context of incremental software development.
Objective
This paper compares the value@GRL language with the i* language, with respect to the quality of goal models, the participants’ modeling time and productivity when creating the models, and their perceptions regarding ease of use and usefulness.
Method
A family of experiments was carried out with 184 students and practitioners in which the participants were asked to specify a goal model using each of the languages. The participants also filled in a questionnaire that allowed us to assess their perceptions.
Results
The results of the individual experiments and the meta-analysis indicate that the quality of goal models obtained with value@GRL is higher than that of i*, but that the participants required less time to create the goal models when using i*. The results also show that the participants perceived value@GRL to be easier to use and more useful than i* in at least two experiments of the family.
Conclusions
value@GRL makes it possible to obtain goal models with good quality when compared to i*, which is one of the most frequently used goal-oriented 
modeling languages
. It can, therefore, be considered as a promising emerging approach in this area. Several insights emerged from the study and opportunities for improving both languages are outlined.",21 Mar 2025,7,"The comparison of value@GRL and i* languages provides valuable insights for stakeholders and practitioners in early requirements of software development, offering a promising emerging approach."
https://www.sciencedirect.com/science/article/pii/S0950584919301685,Package-Level stability evaluation of object-oriented systems,December 2019,Information and Software Technology,Not Found,Jawad Javed Akbar=Baig: Not Found; Sajjad=Mahmood: smahmood@kfupm.edu.sa; Mohammad=Alshayeb: Not Found; Mahmood=Niazi: Not Found,"Abstract
Context
Software stability is an important object-oriented design characteristic that contributes to the 
maintainability
 
quality attribute
. Software stability quantifies a given systems sensitivity to change between different versions. Stable software tends to reduce the maintenance effort. Assessing software stability during the object-oriented design phase is one of the measures to obtain maintainable software. To determine software stability, there are several metrics at the architecture, system and class levels, but few studies have investigated stability at the package level.
Objective
In this paper, we propose a new package stability metrics (PSM) based on the notion of change between package contents, intra-package connections and inter-package connections.
Method
We validate the PSM theoretically and empirically. The theoretical validation is based on a study of the 
mathematical properties
 of the metrics. The empirical validation is carried out using five 
open source software
 programs and we also present a comparison with comparable existing stability metrics packages. For the empirical validation, we perform correlation analysis, 
principal component
 analysis and prediction analysis.
Results
Correlation analysis shows that our proposed metrics provides a better indication of package stability than the existing stability metrics and they are negatively correlated with the maintenance effort. Principal component analysis shows that the proposed metrics captures new dimensions of package stability and helps to increase the maintenance prediction accuracy.
Conclusion
We found there was a negative correlation between our metric and maintenance effort. We also found a 
positive correlation
 between the existing package stability metrics which are based on changes in lines of code and class names.",21 Mar 2025,9,The introduction of package stability metrics at different levels and validation through theoretical and empirical analysis presents a significant contribution to software stability and maintainability.
https://www.sciencedirect.com/science/article/pii/S0950584919301727,Finding key classes in object-oriented software systems by techniques based on static analysis,December 2019,Information and Software Technology,Not Found,Ioana=Şora: ioana.sora@cs.upt.ro; Ciprian-Bogdan=Chirila: Not Found,"Abstract
Context
Software maintenance is burdened by 
program comprehension
 activities which consume a big part of project resources. Program comprehension is difficult because the code to be analyzed is very large and the documentation may not be well structured to help navigating through the code.
Objective
Tools should support the early stages of program comprehension. Our goal is to build tools that analyze the code and filter this large amount of information such that only the most important information is presented to the software 
maintenance team
. In the case of object-oriented systems, finding the important information means finding the most important classes, also called the key classes of the system.
Method
In this work, we formulate and explore several hypotheses regarding which are the class attributes that characterize important classes. By class attributes, we understand here different metrics that quantify properties of the class such as its connections and relationships with other classes. All the necessary input data for computing class attributes are extracted from code by 
static analysis
. We experimentally investigate which attributes are best suited to rank classes according to their importance, doing an extensive empirical study on fifteen software systems.
Result
Attributes from the categories of 
direct connections
 and network centrality are the best for finding key classes. We identified three class attributes which are best as class ranking criteria: PR-U2-W and CONN-TOTAL-W when the target set of key classes is small and CONN-TOTAL when the target set has a large and variable size. We show that the method of ranking classes based on these attributes outperforms known related work approaches of finding key classes.
Conclusions
Our method allows us to build easy-to-use fully automatic tools which find almost instantly the key classes of a software system starting from its code.",21 Mar 2025,8,"The development of tools to identify key classes in object-oriented systems through empirical study of class attributes provides practical support for software maintenance teams, offering an efficient and effective solution."
https://www.sciencedirect.com/science/article/pii/S0950584919301636,PrioriTTVs: A process aimed at supporting researchers to prioritize threats to validity and their mitigation actions when planning controlled experiments in SE,November 2019,Information and Software Technology,Not Found,Eudis=Teixeira: eot@cin.ufpe.br; Liliane=Fonseca: lss4@cin.ufpe.br; Bruno=Cartaxo: email@brunocartaxo.com; Sergio=Soares: scbs@cin.ufpe.br,"Abstract
Context
Researchers argue that a critical component of any empirical study in 
Software Engineering
 (SE) is to identify, analyze, and mitigate threats to validity.
Objective
We propose PrioriTTVs, a process to support researchers in identifying and prioritizing threats to validity and their corresponding 
mitigation actions
 when planning controlled experiments in SE. We also introduce a tool to support the entire process.
Method
Empirical studies were conducted with six experts and 20 
postgraduate students
 to evaluate the ease of use, learning, and perceptions of satisfaction regarding PrioriTTVs.
Results
So far, participants have considered PrioriTTVs to be useful (83%), significantly contributing to learning (90%), and satisfaction (75%).
Conclusions
We believe both novice and expert users can benefit from the process we propose for addressing threats to validity when conducting SE experiments. We also intend to extend our approach to manage threats specific to different SE experiment contexts.",21 Mar 2025,6,"The PrioriTTVs process for addressing threats to validity in SE experiments offers a useful tool for both novice and expert users, contributing to learning and satisfaction in experimental design."
https://www.sciencedirect.com/science/article/pii/S0950584919301703,Evaluating probabilistic software development effort estimates: Maximizing informativeness subject to calibration,November 2019,Information and Software Technology,Not Found,Magne=Jørgensen: magnej@simula.no,"Abstract
Context
Probabilistic effort estimates inform about the uncertainty and may give useful input to plans, budgets and investment analyses.
Objective & method
This paper introduces, motivates and illustrates two principles on how to evaluate the accuracy and other performance criteria of probabilistic effort estimates in software development contexts.
Results
The first principle emphasizes a consistency between the 
estimation error
 measure and the loss function of the chosen type of probabilistic single point effort estimates. The second 
principle points
 at the importance of not just measuring calibration, but also informativeness of estimated prediction intervals and distributions. The relevance of the evaluation principles is illustrated by a performance evaluation of estimates from twenty-eight software professionals using two different uncertainty assessment methods to estimate the effort of the same thirty software maintenance tasks.",21 Mar 2025,7,"The principles introduced for evaluating probabilistic effort estimates in software development contexts provide valuable guidelines for assessing accuracy and performance criteria, contributing to better planning, budgets, and investment analyses."
https://www.sciencedirect.com/science/article/pii/S0950584919301661,Taking the emotional pulse of software engineering — A systematic literature review of empirical studies,November 2019,Information and Software Technology,Not Found,Mary=Sánchez-Gordón: mary.sanchez-gordon@hiof.no; Ricardo=Colomo-Palacios: ricardo.colomo-palacios@hiof.no,"Abstract
Context
Over the past 50 years of 
Software Engineering
, numerous studies have acknowledged the importance of human factors. However, software developers’ emotions are still an area under investigation and debate that is gaining relevance in the software 
industry
.
Objective
In this study, a 
systematic literature review
 (SLR) was carried out to identify, evaluate, and synthesize research published concerning software developers’ emotions as well as the measures used to assess its existence.
Method
By searching five major 
bibliographic databases
, authors identified 7172 articles related to emotions in 
Software Engineering
. We selected 66 of these papers as primary studies. Then, they were analyzed in order to find empirical evidence of the intersection of emotions and software engineering.
Results
Studies report a total of 40 discrete emotions but the most frequent were: 
anger, fear, disgust, sadness, joy, love, and happiness
. There are also 2 different dimensional approaches and 10 datasets related to this topic which are publicly available on the Web. The findings also showed that self-reported 
mood
 instruments (e.g., 
SAM
, PANAS), 
physiological measures
 (e.g., heart rate, perspiration) or behavioral measures (e.g., keyboard use) are the least reported tools, although, there is a recognized intrinsic problem with the accuracy of current state of the art 
sentiment analysis
 tools. Moreover, most of the studies used software practitioners and/or datasets from industrial context as subjects.
Conclusions
The study of emotions has received a growing attention from the research community in the recent years, but the management of emotions has always been challenging in practice. Although it can be said that this field is not mature enough yet, our results provide a holistic view that will benefit researchers by providing the latest trends in this area and identifying the corresponding research gaps.",21 Mar 2025,5,The study of emotions in software engineering is interesting but may have limited direct impact on practical applications for early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584919301648,Bug report severity level prediction in open source software: A survey and research opportunities,November 2019,Information and Software Technology,Not Found,Luiz Alberto Ferreira=Gomes: luizgomes@pucpcaldas.br; Ricardo da Silva=Torres: rtorres@ic.unicamp.br; Mario Lúcio=Côrtes: cortes@ic.unicamp.br,"Abstract
Context:
 The 
severity level
 attribute of a 
bug report
 is considered one of the most critical variables for planning evolution and maintenance in Free/Libre 
Open Source Software
. This variable measures the impact the bug has on the successful execution of the software system and how soon a bug needs to be addressed by the development team. Both business and academic community have made an extensive investigation towards the proposal methods to automate the 
bug report
 severity prediction.
Objective:
 This paper aims to provide a comprehensive mapping study review of recent research efforts on automatically bug report severity prediction. To the best of our knowledge, this is the first review to categorize quantitatively more than ten aspects of the experiments reported in several papers on bug report severity prediction.
Method:
 The mapping study review was performed by searching four electronic databases. Studies published until December 2017 were considered. The initial resulting comprised of 54 papers. From this set, a total of 18 papers were selected. After performing snowballing, more nine papers were selected.
Results:
 From the mapping study, we identified 27 studies addressing bug report severity prediction on Free/Libre 
Open Source Software
. The gathered data confirm the relevance of this topic, reflects the scientific maturity of the research area, as well as, identify gaps, which can motivate new research initiatives.
Conclusion:
 The message drawn from this review is that unstructured text features along with traditional 
machine learning algorithms
 and text mining methods have been playing a central role in the most proposed methods in literature to predict bug 
severity level
. This scenario suggests that there is room for improving prediction results using state-of-the-art 
machine learning
 and text mining algorithms and techniques.",21 Mar 2025,7,Automating bug report severity prediction in Free/Libre Open Source Software can significantly benefit early-stage ventures by improving software maintenance and evolution planning.
https://www.sciencedirect.com/science/article/pii/S0950584919301478,Multi-reviewing pull-requests: An exploratory study on GitHub OSS projects,November 2019,Information and Software Technology,Not Found,Dongyang=Hu: hudongyang17@163.com; Yang=Zhang: yangzhang15@nudt.edu.cn; Junsheng=Chang: Not Found; Gang=Yin: Not Found; Yue=Yu: Not Found; Tao=Wang: Not Found,"Abstract
Context:
GitHub has enabled developers to easily contribute their review comments on multiple pull-requests and switch their review focus 
between
 different pull-requests, 
i.e.
, multi-reviewing. Reviewing multiple pull-requests simultaneously may enhance work efficiency. However, multi-reviewing also relies on developers’ rationally allocating their focus, which may bring a different influence to the resolution of pull-requests.
Objective:
 In this paper, we present an ongoing study of the impact of multi-reviewing on pull-request resolution in GitHub 
open source projects
.
Method:
 We collected and analyzed 1,836,280 pull-requests from 760 GitHub projects to explore how multi-reviewing affects the resolution of a pull-request.
Results:
 We find that multi-reviewing is a common behavior in GitHub. However, more multi-reviewing behaviors tend to bring longer pull-request resolution latency.
Conclusion:
 Multi-reviewing is a complex behavior of developers, and has an important impact on the efficiency of pull-request resolution. Our study motivates the need for more research on multi-reviewing.",21 Mar 2025,4,"The study on multi-reviewing in GitHub projects, while interesting, may not directly impact European early-stage ventures in a significant way."
https://www.sciencedirect.com/science/article/pii/S0950584919301624,Using cognitive dimensions to evaluate the usability of security APIs: An empirical investigation,November 2019,Information and Software Technology,Not Found,Chamila=Wijayarathna: z5122098@student.unsw.edu.au; Nalin Asanka Gamagedara=Arachchilage: n.arachchilage@latrobe.edu.au,"Abstract
Context
Usability issues
 of security 
Application Programming Interfaces
 (APIs) are a main factor for mistakes programmers make that could result in introducing 
security vulnerabilities
 into applications they develop. This has become a common problem as there is no methodology to evaluate the usability of security APIs. A 
usability evaluation
 methodology for security APIs would allow API developers to 
identify usability issues
 of security APIs and fix them. A Cognitive Dimensions Framework (CDF) based 
usability evaluation
 methodology has been proposed in previous research to empirically evaluate the usability of security APIs.
Objective
In this research, we evaluated the proposed CDF based methodology through four security APIs (Google 
Authentication
 API, Bouncy Castle light weight Crypto API, Java Secure Socket Extension API, OWASP Enterprise Security API).
Method
We conducted four experiments where in each experiment we recruited programmers and they completed a programming task using one of the four security APIs. Participants’ feedback on each cognitive dimension of the particular API was collected using the cognitive dimensions questionnaire. 
Usability issues
 of each API was identified based on this feedback.
Results
Results of the four experiments revealed that over 83% of the usability issues in a security API could be identified by this methodology with a considerably good validity and reliability.
Conclusion
The proposed CDF based usability evaluation methodology provides a good platform to conduct usability evaluation for security APIs.",21 Mar 2025,6,The evaluation methodology for security APIs can have practical value for early-stage ventures by improving the usability of security APIs and reducing security vulnerabilities.
https://www.sciencedirect.com/science/article/pii/S0950584919301612,Automatic recall of software lessons learned for software project managers,November 2019,Information and Software Technology,Not Found,Tamer Mohamed=Abdellatif: tmohame7@uwo.ca; Luiz Fernando=Capretz: lcapretz@uwo.ca; Danny=Ho: danny@nfa-estimation.com,"Abstract
Context
Lessons learned (LL) records constitute the software organization memory of successes and failures. LL are recorded within the organization repository for future reference to optimize planning, gain experience, and elevate market competitiveness. However, manually searching this repository is a daunting task, so it is often disregarded. This can lead to the repetition of previous mistakes or even missing potential opportunities. This, in turn, can negatively affect the organization's profitability and competitiveness.
Objective
We aim to present a novel solution that provides an automatic process to recall relevant LL and to push those LL to project managers. This will dramatically save the time and effort of manually searching the unstructured LL repositories and thus encourage the LL exploitation.
Method
We exploit existing project artifacts to build the LL search queries on-the-fly in order to bypass the tedious manual searching. An empirical 
case study
 is conducted to build the automatic LL recall solution and evaluate its effectiveness. The study employs three of the most popular information 
retrieval models
 to construct the solution. Furthermore, a real-world dataset of 212 LL records from 30 different software projects is used for validation. Top-k and MAP well-known accuracy metrics are used as well.
Results
Our case study results confirm the effectiveness of the automatic LL recall solution. Also, the results prove the success of using existing project artifacts to dynamically build the search query string. This is supported by a discerning accuracy of about 70% achieved in the case of top-k.
Conclusion
The automatic LL recall solution is valid with high accuracy. It will eliminate the effort needed to manually search the LL repository. Therefore, this will positively encourage project managers to reuse the available LL knowledge – which will avoid old pitfalls and unleash hidden business opportunities.",21 Mar 2025,8,"The automatic recall solution for lessons learned in software organizations can greatly benefit early-stage ventures by optimizing planning, gaining experience, and improving competitiveness."
https://www.sciencedirect.com/science/article/pii/S0950584919301697,The usefulness of software metric thresholds for detection of bad smells and fault prediction,November 2019,Information and Software Technology,Not Found,Mariza A.S.=Bigonha: mariza@dcc.ufmg.br; Kecia=Ferreira: kecia@decom.cefetmg.br; Priscila=Souza: priscilinhapsouza@gmail.com; Bruno=Sousa: bruno.luan.sousa@dcc.ufmg.br; Marcela=Januário: marcelajanuario92@hotmail.com; Daniele=Lima: danieleddelima@gmail.com,"Abstract
Context
Software metrics may be an effective tool to assess the 
quality of software
, but to guide their use it is important to define their thresholds. Bad smells and fault also impact the 
quality of software
. Extracting metrics from software systems is relatively low cost since there are tools widely used for this purpose, which makes feasible applying software metrics to identify bad smells and to predict faults.
Objective
To inspect whether thresholds of object-oriented metrics may be used to aid bad smells detection and fault predictions.
Method
To direct this research, we have defined three research questions (RQ), two related to identification of bad smells, and one for identifying fault in software systems. To answer these RQs, we have proposed detection strategies for the bad smells: Large Class, Long Method, Data Class, Feature Envy, and Refused Bequest, based on metrics and their thresholds. To assess the quality of the derived thresholds, we have made two studies. The first one was conducted to evaluate their efficacy on detecting these bad smells on 12 systems. A second study was conducted to investigate for each of the class level software metrics: 
DIT
, LCOM, NOF, NOM, NORM, NSC, NSF, NSM, SIX, and WMC, if the ranges of values determined by thresholds are useful to identify fault in software systems.
Results
Both studies confirm that metric thresholds may support the prediction of faults in software and are significantly and effective in the detection of bad smells.
Conclusion
The results of this work suggest practical applications of metric thresholds to identify bad smells and predict faults and hence, support 
software quality assurance
 activities.Their use may help developers to focus their efforts on classes that tend to fail, thereby minimizing the occurrence of future problems.",21 Mar 2025,8,"This abstract presents a practical application of software metrics to identify bad smells and predict faults, which can significantly impact the quality of software in early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301259,Scaling-up domain-specific modelling languages through modularity services,November 2019,Information and Software Technology,Not Found,Antonio=Garmendia: antonio.garmendia@uam.es; Esther=Guerra: Not Found; Juan=de Lara: Not Found; Antonio=García-Domínguez: Not Found; Dimitris=Kolovos: Not Found,"Abstract
Context
Model-driven engineering (MDE) promotes the active use of models in all phases of software development. Even though models are at a high level of abstraction, large or complex systems still require building monolithic models that prove to be too big for their processing by existing tools, and too difficult to comprehend by users. While modularization techniques are well-known in programming languages, they are not the norm in MDE.
Objective
Our goal is to ease the modularization of models to allow their efficient processing by tools and facilitate their management by users.
Method
We propose five patterns that can be used to extend a 
modelling language
 with services related to modularization and scalability. Specifically, the patterns allow defining model fragmentation strategies, scoping and visibility rules, model indexing services, and scoped constraints. Once the patterns have been applied to the meta-model of a 
modelling language
, we synthesize a customized modelling environment enriched with the defined services, which become applicable to both existing monolithic legacy models and new models.
Results
Our proposal is supported by a tool called EMF-Splitter, combined with the Hawk model indexer. Our experiments show that this tool improves the validation performance of large models. Moreover, the analysis of 224 meta-models from 
OMG
 standards, and a public repository with more than 300 meta-models, demonstrates the applicability of our patterns in practice.
Conclusions
Modularity mechanisms typically employed in programming IDEs can be successfully transferred to MDE, leading to more scalable and structured domain-specific modelling languages and environments.",21 Mar 2025,7,The modularization techniques proposed in this abstract can be beneficial for early-stage ventures in managing complex systems and improving scalability in software development.
https://www.sciencedirect.com/science/article/pii/S095058491930134X,Test case selection-prioritization approach based on memoization dynamic programming algorithm,November 2019,Information and Software Technology,Not Found,Ovidiu=Banias: ovidiu.banias@aut.upt.ro,"Abstract
Context
In the software industry, selection and prioritization techniques become a necessity in the regression and validation testing phases because a lot of test cases are available for reuse, yet time and project specific constraints must be respected.
Objective
In this paper we propose a 
dynamic programming
 approach in solving test case selection-prioritization problems. We focus on low memory consumption in pseudo-polynomial time complexity applicable in both selection and selection-prioritization problems over sets of test cases or test suites. In 
dynamic programming
 optimization solutions, huge amounts of memory are required and unfortunately the memory is limited. Therefore, lower memory consumption leads to a higher number of test cases to be involved in the selection process.
Method
Our approach is suited for medium to large projects where the required memory space is not higher than the order of tens of GBytes. We employed both objective methods as the 
dynamic programming algorithm
 and subjective and empiric human decision as defining the prioritization criteria. Furthermore, we propose a method of employing multiple project specific criteria in evaluating the importance of a test case in the project context.
Results
To evaluate the proposed solution relative to the classical dynamic programming 
knapsack
 solution, we developed a suite of 
comparative case studies
 based on 1000 generated scenarios as close as possible to real project scenarios. The results of the comparative study reported the proposed algorithm requires up to 400 times less memory in the best-case scenarios and about 40 times less memory in average.
Conclusion
The solution delivers optimal results in pseudo-polynomial time complexity, is effective for amounts of test cases up to the order of millions and compared with the classical 
dynamic programming methods
 leads to higher number of test cases to be involved in the selection process due to reduced memory consumption.",21 Mar 2025,9,The dynamic programming approach for test case selection and prioritization with significantly reduced memory consumption can greatly benefit early-stage ventures in improving testing efficiency.
https://www.sciencedirect.com/science/article/pii/S0950584919301363,Simsax: A measure of project similarity based on symbolic approximation method and software defect inflow,November 2019,Information and Software Technology,Not Found,Mirosław=Ochodek: mochodek@cs.put.poznan.pl; Miroslaw=Staron: miroslaw.staron@cse.gu.se; Wilhelm=Meding: wilhelm.meding@ericsson.com,"Abstract
Background
Profiling software development projects, in order to compare them, find similar sub-projects or sets of activities, helps to monitor changes in software processes. Since we lack 
objective measures
 for profiling or hashing, researchers often fall back on manual assessments.
Objective
The goal of our study is to define an objective and intuitive measure of similarity between software development projects based on software defect-inflow profiles.
Method
We defined a measure of project similarity called 
SimSAX
 which is based on segmentation of defect-inflow profiles, coding them into strings (sequences of symbols) and comparing these strings to find so-called motifs. We use simulations to find and calibrate the parameters of the measure. The objects in the simulations are two different large industry projects for which we know the similarity a priori, based on the input from industry experts. Finally, we apply the measure to find similarities between five industrial and six 
open source projects
.
Results
Our results show that the measure provides the most accurate simulated results when the compared motifs are long (32 or more weeks) and we use an alphabet of 5 or more symbols. The measure provides the possibility to calibrate for each industrial case, thus allowing to optimize the method for finding specific patterns in project similarity.
Conclusions
We conclude that our proposed measure provides a 
good approximation
 for project similarity. The industrial evaluation showed that it can provide a 
good starting point
 for finding similar periods in software development projects.",21 Mar 2025,6,The objective measure of similarity between software projects presented in this abstract may have some value for early-stage ventures in profiling software development projects.
https://www.sciencedirect.com/science/article/pii/S0950584919301387,An empirical study of sentiments in code reviews,October 2019,Information and Software Technology,Not Found,Ikram El=Asri: ikram.asri@um5s.net.ma; Noureddine=Kerzazi: Not Found; Gias=Uddin: Not Found; Foutse=Khomh: Not Found; M.A.=Janati Idrissi: Not Found,"Abstract
Context
Modern code reviews are supported by tools to enhance developers’ interactions allowing contributors to submit their opinions for each committed change in form of comments. Although the comments are aimed at discussing potential technical issues, the text might enclose harmful sentiments that could erode the benefits of suggested changes.
Objective
In this paper, we study empirically the impact of sentiment embodied within developers’ comments on the time and outcome of the 
code review process
.
Method
Based on historical data of four long-lived Open Source Software (OSS) projects from a code review system we investigate whether perceived sentiments have any impact on the interval time of code changes acceptance.
Results
We found that (1) contributors frequently express positive and negative sentiments during code review activities; (2) the expressed sentiments differ among the contributors depending on their position within the social network of the reviewers (
e.g.,
 core vs peripheral contributors); (3) the sentiments expressed by contributors tend to be neutral as they progress from the status of newcomer in an OSS project to the status of core team contributors; (4) the reviews with negative comments on average took more time to complete than the reviews with positive/neutral comments, and (5) the reviews with controversial comments took significantly longer time in one project.
Conclusion
Through this work, we provide evidences that text-based sentiments have an impact on the duration of the 
code review process
 as well as the acceptance or rejection of the suggested changes.",21 Mar 2025,5,"While the impact of sentiments in code review process is interesting, its practical value for early-stage ventures may not be as significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919301430,A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing,October 2019,Information and Software Technology,Not Found,Juliana Marino=Balera: juliana.balera@inpe.br; Valdivino Alexandre de=Santiago Júnior: valdivino.santiago@inpe.br,"Abstract
Context
Search-based Software Testing
 (SBST) is a research field where testing a software product is formulated as an 
optimization problem
. It is an active sub-area of 
Search-based 
Software Engineering
 (SBSE) where many studies have been published and some reviews have been carried out. The majority of studies in SBST has been adopted meta-heuristics while hyper-heuristics have a long way to go. Moreover, there is still a lack of studies to perceive the state-of-the-art of the use of hyper-heuristics within SBST.
Objective
The objective of this work is to investigate the adoption of hyper-heuristics for Software Testing highlighting the current efforts and identifying new research directions.
Method
A 
Systematic mapping
 study was carried out with 5 research questions considering papers published up to may/2019, and 4 different bases. The research questions aims to find out, among other things, what are the hyper-heuristics used in the context of Software Testing, for what problems hyper-heuristics have been applied, and what are the objective functions in the scope of Software Testing.
Results
A total of 734 studies were found via the search strings and 164 articles were related to Software Testing. However, from these, only 26 papers were actually in accordance with the scope of this research and 3 more papers were considered due to snowballing or expert’s suggestion, totalizing 29 selected papers. Few different problems and application domains where hyper-heuristics have been considered were identified.
Conclusion
Differently from other communities (Operational Research, Artificial Intelligence), SBST has little explored the benefits of hyper-heuristics which include generalization and less difficulty in parameterization. Hence, it is important to further investigate this area in order to alleviate the effort of practitioners to use such an approach in their testing activities.",21 Mar 2025,6,"The investigation of hyper-heuristics for software testing could potentially impact the efficiency and effectiveness of testing activities for European startups, although the current adoption rate is low."
https://www.sciencedirect.com/science/article/pii/S0950584919301399,Employment of multiple algorithms for optimal path-based test selection strategy,October 2019,Information and Software Technology,Not Found,Miroslav=Bures: miroslav.bures@fel.cvut.cz; Bestoun S.=Ahmed: bestoun@kau.se,"Abstract
Context
Executing various sequences of 
system functions
 in a system under test represents one of the primary techniques in software testing. The natural method for creating effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate tests that satisfy a defined test coverage criterion. Several criteria for preferred test set properties can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined.
Objective
Using this prioritization, the test cases exercise the high-priority parts of the system under test by more path combinations than those with low priority (this prioritization can be combined with the test coverage criterion that determines how many path combinations of the individual parts of the system are tested). Evidence from the literature and our observations confirm that finding a universal algorithm that produces a test set with preferred properties for all test coverage criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide results with the best value of a preferred property. In this paper, we present a portfolio-based strategy to perform the best test selection.
Method
The proposed strategy first employs a set of current algorithms to generate test sets; then, a preferred property of each test set is assessed in terms of the selected criterion, and finally, the test set with the best value of a preferred property is chosen.
Results
The experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided results having the best preferred property value; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set preferred property criteria.
Conclusion
In addition to the used algorithms, the proposed strategy can be used to assess the 
optimality
 of different path-based testing algorithms and choose a 
suitable algorithm
 for the testing.",21 Mar 2025,4,"The strategy for test selection could provide some insights for startups, but the universal algorithm challenge limits its immediate practical value for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301375,Model driven transformation development (MDTD): An approach for developing model to model transformation,October 2019,Information and Software Technology,Not Found,Ana Patrícia Fontes=Magalhaes: apmagalhaes@uneb.br; Aline Maria Santos=Andrade: Not Found; Rita Suzana Pitangueira=Maciel: Not Found,"Abstract
Context
In the Model Driven Development (MDD) approach, model transformations are responsible for the semi-automation of software development process converting models between different abstraction levels. The development of model transformations involves a complexity inherent to the transformation domain, in addition to the complexity of software development in general. Therefore, the construction of model transformations requires software engineering feature such as processes and languages to facilitate its development and maintenance.
Objective
This paper presents a framework to develop unidirectional relational model transformation using the MDD approach itself, which integrates: (i) a software development process suitable for the model transformation domain (ii) a Domain specific language for transformation modeling (iii) a transformation chain, to (semi) automate the proposed process, and (iv) a development environment to support it.
Methods
The proposal systematizes the development of model transformation, following the MDD principles. An iterative and incremental process guides transformation development from requirement specification to transformation codification. The proposal has been evaluated through a 
case study
 and a controlled experiment.
Results
The framework enables model transformation specification at a high abstraction level and (semi) automatically transforms it into models at a low abstraction level until the transformation code. The results of the case study showed that people with different levels of knowledge of MDD, or without experience in transformation languages, were able to develop transformations through the framework and generated executable code.
Conclusions
The framework integrates the essential elements involved in the development of model transformation and enables the abstraction of technological details. The results of the case study and controlled experiment showed the feasibility of the proposal and its use in dealing with the complexity involved in model transformation development.",21 Mar 2025,7,"The framework for model transformation development using MDD principles could offer valuable automation and abstraction benefits to European startups, enhancing their software development processes."
https://www.sciencedirect.com/science/article/pii/S0950584919301417,Startup ecosystem effect on minimum viable product development in software startups,October 2019,Information and Software Technology,Not Found,Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Markku=Oivo: Not Found; Kari=Liukkunen: Not Found; Jouni=Markkula: Not Found,"Abstract
Context
Software startups develop innovative products through which they scale their business rapidly, and thus, provide value to the economy, including job generation. However, most startups fail within two years of their launch because of a poor problem-solution fit and negligence of the learning process during 
minimum viable product
 (MVP) development. An ideal startup ecosystem can assist in MVP development by providing the necessary entrepreneurial education and technical skills to founding team members for identifying problem-solution fit for their product idea, allowing them to find the right product-market fit. However, existing knowledge on the effect of the startup ecosystem elements on the MVP development is limited.
Objective
The empirical study presented in this article aims to identify the effect of the six ecosystem elements (entrepreneurs, technology, market, support factors, finance, and human capital) on MVP development.
Method
We conducted a study with 13 software startups and five supporting organizations (accelerators, 
incubator
, co-working space, and investment firm) in the startup ecosystem of the city of Oulu in Finland. Data were collected through semi-structured interviews, observation, and materials.
Results
The study results showed that 
internal sources
 are most common for identifying requirements for the product idea for MVP development. The findings indicate that supporting factors, such as incubators and accelerators, can influence MVP development by providing young founders with the necessary entrepreneurship skills and education needed to create the right product-market fit.
Conclusions
We conclude from this study of a regional startup ecosystem that the 
MVP development process
 is most affected by founding team members’ experiences and skill sets and by advanced technologies. Furthermore, a constructive startup ecosystem around software startups can boost up the creation of an effective MVP to test product ideas and find a product-market fit.",21 Mar 2025,8,"The study on the effect of ecosystem elements on MVP development provides actionable insights for European startups, potentially improving their chances of success and growth."
https://www.sciencedirect.com/science/article/pii/S0950584919301405,On the use of virtual reality in software visualization: The case of the city metaphor,October 2019,Information and Software Technology,Not Found,Simone=Romano: simone.romano@uniba.it; Nicola=Capece: nicola.capece@unibas.it; Ugo=Erra: ugo.erra@unibas.it; Giuseppe=Scanniello: giuseppe.scanniello@unibas.it; Michele=Lanza: michele.lanza@usi.ch,"Abstract
Background:
 Researchers have been exploring 3D representations for visualizing software. Among these representations, one of the most popular is the 
city metaphor
, which represents a target object-oriented system as a virtual city. Recently, this metaphor has been also implemented in interactive software visualization tools that use virtual reality in an immersive 3D environment medium.
Aims:
 We assessed the city metaphor displayed on a standard 
computer screen
 and in an 
immersive virtual reality
 with respect to the support provided in the comprehension of Java software systems.
Method:
 We conducted a controlled experiment where we asked the participants to fulfill 
program comprehension
 tasks with the support of 
(i)
 an 
integrated development environment
 (Eclipse) with a plugin for gathering 
code metrics
 and identifying bad smells; and 
(ii)
 a visualization tool of the city metaphor displayed on a standard 
computer screen
 and in an 
immersive virtual reality
.
Results:
 The use of the city metaphor displayed on a standard computer screen and in an immersive virtual reality significantly improved the correctness of the solutions to 
program comprehension
 tasks with respect to Eclipse. Moreover, when carrying out these tasks, the participants using the city metaphor displayed in an immersive virtual reality were significantly faster than those visualizing with the city metaphor on a standard computer screen.
Conclusions:
 Virtual reality is a viable means for software visualization.",21 Mar 2025,5,"The assessment of the city metaphor in software visualization, particularly in virtual reality, could have some relevance for startups, but its direct impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919301351,A novel approach for automatic remodularization of software systems using extended ant colony optimization algorithm,October 2019,Information and Software Technology,Not Found,Bright Gee=Varghese R: brightachus@karunya.edu; Kumudha=Raimond: kraimond@karunya.edu; Jeno=Lovesum: jenolovesum@karunya.edu,"Abstract
Context
Software modularization is extremely important to streamline the inner structure of the program modules without influencing its 
core functionality
. As the framework advances during the upkeep stage, the pristine design of the software package gets disintegrated and hence it is arduous to understand and maintain. There are many existing approaches being carried out to automatically remodularize using optimization techniques to ease the maintenance and improve the quality of the system. The outcomes are rather insufficiently optimal and depend on problem-specific operators, which in turn expands the time multifaceted nature to land at an answer. Apart from these limitations, the issues, such as time complexity, scalability and performance need to be addressed.
Objective
In this paper, an efficient automatic software remodularization using extended 
Ant Colony Optimization
 (ACO) has been proposed to remodularize the software systems.
Method
The proposed approach mainly includes two phases: optimised traversal of software system using ACO for finding the order of software files to be processed and remodularization of software system using the proposed approach of extended ACO.
Results
We experimented our proposed approach on seven software systems. The performance is evaluated by using Turbo modularization quality (MQ) which supports 
Module dependency
 graph (MDG) that have edge weights. The time complexity of remodularized software system is evaluated based on number of Turbo MQ.
Conclusion
It can be concluded that when the performance has been compared with the subsisting methodologies, for example, 
Genetic algorithm
 (GA), Hill climbing (HC) and Interactive genetic algorithms (I-GAs), the proposed approach has higher Turbo MQ value with lesser time complexity in the evaluated software systems.",21 Mar 2025,8,"The proposal of an efficient automatic software remodularization using extended Ant Colony Optimization addresses important issues in software maintenance and quality improvement, which can have significant practical value for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301429,Log mining to re-construct system behavior: An exploratory study on a large telescope system,October 2019,Information and Software Technology,Not Found,Michele=Pettinato: Not Found; Juan Pablo=Gil: juan.gil@alma.cl; Patricio=Galeas: patricio.galeas@ufrontera.cl; Barbara=Russo: barbara.russo@unibz.it,"Abstract
Context
A large amount of information about 
system behavior
 is stored in logs that record system changes. Such information can be exploited to discover anomalies of a system and the operations that cause them. Given their large size, manual inspection of logs is hard and infeasible in a desired timeframe (e.g., real-time), especially for 
critical systems
.
Objective
This study proposes a semi-automated method for reconstructing sequences of tasks of a system, revealing system anomalies, and associating tasks and anomalies to code components.
Method
The proposed approach uses unsupervised 
machine learning
 (Latent Dirichlet Allocation) to discover latent topics in messages of log events and introduces a novel technique based on pattern recognition to derive the semantic of such topics (topic labelling). The approach has been applied to the 
big data
 generated by the ALMA telescope system consisting of more than 2000 log events collected in about five hours of telescope operation.
Results
With the application of our approach to such data, we were able to model the behavior of the telescope over 16 different observations. We found five different behavior models and three different types of errors. We use the models to interpret each error and discuss its cause.
Conclusions
With this work, we have also been able to discuss some of the known challenges in log mining. The experience we gather has been then summarized in lessons learned.",21 Mar 2025,7,The semi-automated method for reconstructing sequences of tasks of a system and discovering system anomalies using machine learning can provide valuable insights for European startups dealing with critical systems and real-time data analysis.
https://www.sciencedirect.com/science/article/pii/S0950584919301454,Scheduling sequence selection for generating test data to cover paths of MPI programs,October 2019,Information and Software Technology,Not Found,Baicai=Sun: Not Found; Jinxin=Wang: Not Found; Dunwei=Gong: dwgong@vip.163.com; Tian=Tian: Not Found,"Abstract
Context: As one of key tasks in software testing, test data generation has been receiving widespread attention in recent years. Message-passing Interface (MPI) programs, which are one representative type of parallel programs, have the characteristic of non-determinism, which is reflected by the non-deterministic execution under different scheduling sequences against the same program input. Previous studies have shown that different difficulties are raised in generating test data under different scheduling sequences, suggesting that selecting appropriate scheduling sequences is beneficial to a high efficiency.
Objective: We propose a method of selecting a superior and feasible scheduling sequence for generating test data in the criterion of path coverage against each target path of an MPI program.
Method: In the proposed method, a number of program inputs are first sampled by Latin 
hypercube
 sampling. Then, the program is executed against each sample under each scheduling sequence, and all the scheduling sequences are sorted according to the similarities between the paths traversed by these samples and the target one. Finally, the feasibility of a scheduling sequence with the best quality is investigated based on the symbolic execution.
Results: We apply the proposed method to seven typical MPI programs and compare it with the random one. The experimental results show that test data covering the target path can be generated under the selected scheduling sequence with high success rate and low time consumption.
Conclusion: The proposed method takes the influence of scheduling sequences on generating test data into consideration, thus providing a competent way to test parallel programs.",21 Mar 2025,6,"The proposed method for selecting superior scheduling sequences for test data generation in parallel programs addresses a specific technical challenge, which may have moderate practical impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919301466,Improving defect prediction with deep forest,October 2019,Information and Software Technology,Not Found,Tianchi=Zhou: Not Found; Xiaobing=Sun: xbsun@yzu.edu.cn; Xin=Xia: Not Found; Bin=Li: Not Found; Xiang=Chen: Not Found,"Abstract
Context
Software defect
 prediction is important to ensure the 
quality of software
. Nowadays, many supervised learning techniques have been applied to identify defective instances (
e.g.
, methods, classes, and modules).
Objective
However, the performance of these supervised learning techniques are still far from satisfactory, and it will be important to design more advanced techniques to improve the performance of 
defect prediction
 models.
Method
We propose a new deep forest model to build the defect prediction model (
DPDF
). This model can identify more important defect features by using a new cascade strategy, which transforms 
random forest classifiers
 into a layer-by-layer structure. This design takes full advantage of 
ensemble learning
 and 
deep learning
.
Results
We evaluate our approach on 25 
open source projects
 from four public datasets (
i.e.
, NASA, PROMISE, AEEEM and Relink). Experimental results show that our approach increases AUC value by 5% compared with the best traditional 
machine learning algorithms
.
Conclusion
The deep strategy in 
DPDF
 is effective for software defect prediction.",21 Mar 2025,9,"The new deep forest model for software defect prediction shows promising results in improving the performance of defect prediction models, which can greatly benefit European startups in ensuring software quality and reliability."
https://www.sciencedirect.com/science/article/pii/S0950584917302793,DevOps in practice: A multiple case study of five companies,October 2019,Information and Software Technology,Not Found,Lucy Ellen=Lwakatare: lucylwakatare@yahoo.com; Terhi=Kilamo: Not Found; Teemu=Karvonen: Not Found; Tanja=Sauvola: Not Found; Ville=Heikkilä: Not Found; Juha=Itkonen: Not Found; Pasi=Kuvaja: Not Found; Tommi=Mikkonen: Not Found; Markku=Oivo: Not Found; Casper=Lassenius: Not Found,"Abstract
Context:
 
DevOps
 is considered important in the ability to frequently and reliably update a system in operational state. 
DevOps
 presumes cross-functional collaboration and automation between software development and operations. DevOps adoption and implementation in companies is non-trivial due to required changes in technical, organisational and cultural aspects.
Objectives:
 This 
exploratory study
 presents detailed descriptions of how DevOps is implemented in practice. The context of our empirical investigation is web application and service development in 
small and medium sized companies
.
Method:
 A multiple-case study was conducted in five different development contexts with successful DevOps implementations since its benefits, such as quick releases and minimum deployment errors, were achieved. Data was mainly collected through interviews with 26 practitioners and observations made at the companies. Data was analysed by first coding each case individually using a set of predefined themes and thereafter perform a cross-case synthesis.
Results:
 Our analysis yielded some of the following results: (i) software development team attaining ownership and responsibility to deploy software changes in production is crucial in DevOps. (ii) toolchain usage and support in deployment pipeline activities accelerates the delivery of software changes, bug fixes and handling of production incidents. (ii) the delivery speed to production is affected by context factors, such as manual approvals by the product owner (iii) steep 
learning curve
 for new skills is experienced by both software developers and operations staff, who also have to cope with working under pressure.
Conclusion:
 Our findings contributes to the overall understanding of DevOps concept, practices and its perceived impacts, particularly in 
small and medium sized companies
. We discuss two practical implications of the results.",21 Mar 2025,5,"The detailed descriptions of DevOps implementation in small and medium sized companies provide insights into practical challenges and benefits, but may have limited immediate impact on European early-stage ventures in other industries."
https://www.sciencedirect.com/science/article/pii/S0950584919301442,A conceptual perspective on interoperability in context-aware software systems,October 2019,Information and Software Technology,Not Found,Rebeca C.=Motta: rmotta@cos.ufrj.br; Káthia M.=de Oliveira: Not Found; Guilherme H.=Travassos: Not Found,"Abstract
Context
Context-aware software systems can interact with different devices to complete their tasks and act according to the context, regardless of their development and organizational differences. Interoperability is a big challenge in the engineering of such systems.
Objective
To discuss how interoperability has been addressed in context-aware software systems, strengthening the scientific basis for its understanding and conceptualization.
Method
A 
quasi
-systematic literature review was undertaken to observe interoperability in such context-aware software systems to support the discussions. Its dataset includes 17 from 408 papers identified in the technical literature. The extracted information was qualitatively analyzed by following the principles of Grounded Theory.
Results
The analysis allowed to identify ten interoperability concepts, organized into a Theoretical Framework according to structural and behavioral perspectives, which deals with interoperability as the ability of things (an object, a place, an application or anything that can engage interaction with a system) to interact for a particular purpose, once their differences (development platforms, 
data formats
, culture, legal issues) have been overcome. Once the interoperability is established from structural concepts (context, perspective, purpose, the level of provided support and system attributes), it can be measured, improved and observed from the behavioral concepts (evaluation method, challenges, issues, and benefits).
Conclusions
The Interoperability Theoretical Framework provides relevant information to organize the knowledge related to interoperability, considering context, and can be used to guide the evolution of software systems regarding changes focused on interoperability.",21 Mar 2025,5,"The abstract discusses the challenges of interoperability in context-aware software systems, providing a theoretical framework for understanding and improving it. While relevant for software systems, the impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584918301216,"A large-scale, in-depth analysis of developers’ personalities in the Apache ecosystem",October 2019,Information and Software Technology,Not Found,Fabio=Calefato: fabio.calefato@uniba.it; Filippo=Lanubile: filippo.lanubile@uniba.it; Bogdan=Vasilescu: vasilescu@cmu.edu,"Abstract
Context
Large-scale distributed projects are typically the results of collective efforts performed by multiple developers with heterogeneous personalities.
Objective
We aim to find evidence that personalities can explain developers’ behavior in large scale-distributed projects. For example, the propensity to trust others — a critical factor for the success of global 
software engineering
 — has been found to influence positively the result of code reviews in distributed projects.
Method
In this paper, we perform a quantitative analysis of ecosystem-level data from the code commits and email messages contributed by the developers working on the Apache Software Foundation (ASF) projects, as representative of large scale-distributed projects.
Results
We find that there are three common types of personality profiles among Apache developers, characterized in particular by their level of Agreeableness and Neuroticism. We also confirm that developers’ personality is stable over time. Moreover, personality traits do not vary with their role, membership, and extent of contribution to the projects. We also find evidence that more open developers are more likely to make contributors to Apache projects.
Conclusion
Overall, our findings reinforce the need for future studies on human factors in 
software engineering
 to use psychometric tools to control for differences in developers’ personalities.",21 Mar 2025,7,"The abstract explores the impact of developers' personalities on large-scale distributed projects, emphasizing the need for future studies on human factors in software engineering. Understanding developers' behaviors can be valuable for early-stage ventures in team collaboration and project success."
https://www.sciencedirect.com/science/article/pii/S0950584918301228,Resilience of distributed student teams to stress factors: A longitudinal case-study,October 2019,Information and Software Technology,Not Found,Igor=Čavrak: igor.cavrak@fer.hr; Ivana=Bosnić: Not Found; Federico=Ciccozzi: Not Found; Raffaela=Mirandola: Not Found,"Abstract
Context:
 Teaching global 
software engineering
 is continuously evolving and improving to prepare future software engineers adequately. Geographically distributed work in project-oriented software development courses is both demanding and rewarding for student teams, who are susceptible to various risks stemming from different internal and external factors, being the sources of stress and impacting team performance.
Objective:
 In this paper, we analyze the resilience of teams of students working in a geographically fully distributed setting. Resilience is analyzed in relation to two representative stress factors: non-contributing team members and changes to customer project requirements. We also reason on team collaboration patterns and analyze potential dependencies among these collaboration patterns, team resilience and stress factors.
Method:
 We conduct a longitudinal case-study over five years on our Distributed Software Development (DSD) course. Based on empirical data, we study team resilience to two stress factors by observing their impact on process and product 
quality aspects
 of team performance. The same performance aspects are studied for identified collaboration patterns, and bidirectional influence between patterns and resilience is investigated.
Results:
 Teams with up to two non-contributing members experience 
graceful degradation
 of performance indicators. A large number of non-contributing students almost guarantees the occurrence of educationally undesirable collaboration patterns. Exposed to requirements change stress, less resilient teams tend to focus on delivering the functional product rather than retaining a proper 
development process
.
Conclusions:
 Practical recommendations to be applied in contexts similar to our case have been provided at the end of the study. They include suggestions to mitigate the sources of stress, for example, by careful planning the team organization and balancing the number of regular and exchange students, or by discussing the issue of changing requirements with the external customers before the start of the project.",21 Mar 2025,8,"The abstract analyzes team resilience and stress factors in geographically distributed software development courses, providing practical recommendations for team organization and stress mitigation. Insights into team dynamics and collaboration patterns can be highly valuable for early-stage ventures in optimizing team performance."
https://www.sciencedirect.com/science/article/pii/S0950584918301721,Pareto efficient multi-objective black-box test case selection for simulation-based testing,October 2019,Information and Software Technology,Not Found,Aitor=Arrieta: aarrieta@mondragon.edu; Shuai=Wang: shuai.wang@testify.no; Urtzi=Markiegi: umarkiegi@mondragon.edu; Ainhoa=Arruabarrena: eibek03@mondragon.edu; Leire=Etxeberria: letxeberria@mondragon.edu; Goiuria=Sagardui: gsagardui@mondragon.edu,"Abstract
Context:
 In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the 
historical data
 related to failures might not be available.
Objective:
 The objective of the approach presented in this paper is to cost-effectively select test cases without making use of white-box coverage information or 
historical data
 related to fault detection.
Method:
 We propose a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total six effectiveness measures and one cost measure followed by deriving in total 21 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed six effectiveness metrics are specific to simulation models and are based on anti-patterns and similarity measures.
Results:
 We empirically evaluated our approach with these 21 combinations using six 
case studies
 by employing mutation testing to assess the fault revealing capability. We compared our approach with Random Search (RS), two many-objective algorithm, as well as three white-box metrics. The results demonstrated that our approach managed to improve Random Search by up to around 28% in terms of the Hypervolume quality indicator. Similarly, black-box metrics-based test case selection also significantly outperformed those of white-box metrics.
Conclusion:
 We demonstrate that test case selection is a non-trivial problem in the context of simulation models. We also show that the proposed effectiveness metrics performed significantly better than traditional white-box metrics. Thus, we show that black-box test selection approaches are appropriate to solve the test case selection problem within simulation models.",21 Mar 2025,7,"The abstract presents a cost-effective approach for test case selection in simulation models, demonstrating significant improvements over traditional white-box metrics. The approach's benefits in optimizing testing processes can be valuable for early-stage ventures in ensuring software quality and efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584918301630,Standing on the shoulders of giants: Seeding search-based multi-objective optimization with prior knowledge for software service composition,October 2019,Information and Software Technology,Not Found,Tao=Chen: txc919@gmail.com; Miqing=Li: m.li.8@cs.bham.ac.uk,"Abstract
Context
Search-Based 
Software Engineering
, in particular multi-objective 
evolutionary algorithm
, is a promising approach to engineering software service composition while simultaneously optimizing multiple conflicting Quality-of-Service (QoS) objectives. Yet, existing applications of 
evolutionary algorithms
 have failed to consider domain knowledge about the problem into the optimization, which is a perhaps obvious but challenging task.
Objective
This paper aims to investigate different strategies of exploring and injecting knowledge about the problem into the Multi-Objective Evolutionary Algorithm (MOEA) by 
seeding
. Further, we investigate various factors that contribute to the effectiveness of seeding, including the number of seeds, the importance of crossover operation and the similarity of historical problems.
Method
We conduced empirical evaluations with NSGA-II, MOEA/D and IBEA based on a wide spectrum of problem instances, including 10 different workflow structures, from 5 to 100 abstract services and 510 to 5.529  × 10
203
 candidate concrete services with diverse QoS on latency, throughput and cost, which was chosen from the real-world WS-DREAM dataset that contains 4500 QoS values.
Results
We found that, (i) all seeding strategies generally outperform their non-seeded counterparts under the same search budget with large statistical significance. Yet, they may involve relatively smaller compromise on one or two of the 
quality aspects
 among convergence, uniformity and spread. (ii) The implication of the number of seeds on the service 
composition problems
 is minimal in general (except for IBEA). (iii) In contrast to the non-seeded counterparts, the seeding strategies suffer much less implications by the crossover operation. (iv) The differences of historical problems, which are important for two proposed seeding strategies, can indeed affect the results in a non-linear manner; however, the results are still greatly better than the non-seeded counterparts even with up to 90% difference of the problem settings.
Conclusion
The paper concludes that (i) When applying the seeding strategies, the number of seeds to be placed in is less important in general, except for the pre-optimization based strategies under IBEA. (ii) Eliminating or having less crossover is harmful for multi-objective service composition optimization, but the seeding strategies are much less sensitive to this operator than their non-seeded counterparts. (iii) For the history based seeding strategies, the seeds do not have to come from the most similar historical composition problem to achieve the best HV value, but a largely different historical problem should usually be avoided, unless they are the only available seeds.",21 Mar 2025,6,"The abstract investigates strategies for injecting domain knowledge into Multi-Objective Evolutionary Algorithms for software service composition optimization. While the findings are valuable for software engineering, the direct practical impact on early-stage ventures may be somewhat limited."
https://www.sciencedirect.com/science/article/pii/S0950584919300990,A systematic literature review of test breakage prevention and repair techniques,September 2019,Information and Software Technology,Not Found,Javaria=Imtiaz: javaria.imtiaz@questlab.pk; Salman=Sherin: salman.sherin@questlab.pk; Muhammad Uzair=Khan: uzair.khan@questlab.pk; Muhammad Zohaib=Iqbal: zohaib.iqbal@questlab.pk,"Abstract
Context
When an application evolves, some of the developed test cases break. Discarding broken test cases causes a significant waste of effort and leads to test suites that are less effective and have lower coverage. Test repair approaches evolve test suites along with applications by repairing the broken test cases.
Objective
Numerous studies are published on test repair approaches every year. It is important to summarise and consolidate the existing knowledge in the area to provide directions to researchers and practitioners. This research work provides a systematic literature review in the area of test case repair and breakage prevention, aiming to guide researchers and practitioners in the field of software testing.
Method
We followed the standard protocol for conducting a systematic literature review. First, research goals were defined using the Goal Question Metric (GQM). Then we formulate research questions corresponding to each goal. Finally, metrics are extracted from the included papers. Based on the defined selection criteria a final set of 41 primary studies are included for analysis.
Results
The selection process resulted in 5 journal papers, and 36 
conference papers
. We present a taxonomy that lists the causes of test case breakages extracted from the literature. We found that only four proposed test repair tools are publicly available. Most studies evaluated their approaches on open-source 
case studies
.
Conclusion
There is significant room for future research on test repair techniques. Despite the positive trend of evaluating approaches on large scale open source studies, there is a clear lack of results from studies done in a real industrial context. Few tools are publicly available which lowers the potential of adaption by industry practitioners.",21 Mar 2025,5,"While the research on test repair approaches is valuable, the lack of real industrial context results and limited availability of tools somewhat limit the practical impact on European early-stage ventures at this point."
https://www.sciencedirect.com/science/article/pii/S0950584919301004,Interactive semi-automated specification mining for debugging: An experience report,September 2019,Information and Software Technology,Not Found,Mohammad Jafar=Mashhadi: mohammadjafar.mashha@ucalgary.ca; Taha R.=Siddiqui: trsiddiqui1989@gmail.com; Hadi=Hemmati: hadi.hemmati@ucalgary.ca; Howard=Loewen: hloewen@micropilot.com,"Abstract
Context
Specification mining techniques are typically used to extract the specification of a software in the absence of (up-to-date) specification documents. This is useful for 
program comprehension
, testing, and 
anomaly detection
. However, specification mining can also potentially be used for debugging, where a faulty behavior is abstracted to give developers a context about the bug and help them locating it.
Objective
In this project, we investigate this idea in an industrial setting. We propose a very basic semi-automated specification mining approach for debugging and apply that on real reported issues from an AutoPilot software system from our industry partner, MicroPilot Inc. The objective is to assess the feasibility and usefulness of the approach in a real-world setting.
Method
The approach is developed as a prototype tool, working on C code, which accept a set of relevant state fields and functions, per issue, and generates an extended 
finite state machine
 that represents the faulty behavior, abstracted with respect to the relevant context (the selected fields and functions).
Results
We qualitatively evaluate the approach by a set of interviews (including observational studies) with the company’s developers on their real-world reported bugs. The results show that (a) our approach is feasible, (b) it can be automated to some extent, and (c) brings advantages over only using their code-level debugging tools. We also compared this approach with traditional fully automated state-merging algorithms and reported several issues when applying those techniques on a real-world debugging context.
Conclusion
The main conclusion of this study is that the idea of an “interactive” specification mining rather than a fully automated mining tool is NOT impractical and indeed is useful for the debugging use case.",21 Mar 2025,7,The research on semi-automated specification mining for debugging in an industrial setting provides a tangible and potentially useful tool for European early-stage ventures to improve software development processes.
https://www.sciencedirect.com/science/article/pii/S095058491930103X,Adopting configuration management principles for managing experiment materials in families of experiments,September 2019,Information and Software Technology,Not Found,Edison=Espinosa: egespinosa1@espe.edu.ec; Silvia Teresita=Acuña: silvia.acunna@uam.es; Sira=Vegas: svegas@fi.upm.es; Natalia=Juristo: natalia@fi.upm.es,"Abstract
Context
Replication is a key component of experimentation for verifying previous results and findings. Experiment replication requires products like documentation describing the baseline experiment and a version of the experimental material. When replicating an experiment, changes may have to be made to some of the products, leading to new or modified versions of materials. After the replication has been conducted, part of or all the materials should be added to the family history or to the baseline experiment documentation. As the number of replications increases, more versions of the materials are generated. This can lead to product management chaos in replications sharing the same protocol.
Objective
The aim of this paper is to adopt 
configuration management
 principles to manage experimental materials. We apply and validate these principles in a code inspection technique comparison experiment and a personality quasi-experiment.
Method
The study was conducted within a research group with lengthy experience in experiment replication. This research group has had trouble with the management of the materials used to run some of the experiments replicated by other colleagues. This is a suitable context for applying action research. We used action research to adopt the 
configuration management
 principles and build a materials management framework.
Result
We generated the instances of an experiment and a quasi-experiment, identifying the status and traceability of the materials. Additionally, we documented the workload required for 
instantiation
 in person-hours. We also checked the ease of use and understanding of the framework for instantiating the personality quasi-experiment configuration plan executed by researchers who did not develop the framework, as well as its usefulness for managing the experimental materials.
Conclusion
The experimental materials management framework is useful for establishing the status and traceability of the experimental materials. Additionally, it improves the storage, search, location and retrieval of the experimental material versions.",21 Mar 2025,6,"The adoption of configuration management principles for experimental materials management can benefit European early-stage ventures involved in research and experimentation, but the impact may vary depending on the specific context."
https://www.sciencedirect.com/science/article/pii/S0950584919301053,Multi-armed bandits in the wild: Pitfalls and strategies in online experiments,September 2019,Information and Software Technology,Not Found,David=Issa Mattos: davidis@chalmers.se; Jan=Bosch: jan.bosch@chalmers.se; Helena Holmström=Olsson: helena.holmstrom.olsson@mah.se,"Abstract
Context
Delivering faster value to customers with online experimentation is an emerging practice in industry. Multi-Armed Bandit (MAB) based experiments have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, the incorrect use of MAB-based experiments can lead to incorrect conclusions that can potentially hurt the company's business.
Objective
The objective of this study is to understand the pitfalls and restrictions of using MABs in online experiments, as well as the strategies that are used to overcome them.
Method
This research uses a multiple 
case study method
 with eleven experts across five software companies and simulations to triangulate the data of some of the identified limitations.
Results
This study analyzes some limitations faced by companies using MAB and discusses strategies used to overcome them. The results are summarized into practitioners’ guidelines with criteria to select an appropriated 
experimental design
.
Conclusion
MAB algorithms have the potential to deliver even faster results with a better allocation of resources over traditional A/B experiments. However, potential mistakes can occur and hinder the 
potential benefits
 of such approach. Together with the provided guidelines, we aim for this paper to be used as reference material for practitioners during the design of an online experiment.",21 Mar 2025,8,"Understanding the pitfalls and restrictions of MABs in online experiments and providing guidelines for practitioners offer valuable insights for European early-stage ventures engaging in online experimentation, potentially helping them make better decisions."
https://www.sciencedirect.com/science/article/pii/S0950584919301156,Specifying quantities in software models,September 2019,Information and Software Technology,Not Found,Loli=Burgueño: lburguenoc@uoc.edu; Tanja=Mayerhofer: Not Found; Manuel=Wimmer: Not Found; Antonio=Vallecillo: Not Found,"Abstract
Context
An essential requirement for the design and development of any 
engineering application
 that deals with real-world 
physical systems
 is the formal representation and processing of 
physical quantities
, comprising both measurement uncertainty and units. Although solutions exist for several programming languages and simulation frameworks, this problem has not yet been fully solved for software models.
Objective
This paper shows how both measurement uncertainty and units can be effectively incorporated into software models, becoming part of their basic type systems.
Method
We introduce the main concepts and mechanisms needed for representing and handling 
physical quantities
 in software models. More precisely, we describe an extension of basic type Real, called Quantity, and a set of operations defined for the values of that type, together with a ready-to-use library of 
dimensions and units
, which can be added to any modeling project.
Results
We show how our approach permits modelers to safely represent and operate with physical quantities, statically ensuring type- and unit-safe assignments and operations, prior to any simulation of the system or implementation in any programming language.
Conclusion
Our approach improves the 
expressiveness
 and type-safety of software models with respect to measurement uncertainty and units of physical quantities, and its effective use in modeling projects of 
physical systems
.",21 Mar 2025,7,"Incorporating measurement uncertainty and units into software models can enhance the expressiveness and safety of physical system modeling, which can be beneficial for European early-stage ventures working on engineering applications."
https://www.sciencedirect.com/science/article/pii/S095058491930117X,An HMM-based approach for automatic detection and classification of duplicate bug reports,September 2019,Information and Software Technology,Not Found,Neda=Ebrahimi: n_ebr@ece.concordia.ca; Abdelaziz=Trabelsi: trabelsi@ece.concordia.ca; Md. Shariful=Islam: mdsha_i@ece.concordia.ca; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca; Kobra=Khanmohammadi: k_khanm@ece.concordia.ca,"Abstract
Context
Software projects rely on their issue tracking systems to guide maintenance activities of software developers. 
Bug reports
 submitted to the issue tracking systems carry crucial information about the nature of the crash (such as texts from users or developers and execution information about the running functions before the occurrence of a crash). Typically, big software projects receive thousands of reports every day.
Objective
The aim is to reduce the time and effort required to fix bugs while improving software quality overall. Previous studies have shown that a large amount of bug reports are duplicates of previously reported ones. For example, as many as 30% of all reports in for Firefox are duplicates.
Method
While there exist a wide variety of approaches to automatically detect duplicate bug reports by 
natural language processing
, only a few approaches have considered execution information (the so-called stack traces) inside bug reports. In this paper, we propose a novel approach that automatically detects duplicate bug reports using stack traces and Hidden Markov Models.
Results
When applying our approach to Firefox and GNOME datasets, we show that, for Firefox, the average recall for Rank 
k
 = 1 is 59%, for Rank 
k
 = 2 is 75.55%. We start reaching the 90% recall from 
k
 = 10. The 
Mean Average Precision
 (MAP) value is up to 76.5%. For GNOME, The recall at 
k
 = 1 is around 63%, while this value increases by about 10% for 
k
 = 2. The recall increases to 97% for 
k
 = 11. A MAP value of up to 73% is achieved.
Conclusion
We show that HMM and stack traces are a powerful combination for detecting and classifying duplicate bug reports in large bug repositories.",21 Mar 2025,7,"The proposed approach of automatically detecting duplicate bug reports using stack traces and Hidden Markov Models has the potential to greatly reduce time and effort required to fix bugs in large software projects, thus improving software quality and efficiency for startups."
https://www.sciencedirect.com/science/article/pii/S0950584919301223,On the use of usage patterns from telemetry data for test case prioritization,September 2019,Information and Software Technology,Not Found,Jeff=Anderson: jeffand@microsoft.com; Maral=Azizi: maraazizi@my.unt.edu; Saeed=Salem: saeed.salem@ndsu.edu; Hyunsook=Do: hyunsook.do@unt.edu,"Abstract
Context
Modern applications contain pervasive telemetry to ensure reliability and enable monitoring and diagnosis. This presents a new opportunity in the area of regression testing techniques, as we now have the ability to consider usage profiles of the software when making decisions on test execution.
Objective
The results of our prior work on test prioritization using telemetry data showed improvement rate on test suite reduction, and test 
execution time
. The objective of this paper is to further investigate this approach and apply prioritization based on multiple prioritization algorithms in an enterprise level cloud application as well as 
open source projects
. We aim to provide an effective prioritization scheme that practitioners can implement with minimum effort. The other objective is to compare the results and the benefits of this technique factors with code coverage-based prioritization approaches, which is the most commonly used test prioritization technique.
Method
We introduce a method for identifying usage patterns based on telemetry, which we refer to as “telemetry fingerprinting.” Through the use of various algorithms to compute fingerprints, we conduct empirical studies on multiple software products to show that telemetry fingerprinting can be used to more effectively prioritize 
regression tests
.
Results
Our experimental results show that the proposed techniques were able to reduce over 30% in regression test suite run times compared to the coverage-based prioritization technique in detecting discoverable faults. Further, the results indicate that fingerprints are effective in identifying usage patterns, and that the fingerprints can be applied to improve regression testing techniques.
Conclusion
In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms to compute fingerprints and conduct empirical studies that show that fingerprints are effective in identifying distinct usage patterns. By applying these techniques, we believe that regression testing techniques can be improved beyond the current state-of-the-art, yielding additional cost and quality benefits.",21 Mar 2025,8,"The use of telemetry data to prioritize regression tests effectively and reduce test suite run times by over 30% shows significant practical value for startups in improving testing processes and software quality, with potential cost and quality benefits."
https://www.sciencedirect.com/science/article/pii/S0950584919301247,M-Lean: An end-to-end development framework for predictive models in B2B scenarios,September 2019,Information and Software Technology,Not Found,Mona=Nashaat: nashaata@ualberta.ca; Aindrila=Ghosh: Not Found; James=Miller: jimm@ualberta.ca; Shaikh=Quader: Not Found; Chad=Marston: Not Found,"Abstract
Context
The need for 
business intelligence
 has led to advances in 
machine learning
 in the business domain, especially with the rise of 
big data analytics
. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine 
learning systems
 in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying 
machine learning
 in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents 
MLean
, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the 
Lean Startup
 methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a 
case study
 to build a predictive product. The 
case study
 resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.",21 Mar 2025,6,"The MLean framework aiming to guide businesses in designing, developing, evaluating, and deploying predictive systems could provide valuable insights to startups on transforming data into actions, but the impact may be limited compared to other abstracts focusing directly on software development."
https://www.sciencedirect.com/science/article/pii/S0950584919300709,Ranking of software developers based on expertise score for bug triaging,August 2019,Information and Software Technology,Not Found,Asmita=Yadav: asmita.yadav85@gmail.com; Sandeep Kumar=Singh: sandeepk.singh@jiit.ac.in,"Abstract
Context
Existing bug triage approaches for developer recommendation systems are mainly based on 
machine learning
 (ML) techniques. These approaches have shown low prediction accuracy and high bug tossing length (BTL).
Objective
The objective of this paper is to develop a robust algorithm for reducing BTL based on the concept of developer expertise score (DES).
Method
None of the existing approaches to the best of our knowledge have utilized metrics to build developer expertise score. The novel strategy of DES is consisted of two stages: Stage-I consisted of an offline process for detecting the developers based on DES which computes the score using priority, versatility and average fix-time for his individual contributions. The online system process consisted of finding the capable developers using three kinds of similarity measures (feature-based, cosine-similarity and Jaccard). Stage-II of the online process consisted of simply ranking the developers. Hit-ratio and reassignment accuracy were used for performance evaluation. We compared our system against the ML-based bug triaging approaches using three types of classifiers: Navies Bayes, Support Vector Machine and C4.5 paradigms.
Results
By adapting the five open source databases, namely: Mozilla, Eclipse, Netbeans, Firefox, and Freedesktop, covering 41,622 
bug reports
, our novel DES system yielded a mean accuracy, precision, recall rate and F-score of 
89.49%, 89.53%, 89.42%
 and 
89.49%
, respectively, reduced BTLs of up to 
88.55%
. This demonstrates an improvement of up to 
20%
 over existing strategies.
Conclusion
This work presented a novel developer recommendation algorithm to rank the developers based on a metric-based integrated score for bug triaging. This integrated score was based on the developer's expertise with an objective to improve (i) bug assignment and (ii) reduce the bug tossing length. Such architecture has an application in software bug triaging frameworks.",21 Mar 2025,9,"The novel DES system for reducing bug tossing length and ranking developers based on integrated scores shows high accuracy, precision, and recall rates, with a significant improvement over existing ML-based bug triaging approaches. This could greatly benefit startups in optimizing bug assignment processes."
https://www.sciencedirect.com/science/article/pii/S0950584919300710,Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions,August 2019,Information and Software Technology,Not Found,Markus=Borg: markus.borg@ri.se; Panagiota=Chatzipetrou: Not Found; Krzysztof=Wnuk: Not Found; Emil=Alégroth: Not Found; Tony=Gorschek: Not Found; Efi=Papatheocharous: Not Found; Syed Muhammad Ali=Shah: Not Found; Jakob=Axelsson: Not Found,"Abstract
Context
Component-based software engineering (CBSE) is a common approach to develop and evolve contemporary software systems. When evolving a system based on components, make-or-buy decisions are frequent, i.e., whether to develop components internally or to acquire them from 
external sources
. In CBSE, several different sourcing options are available: (1) developing software in-house, (2) outsourcing development, (3) buying commercial-off-the-shelf software, and (4) integrating 
open source software
 components.
Objective
Unfortunately, there is little available research on how organizations select component sourcing options (CSO) in industry practice. In this work, we seek to contribute empirical evidence to CSO selection.
Method
We conduct a cross-domain survey on CSO selection in industry, implemented as an online questionnaire.
Results
Based on 188 responses, we find that most organizations consider multiple CSOs during software evolution, and that the CSO decisions in industry are dominated by 
expert judgment
. When choosing between candidate components, functional suitability acts as an initial filter, then reliability is the most important quality.
Conclusion
We stress that future solution-oriented work on decision support has to account for the dominance of expert judgment in industry. Moreover, we identify considerable variation in CSO decision processes in industry. Finally, we encourage software development organizations to reflect on their decision processes when choosing whether to make or buy components, and we recommend using our survey for a first benchmarking.",21 Mar 2025,5,"The research on component sourcing options (CSO) selection in industry provides valuable insights, but the practical impact on startups may be limited compared to other abstracts focusing on specific software development processes and improvements."
https://www.sciencedirect.com/science/article/pii/S095058491930076X,“Bad smells” in software analytics papers,August 2019,Information and Software Technology,Not Found,Tim=Menzies: timm@ieee.org; Martin=Shepperd: Not Found,"Abstract
Context
There has been a rapid growth in the use of 
data analytics
 to underpin evidence-based 
software engineering
. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies.
Objective
Our goal is to provide guidance for producers and consumers of 
software analytics
 studies (computational experiments and correlation studies).
Method
We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in 
software analytics
 studies.
Results
We list 12 “bad smells” in software analytics papers (and show their impact by examples).
Conclusions
We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time).",21 Mar 2025,6,"The study provides guidance for producers and consumers of software analytics studies, which can be valuable for startups in improving the reliability of their studies. The proposed 'bad smells' concept could aid in identifying underlying problems."
https://www.sciencedirect.com/science/article/pii/S0950584919300771,A critical appraisal tool for systematic literature reviews in software engineering,August 2019,Information and Software Technology,"Systematic literature reviews, Quality assessment, Software engineering, Critical appraisal tools, AMSTAR",Nauman bin=Ali: nauman.ali@bth.se; Muhammad=Usman: muu@bth.se,"Abstract
Context:
 Methodological research on 
systematic literature reviews
 (SLRs) in 
Software Engineering
 (SE) has so far focused on developing and evaluating guidelines for conducting 
systematic reviews
. However, the support for quality assessment of completed SLRs has not received the same level of attention.
Objective:
 To raise awareness of the need for a critical appraisal tool (CAT) for assessing the quality of SLRs in SE. To initiate a community-based effort towards the development of such a tool.
Method:
 We reviewed the literature on the quality assessment of SLRs to identify the frequently used CATs in SE and other fields. 
Results:
 We identified that the CATs currently used is SE were borrowed from medicine, but have not kept pace with substantial advancements in the field of medicine.
Conclusion:
 In this paper, we have argued the need for a CAT for quality appraisal of SLRs in SE. We have also identified a tool that has the potential for application in SE. Furthermore, we have presented our approach for adapting this state-of-the-art CAT for assessing SLRs in SE.",21 Mar 2025,5,"The need for a critical appraisal tool for assessing the quality of systematic literature reviews in Software Engineering is important, but the impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584919300904,Dynamic selection of fitness function for software change prediction using Particle Swarm Optimization,August 2019,Information and Software Technology,Not Found,Ruchika=Malhotra: ruchikamalhotra@dtu.ac.in; Megha=Khanna: Not Found,"Abstract
Context
Over the past few years, researchers have been actively searching for an effective classifier which correctly predicts change prone classes. Though, few researchers have ascertained the predictive capability of search-based algorithms in this domain, their effectiveness is highly dependent on the selection of an optimum fitness function. The criteria for selecting one fitness function over the other is the improved predictive capability of the developed model on the entire dataset. However, it may be the case that various subsets of instances of a dataset may give best results with a different fitness function.
Objective
The aim of this study is to choose the best fitness function for each instance rather than the entire dataset so as to create models which correctly ascertain the change prone nature of majority of instances. Therefore, we propose a novel framework for the adaptive selection of a dynamic optimum fitness function for each instance of the dataset, which would correctly determine its change prone nature.
Method
The 
predictive models
 in this study are developed using seven different fitness variants of Particle Swarm Optimization (PSO) algorithm. The proposed framework predicts the best suited fitness variant amongst the seven investigated fitness variants on the basis of structural characteristics of a corresponding instance.
Results
The results of the study are empirically validated on fifteen datasets collected from popular open-source software. The proposed adaptive framework was found efficient in determination of change prone classes as it yielded improved results when compared with models developed using individual fitness variants and fitness-based voting 
ensemble classifiers
.
Conclusion
The performance of the models developed using the proposed adaptive framework were statistically better than the models developed using individual fitness variants of PSO algorithm and competent to models developed using 
machine learning
 
ensemble classifiers
.",21 Mar 2025,8,The proposed adaptive selection of a dynamic fitness function for predictive models shows promise in correctly determining change prone classes. This could be valuable for startups looking to improve model predictions and efficiency.
https://www.sciencedirect.com/science/article/pii/S0950584919300898,Towards a reduction in architectural knowledge vaporization during agile global software development,August 2019,Information and Software Technology,Not Found,Gilberto=Borrego: gilberto.borrego@uabc.edu.mx; Alberto L.=Morán: Not Found; Ramón R.=Palacio: Not Found; Aurora=Vizcaíno: Not Found; Félix O.=García: Not Found,"Abstract
Context
The adoption of agile methods is a trend in 
global software development
 (GSD), but may result in many challenges. One important challenge is architectural knowledge (AK) management, since agile developers prefer 
sharing knowledge
 through face-to-face interactions, while in GSD the preferred manner is documents. Agile knowledge-sharing practices tend to predominate in GSD companies that practice 
agile development
 (AGSD), leading to a lack of documents, such as 
architectural designs
, data models, deployment specifications, etc., resulting in the loss of AK over time, i.e., it vaporizes.
Objective
In a previous study, we found that there is important AK in the log files of unstructured textual electronic media (UTEM), such as instant messengers, emails, forums, etc., which are the preferred means employed in AGSD to contact remote teammates. The objective of this paper is to present and evaluate a proposal with which to recover AK from UTEM logs. We developed and evaluated a prototype that implements our proposal in order to determine its feasibility.
Method
The evaluation was performed by conducting a study with agile/global developers and students, who used the prototype and different UTEM to execute tasks that emulate common situations concerning AGSD teams’ lack of documentation during development phases.
Results
Our prototype was considered a useful, usable and unobtrusive tool when retrieving AK from UTEM logs. The participants also preferred our prototype when searching for AK and found AK faster with the prototype than with UTEM when the origin of the AK required was unknown.
Conclusion
The participants’ performance and perceptions when using our prototype provided evidence that our proposal could reduce AK vaporization in AGSD environments. These results encourage us to evaluate our proposal in a long-term test as future work.",21 Mar 2025,7,The proposal to recover architectural knowledge from unstructured textual electronic media using a prototype could be beneficial for startups facing challenges in agile/global software development. The tool's usability and effectiveness are highlighted.
https://www.sciencedirect.com/science/article/pii/S0950584919300916,Using a many-objective approach to investigate automated refactoring,August 2019,Information and Software Technology,Not Found,M.=Mohan: Not Found; D.=Greer: des.greer@qub.ac.uk,"Abstract
Context
Software maintenance is expensive and so anything that can be done to reduce its cost is potentially of huge benefit. However, it is recognised that some maintenance, especially refactoring, can be automated. Given the number of possible refactorings and combinations of refactorings, a search-based approach may provide the means to optimise refactorings.
Objective
This paper describes the investigation of a many-objective 
genetic algorithm
 used to automate software refactoring, implemented as a Java tool, MultiRefactor.
Method
The approach and tool is evaluated using a set of open source Java programs. The tool contains four separate measures of software looking at the software quality as well as measures of code priority, refactoring coverage and element recentness. The many-objective algorithm combines the four objectives to improve the software in a holistic manner. An experiment has been constructed to compare the many-objective approach against a mono-objective approach that only uses a single objective to measure software quality. Different permutations of the objectives are also tested and compared to see how well the different objectives can work together in a multi-objective refactoring approach. The eight approaches are tested on six different open source Java programs.
Results
The many-objective approach is found to give better objective scores on average than the mono-objective approach and in less time. However, the priority and element recentness objectives are both found to be less successful in multi/many-objective setups when they are used together.
Conclusion
A many-objective approach is suitable and effective for optimising 
automated refactoring
 to improve quality. Including other objectives does not unduly degrade the quality improvements, but is less effective for those objectives than if they were used in a mono-objective approach.",21 Mar 2025,4,"The investigation of a many-objective genetic algorithm for automating software refactoring is valuable for reducing maintenance costs, but the direct impact on early-stage ventures may be limited as they might not be heavily involved in software maintenance."
https://www.sciencedirect.com/science/article/pii/S0950584919300928,A Community Strategy Framework – How to obtain influence on requirements in meritocratic open source software communities?,August 2019,Information and Software Technology,Not Found,J.=Linåker: johan.linaker@cs.lth.se; B.=Regnell: bjorn.regnell@cs.lth.se; D.=Damian: damian.daniela@gmail.com,"Abstract
Context:
 In the 
Requirements Engineering
 (RE) process of an 
Open Source Software
 (OSS) community, an involved firm is a stakeholder among many. Conflicting agendas may create miss-alignment with the firm’s internal requirements strategy. In communities with meritocratic governance or with aspects thereof, a firm has the opportunity to affect the RE process in line with their own agenda by gaining influence through active and symbiotic engagements.
Objective:
 The focus of this study has been to identify what aspects that firms should consider when they assess their need of influencing the RE process in an OSS community, as well as what engagement practices that should be considered in order to gain this influence.
Method:
 Using a design science approach, 21 interviews with 18 industry professionals from 12 different software-intensive firms were conducted to explore, design and validate an artifact for the problem context.
Results:
 A Community Strategy Framework (CSF) is presented to help firms create community strategies that describe if and why they need influence on the RE process in a specific (meritocratic) OSS community, and how the firm could gain it. The framework consists of aspects and engagement practices. The aspects help determine how important an 
OSS project
 and its community is from business and technical perspectives. A community perspective is used when considering the feasibility and potential in gaining influence. The engagement practices are intended as a tool-box for how a firm can engage with a community in order to build influence needed.
Conclusion:
 It is concluded from interview-based validation that the proposed CSF may provide support for firms in creating and tailoring community strategies and help them to focus resources on communities that matter and gain the influence needed on their respective RE processes.",21 Mar 2025,8,"The study provides a practical framework for firms to influence the Requirements Engineering process in OSS communities, which can benefit early-stage ventures by helping them focus resources on communities that matter."
https://www.sciencedirect.com/science/article/pii/S0950584919300941,Enhancing context specifications for dependable adaptive systems: A data mining approach,August 2019,Information and Software Technology,Not Found,Arthur=Rodrigues: arthy.rf@gmail.com; Genaína Nunes=Rodrigues: genaina@cic.unb.br; Alessia=Knauss: alessia.knauss@chalmers.se; Raian=Ali: rali@bournemouth.ac.uk; Hugo=Andrade: sica@chalmers.se,"Abstract
Context:
 Adaptive systems are expected to cater for various 
operational contexts
 by having multiple strategies in achieving their objectives and the logic for matching strategies to an actual context. The prediction of relevant contexts at design time is paramount for dependability. With the current trend on using data mining to support the 
requirements engineering
 process, this task of understanding context for adaptive system at design time can benefit from such techniques as well.
Objective:
 The objective is to provide a method to refine the specification of contextual variables and their relation to strategies for dependability. This refinement shall detect dependencies between such variables, priorities in monitoring them, and decide on their relevance in choosing the right strategy in a 
decision tree
.
Method:
 Our requirements-driven approach adopts the contextual goal modelling structure in addition to the operationalization values of sensed information to map contexts to the system’s behaviour. We propose a design time analysis process using a subset of 
data mining algorithms
 to extract a list of relevant contexts and their related variables, tasks, and/or goals.
Results:
 We experimentally evaluated our proposal on a Body Sensor Network system (BSN), simulating 12 resources that could lead to a variability space of 4096 possible context conditions. Our approach was able to elicit subtle contexts that would significantly affect the service provided to assisted patients and relations between contexts, assisting the decision on their need, and priority in monitoring.
Conclusion:
 The use of some 
data mining techniques
 can mitigate the lack of precise definition of contexts and their relation to system strategies for dependability. Our method is practical and supportive to traditional requirements specification methods, which typically require intense human intervention.",21 Mar 2025,6,"The method proposed focuses on refining contextual variables and strategies for dependability, which could be beneficial for startups working on adaptive systems, although the impact might be limited to a specific domain."
https://www.sciencedirect.com/science/article/pii/S0950584919300953,Using Squeeziness to test component-based systems defined as Finite State Machines,August 2019,Information and Software Technology,Not Found,Alfredo=Ibias: aibias@ucm.es; Robert M.=Hierons: r.hierons@sheffield.ac.uk; Manuel=Núñez: manuelnu@ucm.es,"Abstract
Context:
Testing is the main validation technique used to increase the reliability of software systems. The effectiveness of testing can be strongly reduced by 
Failed 
Error Propagation
. This situation happens when the System Under Test executes a faulty statement, the state of the system is affected by this fault, but the expected output is observed. Squeeziness is an information 
theoretic measure
 designed to quantify the likelihood of Failed Error Propagation and previous work has shown that Squeeziness correlates strongly with Failed Error Propagation in white-box scenarios. Despite its usefulness, this measure, in its current formulation, cannot be used in a black-box scenario where we do not have access to the 
source code
 of the components.
Objective:
The main goal of this paper is to adapt Squeeziness to a black-box scenario and evaluate whether it can be used to estimate the likelihood that a component of a software system introduces Failed Error Propagation.
Method:
 First, we defined our black-box scenario. Specifically, we considered the Failed Error Propagation that a component introduces when it receives its input from another component. We were interested in this since such fault masking makes it more difficult to find faults in the 
previous
 component when testing. Second, we defined our notion of Squeeziness in this framework. Finally, we carried out experiments in order to evaluate our measure.
Results:
 Our experiments showed a strong correlation between the likelihood of Failed Error Propagation and Squeeziness.
Conclusion:
 We can conclude that our new notion of Squeeziness can be used as a measure that estimates the probability of Failed Error Propagation being introduced by a component. As a result, it has the potential to be used as a measure of 
testability
, allowing testers to assess how easy it is to test either the whole system or a single component. We considered a simple model (Finite State Machines) but the notions and results can be extended/adapted to deal with more complex state-based models, in particular, those containing data.",21 Mar 2025,7,"Adapting Squeeziness for black-box scenarios could be useful for startups to estimate the likelihood of Failed Error Propagation, enhancing the quality and testability of software components."
https://www.sciencedirect.com/science/article/pii/S0950584919300965,Source code properties of defective infrastructure as code scripts,August 2019,Information and Software Technology,Not Found,Akond=Rahman: aarahman@ncsu.edu; Laurie=Williams: Not Found,"Abstract
Context
In continuous deployment, software and services are rapidly deployed to end-users using an automated deployment pipeline. Defects in infrastructure as code (IaC) scripts can hinder the reliability of the automated deployment pipeline. We hypothesize that certain properties of IaC 
source code
 such as lines of code and hard-coded strings used as configuration values, show correlation with defective IaC scripts.
Objective
The objective of this paper is to help practitioners in increasing the quality of infrastructure as code (IaC) scripts through an empirical study that identifies 
source code
 properties of defective IaC scripts.
Methodology
We apply qualitative analysis on defect-related commits mined from 
open source software
 repositories to identify source code properties that correlate with defective IaC scripts. Next, we survey practitioners to assess the practitioner’s agreement level with the identified properties. We also construct 
defect prediction
 models using the identified properties for 2439 scripts collected from four datasets.
Results
We identify 10 source code properties that correlate with defective IaC scripts. Of the identified 10 properties we observe lines of code and hard-coded string i.e. putting strings as configuration values, to show the strongest correlation with defective IaC scripts. According to our survey analysis, majority of the practitioners show agreement for two properties: include, the property of executing external modules or scripts, and hard-coded string. Using the identified properties, our constructed 
defect prediction
 models show a precision of 0.70
∼
0.78, and a recall of 0.54
∼
0.67.
Conclusion
Based on our findings, we recommend practitioners to allocate sufficient inspection and testing efforts on IaC scripts that include any of the identified 10 source code properties of IaC scripts.",21 Mar 2025,9,"Identifying source code properties correlated with defective IaC scripts can significantly benefit early-stage ventures in improving the quality of their infrastructure as code, potentially avoiding deployment issues."
https://www.sciencedirect.com/science/article/pii/S0950584919300977,A bug finder refined by a large set of open-source projects,August 2019,Information and Software Technology,Not Found,Jaechang=Nam: jcnam@handong.edu; Song=Wang: song.wang@uwaterloo.ca; Yuan=Xi: y25xi@uwaterloo.ca; Lin=Tan: lintan@uwaterloo.ca,"Abstract
Context
Static bug detection techniques are commonly used to automatically detect software bugs. The biggest obstacle to the wider adoption of static bug detection tools is 
false positives
, i.e., reported bugs that developers do not have to act on.
Objective
The objective of this study is to reduce 
false positives
 resulting from static bug detection tools and to detect new bugs by exploring the effectiveness of a feedback-based bug detection rule design.
Method
We explored a large number of software projects and applied an iterative feedback-based process to design bug detection rules. The outcome of the process is a set of ten bug detection rules, which we used to build a feedback-based bug finder, 
FeeFin
. Specifically, we manually examined 1622 patches to identify bugs and fix patterns, and implement bug detection rules. Then, we refined the rules by repeatedly using feedback from a large number of software projects.
Results
We applied 
FeeFin
 to the latest versions of the 1880 projects on GitHub to detect previously unknown bugs. 
FeeFin
 detected 98 new bugs, 63 of which have been reviewed by developers: 57 were confirmed as true bugs, and 9 were confirmed as false positives. In addition, we investigated the benefits of our 
FeeFin
 process in terms of new and improved bug patterns. We verified our bug patterns with four existing tools, namely PMD, FindBugs, Facebook Infer, and Google Error Prone, and found that our FeeFin process has the potential to identify new bug patterns and also to improve existing bug patterns.
Conclusion
Based on the results, we suggest that static bug detection tool designers identify new bug patterns by mining real-world patches from a large number of software projects. In addition, the 
FeeFin
 process is helpful in mitigating false positives generated from existing tools by refining their bug detection rules.",21 Mar 2025,8,"The feedback-based bug detection rule design proposed in this study can help reduce false positives in static bug detection tools, leading to more efficient bug detection in software projects, which is valuable for startups."
https://www.sciencedirect.com/science/article/pii/S0950584918301290,Internal and external quality in the evolution of mobile software: An exploratory study in open-source market,August 2019,Information and Software Technology,Not Found,Bahar=Gezici: bahargezici@hacettepe.edu.tr; Ayça=Tarhan: atarhan@hacettepe.edu.tr; Oumout=Chouseinoglou: uhus@hacettepe.edu.tr,"Abstract
Context
Mobile applications evolve rapidly and grow constantly to meet user requirements. Satisfying these requirements may lead to poor design choices that can degrade internal quality and performance, and consequently external quality and quality in use. Therefore, monitoring the characteristics of mobile applications through their evolution is important to facilitate maintenance and development.
Objective
This study aims to explore internal quality, external quality and the relation between these two by carrying out an embedded, multiple 
case study
 that includes two cases in different functional domains. In each 
case study
, the evolution of three open-source mobile applications having similar features in the same domain and platform is investigated with the analysis of a number of code-based and community-based metrics, to understand whether they are significantly related to 
quality characteristics
.
Method
A total of 105 releases of the six mobile applications are analyzed to understand internal quality, where code-based characteristics are employed in the light of Lehman’s Increasing Complexity, Continuous Growth, and Decreasing Quality laws. External quality is explored by adapting DeLone and McLean model of 
information system
 success and using community-based metrics, when data is available for the included releases, to derive a corresponding success index. Finally, internal and external quality relationship is investigated by applying Spearman’s correlation analysis on metrics data from 91 corresponding releases.
Results
The analysis of Lehman’s laws shows that only the law of Continuous Growth is validated for the selected mobile applications in both case studies. Spearman’s analysis results indicate that the internal 
quality attribute
 of ‘Understandability’ is negatively related to ‘Success Index’ for Case Study A and ‘LCOM’ is negatively related to ‘Success Index’ for Case Study B. No other significant relationship between the internal quality attributes and the Success Index is observed; but specific to community-based metrics, some significant relationships with code-based attributes were determined.
Conclusion
Our 
exploratory study
 is unique for the method it employs for exploring the relationship between internal and external quality in the evolution of mobile applications. Yet, our findings should be used with caution as they are derived from a limited number of applications. Therefore, this study should be considered to provide initial evidence for applicability of the method and a degree of confidence for repeating similar studies in wider contexts.",21 Mar 2025,5,"While the study explores important aspects of mobile application evolution, it may have limited direct practical impact on early-stage ventures or startups in Europe."
https://www.sciencedirect.com/science/article/pii/S0950584919300539,Enactment of adaptation in data stream processing with latency implications—A systematic literature review,July 2019,Information and Software Technology,Not Found,Cui=Qin: qin@sse.uni-hildesheim.de; Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Klaus=Schmid: schmid@sse.uni-hildesheim.de,"Abstract
Context
Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data 
processing tasks
. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established.
Objective
This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension.
Method
We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, 
evaluation metrics
 as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation.
Results
We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 
evaluation metrics
 and 12 evaluation parameters according to the extracted data properties.
Conclusion
We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.",21 Mar 2025,7,The categorization of runtime adaptation approaches in stream processing can provide valuable insights for startups dealing with data processing tasks and optimizing latency.
https://www.sciencedirect.com/science/article/pii/S0950584919300540,Search-based test case implantation for testing untested configurations,July 2019,Information and Software Technology,Not Found,Dipesh=Pradhan: dipesh@simula.no; Shuai=Wang: shuai.wang@testify.no; Tao=Yue: tao@simula.no; Shaukat=Ali: shaukat@simula.no; Marius=Liaaen: marliaae@cisco.com,"Abstract
Context
Modern large-scale software systems are highly configurable, and thus require a large number of test cases to be implemented and revised for testing a variety of system configurations. This makes testing highly configurable systems very expensive and time-consuming.
Objective
Driven by our industrial collaboration with a video conferencing company, we aim to automatically analyze and implant existing test cases (i.e., an original test suite) to test the untested configurations.
Method
We propose a search-based test case implantation approach (named as SBI) consisting of two key components: 1) 
Test case analyzer
 that statically analyzes each test case in the original test suite to obtain the program 
dependence graph
 for test case statements and 2) 
Test case implanter
 that uses multi-objective search to select suitable test cases for implantation using three operators, i.e., selection, crossover, and mutation (at the test suite level) and implants the selected test cases using a 
mutation operator
 at the test case level including three operations (i.e., addition, modification, and deletion).
Results
We empirically evaluated SBI with an industrial 
case study
 and an open source 
case study
 by comparing the implanted test suites produced by three variants of SBI with the original test suite using 
evaluation metrics
 such as 
statement coverage
 (
SC
), branch coverage (
BC
), and mutation score (
MS
). Results show that for both the case studies, the test suites implanted by the three variants of SBI performed significantly better than the original test suites. The best variant of SBI achieved on average 19.3% higher coverage of configuration 
variable values
 for both the case studies. Moreover, for the open source case study, the best variant of SBI managed to improve 
SC, BC
, and 
MS
 with 5.0%, 7.9%, and 3.2%, respectively.
Conclusion
SBI can be applied to automatically implant a test suite with the aim of testing untested configurations and thus achieving higher configuration coverage.",21 Mar 2025,8,"The SBI approach for automatic test case implantation shows promising results for improving configuration coverage, which can be highly beneficial for early-stage ventures in software testing."
https://www.sciencedirect.com/science/article/pii/S0950584919300564,Bootstrapping cookbooks for APIs from crowd knowledge on Stack Overflow,July 2019,Information and Software Technology,Not Found,Lucas B.L.=Souza: Not Found; Eduardo C.=Campos: eccampos@ufu.br; Fernanda=Madeiral: fernanda.madeiral@ufu.br; Klérisson=Paixão: klerisson@ufu.br; Adriano M.=Rocha: Not Found; Marcelo de Almeida=Maia: marcelo.maia@ufu.br,"Abstract
Context
Well established libraries typically have 
API
 documentation. However, they frequently lack examples and explanations, possibly making difficult their effective reuse. Stack Overflow is a question-and-answer website oriented to issues related to software development. Despite the increasing adoption of Stack Overflow, the information related to a particular topic (e.g., an API) is spread across the website. Thus, Stack Overflow still lacks organization of the 
crowd knowledge
 available on it.
Objective
Our target goal is to address the problem of the poor quality documentation for APIs by providing an alternative artifact to document them based on the crowd knowledge available on Stack Overflow, called 
crowd cookbook
. A 
cookbook
 is a recipe-oriented book, and we refer to our cookbook as 
crowd cookbook
 since it contains 
content generated
 by a crowd. The cookbooks are meant to be used through an exploration process, i.e. browsing.
Method
In this paper, we present a semi-automatic approach that organizes the crowd knowledge available on Stack Overflow to build cookbooks for APIs. We have generated cookbooks for three APIs widely used by the software development community: SWT, LINQ and QT. We have also defined 
desired properties
 that crowd cookbooks must meet, and we conducted an evaluation of the cookbooks against these properties with 
human subjects
.
Results
The results showed that the cookbooks built using our approach, in general, meet those properties. As a highlight, most of the recipes were considered appropriate to be in the cookbooks and have self-contained information.
Conclusion
We concluded that our approach is capable to produce adequate cookbooks automatically, which can be as useful as manually produced cookbooks. This opens an opportunity for 
API designers
 to enrich existent cookbooks with the different points of view from the crowd, or even to generate initial versions of new cookbooks.",21 Mar 2025,6,"The crowd cookbook approach for API documentation organization presents an interesting concept, but its direct impact on European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919300576,The relationship between personality and decision-making: A Systematic literature review,July 2019,Information and Software Technology,Not Found,Fabiana Freitas=Mendes: fabianamendes@unb.br; Emilia=Mendes: emilia.mendes@bth.se; Norsaremah=Salleh: norsaremah@iium.edu.my,"Abstract
Context
From a point of view, software development is a set of decisions that need to be made while the software is developed. Many alternatives should be considered, such as the technology to employ, or the most important features to implement. However, many factors can influence one’s decision-making, such as the decision maker’s personality.
Objective
This paper reports the state of the art with regard to the relationship between decision-makers’ personality and decision-making aspects.
Method
We conducted a 
Systematic Literature Review
 to search and analyze published primary studies that discuss the abovementioned relationship in the context of companies that develop any kind of product or service.
Results
Despite the recognized influence of personality in decision-making activities, we were not able to find any study in 
Software Engineering
 field that discusses this relationship. We included 15 studies and most of them are from Management field, excluding one from 
Information System
 field. From these studies, we identified 75 reported relationships between 28 different personality aspects and 30 different decision-making aspects.
Conclusion
The interest in this topic born on 80’s and it has grown after 2002. However, despite the number of reported relationships, and the number of personalities and decision-making aspects investigated, more research on this topic is necessary. In particular, it is important to verify how someone’s personality influences the decision-making considering the software development context. This can help in improving how a decision is made in software engineering context.",21 Mar 2025,4,"The exploration of the relationship between decision-makers' personality and decision-making in software development is insightful, but its practical application for European startups may be unclear."
https://www.sciencedirect.com/science/article/pii/S095058491930059X,A reference model-based user requirements elicitation process: Toward operational business-IT alignment in a co-creation value network,July 2019,Information and Software Technology,Not Found,Samaneh=Bagheri: s.bagheri@tue.nl; R.J.=Kusters: r.j.kusters@tue.nl; J.J.M.=Trienekens: j.j.m.trienekens@tue.nl; P.W.P.J.=Grefen: p.w.p.j.grefen@tue.nl,"Abstract
Context
To improve operational business-IT alignment (BITA), the development of IT-based systems should be derived from business requirements. However, the 
requirements elicitation
 process is challenging and encounters several problems which might lead to acquiring low-quality user requirements and failure of systems development projects. Many of 
elicitation
 problems are also identified as being relevant in the BITA literature. We focus on one category of well-known 
elicitation
 problems, such as communication flaws.
Until now, the majority of 
requirements elicitation
 studies with the aim of addressing operational BITA are based on an asking strategy. This elicitation strategy is suitable for relatively stable situations. To compensate for the limitation of this strategy in a more complex situation, e.g., a co-creation value network (VN) setting, using it in conjunction with other elicitation strategies is more likely to yield satisfactory results.
Objective
To contribute to operational BITA improvement in a VN setting by addressing one category of elicitation problems. For this purpose, we design and evaluate a reference model-based approach to facilitate the user requirements 
elicitation process
.
Method
Two-phase research according to the design science approach is followed. In the design phase, a reference model-based user requirements 
elicitation process
 is designed. Also, as a proof of concept, two instances of this artifact are designed. Two reference models, respectively, describing customer 
knowledge management
 processes and customer 
knowledge management
 challenges in a VN setting are used separately in designing these two instances. In the evaluation phase, the applicability and usefulness of these instances are evaluated in two separate studies.
Results
A reference model supports asking-based user requirements elicitation process via a Delphi method in a complex context of a VN. It improves the user requirements elicitation process by addressing a set of recognized elicitation problems.
Conclusions
The reference model-based approach, by addressing the elicitation problems, contributes to user requirements elicitation process improvement in general and to a better operational BITA in the complex situation of a VN in particular.",21 Mar 2025,7,The development of a reference model-based approach to improve user requirements elicitation process can have a positive impact on operational BITA in a complex setting like a value network.
https://www.sciencedirect.com/science/article/pii/S0950584919300606,A domain analysis of resource and requirements monitoring: Towards a comprehensive model of the software monitoring domain,July 2019,Information and Software Technology,Not Found,Rick=Rabiser: rick.rabiser@jku.at; Klaus=Schmid: schmid@sse.uni-hildesheim.de; Holger=Eichelberger: eichelberger@sse.uni-hildesheim.de; Michael=Vierhauser: mvierhau@nd.edu; Sam=Guinea: sam.guinea@polimi.it; Paul=Grünbacher: paul.gruenbacher@jku.at,"Abstract
[Context]
 Complex and heterogeneous software systems need to be monitored as their full behavior often only emerges at runtime, e.g., when interacting with other systems or the environment. Software monitoring approaches observe and check properties or quality attributes of software systems during operation. Such approaches have been developed in diverse communities for various kinds of systems and purposes. For instance, requirements monitoring aims to check at runtime whether a software system adheres to its requirements, while resource or performance monitoring collects information about the consumption of computing resources by the monitored system. Many venues publish research on software monitoring, often using diverse terminology, and focusing on different monitoring aspects and phases. The lack of a comprehensive overview of existing research often leads to re-inventing the wheel. 
[Objective]
 We provide a domain model to structure and systematize the field of software monitoring, starting with requirements and resource monitoring. 
[Method]
 We developed an initial domain model based on (i) our extensive experiences with requirements and resource monitoring, (ii) earlier efforts to develop a comparison framework for monitoring approaches, and (iii) an earlier systematic literature review on requirements monitoring frameworks. We then systematically analyzed 47 existing requirements and resource monitoring approaches to iteratively refine the domain model and to develop a reference architecture for software monitoring approaches. 
[Results]
 Our domain model covers the key elements of monitoring approaches and allows analyzing their commonalities and differences. Together with the reference architecture, our domain model supports the development of integrated monitoring solutions. We provide details on 47 approaches we analyzed with the model to assess its coverage. We also evaluate the reference architecture by instantiating it for five different monitoring solutions. 
[Conclusions]
 We conclude that requirements and resource monitoring have more commonalities than differences, which is promising for the future integration of existing monitoring solutions.",21 Mar 2025,9,"The domain model and reference architecture developed for software monitoring provide a comprehensive overview to prevent reinventing the wheel, which can greatly benefit the development of monitoring solutions."
https://www.sciencedirect.com/science/article/pii/S0950584919300734,Leveraging keyword-guided exploration to build test models for web applications,July 2019,Information and Software Technology,Not Found,Xiao-Fang=Qi: xfqi@seu.edu.cn; Yun-Long=Hua: Not Found; Peng=Wang: pwang@seu.edu.cn; Zi-Yuan=Wang: wangziyuan@njupt.edu.cn,"Abstract
Context
Dynamic exploration techniques, which automatically exercise possible 
user interface elements
, have been used to explore user interface state flow graphs as test models for web applications. An exhaustive exploration may incur the well-known state explosion problem. In a limited amount of time, most existing dynamic exploration techniques tend to become mired in local or irrelevant regions of the web application due to not considering functionality semantics information. Hence, generated test models have often inadequate functionality coverage for deriving effective test cases.
Objective
This paper proposes a keyword-guided exploration strategy for automatic construction of web application test models. The goal is to generate incomplete test models with adequate functionality coverage in a given time budget for deriving test cases w.r.t. specified functionalities.
Method
Given very few keywords that describe specified functionalities, our strategy guides the exploration to discover user interface states and transitions among them that are relevant to the specified functionalities by computing similarity scores between text contents in web pages and given keywords. We use nine representative web applications to perform dynamic explorations in a given time budget and empirically evaluate functionality coverage, and other metrics, e.g., code coverage, the size of test model, the number of the test suite, path diversity, and 
DOM
 diversity.
Results
Our keyword-guided exploration strategy achieves a higher functionality coverage as compared with the generic and feedback-directed exploration strategies. Yet the significant improvement of functionality coverage achieved by our strategy is not exchanged at the cost of other metrics.
Conclusion
Our keyword-guided exploration strategy is more effective than the generic and feedback-directed exploration strategies in terms of functionality coverage. In a limited amount of time, test models generated with our strategy can be used to derive effective web application test cases.",21 Mar 2025,8,"The keyword-guided exploration strategy proposed for web application test models shows significant improvement in functionality coverage without compromising other important metrics, which can be beneficial for web application testing."
https://www.sciencedirect.com/science/article/pii/S095058491830106X,State of the art in hybrid strategies for context reasoning: A systematic literature review,July 2019,Information and Software Technology,Not Found,Roger S.=Machado: rdsmachado@inf.ufpel.edu.br; Ricardo B.=Almeida: rbalmeida@inf.ufpel.edu.br; Ana Marilza=Pernas: marilza@inf.ufpel.edu.br; Adenauer C.=Yamin: adenauer@inf.ufpel.edu.br,"Abstract
Context
Several strategies have been used to implement context reasoning, and a strategy that can be applied satisfactorily in different smart systems applications has not yet been found. Because of this, hybrid proposals for context reasoning are gaining prominence. These proposals allow the combination of two or more strategies.
Objective
This work aims to identify the state of the art in the 
context awareness
 field, considering papers that use 
hybrid strategies
 for context reasoning.
Method
A Systematic Literature Review was explored, contributing to the identification of relevant works in the field, as well as the specification of criteria for its selection. In this review, we analyzed papers published between 2004 and 2018.
Results
During the process, we identified 3241 papers. After applying filtering and conditioning processes, ten papers about 
hybrid strategies
 for context reasoning were selected. We described, discussed, and compared the selected papers.
Conclusion
The Systematic Literature Review showed that some researchers explore hybrid proposals, but these proposals do not offer flexibility regarding the reasoning strategies used. Thus, we noted that research efforts related to the topic are still necessary, mainly focusing on the development of dynamic approaches that allow the applications to choose how they want to use the different resources available.",21 Mar 2025,6,"Identifying the state of the art in context reasoning and hybrid strategies is valuable, but the conclusion that more research efforts are needed does not provide immediate practical impact on European startups."
https://www.sciencedirect.com/science/article/pii/S0950584918301113,An overview of a novel analysis approach for enhancing context awareness in smart environments,July 2019,Information and Software Technology,Not Found,Nesrine=Khabou: nesrine.khabou@redcad.org; Ismael=Bouassida Rodriguez: bouassida@redcad.org; Mohamed=Jmaiel: mohamed.jmaiel@enis.rnu.tn,"Abstract
Context
This work is part of 
context aware applications
 design and development, and smart environments in which context changes frequently.
Objective
The objective of the work is to facilitate the design and the development of 
context aware applications
 able to detect context changes and to predict context.
Method
In the paper, two analysis tasks are proposed. An analysis task for detection aiming at supporting application designers to conceive easily context aware applications able to detect context changes and an analysis task for prediction aiming at helping the application designers to conceive context aware applications able to predict context. The paper details also an analysis module that implements the functionalities of the analysis tasks. The analysis module helps the application developers to develop context aware applications. Finally, the paper introduces a 
case study
 related to smart buildings in order to show the usefulness of the analysis tasks.
Results
The paper shows an application scenario related to smart buildings and particularly water consumption prediction. Also the paper presents experiments related to memory consumption introduced by the use of our analysis module.
Conclusions
The application scenario illustrates the usefulness of the analysis approach. The overhead introduced by the analysis module is negligeable.",21 Mar 2025,5,"Facilitating the design and development of context-aware applications is relevant, but the specific focus on smart buildings and negligible overhead may not have a direct impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918301137,A distributed event-driven architectural model based on situational awareness applied on internet of things,July 2019,Information and Software Technology,Not Found,Ricardo Borges=Almeida: rbalmeida@inf.ufpel.edu.br; Victor Renan Covalski=Junes: vrcjunes@inf.ufpel.edu.br; Roger da Silva=Machado: rdsmachado@inf.ufpel.edu.br; Diórgenes Yuri Leal da=Rosa: diorgenes@inf.ufpel.edu.br; Lucas Medeiros=Donato: lucas.donato@my365.dmu.ac.uk; Adenauer Corrêa=Yamin: adenauer@inf.ufpel.edu.br; Ana Marilza=Pernas: marilza@inf.ufpel.edu.br,"Abstract
Context
The 
IoT
 network is comprised of numerous and heterogeneous devices that are capable of generating large amounts of events. To enable the 
IoT
 paradigm, it is necessary to integrate, process, and react to events on the fly.
Objective
The goal of this paper is to support the increased demands of scalability, flexibility, autonomy, and heterogeneity for IoT event processing. A distributed 
architectural model
 based on 
Situational Awareness
, named EXEHDA-SA, was designed to provide event collection, hybrid processing, and customizable and dynamic reaction features.
Method
The conception of the model was based on a middleware for 
ubiquitous computing
 called EXEHDA, thus benefiting from its already defined strategies. The proposal follows a multi-level strategy and consists of three hierarchically interconnected modular components.
Results
Our main contribution is the conception and validation of a model for event collection, processing and reaction for modern distributed environments. The contribution is evidenced through experiments performed on a prototype implemented on consolidated free and 
open source technologies
. The experiments are made up of five 
case studies
 where each one evaluates a scenario for IoT demands.
Conclusion
Through these 
case studies
 which were proposed in information security area, we demonstrated the feasibility of this proposal for deployment in IoT production environments. Furthermore, EXEHDA-SA is able to operate on different scenarios due to each component modularity and its consequent extensibility.",21 Mar 2025,7,The abstract presents a model for event processing in IoT environments which can be valuable for early-stage ventures working in IoT technologies.
https://www.sciencedirect.com/science/article/pii/S0950584918301344,GoalD: A Goal-Driven deployment framework for dynamic and heterogeneous computing environments,July 2019,Information and Software Technology,Not Found,Gabriel S.=Rodrigues: gabrielsr@aluno.unb.br; Felipe P.=Guimarães: felipe.guimaraes@aeb.gov.br; Genaína N.=Rodrigues: genaina@unb.br; Alessia=Knauss: alessia.knauss@chalmers.se; João Paulo C.=de Araújo: Not Found; Hugo=Andrade: sica@chalmers.se; Raian=Ali: rali@bournemouth.ac.uk,"Abstract
Context
Emerging paradigms like 
Internet of Things
 and Smart Cities utilize advanced sensing and communication infrastructures, where heterogeneity is an inherited feature. Applications targeting such environments require adaptability and context-sensitivity to uncertain availability and failures in resources and their ad-hoc networks. Such heterogeneity is often hard to predict, making the 
deployment process
 a challenging task.
Objective
This paper proposes GoalD as a goal-driven framework to support autonomous deployment of heterogeneous 
computational resources
 to fulfill requirements, seen as goals, and their correlated components on one hand, and the variability space of the hosting computing and sensing environment on the other hand.
Method
GoalD comprises an offline and an online stage to fulfill autonomous deployment by leveraging the use of goals. Deployment configuration strategies arise from the variability structure of the Contextual Goal Model as an underlying structure to guide autonomous planning by selecting available as well as suitable resources at runtime.
Results
We evaluate GoalD on an existing exemplar from the self-adaptive systems community – the Tele Assistance Service provided by Weyns and Calinescu [1]. Furthermore, we evaluate the scalability of GoalD on a repository consisting of 430,500 artifacts. The evaluation results demonstrate the usefulness and scalability of GoalD in planning the deployment of a system with thousands of components in a few milliseconds.
Conclusion
GoalD is a framework to systematically tackle autonomous deployment in highly 
heterogeneous computing
 environments, partially unknown at design-time following a goal-oriented approach to achieve the user goals in a target environment. GoalD has demonstrated itself able to scale for deployment planning dealing with thousands of components in a few milliseconds.",21 Mar 2025,9,"The framework proposed in this abstract can significantly impact early-stage ventures by supporting autonomous deployment in heterogeneous computing environments, demonstrating scalability and efficiency."
https://www.sciencedirect.com/science/article/pii/S0950584918300715,Detecting terminological ambiguity in user stories: Tool and experimentation,June 2019,Information and Software Technology,Not Found,Fabiano=Dalpiaz: f.dalpiaz@uu.nl; Ivor=van der Schalk: Not Found; Sjaak=Brinkkemper: Not Found; Fatma Başak=Aydemir: Not Found; Garm=Lucassen: Not Found,"Abstract
Context.
 Defects such as ambiguity and incompleteness are pervasive in software requirements, often due to the limited time that practitioners devote to writing good requirements. 
Objective.
We study whether a synergy between humans’ analytic capabilities and 
natural language processing
 is an effective approach for quickly identifying near-synonyms, a possible source of terminological ambiguity. 
Method.
We propose a tool-supported approach that blends 
information visualization
 with two 
natural language processing
 techniques: conceptual model extraction and semantic similarity. We evaluate the precision and recall of our approach compared to a pen-and-paper manual inspection session through a controlled quasi-experiment that involves 57 participants organized into 28 groups, each group working on one real-world requirements data set. 
Results.
The experimental results indicate that manual inspection delivers higher recall (statistically significant with 
p
 ≤ 0.01) and non-significantly higher precision. Based on qualitative observations, we analyze the quantitative results and suggest interpretations that explain the advantages and disadvantages of each approach. 
Conclusions.
Our experiment confirms conventional wisdom in 
requirements engineering
: identifying terminological ambiguities is time consuming, even when with tool support; and it is hard to determine whether a near-synonym may challenge the correct development of a software system. The results suggest that the most effective approach may be a combination of manual inspection with an improved version of our tool.",21 Mar 2025,5,"While the approach presented is interesting, the impact on early-stage ventures may be limited as it focuses on identifying terminological ambiguities in software requirements."
https://www.sciencedirect.com/science/article/pii/S0950584918301599,GuideGen: An approach for keeping requirements and acceptance tests aligned via automatically generated guidance,June 2019,Information and Software Technology,Not Found,Sofija=Hotomski: hotomski@ifi.uzh.ch; Martin=Glinz: glinz@ifi.uzh.ch,"Abstract
Context
When software-based systems evolve, their requirements change. The changes in requirements affect the associated 
acceptance tests
, which should be adapted accordingly. In practice, however, requirements and their 
acceptance tests
 are not always kept up-to-date nor aligned. Such inconsistencies may introduce software quality problems, unintended costs and project delays.
Objective
In order to keep evolving requirements and their acceptance tests aligned, we are developing an approach called GuideGen. GuideGen automatically generates guidance in natural language about how to adapt the impacted acceptance tests when their requirements change.
Method
We have implemented GuideGen as a prototype tool and evaluated it in two studies: first, by assessing the correctness, completeness, 
understandability
 and relevance of the generated guidance using three data sets from industry and second, by assessing the applicability and usefulness of the approach and the tool with 23 practitioners from ten companies.
When a requirement having more than one associated acceptance test is changed, GuideGen currently generates guidance for all of them together. As a first step towards overcoming this limitation, we assessed how well existing methods for change impact analysis can identify the tests actually impacted by the changes in a requirement.
Results
In the first study, we found that GuideGen produced correct guidance in about 67 to 89 percent of all changes. Our approach performed better for agile requirements than for traditional ones. The results of the second study show that GuideGen is perceived to be useful, but that the practitioners would prefer a GuideGen plug-in for commercial tools instead of a standalone tool. Further, in our experiment we could correctly identify the affected acceptance tests for 63% to 91% of the changes in the requirements.
Conclusion
Our approach facilitates the alignment of acceptance tests with the actual requirements and can improve the communication between requirements engineers and testers.",21 Mar 2025,8,The development of GuideGen to automatically generate guidance for adapting acceptance tests can have a significant practical value for startups by improving alignment between requirements and tests.
https://www.sciencedirect.com/science/article/pii/S0950584918300739,Quality requirements challenges in the context of large-scale distributed agile: An empirical study,June 2019,Information and Software Technology,Not Found,Wasim=Alsaqaf: w.h.a.alsaqaf@utwente.nl; Maya=Daneva: m.daneva@utwente.nl; Roel=Wieringa: r.j.wieringa@utwente.nl,"Abstract
Context
Engineering quality requirements in agile projects does not fit organically with agile methods. Despite the agile community acknowledges this, little empirical evidence has been published on this topic.
Objective
This exploratory qualitative interview-based study explicates the challenging situations experienced by practitioners in engineering the quality requirements in the context of large-scale distributed agile projects. Moreover, this study describes the practices that agile distributed teams currently use which could contribute by dealing with the identified challenges.
Method
The challenging situations and possible mitigation practices were studied from the perspective of 17 practitioners from large distributed agile project teams in six organizations in The Netherlands. Qualitative data were collected using semi-structured, open-ended interviews. Qualitative coding techniques were used for data analysis, to identify the challenges of engineering quality requirements, the mechanisms behind the challenges and the practices used that could mitigate the impact of those challenges. Further, by using dialog mapping technique for qualitative data structuring, we have mapped the identified mechanisms and practices to the challenges.
Results
From the perspective of the participating practitioners, our 
exploratory study
 revealed 15 challenges classified in five categories: (1) team coordination and communication, (2) quality assurance, (3) quality 
requirements elicitation
, (4) conceptual challenges, and (5) software architecture. The study has also disclosed 13 mechanisms behind the challenges and 9 practices that could mitigate the impact of those challenges.
Conclusions
The main contributions of the paper are: (1) the explication of the challenges from practitioners’ perspective and the comparison of our findings with previously published results, (2) the description of the mechanisms behind the challenges, and (3) the identification of the practices currently used by agile teams that could mitigate the impact of the challenges. The findings of this study provide useful input into the process of designing possible solution approaches to overcome the challenges.",21 Mar 2025,6,The study on engineering quality requirements in agile projects provides insights that could be beneficial for early-stage ventures involved in large-scale distributed agile projects.
https://www.sciencedirect.com/science/article/pii/S095058491930031X,Investigation on test effort estimation of mobile applications: Systematic literature review and survey,June 2019,Information and Software Technology,Not Found,Anureet=Kaur: anumahal@gmail.com; Kulwant=Kaur: kulwantkaur@apjimtc.org,"Abstract
Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of 
agile methodology
, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.",21 Mar 2025,7,"The study provides insights into improving test estimation models for mobile applications, taking into account specific characteristics and suggestions from experienced developers and testers."
https://www.sciencedirect.com/science/article/pii/S0950584919300321,Towards functional change decision support based on COSMIC FSM method,June 2019,Information and Software Technology,Not Found,Mariem=Haoues: mariem.haoues@isims.usf.tn; Asma=Sellami: asma.sellami@isims.usf.tn; Hanêne=Ben-Abdallah: hbenabdallah@hct.ac.ae,"Abstract
Context:
 Managing requirements change is a central issue in the software development industry. In fact, inappropriate decisions about a change request may jeopardize the project development progress by going over budget/time or delivering a software with functional requirements that do not fully meet the user’s needs. Hence, a change decision support is required for the success of the software development.
Objective:
 This paper has a three-fold objective: (i) explore the applicability of the ISO standard COSMIC FSM method to evaluate a change request; (ii) investigate the use of estimation models to predict the effort required to handle a functional change and its impact on the initially estimated software development effort; and (iii) propose a decision support method that offers the appropriate information for the 
change advisory board
 members to decide whether to accept, deny or defer a functional change request.
Method:
 To guide the decision on a change request, the method proposed in this paper accounts for the most important factors when evaluating a change request, namely the functional change status, the preference of the change requester, and the effort required to handle the change. The functional change status is identified based on the sensitivity of the changed functionality and the functional size of the functional change. The functional change effort can be estimated using several ways including the COCOMO II model, the 
Simple Linear Regression
 Model and 
expert judgment
. Furthermore, this paper proposes a prototype to determine automatically the functional change status and offers pertinent information that the 
change advisory board
 can use to determine how to handle a change request. The use of the decision support method and tool is illustrated through three 
case studies
.
Results:
 A decision support method to help decision-makers respond to a functional change request is provided. This method takes into account the functional change status, the preference of the change requester and the functional change effort. The empirical evaluation of the proposed method is illustrated through three 
case studies
. The role of experiments here is primarily to provide a proof-of-concept rather than an exhaustive evaluation.
Conclusion:
 Using COSMIC FSM method, it is possible to identify functional changes leading to a potential impact on the software development progress. Based on the evaluation of the functional change, the change advisory board members can make judicious decisions about whether to accept, defer or deny a functional change request.",21 Mar 2025,8,"The paper addresses the important issue of managing requirements change in software development industry, offering a decision support method for change advisory board members to make informed decisions."
https://www.sciencedirect.com/science/article/pii/S0950584919300400,CaMeLOT: An educational framework for conceptual data modelling,June 2019,Information and Software Technology,Not Found,Daria=Bogdanova: daria.bogdanova@kuleuven.be; Monique=Snoeck: monique.snoeck@kuleuven.be,"Abstract
Context
Teaching 
conceptual data modelling
 (CDM) remains a challenging task for educators. Despite the fact that CDM is an integral part of 
software engineering
 curricula, there is no generally accepted educational framework for the subject. Moreover, the existing educational literature shows significant gaps when it comes to pursued 
learning outcomes
 and their assessment.
Objective
In this paper, we propose an educational framework for 
conceptual data modelling
, based on the revised Bloom's taxonomy of educational objectives, and provide necessary examples of systemized 
learning outcomes
.
Method
We utilized the revised Bloom's taxonomy to develop an adapted framework specifically for learning outcomes related to CDM. We validated the framework by mapping learning outcomes distilled from the existing course material to the framework, by presenting the framework for feedback to the experts in the field and further elaborating and refining it based on the feedback and experiences from these validation activities.
Results
CaMeLOT is an adaptation of the Bloom's taxonomy specifically for learning outcomes related to CDM. We identified different content areas and indicated the necessary scaffolding. Based on the framework, we worked out 17 example tables of learning outcomes related to content areas at different levels of scaffolding, exemplifying the different knowledge and cognitive levels. We clarify the differences in learning outcomes related to different knowledge and cognitive levels and thereby provide a domain specific clarification of the classification guidelines.
Conclusion
CaMeLOT gives educators an opportunity to enhance the CDM part of 
software engineering
 curricula with a systemized set of learning outcomes to be pursued, and open the path for creating more complete, useful and effective assessment packages. The adoption of our educational framework may reduce the time spent on designing educational material and, at the same time, improve its quality.",21 Mar 2025,6,"The educational framework proposed for conceptual data modeling can enhance software engineering curricula, providing systemized learning outcomes for educators to improve the quality of teaching."
https://www.sciencedirect.com/science/article/pii/S0950584919300424,Why is my code change abandoned?,June 2019,Information and Software Technology,Not Found,Qingye=Wang: wqyy@zju.edu.cn; Xin=Xia: xin.xia@monash.edu; David=Lo: davidlo@smu.edu.sg; Shanping=Li: shan@zju.edu.cn,"Abstract
Context
: Software developers contribute numerous changes every day to the code review systems. However, not all submitted changes are merged into a codebase because they might not pass the 
code review process
. Some changes would be abandoned or be asked for resubmission after improvement, which results in more workload for developers and reviewers, and more delays to deliverables.
Objective
: To understand the underlying reasons why changes are abandoned, we conduct an empirical study on the code review of four 
open source projects
 (Eclipse, LibreOffice, OpenStack, and Qt).
Method
: First, we manually analyzed 1459 abandoned changes. Second, we leveraged the open card sorting method to label these changes with reasons why they were abandoned, and we identified 12 categories of reasons. Next, we further investigated the frequency distribution of the categories across projects. Finally, we studied the relationship between the categories and time-to-abandonment.
Results
: Our findings include the following: (1) 
Duplicate
 changes are the majority of the abandoned changes; (2) the frequency distribution of abandoned changes across the 12 categories is similar for the four 
open source projects
; (3) 98.39% of the changes are abandoned within a year.
Conclusion
: Our study concluded the root causes of abandoned changes, which will help developers submit high-quality code changes.",21 Mar 2025,5,"The empirical study on abandoned changes in code review process offers insights on improving code quality, but may have limited direct impact on startups or early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919300436,FineLocator: A novel approach to method-level fine-grained bug localization by query expansion,June 2019,Information and Software Technology,Not Found,Wen=Zhang: zhangwen@bjut.buct.edu.cn; Ziqiang=Li: 2016200859@mail.buct.edu.cn; Qing=Wang: wq@itechs.iscas.ac.cn; Juan=Li: lijuan@bjut.edu.cn,"Abstract
Context
Bug localization, namely, to locate suspicious snippets from 
source code
 files for developers to fix the bug, is crucial for 
software quality assurance
 and software maintenance. Effective bug localization technique is desirable for software developers to reduce the effort involved in bug resolution. State-of-the-art bug localization techniques concentrate on file-level coarse-grained localization by lexical matching 
bug reports
 and 
source code files
. However, this would bring about a heavy burden for developers to locate feasible code snippets to make change with the goal of fixing the bug.
Objective
This paper proposes a novel approach called FineLocator to method-level fine-grained bug localization by using semantic similarity, temporal proximity and call dependency for method expansion.
Method
Firstly, the 
bug reports
 and the methods of 
source code
 are represented by numeric vectors using 
word embedding
 (word2vec) and the TF-IDF method. Secondly, we propose three 
query expansion
 scores as semantic similarity score, temporal proximity score and call dependency score to address the representation sparseness problem caused by the short lengths of methods in the source code. Then, the representation of a method with short length is augmented by elements of its neighboring methods with 
query expansion
. Thirdly, when a new bug report is incoming, FineLocator will retrieve the methods in source code by similarity ranking on the bug report and the augmented methods for bug localization.
Results
We collect bug repositories of ArgoUML, Maven, Kylin, Ant and AspectJ projects to investigate the performance of the proposed FineLocator approach. Experimental results demonstrate that the proposed FineLocator approach can improve the performances of method-level bug localization at average by 20%, 21% and 17% measured by Top-N indicator, 
MAP
 and MRR respectively, in comparison with state-of-the-art techniques.
Conclusion
This is the first paper to demonstrate how to make use of method expansion to address the representation sparseness problem for method-level fine-grained bug localization.",21 Mar 2025,9,"The proposed FineLocator approach for method-level bug localization demonstrates significant performance improvements, which can be valuable for startups aiming to enhance software quality assurance."
https://www.sciencedirect.com/science/article/pii/S0950584919300461,Revisiting the refactoring mechanics,June 2019,Information and Software Technology,Not Found,Jonhnanthan=Oliveira: jonhnanthan@copin.ufcg.edu.br; Rohit=Gheyi: rohit@dsc.ufcg.edu.br; Melina=Mongiovi: melina@computacao.ufcg.edu.br; Gustavo=Soares: gustavo.soares@microsoft.com; Márcio=Ribeiro: marcio@ic.ufal.br; Alessandro=Garcia: afgarcia@inf.puc-rio.br,"Abstract
Context
Refactoring is a key practice in 
agile methodologies
 used by a number of developers, and available in popular IDEs. However, it is unclear whether the refactoring mechanics have the same meaning for developers.
Objective
In this article, we revisit the refactoring mechanics.
Method
We conduct a survey with 107 developers of popular Java projects on GitHub. We asked them about the output of seven refactoring types applied to small programs.
Results
Developers do not expect the same outputs in all questions. The refactoring mechanics is based on developers’ experience for a number of them (71.02%). Some developers (75.70%) use IDEs to apply refactorings. However, the output yielded by the preferred IDE is different from what they want.
Conclusion
Developers and IDE developers use different mechanics for most refactoring types considered in our survey, and this may impact developers’ communication.",21 Mar 2025,4,"The study on refactoring mechanics is important for developers, but the impact on early-stage ventures may be limited as it focuses more on developer communication."
https://www.sciencedirect.com/science/article/pii/S0950584919300503,Images don’t lie: Duplicate crowdtesting reports detection with screenshot information,June 2019,Information and Software Technology,Not Found,Junjie=Wang: wangjunjie@itechs.iscas.ac.cn; Mingyang=Li: limingyang@itechs.iscas.ac.cn; Song=Wang: song.wang@uwaterloo.ca; Tim=Menzies: tim@menzies.us; Qing=Wang: wq@itechs.iscas.ac.cn,"Abstract
Context
: Crowdtesting is effective especially when it comes to the feedback on GUI systems, or subjective opinions about features. Despite of this, we find crowdtesting reports are highly duplicated, i.e., 82% of them are duplicates of others. Most of the existing approaches mainly adopted textual information for 
duplicate detection
, and suffered from low accuracy because of the lexical gap. Our observation on real industrial crowdtesting data found that when dealing with crowdtesting reports of GUI systems, the reports would be accompanied with images, i.e., the screenshots of the tested app. We assume the screenshot to be valuable for duplicate crowdtesting report detection because it reflects the real context of the bug and is not affected by the variety of natural languages.
Objective
: We aim at automatically detecting duplicate crowdtesting reports that could help reduce triaging effort.
Method
: In this work, we propose SETU which combines information from the ScrEenshots and the TextUal descriptions to detect duplicate crowdtesting reports. We extract four types of features to characterize the screenshots (i.e., image structure feature and image color feature) and the textual descriptions (i.e., TF-IDF feature and 
word embedding
 feature), and design a hierarchical algorithm to detect duplicates based on the four similarity scores derived from the four features respectively.
Results
: We investigate the effectiveness of SETU on 12 projects with 3,689 reports from one of the Chinese largest crowdtesting platforms. Results show that recall@1 achieved by SETU is 0.44 to 0.79, recall@5 is 0.66 to 0.92, and 
MAP
 is 0.21 to 0.58 across all experimental projects. Furthermore, SETU can outperform existing state-of-the-art approaches significantly and substantially.
Conclusion
: Through combining the screenshots and textual descriptions, our proposed SETU can improve the duplicate crowdtesting reports detection performance.",21 Mar 2025,9,"The research on duplicate crowdtesting reports detection using SETU has high practical value for startups, as it can reduce triaging effort significantly and outperform existing approaches."
https://www.sciencedirect.com/science/article/pii/S0950584919300515,Reusability in goal modeling: A systematic literature review,June 2019,Information and Software Technology,Not Found,Mustafa Berk=Duran: berk.duran@mail.mcgill.ca; Gunter=Mussbacher: gunter.mussbacher@mcgill.ca,"Abstract
Context:
 Goal modeling is an important instrument for the 
elicitation
, specification, analysis, and validation of early requirements. Goal models capture hierarchical representations of stakeholder objectives, requirements, possible solutions, and their relationships to help requirements engineers understand 
stakeholder goals
 and explore solutions based on their impact on these goals. To reuse a goal model and benefit from the strengths of goal modeling, we argue that it is necessary (i) to make sure that analysis and validation of goal models is possible through reuse hierarchies, (ii) to provide the means to delay decision making to a later point in the reuse hierarchy, (iii) to take constraints imposed by other 
modeling notations
 into account during analysis, (iv) to allow context dependent information to be modeled so that the goal model can be used in various reuse contexts, and (v) to provide an interface for reuse.
Objective:
 In this two-part systematic literature review, we (i) evaluate how well existing goal modeling approaches support reusability with our five desired characteristics of contextual and reusable goal models, (ii) categorize these approaches based on language constructs for context modeling and connection to other modeling formalisms, and then (iii) draw our conclusions on future research themes.
Method:
 Following guidelines by Kitchenham, the review is conducted on seven major academic search engines. Research questions, 
inclusion criteria
, and categorization criteria are specified, and threats to validity are discussed. A final list of 146 publications and 34 comparisons/assessments of goal modeling approaches is discussed in more detail.
Results:
 Five major research themes are derived to realize reusable goal models with context dependent information.
Conclusion:
 The results indicate that existing goal modeling approaches do not fully address the required capabilities for reusability in different contexts and that further research is needed to fill this gap in the landscape of goal modeling approaches.",21 Mar 2025,8,The evaluation of existing goal modeling approaches for reusability provides insights that can benefit early-stage ventures in the requirements engineering process.
https://www.sciencedirect.com/science/article/pii/S0950584919300527,Reference Coupling: An exploration of inter-project technical dependencies and their characteristics within large software ecosystems,June 2019,Information and Software Technology,Not Found,Kelly=Blincoe: kblincoe@acm.org; Francis=Harrison: francish@uvic.ca; Navpreet=Kaur: nkaur@uvic.ca; Daniela=Damian: danielad@uvic.ca,"Abstract
Context
Software projects often depend on other projects or are developed in tandem with other projects. Within such software ecosystems, knowledge of cross-project technical dependencies is important for (1) practitioners understanding of the impact of their code change and coordination needs within the ecosystem and (2) researchers in exploring properties of software ecosystems based on these technical dependencies. However, identifying technical dependencies at the ecosystem level can be challenging.
Objective
In this paper, we describe Reference Coupling, a new method that uses solely the information in developers online interactions to detect technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We then use the output of this method to explore the properties of large software ecosystems.
Method
We validate our method on two datasets — one from open-source projects hosted on GitHub and one commercial dataset of IBM projects. We manually analyze the identified dependencies, categorize them, and compare them to dependencies specified by the development team. We examine the types of projects involved in the identified ecosystems, the structure of the identified ecosystems, and how the ecosystems structure compares with the social behavior of project contributors and owners.
Results
We find that our Reference Coupling method often identifies technical dependencies between projects that are untracked by developers. We describe empirical insights about the characteristics of large software ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. By exploring the socio-technical alignment within the GitHub ecosystems, we also found that the project owners social behavior aligns well with the technical dependencies within the ecosystem, but the project contributors social behavior does not align with these dependencies.
Conclusions
We conclude with a discussion on future research that is enabled by our Reference Coupling method.",21 Mar 2025,10,The Reference Coupling method for detecting technical dependencies between projects using developers' interactions can have a significant impact on startups by providing insights into software ecosystems and improving coordination needs.
https://www.sciencedirect.com/science/article/pii/S0950584919300047,Is deep learning better than traditional approaches in tag recommendation for software information sites?,May 2019,Information and Software Technology,Not Found,Pingyi=Zhou: zhou_pinyi@whu.edu.cn; Jin=Liu: jinliu@whu.edu.cn; Xiao=Liu: xiao.liu@deakin.edu.au; Zijiang=Yang: zijiang.yang@wmich.edu; John=Grundy: john.grundy@monash.edu,"Abstract
Context
Inspired by the success of 
deep learning
 in other domains, this new technique been gaining widespread recent interest in being applied to diverse data analysis problems in 
software engineering
. Many 
deep learning
 models, such as 
CNN
, 
DBN
, 
RNN
, 
LSTM
 and 
GAN
, have been proposed and recently applied to 
software engineering
 tasks including effort estimation, 
vulnerability analysis
, code clone detection, test case selection, requirements analysis and many others. However, there is a perception that applying 
deep learning
 is a ”silver bullet” if it can be applied to a software engineering data analysis problem.
Object
This motivated us to ask the question as to whether 
deep learning
 is better than traditional approaches in 
tag recommendation
 task for software information sites.
Method
In this paper we test this question by applying both the latest deep 
learning approaches
 and some traditional approaches on 
tag recommendation
 task for software information sites. This is a typical Software 
Engineering automation
 problem where intensive data processing is required to link disparate information to assist developers. Four different deep 
learning approaches
 – TagCNN, TagRNN, TagHAN and TagRCNN – are implemented and compared with three advanced traditional approaches – EnTagRec, TagMulRec, and FastTagRec.
Results
Our comprehensive experimental results show that the performance of these different deep learning approaches varies significantly. The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks.
Conclusion
Therefore, using appropriate deep learning approaches can indeed achieve better performance than traditional approaches in tag recommendation tasks for software information sites.",21 Mar 2025,7,The comparison of deep learning and traditional approaches for tag recommendation in software information sites can be valuable for startups looking to automate software engineering tasks.
https://www.sciencedirect.com/science/article/pii/S0950584919300059,"Formal Quality of Service assurances, ranking and verification of cloud deployment options with a probabilistic model checking method",May 2019,Information and Software Technology,Not Found,Petar=Kochovski: Not Found; Pavel D.=Drobintsev: Not Found; Vlado=Stankovski: vlado.stankovski@fgg.uni-lj.si,"Abstract
Context
: Existing software workbenches allow for the deployment of cloud applications across a variety of Infrastructure-as-a-Service (IaaS) providers. The expected workload, 
Quality of Service
 (QoS) and Non-Functional Requirements (NFRs) must be considered before an appropriate infrastructure is selected. However, this decision-making process is complex and time-consuming. Moreover, the software engineer needs assurances that the selected infrastructure will lead to an adequate QoS of the application.
Objective
: The goal is to develop a new method for selection of an optimal cloud 
deployment option
, that is, an infrastructure and configuration for deployment and to verify that all hard and as many soft QoS requirements as possible will be met at runtime.
Method
: A new Formal QoS Assurances Method (FoQoSAM), which relies on stochastic Markov models is introduced to facilitate an automated decision-making process. For a given workload, it uses QoS 
monitoring data
 and a user-related metric in order to automatically generate a probabilistic model. The probabilistic model takes the form of a 
finite automaton
. It is further used to produce a rank list of cloud deployment options. As a result, any of the cloud deployment options can be verified by applying a probabilistic model checking approach.
Results
: Testing was performed by ranking deployment options for two cloud applications, File Upload and Video-conferencing. The FoQoSAM method was compared to a baseline 
Analytic Hierarchy Process
 (AHP). The results show that the first ranked cloud deployment options satisfy all hard and at least one of the soft requirements for both methods, however, the FoQoSAM method always satisfies at least an additional QoS requirement compared to the baseline AHP method.
Conclusions
: The proposed new FoQoSAM method is appropriate and can be used in decision-making when ranking and verifying cloud deployment options. Due to its practical utility it was integrated into the SWITCH workbench.",21 Mar 2025,5,"The development of a new method for selecting optimal cloud deployment options can be beneficial for early-stage ventures utilizing cloud services, but the impact may be limited to specific scenarios."
https://www.sciencedirect.com/science/article/pii/S0950584919300035,Deriving architectural models from requirements specifications: A systematic mapping study,May 2019,Information and Software Technology,Not Found,Eric=Souza: er.souza@campus.fct.unl.pt; Ana=Moreira: amm@fct.unl.pt; Miguel=Goulão: mgoul@fct.unl.pt,"Abstract
Context
Software architecture design
 creates and documents the high-level structure of a software system. Such structure, expressed in 
architectural models
, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive 
architectural models
 from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects.
Object
The purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation.
Method
To achieve this goal, we performed a 
systematic mapping study
 following the good practices from the Evidence-Based 
Software Engineering
 field.
Results
This study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved.
Conclusion
The major findings indicate that current architectural derivation methods rely heavily on the architects’ 
tacit knowledge
 (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners.",21 Mar 2025,3,"While providing an overview of existing methods for deriving architectural models, the paper focuses more on research challenges and lacks direct practical application for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919300072,On the need to update systematic literature reviews,May 2019,Information and Software Technology,Not Found,Vilmar=Nepomuceno: vsn@cin.ufpe.br; Sergio=Soares: scbs@cin.ufpe.br,"Abstract
Context
Many 
Systematic Literature Reviews
 (SLRs) were performed in the recent past, but just a few are being updated. Keeping SLRs updated is essential to prolong their lifespan.
Objective
To give a picture about how SLRs are being updated and what researchers think about SLRs updates.
Method
In this work, we present a Systematic Mapping (SM) study about SLRs updates and a survey with 
EBSE
 researchers that published their SLRs between 2011 and 2015.
Results
We included 22 studies in the 
SM
, where 15 changed some artifact from the original study, including changes in research questions. We obtained 28 answers in our survey with SLRs authors that, in general, consolidate interpretations retrieved from the 
SM
, but some answers did not.
Conclusion
SLRs may lose their impact over the years. Identifying actions to keep them updated is of great importance to SLR research field.",21 Mar 2025,3,"The study on keeping Systematic Literature Reviews updated is relevant for research methods, but the direct impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584919300096,"Euphoria: A Scalable, event-driven architecture for designing interactions across heterogeneous devices in smart environments",May 2019,Information and Software Technology,Not Found,Ovidiu-Andrei=Schipor: schipor@eed.usv.ro; Radu-Daniel=Vatavu: http://www.eed.usv.ro/~vatavu; Jean=Vanderdonckt: jean.vanderdonckt@uclouvain.be,"Abstract
Context:
 From personal mobile and wearable devices to public ambient displays, our 
digital ecosystem
 has been growing with a large variety of smart sensors and devices that can capture and deliver insightful data to 
connected applications
, creating thus the need for new software architectures to enable fluent and flexible interactions in such smart environments.
Objective:
 We introduce 
Euphoria
, a new 
software architecture design
 and implementation that enables easy prototyping, deployment, and evaluation of adaptable and flexible interactions across heterogeneous devices in smart environments.
Method:
 We designed 
Euphoria
 by following the requirements of the ISO/IEC 25010:2011 standard on Software Quality Requirements and Evaluation applied to the specific context of smart environments.
Results:
 To demonstrate the adaptability and flexibility of 
Euphoria
, we describe three application scenarios for contexts of use involving multiple users, multiple input/output devices, and various types of smart environments, as follows: (1) wearable user interfaces and whole-body gesture input for interacting with public ambient displays, (2) multi-device interactions in physical-digital spaces, and (3) interactions on smartwatches for a connected car application scenario. We also perform a technical evaluation of 
Euphoria
 regarding the main factors responsible for the magnitudes of the request-response times for producing, broadcasting, and consuming messages inside the architecture. We deliver the source code of 
Euphoria
 free to download and use for research purposes.
Conclusion:
 By introducing 
Euphoria
 and discussing its applicability, we hope to foster advances and developments in new software architecture initiatives for our increasingly complex smart environments, but also to readily support implementations of novel interactive 
systems and applications
 for smart environments of all kinds.",21 Mar 2025,7,The introduction of Euphoria software architecture for smart environments and its applicability can have a significant impact on early-stage ventures looking to develop interactions across heterogeneous devices.
https://www.sciencedirect.com/science/article/pii/S0950584919300102,A new benchmark for evaluating pattern mining methods based on the automatic generation of testbeds,May 2019,Information and Software Technology,Not Found,B.=Bafandeh Mayvan: Not Found; A.=Rasoolzadegan: rasoolzadegan@um.ac.ir; A.M.=Ebrahimi: Not Found,"Abstract
Context
Mining patterns is one of the most attractive topics in the field of software design. Knowledge about the number, type, and location of pattern instances is crucial to understand the original design decisions. Several techniques and tools have been presented in the literature for mining patterns in a software system. However, evaluating the quality of the detection results is usually done manually or subjectively. This can significantly affect the evaluation results. Therefore, a fair comparison of the quality of the various mining methods is not possible.
Objective
This paper describes a new benchmark to evaluate pattern mining methods in source code or design. Our work aims at overcoming the challenges faced in benchmarking in pattern detection. The proposed benchmark is comprehensive, fair, and objective, with a repeatable evaluation process.
Method
Our proposed benchmark is based on automatic generation of testbeds using graph theory. The generated testbeds are Java source codes and their corresponding class diagrams in which various types of patterns and their variants are inserted in different locations. The generated testbeds differ in their levels of complexity and full information is available on the utilized patterns.
Results
The results show that our proposed benchmark is able to evaluate the pattern mining methods quantitatively and objectively. Also, it can be used to compare pattern mining methods in a fair and repeatable manner.
Conclusions
Based on our findings, it can be argued that benchmarking in the pattern mining field is significantly less mature than topics such as presenting a new detection method. Therefore, special attention is needed in the pattern evaluation topic. Our proposed benchmark is a step towards achieving a comparative understanding of the effectiveness of detection methods and demonstrating their strengths and weaknesses.",21 Mar 2025,6,The new benchmark for evaluating pattern mining methods in software design can provide valuable insights for early-stage ventures seeking to improve their software quality and design.
https://www.sciencedirect.com/science/article/pii/S0950584919300084,Mining software repositories for adaptive change commits using machine learning techniques,May 2019,Information and Software Technology,Not Found,Omar=Meqdadi: ommeqdadi@just.edu.jo; Nouh=Alhindawi: hindawi@jadara.edu.jo; Jamal=Alsakran: j.alsakran@ju.edu.jo; Ahmad=Saifan: ahmads@yu.edu.jo; Hatim=Migdadi: hatims@hu.edu.jo,"Abstract
Context
Version Control Systems, such as Subversion, are standard repositories that preserve all of the maintenance changes undertaken to source code artifacts during the evolution of a software system. The documented data of the version history are organized as commits; however, these commits do not keep a tag that would identify the purpose of the relevant undertaken change of a commit, thus, there is rarely enough detail to clearly direct developers to the changes associated with a specific type of maintenance.
Objective
This work examines the version histories of an 
open source system
 to automatically classify version commits into one of two categories, namely adaptive commits and non-adaptive commits.
Method
We collected the commits from the version history of three open source systems, then we obtained eight different code change metrics related to, for example, the number of changed statements, methods, hunks, and files. Based on these change metrics, we built a 
machine learning approach
 to classify whether a commit was adaptive or not.
Results
It is observed that code change metrics can be indicative of adaptive maintenance activities. Also, the classification findings show that the 
machine learning
 classifier developed has approximately 75% prediction accuracy within labeled change histories.
Conclusion
The proposed method automates the process of examining the version history of a software system and identifies which commits to the system are related to an adaptive maintenance task. The evaluation of the method supports its applicability and efficiency. Although the evaluation of the proposed classifier on unlabeled change histories shows that it is not much better than the random guessing in terms of 
F
-measure, we feel that our classifier would serve as a better basis for developing advanced classifiers that have 
predictive power
 of adaptive commits without the need of manual efforts.",21 Mar 2025,7,The automated classification of version commits into adaptive and non-adaptive categories using machine learning can significantly streamline software development processes for startups.
https://www.sciencedirect.com/science/article/pii/S095058491930028X,A model of requirements engineering in software startups,May 2019,Information and Software Technology,Not Found,Jorge=Melegati: jmelegatigoncalves@unibz.it; Alfredo=Goldman: Not Found; Fabio=Kon: Not Found; Xiaofeng=Wang: Not Found,"Abstract
Context
Over the past 20 years, software startups have created many products that have changed human life. Since these companies are creating brand-new products or services, requirements are difficult to gather and highly volatile. Although scientific interest in software development in this context has increased, the studies on 
requirements engineering
 in software startups are still scarce and mostly focused on elicitation activities.
Objective
This study overcomes this gap by answering how 
requirements engineering
 practices are performed in this context.
Method
We conducted a grounded theory study based on 17 interviews with software startups practitioners.
Results
We constructed a model to show that software startups do not follow a single set of practices but, instead, build a custom process, changed throughout the development of the company, combining different practices according to a set of influences (Founders, Software Development Manager, Developers, Market, Business Model and Startup Ecosystem).
Conclusion
Our findings show that requirements engineering activities in software startups are similar to those in agile teams, but some steps vary as a consequence of the lack of an accessible customer.",21 Mar 2025,8,"The study provides insights into how requirements engineering practices are carried out in software startups, offering valuable knowledge for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584919300291,Empirical evaluation and proposals for bands-based COSMIC early estimation methods,May 2019,Information and Software Technology,Not Found,Sandro=Morasca: Not Found,"Abstract
Background
. In the early phases of software development projects, thorough application of the 
COSMIC
 functional size measurement method may require more time and effort than available. Thus, early approximate methods have been proposed for estimating the 
COSMIC
 functional size of an application, instead of measuring it.
Objective
. The goal of this paper is to empirically evaluate the accuracy of the COSMIC early size estimation methods that are based on evaluations at the functional process level, for which 
historical data
 are available. The goal is to provide practitioners with empirical evidence on the accuracy of these methods.
Method
. We evaluated the Average Functional Process and the Equal Size Bands methods. We also proposed and evaluated two new approaches for defining bands in the Fixed Size Classification method. The estimation was performed by applying these methods to a set of 
software applications
 for which the data necessary to perform estimations were available, having been previously measured according to the standard COSMIC method.
Results
. Our analyses show that the Average Functional Process method generally provides estimates that are reasonable for early and quick sizing, but in some cases its 
estimation errors
 are too large to be acceptable. On the contrary, the methods using bands can provide quite accurate estimates. We determine the level of accuracy that can be obtained based on the type of method used, the number of bands used, and the quantitative characterization of the ability to classify each functional process in the correct band.
Conclusions
. The Average Functional Process method may be unreliable, as it occasionally yields quite large errors. Organizations using bands-based methods cannot just follow the prescribed estimation process: they need to properly train people in charge of classifying functional processes in the correct size band.",21 Mar 2025,6,"The evaluation of early size estimation methods in software development projects can help startups make informed decisions, but the reliance on historical data may limit its practical applicability."
https://www.sciencedirect.com/science/article/pii/S0950584918302477,Model-based test suite generation for graph transformation system using model simulation and search-based techniques,April 2019,Information and Software Technology,Not Found,Akram=Kalaee: a-kalaee@arshad.araku.ac.ir; Vahid=Rafe: v-rafe@araku.ac.ir,"Abstract
Context
Test generation by model checking is a useful technique in model-based testing that allows automatic generation of test cases from models by utilizing the counter-examples/witnesses produced through a 
model checker
. However, generating redundant test cases and state space explosion problem are two major obstacles to transfer this technique into industrial practice.
Objective
An idea to cope with these challenges consists in an intelligent model checking for exploring only a portion of the state space according to the test objectives. Motivated by this idea, we propose an approach that exploits meta-heuristic algorithms to adapt a 
model checker
 when used for integration testing of systems formally specified by graph transformations.
Method
This method is not based on 
model checking algorithms
, but rather uses the 
modeling and simulation
 features of the underlying model checker. In the proposed approach, a population of test suites that each of which is a set of paths on the state space, is evolved towards satisfying the all def-use test objectives. Consequently, a test suite with high coverage is generated.
Results
To assess the efficiency of our approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Empirical results based on some 
case studies
, confirm a significant improvement in terms of coverage, speed and memory usage, in comparison with the 
state of the art techniques
.
Conclusion
Our analysis reveals that intelligent model checking can appropriately address the challenges of traditional model-checking-assisted testing. We further conclude that graph transformation specification is an efficient modeling solution to behavioral testing and graph transformation tools have a great potential for developing a model-based testing tool.",21 Mar 2025,9,"The proposed approach for intelligent model checking demonstrates significant improvements in test coverage, speed, and memory usage, which can greatly benefit software startups in system integration testing."
https://www.sciencedirect.com/science/article/pii/S0950584918302489,Software feature refinement prioritization based on online user review mining,April 2019,Information and Software Technology,Not Found,Jianzhang=Zhang: jianzhang.zhang2017@gmail.com; Yinglin=Wang: wang.yinglin@shufe.edu.cn; Tian=Xie: xietiansh@gmail.com,"Abstract
Context
Online software reviews have provided a wealth of user feedback on 
software applications
. User reviews along with ratings have been influential in a series of 
software engineering
 tasks e.g. software maintenance and release planning.
Objective
Our research aims to assist managers in prioritizing features to be refined in next release from the perspective of enhancing user ratings via mining online reviews.
Method
We first extract software features from user reviews and determine their probability distribution in each review with 
LDA
. Then the ground truth rating of each feature is estimated by linear regression under the assumption that the software functionality rating is a 
convex combination
 of all feature ratings weighted by their distribution probabilities over the review. Finally, we formalize feature refinement prioritization as an 
optimization problem
 which maximizes user group’s rating on the software functionality under the constraint of development budget.
Results
The proposed approach can use topic model to jointly extract features from user reviews semi-supervisedly and determine each feature’s weight in each user’s rating on the software functionality. The estimated ground truth ratings of all features reveal how reviewer group evaluate those features. Finally, we provide an illustrative example to demonstrate the key idea of our framework.
Conclusion
Our proposed framework is general to various software products with mass user reviews and semi-automatic without much human efforts and intervention. The framework’s 
interpretability
 helps managers better understand user feedback on the software functionality and make feature refinement plan for the upcoming releases.",21 Mar 2025,8,"The research on prioritizing feature refinements based on user ratings from online reviews can assist software managers in making data-driven decisions for product enhancement, offering practical value to startups."
https://www.sciencedirect.com/science/article/pii/S0950584918302490,A survey on software testability,April 2019,Information and Software Technology,Not Found,Vahid=Garousi: vahid.garousi@wur.nl; Feyza Nur=Kılıçaslan: feyzanur@cs.hacettepe.edu.tr,"Abstract
Context
Software testability is the degree to which a software system or a unit under test supports its own testing. To predict and improve software testability, a large number of techniques and metrics have been proposed by both practitioners and researchers in the last several decades. Reviewing and getting an overview of the entire state-of-the-art and state-of-the-practice in this area is often challenging for a practitioner or a new researcher.
Objective
Our objective is to summarize the body of knowledge in this area and to benefit the readers (both practitioners and researchers) in preparing, measuring and improving software testability.
Method
To address the above need, the authors conducted a survey in the form of a systematic literature mapping (classification) to find out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our final pool included 208 papers (published between 1982 and 2017).
Results
The area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors affecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability.
Conclusion
This paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess 
potential benefits
 of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneficial in their testing activities. Our results can also benefit researchers in observing the trends in this area and identify the topics that require further investigation.",21 Mar 2025,7,"The abstract provides a comprehensive review of software testability, offering valuable insights for both researchers and practitioners. The practical recommendations and positive feedback from industrial collaborators demonstrate its potential impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302507,A systematic mapping study of infrastructure as code research,April 2019,Information and Software Technology,Not Found,Akond=Rahman: aarahman@ncsu.edu; Rezvan=Mahdavi-Hezaveh: Not Found; Laurie=Williams: Not Found,"Abstract
Context:
 Infrastructure as code (IaC) is the practice to automatically configure system dependencies and to provision local and remote instances. Practitioners consider IaC as a fundamental pillar to implement 
DevOps
 practices, which helps them to rapidly deliver software and services to end-users. Information technology (IT) organizations, such as GitHub, Mozilla, Facebook, Google and Netflix have adopted IaC. A 
systematic mapping study
 on existing IaC research can help researchers to identify potential research areas related to IaC, for example defects and security flaws that may occur in IaC scripts.
Objective:
 The objective of this paper is to help researchers identify research areas related to infrastructure as code (IaC) by conducting a 
systematic mapping study
 of IaC-related research.
Method:
 We conduct our research study by searching five scholar databases. We collect a set of 31,498 publications by using seven search strings. By systematically applying inclusion and exclusion criteria, which includes removing duplicates and removing non-English and non peer-reviewed publications, we identify 32 publications related to IaC. We identify topics addressed in these publications by applying qualitative analysis.
Results:
 We identify four topics studied in IaC-related publications: (i) framework/tool for infrastructure as code; (ii) adoption of infrastructure as code; (iii) empirical study related to infrastructure as code; and (iv) testing in infrastructure as code. According to our analysis, 50.0% of the studied 32 publications propose a framework or tool to implement the practice of IaC or extend the functionality of an existing IaC tool.
Conclusion:
 Our findings suggest that framework or tools is a well-studied topic in IaC research. As defects and security flaws can have serious consequences for the deployment and development environments in 
DevOps
, we observe the need for research studies that will study defects and security flaws for IaC.",21 Mar 2025,8,The systematic mapping study on Infrastructure as code (IaC) provides a valuable overview of research areas in a rapidly developing field. The identification of common topics and the emphasis on defects and security flaws address practical concerns for startups and early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584918302519,Impact of model notations on the productivity of domain modelling: An empirical study,April 2019,Information and Software Technology,Not Found,Cristina=Cachero: ccachero@dlsi.ua.es; Santiago=Meliá: santi@dlsi.ua.es; Jesús M.=Hermida: jhermida@dlsi.ua.es,"Abstract
Context
The intensive use of models is a cornerstone of the Model-Driven Engineering (MDE) paradigm and its claimed gains in productivity. However, in order to maximize these productivity gains, it is important to adequately select the modeling formalism to be used. Unfortunately, the MDE community still lacks empirical data to support such choice.
Objective
This paper aims at contributing to filling this gap by reporting an empirical study in which two types of domain model notations, graphical vs. textual, are compared regarding their efficiency and effectiveness during the creation of domain models.
Method
A quasi-experiment was designed in which 127 participants were randomly classified in four groups. Then, each group was randomly assigned to a different combination of notation and application. All the participants were students enrolled in the 6th semester of the Computer Engineering degree at the University of Alicante. The statistical procedure applied was a two-factor multivariate analysis of variance (two-way MANOVA).
Results
The data shows a statistically significant effect of notation type on the efficiency and effectiveness of domain modelling activities, independently from the application being modelled.
Conclusion
The joint examination of our results and those of previous studies suggests that, in MDE, different tasks call for different types of notations. Therefore, MDE environments should offer both textual and graphical notations, and assist developers in selecting the most suitable one depending on the task being carried out. In particular, our data suggest that domain model creation tasks are better supported by graphical notations. To augment the validity of the conclusions of this paper, the experiment should be replicated with different subject profiles, notations, domain model sizes, tasks and application types.",21 Mar 2025,6,"The empirical study comparing graphical vs. textual domain model notations contributes to the MDE community. While the findings offer insights into notation efficiency, the practical implications for early-stage ventures may require further exploration and validation in real-world settings."
https://www.sciencedirect.com/science/article/pii/S095058491830243X,Little’s law based validation framework for load testing,April 2019,Information and Software Technology,Not Found,Raghu=Ramakrishnan: raghuramakrishnan71@gmail.com; Arvinder=Kaur: arvinder70@gmail.com,"Abstract
Context:
 Performance is a key quality consideration for large-scale software systems which supports thousands of concurrent users. Load testing is an integral part of the 
development lifecycle
 and is used to address performance issues before deploying the system in production. But, how do we validate the output reported by load testing tools? Little’s Law is useful for validating the accuracy of load testing output. Though IT industry is flooded with various enterprise and 
open source tools
 for load testing, but they do not offer support for validation of its result using Little’s Law. 
Manual validation
 is time intensive and infeasible with increase in the complexity of testing scripts.
Objective:
 In this paper we provide a Little’s law-based validation framework which will enable the researchers and industry practitioners to validate load testing results. The implementation of the framework is also demonstrated.
Method:
 To understand the constructs commonly used in 
load test
 scripts, we analysed scripts of two open source 
benchmark applications
 and eight large-scale software systems used in industry. We found that transactions are arranged using control flow patterns like sequential, loop and conditional. Based on the analysis, we devised the framework.
Results:
 The efficacy of the proposed framework is successfully evaluated on two systems - an open source Dell DVD Store benchmarking application and a real-world 
large scale system
 used in industry. The framework is independent of load testing tool used and can be used with complex testing scripts.
Conclusions
 There are no known frameworks or inbuilt support in existing load testing tools for guiding practitioners on applying Little’s Law using output generated by tools. We address this significant gap by providing a framework which combines information from test scripts/reports and Little’s Law to determine whether the results are valid. The provided implementation can be easily integrated with existing load testing tools.",21 Mar 2025,9,The Little's law-based validation framework for load testing results fills a significant gap in the industry. The practical application and successful evaluation on real-world systems highlight its potential impact on improving performance testing practices for startups and early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584918302593,Live programming in practice: A controlled experiment on state machines for robotic behaviors,April 2019,Information and Software Technology,Not Found,Miguel=Campusano: mcampusa@dcc.uchile.cl; Johan=Fabry: jfabry@gmail.com; Alexandre=Bergel: abergel@dcc.uchile.cl,"Abstract
Context
Live programming environments are gaining momentum across multiple programming languages. A tenet of live programming is a development feedback cycle, resulting in faster development practices. Although practitioners of live programming consider it a positive inclusion in their workflow, no in-depth investigations have yet been conducted on its benefits in a realistic scenario, nor using complex API.
Objective
This paper carefully studies the advantage of using live programming in defining nested state machines for robot behaviors. We analyzed two important aspects of developing robotic behaviors using these machines: 
program comprehension
 and program writing. We analyzed both development practices in terms of speed and accuracy.
Method
We conducted two controlled experiments, one for 
program comprehension
 and another for program writing. We measured the speed and accuracy of randomized assigned participants on completing programming tasks, against a baseline.
Results
In a robotic behavior context, we found that a live programming system for nested state machine programs does not significantly outperform a non-live language in program comprehension nor in program writing in terms of speed and accuracy. However, the feedback of test subjects indicates their preference for the live programming system.
Conclusions
The results of this work seem to contradict the studies of live programming in other areas, even while participants still favor using live programming techniques. We learned that the complex API chosen in this work has a strong negative influence on the results. To the best of our knowledge, this is the first in-depth live programming experiment in a complex domain.",21 Mar 2025,5,"The study on live programming in robotic behaviors provides valuable insights, but the conflicting results and negative influence of a complex API limit its immediate practical relevance for early-stage ventures. Further research may be needed to validate the findings in different contexts."
https://www.sciencedirect.com/science/article/pii/S0950584918302623,Machine learning techniques for code smell detection: A systematic literature review and meta-analysis,April 2019,Information and Software Technology,Not Found,Muhammad Ilyas=Azeem: azeem@itechs.iscas.ac.cn; Fabio=Palomba: palomba@ifi.uzh.ch; Lin=Shi: shilin@itechs.iscas.ac.cn; Qing=Wang: wq@itechs.iscas.ac.cn,"Abstract
Background
: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of 
machine learning techniques
 represents an ever increasing research area.
Objective
: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how 
machine learning approaches
 have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of 
machine learning approaches
 in the field of code smells.
Method
: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far.
Results
: The analyses performed show that 
God Class, Long Method, 
Functional Decomposition
, and 
Spaghetti Code
 have been heavily considered in the literature. 
Decision Trees
 and 
Support Vector Machines
 are the most commonly used 
machine learning algorithms
 for code smell detection. Models based on a large set of independent variables have performed well. 
JRip
 and 
Random Forest
 are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future.
Conclusion
: Based on our findings, we argue that there is still room for the improvement of 
machine learning techniques
 in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.",21 Mar 2025,8,"This abstract addresses an important issue in software development with practical implications for code smell detection, utilizing machine learning techniques to improve the process. The results provide valuable insights into the performance of different machine learning algorithms and open issues for future research."
https://www.sciencedirect.com/science/article/pii/S0950584919300011,The current state of software license renewals in the I.T. industry,April 2019,Information and Software Technology,Not Found,Aindrila=Ghosh: aindrila@ualberta.ca; Mona=Nashaat: nashaata@ualberta.ca; James=Miller: jimm@ualberta.ca,"Abstract
Context
The software industry has changed significantly in the 21st century; no longer is it dominated by organizations seeking to sell products directly to customers, instead most 
multinational organizations
 nowadays provide services via licensing agreements. These licenses are for a fixed-duration; and hence, the question of their renewal becomes of 
paramount importance
 for the selling organization’s revenue.
Objective
Despite its financial impact, the topic of license renewal strategies, processes, tools, and support receives very limited attention in the research literature. Hence, it is believed that an interesting research question is: What is the state of current industrial practice in this essential field?
Method
To initially explore the topic of license renewals, this paper implements the 
Grounded theory method
. To implement the method, semi-structured, cross-sectional, anonymous, selfreported interviews are carried out with 20 professionals from multiple organizations, later the Constant Comparative Method is used to analyse the 
collected data
.
Results
This paper presents a synthesized picture of the current industrial practice of the end-to-end software license 
renewal process
. Alongside, it also identifies a set of challenges and risk factors that impact on renewal decisions of customers, hence on the overall revenue of seller organizations. Finally, using structured brainstorming techniques, this paper identifies 11 future research directions, that can help organizations with the mitigation of the risks in the license 
renewal process
.
Conclusion
It is concluded that lack of effective communication among stakeholders, the absence of customer trust, and scarcity of value generated from purchased licenses are among the primary drivers that influence renewal decisions. Also, there is a need to invest in intelligent automation along with 
artificial intelligence
 enabled analytics in order to enhance customer satisfaction.",21 Mar 2025,6,"The abstract covers an important topic related to license renewal processes in the software industry. The study identifies challenges and provides future research directions, which can benefit organizations in revenue generation. However, the practical impact may be more limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058491830209X,An extensible collaborative framework for monitoring software quality in critical systems,March 2019,Information and Software Technology,Not Found,Marisol=García-Valls: mvalls@it.uc3m.es; Julio=Escribano-Barreno: jebarreno@indra.es; Javier=García-Muñoz: 100291551@alumnos.uc3m.es,"Abstract
Context
Current practices on software quality monitoring for critical software systems development rely on the manual integration of the information provided by a number of independent commercial tools for code analysis; some external tools for code analysis are mandatory in some critical software projects that must comply with specific norms. However, there are no approaches to providing an integrated view over the analysis results of independent external tools into a unified software quality framework.
Objective
This paper presents the design and development of ESQUF (Enhanced Software Quality Monitoring Framework) suitable for critical software systems. It provides the above enriched quality results presentation derived not only from multiple external tools but from the local analysis functions of the framework.
Method
An analysis of the norms and standards that apply to critical software systems is provided. The detailed and modular design of ESQUF adjusts to the integration requirements for external tools. UML is used for designing the framework, and Java is used to provide the detailed design. The framework is validated with a prototype implementation that integrates two different external tools and their respective quality results over a real software project 
source code
.
Results
The integration of results files and data from external tools as well as from internal analysis functions is enabled. The analysis of critical software projects is made posible yielding a 
collaborative space
 where 
verification engineers
 
share information
 about code analysis activities of specific projects; and single 
presentation space
 with rich static and dynamic analysis information of software projects that comply with the required development norms.",21 Mar 2025,7,"The abstract introduces a framework for software quality monitoring, addressing the integration of multiple tools for code analysis. This has practical value for critical software systems development and enables collaborative sharing of analysis information. The results provide a foundation for effective software quality assessment."
https://www.sciencedirect.com/science/article/pii/S0950584918302210,Slimming javascript applications: An approach for removing unused functions from javascript libraries,March 2019,Information and Software Technology,Not Found,H.C.=Vázquez: hvazquez@exa.unicen.edu.ar; A.=Bergel: abergel@dcc.uchile.cl; S.=Vidal: svidal@exa.unicen.edu.ar; J.A.=Díaz Pace: adiaz@exa.unicen.edu.ar; C.=Marcos: cmarcos@exa.unicen.edu.ar,"Abstract
Context
A 
common practice
 in JavaScript development is to ship and deploy an application as a large file, called 
bundle
, which is the result of combining the application code along with the code of all the libraries the application depends on. Despite the benefits of having a single bundle per application, this approach leads to applications being shipped with significant portions of code that are actually not used, which unnecessarily inflates the JavaScript bundles and could slow down website loading because of the extra unused code. Although some 
static analysis
 techniques exist for removing unused code, our investigations suggest that there is still room for improvements.
Objective
The goal of this paper is to address the problem of reducing the size of bundle files in JavaScript applications.
Method
In this context, we define the notion of Unused Foreign Function (UFF) to denote a JavaScript function contained in dependent libraries that is not needed at runtime. Furthermore, we propose an approach based on dynamic analysis that assists developers to identify and remove UFFs from JavaScript bundles.
Results
We report on a case-study performed over 22 JavaScript applications, showing evidence that our approach can produce size reductions of 26% on average (with reductions going up to 66% in some applications).
Conclusion
It is concluded that removing unused foreign functions from JavaScript bundles helps reduce their size, and thus, it can boost the results of existing 
static analysis
 techniques.",21 Mar 2025,9,"This abstract focuses on optimizing JavaScript bundle file sizes by removing unused code, which is a prevalent issue in web development. The proposed approach shows significant size reductions in real applications, demonstrating practical relevance and immediate impact on optimizing website loading times."
https://www.sciencedirect.com/science/article/pii/S0950584918302222,API recommendation for event-driven Android application development,March 2019,Information and Software Technology,Not Found,Weizhao=Yuan: weizhaoy@163.com; Hoang H.=Nguyen: mr.erichoang@gmail.com; Lingxiao=Jiang: lxjiang@smu.edu.sg; Yuting=Chen: chenyt@cs.sjtu.edu.cn; Jianjun=Zhao: zhao@ait.kyushu-u.ac.jp; Haibo=Yu: haibo_yu@sjtu.edu.cn,"Abstract
Context
Software development is increasingly dependent on existing libraries. Developers need help to find suitable library APIs. Although many studies have been proposed to recommend relevant functional APIs that can be invoked for implementing a functionality, few studies have paid attention to an orthogonal need associated with event-driven programming frameworks, such as the 
Android
 framework. In addition to invoking functional APIs, Android developers need to know where to place functional code according to various events that may be triggered within the framework.
Objective
This paper aims to develop an API recommendation engine for 
Android application
 development that can recommend both (1) functional APIs for implementing a functionality and (2) the event callback APIs that are to be overridden to contain the functional code.
Method
We carry out an empirical study on actual Android programming questions from StackOverflow to confirm the need of recommending callbacks. Then we build Android-specific API databases to contain the correlations among various functionalities and APIs, based on customized 
parsing
 of code snippets and 
natural language processing
 of texts in Android tutorials and SDK documents, and then textual and code similarity metrics are adapted for recommending relevant APIs.
Results
We have evaluated our prototype recommendation engine, named LibraryGuru, with about 1500 questions on Android programming from StackOverflow, and demonstrated that our top-5 results on recommending callbacks and functional APIs can on estimate achieve up to 43.5% and 50.9% respectively in precision, 24.6% and 32.5% respectively in 
mean average precision
 (MAP) scores, and 51.1% and 44.0% respectively in recall.
Conclusion
We conclude that it is important and possible to recommend both functional APIs and callbacks for 
Android application
 development, and future work is needed to take more data sources into consideration to make more relevant recommendations for developers’ needs.",21 Mar 2025,6,"The abstract presents an API recommendation engine for Android application development, addressing the need for recommending functional APIs and event callback APIs. While the results show promising precision, recall, and MAP scores, the practical impact may be more specific to Android developers and may have a narrower focus compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918302234,Adapting usability techniques for application in open source Software: A multiple case study,March 2019,Information and Software Technology,Not Found,Lucrecia=Llerena: lucrecia.llerena@estudiante.uam.es; Nancy=Rodriguez: nancy.rodriguez@estudiante.uam.es; John W.=Castro: john.castro@uda.cl; Silvia T.=Acuña: silvia.acunna@uam.es,"Abstract
Context
As a result of the growth of non-developer users of 
OSS applications
, usability has over the last ten years begun to attract the interest of the open source software (OSS) community. The 
OSS
 community has some special characteristics (such as worldwide geographical distribution of both users and developers and missing resources) which are an obstacle to the direct adoption of many usability techniques as specified in the human-computer interaction (HCI) field.
Objective
The aim of this research is to adapt and evaluate the feasibility of applying four usability techniques: user profiles, personas, direct observation and post-test information to four 
OSS projects
 from the viewpoint of the development team.
Method
The applied research method was a multiple 
case study
 of the following 
OSS projects
: Quite Universal Circuit Simulator, PSeInt, FreeMind and OpenOffice Writer.
Results
We formalized the application procedure of each of the adapted usability techniques. We found that either there were no procedures for adopting usability techniques in 
OSS
 or they were not fully systematized. Additionally, we identified the adverse conditions that are an obstacle to their adoption in OSS and propose the special adaptations required to overcome the obstacles. To avoid some of the adverse conditions, we created web artefacts (online survey, 
wiki
 and forum) that are very popular in the OSS field.
Conclusion
It is necessary to adapt usability techniques for application in OSS projects considering their idiosyncrasy. Additionally, we found that there are obstacles (for example, number of participant users, biased information provided by developers) to the application of the techniques. Despite these obstacles, it is feasible to apply the adapted techniques in OSS projects.",21 Mar 2025,7,"The research focuses on adapting usability techniques for OSS projects, addressing obstacles and proposing adaptations, which can be valuable for early-stage ventures looking to improve user experience and adoption."
https://www.sciencedirect.com/science/article/pii/S095058491830226X,On semantic detection of cloud API (anti)patterns,March 2019,Information and Software Technology,Not Found,Hayet=Brabra: hayet.brabra@telecom-sudparis.eu; Achraf=Mtibaa: achraf.mtibaa@enetcom.usf.tn; Fabio=Petrillo: fabio@petrillo.com; Philippe=Merle: philippe.merle@inria.fr; Layth=Sliman: layth.sliman@efrei.fr; Naouel=Moha: moha.naouel@uqam.ca; Walid=Gaaloul: walid.gaaloul@telecom-sudparis.eu; Yann-Gaël=Guéhéneuc: yann-gael.gueheneuc@polymtl.ca; Boualem=Benatallah: boualem@cse.unsw.edu.au; Faïez=Gargouri: faiez.gargouri@isims.usf.tn,"Abstract
Context
Open standards are urgently needed for enabling software interoperability in 
Cloud Computing
. Open 
Cloud Computing
 Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their 
understandability
 and 
reusability
.
Objective
We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles.
Method
First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)patterns and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility.
Results
We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an 
acceptable level
 of maturity regarding REST principles.
Conclusion
Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.",21 Mar 2025,8,"The study provides a method for enhancing REST management APIs in Cloud Computing, offering compliance evaluation and recommendations. This can benefit startups in developing interoperable, understandable, and reusable Cloud management APIs."
https://www.sciencedirect.com/science/article/pii/S0950584918302313,Information quality requirements engineering with STS-IQ,March 2019,Information and Software Technology,Not Found,Mohamad=Gharib: mohamad.gharib@unifi.it; Paolo=Giorgini: paolo.giorgini@unitn.it,"Abstract
Context
Information Quality (IQ) is particularly important for organizations: they depend on information for managing their daily tasks and relying on low-quality information may negatively influence their overall performance. Despite this, the literature shows that most software development approaches do not consider IQ requirements during the system design, which leaves the system open to different kinds of vulnerabilities.
Objective
The main objective of this research is proposing a framework for modeling and analyzing IQ requirements for Socio-Technical Systems (STS).
Method
We propose STS-IQ, a goal-oriented framework for modeling and analyzing IQ requirements in their social and organizational context since the early phases of the system design. The framework extends and refines our previous work, and it consists of: (i) a 
modeling language
 that provides concepts and constructs for modeling IQ requirements; (ii) a set of analysis techniques that support the verification of the correctness and consistency of the IQ requirements model; (iii) a mechanism for deriving the final IQ specifications in terms of IQ policies; (iv) a methodology to assist software engineers during the system design; and (v) a CASE tool, namely STS-IQ Tool.
Result
We demonstrated the applicability, usefulness, and scalability of the modeling and reasoning techniques within a stock market 
case study
, and we also evaluated the usability and utility of the framework with end-users.
Conclusion
We conclude that the STS-IQ framework supports the modeling and analysis of IQ requirements, and also the derivation of precise IQ specifications in terms of IQ policies. Therefore, we believe it has potential in practice.",21 Mar 2025,9,"The proposed framework for modeling and analyzing IQ requirements for Socio-Technical Systems can be highly valuable for organizations, including startups, to improve information quality and performance."
https://www.sciencedirect.com/science/article/pii/S0950584918302325,Exploratory testing: Do contextual factors influence software fault identification?,March 2019,Information and Software Technology,Not Found,Fredrik=Asplund: fasplund@kth.se,"Abstract
Context:
 Exploratory Testing (ET) is a manual approach to software testing in which learning, test design and test execution occurs simultaneously. Still a developing topic of interest to academia, although as yet insufficiently investigated, most studies focus on the skills and experience of the individual tester. However, contextual factors such as project processes, test scope and organisational boundaries are also likely to affect the approach.
Objective:
 This study explores contextual differences between teams of testers at a MedTec firm developing safety-critical products to ascertain whether contextual factors can influence the outcomes of ET, and what associated implications can be drawn for test management.
Method:
 A development project was studied in two iterations, each consisting of a quantitative phase testing hypotheses concerning when ET would identify faults in comparison to other testing approaches and a qualitative phase involving interviews.
Results:
 Influence on ET is traced to how the scope of tests focus learning on different types of knowledge and imply an asymmetry in the strength and number of information flows to test teams.
Conclusions:
 While test specialisation can be attractive to software development organisations, results suggest changes to processes and organisational structures might be required to maintain test efficiency throughout projects: the responsibility for test cases might need to be rotated late in projects, and asymmetries in information flows might require management to actively strengthen the presence and connections of test teams throughout the firm. However, further research is needed to investigate whether these results also hold for non safety-critical faults.",21 Mar 2025,6,"The study explores contextual differences in exploratory testing within a MedTec firm, providing insights for test management. While the findings are valuable, the direct impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584918302404,Athena: A framework to automatically generate security test oracle via extracting policies from source code and intended software behaviour,March 2019,Information and Software Technology,Not Found,Hossein=Homaei: homayi@aut.ac.ir; Hamid Reza=Shahriari: shahriari@aut.ac.ir,"Abstract
Context:
 Software security testing aims to check the security behaviour of a program. To determine whether the program behaves securely on a particular execution, we need an oracle who knows the expected security behaviour. Security test oracle decides whether test cases violate the intended security policies of the program. Thus, it is necessary for the oracle to model the detailed security policies. Unfortunately, these policies are usually poorly documented. Even worse, in some cases, the source code is the only available document of the program.
Objective:
 We propose a method to automatically extract the intended security policies of the program under test from the source code and expected execution traces. We introduce a security test oracle, Athena, which utilises these policies to differentiate between the secure and potentially insecure behaviour of the program.
Method:
 We use a hybrid analysis approach to obtain the intended security policies. We investigate the program statements (gates) in which the software communicates with the environment. We analyse the transmitted messages in the gates and the control and data flow of the program to extract some security properties. Moreover, we specify the intended navigation paths of the program. These properties and paths form the expected security policies. Athena utilises these policies to detect potential 
security breaches
.
Results:
 Investigating common types of software vulnerabilities illustrates the flexibility of Athena in modelling various kinds of security policies. Moreover, we show the usefulness of the method by applying it to the real web applications and evaluating its capability to detect actual attacks.
Conclusions:
 Our proposed approach takes a step towards solving the test oracle automation problem in the domain of security testing.",21 Mar 2025,8,The proposed method for automatically extracting security policies from source code for security testing can be crucial for startups to ensure the security of their programs and detect potential breaches.
https://www.sciencedirect.com/science/article/pii/S0950584918302416,A two-phase transfer learning model for cross-project defect prediction,March 2019,Information and Software Technology,Not Found,Chao=Liu: liu.chao@cqu.edu.cn; Dan=Yang: dyang@cqu.edu.cn; Xin=Xia: xin.xia@monash.edu; Meng=Yan: mengy@zju.edu.cn; Xiaohong=Zhang: xhongz@cqu.edu.cn,"Abstract
Context:
 Previous studies have shown that a 
transfer learning
 model, TCA+ proposed by Nam et al., can significantly improve the performance of cross-project 
defect prediction
 (CPDP). TCA+ achieves the improvement by reducing 
data distribution
 difference between source (training data) and target (testing data) projects. However, TCA+ is unstable, i.e., its performance varies largely when using different source projects to build prediction models. In practice, it is hard to choose a suitable source project to build the prediction model.
Objective:
 To address the limitation of TCA+, we propose a two-phase 
transfer learning
 model (TPTL) for CPDP.
Method:
 In the first phase, we propose a source project estimator (SPE) to automatically choose two source projects with the highest distribution similarity to a target project from candidates. Next, two source projects that are estimated to achieve the highest values of F1-score and cost-effectiveness are selected. In the second phase, we leverage TCA+ to build two prediction models based on the two selected projects and combine their prediction results to further improve the prediction performance.
Results:
 We evaluate TPTL on 42 defect datasets from PROMISE repository, and compare it with two versions of TCA+ (TCA+_Rnd, randomly selecting one source project; TCA+_All, using all alternative source projects), a related source project selection model TDS proposed by Herbold, a state-of-the-art CPDP model leveraging a log transformation (LT) method, and a 
transfer learning
 model Dycom with better form of TCA. Experiment results show that, on average across 42 datasets, TPTL respectively improves these 
baseline models
 by 19%, 5%, 36%, 27%, and 11% in terms of F1-score; by 64%, 92%, 71%, 11%, and 66% in terms of cost-effectiveness.
Conclusion:
 The proposed TPTL model can solve the instability problem of TCA+, showing substantial improvements over the state-of-the-art and related CPDP models.",21 Mar 2025,8,"The proposed TPTL model addresses the limitation of TCA+ and shows substantial improvements over existing models, which can have a significant impact on the performance of cross-project defect prediction. The practical value of this research for early-stage ventures is high."
https://www.sciencedirect.com/science/article/pii/S0950584918302428,FSCT: A new fuzzy search strategy in concolic testing,March 2019,Information and Software Technology,Not Found,Arash=Sabbaghi: a.sabbaghi@qiau.ac.ir; Hamidreza=Rashidy Kanan: h.rashidykanan@sru.ac.ir; Mohammad Reza=Keyvanpour: keyvanpour@alzahra.ac.ir,"Abstract
Context
Concolic testing is a promising approach to automate structural test data generation. However, combinatorial explosion of the path space, known as path explosion, and also constrained testing budget, makes achieving high code coverage in concolic testing a challenging task.
Objective
All branches of the previously explored paths make up the 
search space
 of concolic testing and search strategy define the mechanism of choosing branches to be flipped to drive the execution toward testing goals. With regard to the large number of candidate branches, choosing the right branch to continue the search is so crucial and has a direct impact on coverage rate and effort. This paper aims to improve the effectiveness of branch testing by considering the characteristics of paths reaching uncovered branches and presenting a novel search strategy for effectively and efficiently exploring the 
search space
.
Method
We model the branch selection process in concolic testing as a decision making system and introduce a new Fuzzy Search Strategy in Concolic Testing (FSCT). FSCT chooses a branch to be filliped in which the most suitable path with respect to the proposed 
coverage factors
 reaches an uncovered branch with the highest priority and this priority is assigned by the designed fuzzy 
expert system
. The proposed 
coverage factors
 effectively help to determine the characteristics of paths.
Results
We implemented FSCT on top of CREST and evaluated it using several popular benchmarks. The experimental results show that FSCT outperforms the state-of-the-art techniques in terms of coverage rate and coverage effort.
Conclusion
FSCT helps concolic testing to better cope with path explosion problem and shows its capabilities to achieve higher code coverage while at the same time decreases testing efforts in terms of both runtime and number of iterations.",21 Mar 2025,7,"The FSCT approach improves concolic testing and helps cope with the path explosion problem, showcasing capabilities to achieve higher code coverage with decreased testing efforts. While impactful, the practical implication for early-stage ventures may not be as direct as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918302441,A survey on software coupling relations and tools,March 2019,Information and Software Technology,Not Found,Enrico=Fregnan: fregnan@ifi.uzh.ch; Tobias=Baum: tobias.baum@inf.uni-hannover.de; Fabio=Palomba: palomba@ifi.uzh.ch; Alberto=Bacchelli: bacchelli@ifi.uzh.ch,"Abstract
Context
Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find.
Goals
The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the 
software engineering
 academic community to extract these metrics.
Method
This work constitutes a systematic literature review in 
software engineering
. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material.
Results
Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an 
independent group
 and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group.
Conclusion
This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. 
Public preprint
 [
https://doi.org/10.5281/zenodo.2002001
].",21 Mar 2025,5,"The systematic literature review on software coupling provides valuable insights and categorization of coupling relations and related metrics. While informative for academics and practitioners, the direct impact on practical applications for early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S0950584918302453,VFL: Variable-based fault localization,March 2019,Information and Software Technology,Not Found,Jeongho=Kim: jeonghodot@skku.edu; Jindae=Kim: jdkim@cse.ust.hk; Eunseok=Lee: leees@skku.edu,"ABSTRACT
Context
Fault localization
 is one of the most important debugging tasks. Hence, many automatic fault localization techniques have been proposed to reduce the burden on developers for such tasks. Among them, Spectrum-based Fault Localization (SFL) techniques leverage coverage information and localize faults based on the coverage difference between the failed and passed test cases.
Objective
However, such SFL techniques cannot localize faults effectively when coverage differences are not clear. To address this issue and improve the fault localization performance of the SFL techniques, we propose a Variable-based Fault Localization (VFL) technique.
Method
The VFL technique identifies suspicious variables and uses them to generate a ranked list of suspicious source code lines. Since it only requires additional information about variables that are also available in the SFL techniques, the proposed technique is lightweight and can be used to improve the performance of existing the SFL techniques.
Results
In an evaluation with 224 real Java faults and 120 C faults, the VFL technique outperforms the SFL techniques using the same similarity coefficient. The average Exam scores of the VFL techniques are reduced by more than 55% compared to the SFL techniques, and the VFL techniques localize faults at a lower rank than the SFL techniques for about 73% of the 344 faults.
Conclusion
We proposed a novel variable-based fault localization technique for more effective debugging. The VFL technique has better performance than the existing techniques and the results were more useful for actual fault localization tasks. In addition, this technique is very lightweight and scalable, so it is very easy to collaborate with other fault localization techniques.",21 Mar 2025,9,"The VFL technique offers a novel approach to fault localization, outperforming existing SFL techniques in terms of effectiveness and efficiency. The lightweight and scalable nature of the proposed technique makes it particularly valuable for early-stage ventures looking to enhance debugging processes."
https://www.sciencedirect.com/science/article/pii/S0950584918301873,Metrics for analyzing variability and its implementation in software product lines: A systematic literature review,February 2019,Information and Software Technology,Not Found,Sascha=El-Sharkawy: elscha@sse.uni-hildesheim.de; Nozomi=Yamagishi-Eichler: Not Found; Klaus=Schmid: schmid@sse.uni-hildesheim.de,"Abstract
Context:
 Software Product Line (SPL) development requires at least concepts for variability implementation and 
variability modeling
 for deriving products from a product line. These variability implementation concepts are not required for the development of single systems and, thus, are not considered in traditional 
software engineering
. Metrics are well established in traditional 
software engineering
, but existing metrics are typically not applicable to SPLs as they do not address variability management. Over time, various specialized product line metrics have been described in literature, but no systematic description of these metrics and their characteristics is currently available.
Objective:
 This paper describes and analyzes variability-aware metrics, designed for the needs of software product lines. More precisely we restrict the scope of our study explicitly to metrics designed for variability models, code artifacts, and metrics taking both kinds of artifacts into account. Further, we categorize the purpose for which these metrics were developed. We also analyze to what extent these metrics were evaluated to provide a basis for researchers for selecting adequate metrics.
Method:
 We conducted a systematic literature review to identify variability-aware implementation metrics. We discovered 42 relevant papers reporting metrics intended to measure aspects of variability models or code artifacts.
Results:
 We identified 57 variability model metrics, 34 annotation-based 
code metrics
, 46 
code metrics
 specific to composition-based implementation techniques, and 10 metrics integrating information from variability model and code artifacts. For only 31 metrics, an evaluation was performed assessing their suitability to draw any qualitative conclusions.
Conclusions:
 We observed several problematic issues regarding the definition and the use of the metrics. Researchers and practitioners benefit from the catalog of variability-aware metrics, which is the first of its kind. Also, the research community benefits from the identified observations in order to avoid those problems when defining new metrics.",21 Mar 2025,6,"The analysis of variability-aware metrics for software product lines provides valuable insights and a categorization of metrics designed for variability models and code artifacts. While informative for SPL development, the immediate practical impact on early-stage ventures may be more limited."
https://www.sciencedirect.com/science/article/pii/S0950584918301903,Cognitive complexity as a quantifier of version to version Java-based source code change: An empirical probe,February 2019,Information and Software Technology,Not Found,Loveleen=Kaur: loveleen.kaur@thapar.edu; Ashutosh=Mishra: ashutosh.mishra@thapar.edu,"Abstract
Context
It has been often argued that it is challenging to modify code fragments from existing software that contains files that are difficult to comprehend. Since systematic software maintenance includes an extensive human activity, cognitive complexity is one of the intrinsic factors that could potentially contribute to or impede an efficient software maintenance practice, the empirical validation of which remains vastly unaddressed.
Objective
This study conducts an experimental analysis in which the software developer's level of difficulty in comprehending the software: the cognitive complexity, is theoretically computed and empirically evaluated for estimating its relevance to actual software change.
Method
For multiple successive releases of two Java-based software projects, where the 
source code
 of a previous release has been substantively used in a novel release, we calculate the change results and the values of the cognitive complexity for each of the version's 
source code
 Java files. We construct eight datasets and build 
predictive models
 using statistical analysis and 
machine learning techniques
.
Results
The pragmatic comparative examination of the estimated cognitive complexity against prevailing metrics of software change and software complexity clearly validates the cognitive complexity metric as a noteworthy measure of version to version source code change.",21 Mar 2025,5,"The study focuses on software maintenance and cognitive complexity, which can be relevant for early-stage ventures dealing with code maintenance. However, the impact on startups may not be immediate or significant."
https://www.sciencedirect.com/science/article/pii/S0950584918301915,Automated metamodel/model co-evolution: A search-based approach,February 2019,Information and Software Technology,Not Found,Wael=Kessentini: kessentw@iro.umontreal.ca; Houari=Sahraoui: Not Found; Manuel=Wimmer: Not Found,"Abstract
Context:
 Metamodels evolve over time to accommodate new features, improve existing designs, and fix errors identified in previous releases. One of the obstacles that may limit the adaptation of new metamodels by developers is the extensive manual changes that have been applied to migrate existing models. Recent studies addressed the problem of automating the metamodel/model co-evolution based on manually defined migration rules. The definition of these rules requires the list of changes at the metamodel level which are difficult to fully identify. Furthermore, different possible alternatives may be available to translate a metamodel change to a model change. Thus, it is hard to generalize these co-evolution rules.
Objective:
 We propose an alternative automated approach for the metamodel/model co-evolution. The proposed approach refines an initial model instantiated from the previous metamodel version to make it as conformant as possible to the new metamodel version by finding the best compromise between three objectives, namely minimizing (
i
) the non-conformities with new metamodel version, (
ii
) the changes to existing models, and (
iii
) the textual and structural dissimilarities between the initial and revised models.
Method:
 We formulated the metamodel/model co-evolution as a multi-objective 
optimization problem
 to handle the different conflicting objectives using the Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Multi-Objective 
Particle Swarm Optimization
 (MOPSO).
Results:
 We evaluated our approach on several evolution scenarios extracted from different widely used metamodels. The results confirm the effectiveness of our approach with average manual correctness, precision and recall respectively higher than 91%, 88% and 89% on the different co-evolution scenarios.
Conclusion:
 A comparison with our previous work confirms the out-performance of our multi-objective formulation.",21 Mar 2025,7,The proposed approach for metamodel/model co-evolution addresses a common challenge in software development and offers an automated solution. This could have a practical impact on startups by reducing manual efforts and improving efficiency.
https://www.sciencedirect.com/science/article/pii/S0950584917300770,Evaluating different i*-based approaches for selecting functional requirements while balancing and optimizing non-functional requirements: A controlled experiment,February 2019,Information and Software Technology,Not Found,Jose=Zubcoff: jose.zubcoff@ua.es; Irene=Garrigós: Not Found; Jose-Norberto=Mazón: Not Found; Jose-Alfonso=Aguilar: Not Found; Francisco=Gomariz-Castillo: Not Found,"Abstract
Context
A relevant question in requirements engineering is which set of functional requirements (FR) to prioritize and implement, while keeping non-functional requirements (NFR) balanced and optimized.
Objective
We aim to provide empirical evidence that requirement engineers may perform better at the task of selecting FRs while optimizing and balancing NFRs using an alternative (automated) i* post-processed model, compared to the original i* model.
Method
We performed a controlled experiment, designed to compare the original i* graphical notation, with our post-processed i* visualizations based on Pareto efficiency (a tabular and a radar chart visualization). Our experiment consisted of solving different exercises of various complexity for selecting FRs while balancing NFR. We considered the efficiency (time spent to correctly answer exercises), and the effectiveness (regarding time: time spent to solve exercises, independent of correctness; and regarding correctness of the answer, independent of time).
Results
The efficiency analysis shows it is 3.51 times more likely to solve exercises correctly with our tabular and radar chart visualizations than with i*. Actually, i* was the most time-consuming (effectiveness regarding time), had a lower number of correct answers (effectiveness regarding correctness), and was affected by complexity. Visual or textual preference of the subjects had no effect on the score. Beginners took more time to solve exercises than experts if i* is used (no distinction if our Pareto-based visualizations are used).
Conclusion
For complex model instances, the Pareto front based tabular visualization results in more correct answers, compared to radar chart visualization. When we consider effectiveness regarding time, the i* graphical notation is the most time consuming visualization, independent of the complexity of the exercise. Finally, regarding efficiency, subjects consume less time when using radar chart visualization than tabular visualization, and even more so compared to the original i* graphical notation.",21 Mar 2025,8,"The study provides empirical evidence of an alternative approach for selecting functional requirements while balancing non-functional requirements, which can be valuable for startups in optimizing product development. The results show a clear improvement in efficiency and effectiveness."
https://www.sciencedirect.com/science/article/pii/S0950584918301927,Creative goal modeling for innovative requirements,February 2019,Information and Software Technology,Not Found,J.=Horkoff: jenho@chalmers.se; N.A.=Maiden: neil.maiden.1@city.ac.uk; D.=Asboth: david.asboth.2@city.ac.uk,"Abstract
Context
 When determining the functions and qualities (a.k.a. requirements) for a system, creativity is key to drive innovation and foster business success. However, creative requirements must be practically operationalized, grounded in concrete functions and system interactions. 
Requirements Engineering
 (RE) has produced a wealth of methods centered around goal modeling, in order to graphically explore the space of alternative requirements, linking functions to goals and dependencies. In parallel work, 
creativity theories
 from the social sciences have been applied to the design of creative requirements workshops, pushing stakeholders to develop innovative systems. Goal models tend to focus on what is known, while creativity workshops are expensive, require a specific skill set to facilitate, and produce mainly paper-based, unstructured outputs. 
Objective
 Our aim in this work is to explore beneficial combinations of the two areas of work in order to overcome these and other limitations, facilitating creative 
requirements elicitation
, supported by a simple extension of a well-known and structured requirements modeling technique. 
Method
 We take a Design Science approach, iterating over 
exploratory studies
, design, and summative validation studies. 
Results
 The result is the Creative Leaf tool and method supporting creative goal modeling for RE. 
Conclusion
 We support creative RE by making creativity techniques more accessible, producing structured digital outputs which better match to existing RE methods with associated analysis procedures and transformations.",21 Mar 2025,6,"The work on combining creativity techniques with requirements engineering could benefit startups by enhancing innovation in product development. The proposed tool and method aim to overcome limitations, although the practical implementation and impact may vary."
https://www.sciencedirect.com/science/article/pii/S0950584918301939,Guidelines for including grey literature and conducting multivocal literature reviews in software engineering,February 2019,Information and Software Technology,Not Found,Vahid=Garousi: vahid.garousi@wur.nl; Mika V.=Mäntylä: mika.mantyla@oulu.fi,"Abstract
Context
A Multivocal Literature Review (MLR) is a form of a 
Systematic Literature Review
 (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in 
software engineering
 (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results.
Objective
There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE.
Method
To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example.
Results
The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations.
Conclusion
Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.",21 Mar 2025,8,The guidelines for conducting Multivocal Literature Reviews in software engineering can be valuable for startups looking to stay updated on state-of-the-art practices and research. The guidelines provide a structured approach that can benefit early-stage ventures in conducting thorough literature reviews.
https://www.sciencedirect.com/science/article/pii/S0950584918301940,A pilot empirical study of applying a usability technique in an open source software project,February 2019,Information and Software Technology,Not Found,Lucrecia=Llerena: lucrecia.llerena@estudiante.uam.es; John W.=Castro: john.castro@alumni.uam.es; Silvia T.=Acuña: silvia.acunna@uam.es,"Abstract
Context
The growth in the number of non-technical 
open source software
 (OSS) application users and the escalating use of these applications have redoubled the need for, and interest in, developing usable OSS. OSS communities are unclear about which techniques to use in each 
development process
 activity.
Objective
The aim of our research is to adapt a usability technique (visual brainstorming) to an 
OSS project
 and evaluate the feasibility of its application.
Method
We used the 
case study
 research method to investigate technique application and participation in a project. To do this, we participated as volunteers in the HistoryCal project.
Results
We identified adverse conditions that were an obstacle to technique application (like it was not easy to recruit OSS users to participate) and modified the technique to make it applicable.
Conclusion
We conclude that these changes were helpful for applying the technique using web artifacts like blogs.",21 Mar 2025,7,"The research on adapting usability techniques for OSS projects can potentially enhance the user experience and development process, which is beneficial for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302040,Exploiting Parts-of-Speech for effective automated requirements traceability,February 2019,Information and Software Technology,Not Found,Nasir=Ali: cnali@memphis.edu; Haipeng=Cai: hcai@eecs.wsu.edu; Abdelwahab=Hamou-Lhadj: abdelw@ece.concordia.ca; Jameleddine=Hassine: jhassine@kfupm.edu.sa,"Abstract
Context
Requirement traceability (RT) is defined as the ability to describe and follow the life of a requirement. RT helps developers ensure that relevant requirements are implemented and that the source code is consistent with its requirement with respect to a set of 
traceability links
 called 
trace links
. Previous work leverages Parts Of Speech (POS) tagging of software artifacts to recover trace links among them. These studies work on the premise that discarding one or more POS tags results in an improved accuracy of Information Retrieval (IR) techniques.
Objective
First, we show empirically that excluding one or more POS tags could negatively impact the accuracy of existing IR-based traceability approaches, namely the 
Vector Space Model
 (VSM) and the Jensen Shannon Model (JSM). Second, we propose a method that improves the accuracy of IR-based traceability approaches.
Method
We developed an approach, called 
ConPOS
, to recover 
trace links
 using constraint-based pruning. 
ConPOS
 uses major POS categories and applies constraints to the recovered trace links for pruning as a filtering process to significantly improve the effectiveness of IR-based techniques. We conducted an experiment to provide evidence that removing POSs does not improve the accuracy of IR techniques. Furthermore, we conducted two empirical studies to evaluate the effectiveness of 
ConPOS
 in recovering trace links compared to existing peer RT approaches.
Results
The results of the first empirical study show that removing one or more POS negatively impacts the accuracy of VSM and JSM. Furthermore, the results from the other empirical studies show that 
ConPOS
 provides 11%-107%, 8%-64%, and 15%-170% higher precision, recall, and 
mean average precision
 (MAP) than VSM and JSM.
Conclusion
We showed that 
ConPos
 outperforms existing IR-based RT approaches that discard some POS tags from the input documents.",21 Mar 2025,9,"The proposal of ConPOS method to improve traceability in software development processes can significantly impact the accuracy and effectiveness, providing valuable insights for startups to enhance their development practices."
https://www.sciencedirect.com/science/article/pii/S0950584918302052,Automatically identifying code features for software defect prediction: Using AST N-grams,February 2019,Information and Software Technology,Not Found,Thomas=Shippey: t.shippey@herts.ac.uk; David=Bowes: dbowes@uclan.ac.uk; Tracy=Hall: t.hall3@lancaster.ac.uk,"Abstract
Context:
 Identifying defects in code early is important. A wide range of static 
code metrics
 have been evaluated as potential defect indicators. Most of these metrics offer only high level insights and focus on particular pre-selected features of the code. None of the currently used metrics clearly performs best in 
defect prediction
.
Objective:
 We use 
Abstract Syntax Tree
 (AST) n-grams to identify features of defective Java code that improve 
defect prediction
 performance.
Method:
 Our approach is bottom-up and does not rely on pre-selecting any specific features of code. We use non-parametric testing to determine relationships between AST n-grams and faults in both open source and commercial systems. We build defect prediction models using three 
machine learning techniques
.
Results:
 We show that AST n-grams are very significantly related to faults in some systems, with very large 
effect sizes
. The occurrence of some frequently occurring AST n-grams in a method can mean that the method is up to three times more likely to contain a fault. AST n-grams can have a large effect on the performance of defect prediction models.
Conclusions:
 We suggest that AST n-grams offer developers a promising approach to identifying potentially defective code.",21 Mar 2025,8,"The use of AST n-grams to improve defect prediction performance offers a practical approach for identifying faulty code early, which can benefit startups in improving software quality and reliability."
https://www.sciencedirect.com/science/article/pii/S0950584918302076,Software defect number prediction: Unsupervised vs supervised methods,February 2019,Information and Software Technology,Not Found,Xiang=Chen: xchencs@ntu.edu.cn; Dun=Zhang: dunnzhang0@gmail.com; Yingquan=Zhao: enockchao@gmail.com; Zhanqi=Cui: czq@bistu.edu.cn; Chao=Ni: jacknichao920209@gmail.com,"Abstract
Context: 
Software defect
 number prediction (SDNP) can rank the program modules according to the prediction results and is helpful for the optimization of testing 
resource allocation
.
Objective: In previous studies, supervised methods vs 
unsupervised methods
 is an active issue for just-in-time 
defect prediction
 and file-level 
defect prediction
 based on effort-aware 
performance measures
. However, this issue has not been investigated for SDNP. To the best of our knowledge, we are the first to make a thorough comparison for these two different types of methods.
Method: In our empirical studies, we consider 7 real open-source projects with 24 versions in total, use 
FPA
 and 
Kendall
 as our effort-aware 
performance measures
, and consider three different performance evaluation scenarios (i.e., within-version scenario, cross-version scenario, and cross-project scenario).
Result: We first identify two 
unsupervised methods
 with best performance. These two methods simply rank modules according to the value of metric LOC and metric RFC from large to small respectively. Then we compare 9 state-of-the-art supervised methods incorporating SMOTEND, which is used for handling 
class imbalance problem
, with the unsupervised method based on LOC metric (i.e., LOC_D method). Final results show that LOC_D method can perform significantly better than or the same as these supervised methods. Later motivated by a recent study conducted by Agrawla and Menzies, we apply differential evolutionary (DE) to optimize parameter value of SMOTEND used by these supervised methods and find that using DE can effectively improve the performance of these supervised methods for SDNP too. Finally, we continue to compare LOC_D with these optimized supervised methods using DE, and LOC_D method still has advantages in the performance, especially in the cross-version and cross-project scenarios.
Conclusion: Based on these results, we suggest that researchers need to use the unsupervised method LOC_D as the 
baseline method
, which is used for comparing their proposed novel methods for SDNP problem in the future.",21 Mar 2025,10,"The comprehensive comparison of supervised and unsupervised methods for software defect number prediction, along with the suggestion of a baseline method for future research, provides valuable insights and guidance for startups to optimize their testing resource allocation."
https://www.sciencedirect.com/science/article/pii/S0950584918302088,Software defect prediction based on kernel PCA and weighted extreme learning machine,February 2019,Information and Software Technology,Not Found,Zhou=Xu: Not Found; Jin=Liu: jinliu@whu.edu.cn; Xiapu=Luo: Not Found; Zijiang=Yang: Not Found; Yifeng=Zhang: Not Found; Peipei=Yuan: Not Found; Yutian=Tang: Not Found; Tao=Zhang: Not Found,"Abstract
Context
Software 
defect prediction
 strives to detect defect-prone software modules by mining the 
historical data
. Effective prediction enables reasonable testing 
resource allocation
, which eventually leads to a more reliable software.
Objective
The complex structures and the imbalanced class distribution in software defect data make it challenging to obtain suitable data features and learn an effective 
defect prediction
 model. In this paper, we propose a method to address these two challenges.
Method
We propose a defect prediction framework called 
KPWE
 that combines two techniques, i.e., 
Kernel 
Principal Component
 A
nalysis (
KPCA
) and 
Weighted 
Extreme Learning Machine
 (
WELM
). Our framework consists of two major stages. In the first stage, KPWE aims to extract representative data features. It leverages the KPCA technique to project the original data into a latent 
feature space
 by 
nonlinear mapping
. In the second stage, KPWE aims to alleviate the 
class imbalance
. It exploits the WELM technique to learn an effective defect prediction model with a weighting-based scheme.
Results
We have conducted extensive experiments on 34 projects from the PROMISE dataset and 10 projects from the NASA dataset. The experimental results show that KPWE achieves promising performance compared with 41 
baseline methods
, including seven basic classifiers with KPCA, five variants of KPWE, eight representative feature selection methods with WELM, 21 imbalanced learning methods.
Conclusion
In this paper, we propose KPWE, a new software defect prediction framework that considers the feature extraction and 
class imbalance
 issues. The empirical study on 44 software projects indicate that KPWE is superior to the 
baseline methods
 in most cases.",21 Mar 2025,9,"The proposal of KPWE framework to address challenges in software defect prediction, along with promising experimental results, offers startups a valuable tool to enhance defect prediction and improve software quality."
https://www.sciencedirect.com/science/article/pii/S0950584918302106,"Identifying, categorizing and mitigating threats to validity in software engineering secondary studies",February 2019,Information and Software Technology,Not Found,Apostolos=Ampatzoglou: apostolos.ampatzoglou@gmail.com; Stamatia=Bibi: Not Found; Paris=Avgeriou: Not Found; Marijn=Verbeek: Not Found; Alexander=Chatzigeorgiou: Not Found,"Abstract
Context
Secondary studies are vulnerable to threats to validity. Although, mitigating these threats is crucial for the credibility of these studies, we currently lack a systematic approach to identify, categorize and mitigate threats to validity for secondary studies.
Objective
In this paper, we review the corpus of secondary studies, with the aim to identify: (a) the trend of reporting threats to validity, (b) the most common threats to validity and corresponding 
mitigation actions
, and (c) possible categories in which threats to validity can be classified.
Method
To achieve this goal we employ the tertiary study research method that is used for synthesizing knowledge from existing secondary studies. In particular, we 
collected data
 from more than 100 studies, published until December 2016 in top quality 
software engineering
 venues (both journals and conference).
Results
Our results suggest that in recent years, secondary studies are more likely to report their threats to validity. However, the presentation of such threats is rather ad hoc, e.g., the same threat may be presented with a different name, or under a different category. To alleviate this problem, we propose a classification schema for reporting threats to validity and possible 
mitigation actions
. Both the classification of threats and the associated mitigation actions have been validated by an empirical study, i.e., Delphi rounds with experts.
Conclusion
Based on the proposed schema, we provide a checklist, which authors of secondary studies can use for identifying and categorizing threats to validity and corresponding mitigation actions, while readers of secondary studies can use the checklist for assessing the validity of the reported results.",21 Mar 2025,8,"The paper provides a systematic approach to identifying, categorizing, and mitigating threats to validity in secondary studies, which can significantly improve the credibility of research in the field of software engineering."
https://www.sciencedirect.com/science/article/pii/S0950584918302192,Heuristics for improving the rigour and relevance of grey literature searches for software engineering research,February 2019,Information and Software Technology,Not Found,Austen=Rainer: austen.rainer@canterbury.ac.nz; Ashley=Williams: ashley.williams@pg.canterbury.ac.nz,"Abstract
Background:
 
Software engineering
 research has a growing interest in grey literature (GL). Aim: To improve the identification of relevant and rigorous GL. Method: We develop and demonstrate heuristics to find more relevant and rigorous GL. The heuristics generate stratified samples of search and post–search datasets using a formally structured set of 
search keywords
. Conclusion: The heuristics require further evaluation. We are developing a tool to implement the heuristics.",21 Mar 2025,5,"The development of heuristics to identify relevant and rigorous grey literature in software engineering is valuable, but further evaluation of the heuristics is needed to determine their practical impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918302209,Challenges and recommended practices for software architecting in global software development,February 2019,Information and Software Technology,Not Found,Outi=Sievi-Korte: outi.sievi-korte@tut.fi; Sarah=Beecham: sarah.beecham@lero.ie; Ita=Richardson: ita.richardson@lero.ie,"Abstract
Context
Global software development
 (GSD), although now a norm in the software industry, carries with it enormous challenges mostly regarding communication and coordination. Aforementioned challenges are highlighted when there is a need to transfer knowledge between sites, particularly when software artifacts assigned to different sites depend on each other. The design of the software architecture and associated 
task dependencies
 play a major role in reducing some of these challenges.
Objective
The current literature does not provide a cohesive picture of how the distributed nature of software development is taken into account during the design phase: what to avoid, and what works in practice. The objective of this paper is to gain an understanding of software architecting in the context of GSD, in order to develop a framework of challenges and solutions that can be applied in both research and practice.
Method
We conducted a systematic literature review (SLR) that synthesises (i) challenges which GSD imposes on 
software architecture design
, and (ii) recommended practices to alleviate these challenges.
Results
We produced a comprehensive set of guidelines for performing 
software architecture design
 in GSD based on 55 selected studies. Our framework comprises nine key challenges with 28 related concerns, and nine recommended practices, with 22 related concerns for software architecture design in GSD. These challenges and practices were mapped to a thematic conceptual model with the following concepts: Organization (Structure and Resources), Ways of Working (Architecture 
Knowledge Management
, Change Management and Quality Management), Design Practices, Modularity and Task Allocation.
Conclusion
The synthesis of findings resulted in a thematic conceptual model of the problem area, a mapping of the key challenges to practices, and a concern framework providing concrete questions to aid the design process in a distributed setting. This is a first step in creating more 
concrete architecture
 design practices and guidelines.",21 Mar 2025,9,"The study on software architecting in the context of global software development provides concrete guidelines and practices to address challenges in distributed software architecture design, which can have a significant positive impact on early-stage ventures operating in a global context."
https://www.sciencedirect.com/science/article/pii/S0950584918301617,What can violations of good practices tell about the relationship between GoF patterns and run-time quality attributes?,January 2019,Information and Software Technology,Not Found,Daniel=Feitosa: d.feitosa@rug.nl; Apostolos=Ampatzoglou: Not Found; Paris=Avgeriou: Not Found; Alexander=Chatzigeorgiou: Not Found; Elisa.Y.=Nakagawa: Not Found,"Abstract
Context
GoF patterns have been extensively studied with respect to the benefit they provide as problem-solving, communication and quality improvement mechanisms. The latter has been mostly investigated through empirical studies, but some aspects of quality (esp. run-time ones) are still under-investigated.
Objective
In this paper, we study if the presence of patterns enforces the conformance to good coding practices. To achieve this goal, we explore the relationship between the presence of GoF 
design patterns
 and violations of good practices related to 
source code
 correctness, performance and security, via 
static analysis
.
Method
Specifically, we exploit 
static analysis
 so as to investigate whether the number of violations of good coding practices identified on classes is related to: (a) their participation in pattern occurrences, (b) the pattern category, (c) the pattern in which they participate, and (d) their role within the pattern occurrence. To answer these questions, we performed a 
case study
 on approximately 13,000 classes retrieved from five open-source projects.
Results
The obtained results suggest that classes not participating in patterns are more probable to violate good coding practices for correctness, performance and security. In a more fine-grained level of analysis, by focusing on specific patterns, we observed that patterns with more complex structure (e.g., Decorator) and pattern roles that are more change-prone (e.g., Subclasses) are more likely to be associated with a higher number of violations (up to 50 times more violations).
Conclusion
This finding implies that investing in a well-thought architecture based on best practices, such as patterns, is often accompanied with cleaner code with fewer violations.",21 Mar 2025,7,"The investigation on the relationship between GoF design patterns and violations of good coding practices can contribute to improving the quality and conformance to best practices in software development, benefiting startups by providing insights into maintaining clean code."
https://www.sciencedirect.com/science/article/pii/S0950584918301654,Improving bug localization with word embedding and enhanced convolutional neural networks,January 2019,Information and Software Technology,Not Found,Yan=Xiao: yanxiao6-c@my.cityu.edu.hk; Jacky=Keung: Jacky.Keung@cityu.edu.hk; Kwabena E.=Bennin: kebennin2-c@my.cityu.edu.hk; Qing=Mi: Qing.Mi@my.cityu.edu.hk,"Abstract
Context:
 Automatic localization of buggy files can speed up the process of bug fixing to improve the efficiency and productivity of 
software quality assurance
 teams. Useful 
semantic information
 is available in 
bug reports
 and 
source code
, but it is usually underutilized by existing bug localization approaches.
Objective:
 To improve the performance of bug localization, we propose DeepLoc, a novel deep learning-based model that makes full use of 
semantic information
.
Method:
 DeepLoc is composed of an enhanced convolutional 
neural network
 (CNN) that considers bug-fixing recency and frequency, together with word-embedding and feature-detecting techniques. DeepLoc uses word embeddings to represent the words in 
bug reports
 and source files that retain their semantic information, and different CNNs to detect features from them. DeepLoc is evaluated on over 18,500 bug reports extracted from AspectJ, Eclipse, JDT, SWT, and Tomcat projects.
Results:
 The experimental results show that DeepLoc achieves 10.87%–13.4% higher MAP (mean average precision) than conventional CNN. DeepLoc outperforms four current state-of-the-art approaches (DeepLocator, HyLoc, LR+WE, and BugLocator) in terms of Accuracy@k (the percentage of bug reports for which at least one real buggy file is located within the top 
k
 rank), MAP, and MRR (mean reciprocal rank) using less 
computation time
.
Conclusion:
 DeepLoc is capable of automatically connecting bug reports to the corresponding buggy files and achieves better performance than four state-of-the-art approaches based on a 
deep understanding
 of semantics in bug reports and 
source code
.",21 Mar 2025,10,"The proposal of DeepLoc, a deep learning-based bug localization model, offers a novel approach to improving software quality assurance processes, which can greatly benefit early-stage ventures by enhancing bug-fixing efficiency and productivity."
https://www.sciencedirect.com/science/article/pii/S0950584918301666,An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study,January 2019,Information and Software Technology,Not Found,Ivan=Švogor: isvogor@foi.hr; Ivica=Crnković: Not Found; Neven=Vrček: Not Found,"Abstract
Context:
 Application of 
component based software engineering
 methods to 
heterogeneous computing
 (HC) enables different 
software configurations
 to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task.
Objective:
 We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and 
average execution time
.
Method:
 We validated SCAF through its 
instantiation
 on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six 
software configurations
 to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with 
solution spaces
 ranging from 10
30
 to 30
70
) and benchmark the results against best performing random configurations.
Results:
 The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the 
genetic algorithm
 and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully).
Conclusion:
 SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model 
architectural constraints
 and preferences, acting as a multi–criterion software architecture decision 
support system
. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach.",21 Mar 2025,8,"The proposal of the Software Component Allocation Framework (SCAF) for software configuration optimization with respect to multiple non-functional properties can have a significant impact on the early design phase of software development, especially for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0950584918301678,On the impact of code smells on the energy consumption of mobile applications,January 2019,Information and Software Technology,Not Found,Fabio=Palomba: fpalomba@unisa.it; Dario=Di Nucci: Not Found; Annibale=Panichella: Not Found; Andy=Zaidman: Not Found; Andrea=De Lucia: Not Found,"Abstract
Context.
 The demand for green 
software design
 is steadily growing higher especially in the context of mobile devices, where the computation is often limited by battery life. Previous studies found how wrong programming solutions have a strong impact on the energy consumption. 
Objective.
 Despite the efforts spent so far, only a little knowledge on the influence of code smells, 
i.e.,
symptoms of poor design or implementation choices, on the energy consumption of mobile applications is available. 
Method.
 To provide a wider overview on the relationship between smells and energy efficiency, in this paper we conducted a large-scale empirical study on the influence of 9 Android-specific code smells on the energy consumption of 60 
Android
 apps. In particular, we focus our attention on the design flaws that are theoretically supposed to be related to non-functional attributes of 
source code
, such as performance and energy consumption. 
Results.
 The results of the study highlight that methods affected by four code smell types, 
i.e.,Internal Setter, Leaking Thread, Member Ignoring Method
, and 
Slow Loop
, consume up to 87 times more than methods affected by other code smells. Moreover, we found that refactoring these code smells reduces energy consumption in all of the situations. 
Conclusions.
 Based on our findings, we argue that more research aimed at designing automatic refactoring approaches and tools for mobile apps is needed.",21 Mar 2025,7,The study on the influence of Android-specific code smells on energy consumption of mobile applications can provide valuable insights for startups focusing on developing energy-efficient software solutions.
https://www.sciencedirect.com/science/article/pii/S095058491830168X,Insights into startup ecosystems through exploration of multi-vocal literature,January 2019,Information and Software Technology,Not Found,Nirnaya=Tripathi: nirnaya.tripathi@oulu.fi; Pertti=Seppänen: Not Found; Ganesh=Boominathan: Not Found; Markku=Oivo: Not Found; Kari=Liukkunen: Not Found,"Abstract
Context: Successful startup firms have the ability to create jobs and contribute to 
economic welfare
. A suitable ecosystem developed around startups is important to form and support these firms. In this regard, it is crucial to understand the startup ecosystem, particularly from researchers’ and practitioners’ perspectives. However, a systematic literature research on the startup ecosystem is limited. Objective: In this study, our objective was to conduct a multi-vocal literature review and rigorously find existing studies on the startup ecosystem in order to organize and analyze them, know the definitions and major elements of this ecosystem, and determine the roles of such elements in startups’ product development. Method: We conducted a multi-vocal literature review to analyze relevant articles, which are published technical articles, white papers, and Internet articles that focused on the startup ecosystem. Our search generated 18,310 articles, of which 63 were considered primary candidates focusing on the startup ecosystem. Results: From our analysis of primary articles, we found four definitions of a startup ecosystem. These definitions used common terms, such as stakeholders, supporting organization, infrastructure, network, and region. Out of 63 articles, 34 belonged to the opinion type, with contributions in the form of reports, whereas over 50% had full relevance to the startup ecosystem. We identified eight major elements (finance, demography, market, education, 
human capital
, technology, entrepreneur, and support factors) of a startup ecosystem, which directly or indirectly affected startups. Conclusions: This study aims to provide the state of the art on the startup ecosystem through a multi-vocal literature review. The results indicate that current knowledge on the startup ecosystem is mainly shared by non-peer-reviewed literature, thus signifying the need for more systematic and empirical literature on the topic. Our study also provides some recommendations for future work.",21 Mar 2025,5,"The analysis of the startup ecosystem can offer valuable information for European startups, but the impact on practical value and early-stage ventures might be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S095058491830171X,An exploratory study of waste in software development organizations using agile or lean approaches: A multiple case study at 14 organizations,January 2019,Information and Software Technology,Not Found,Hiva=Alahyari: hiva@chalmers.se; Tony=Gorschek: tony.gorschek@bth.se; Richard=Berntsson Svensson: richard.berntsson.svensson@bth.se,"Abstract
Context
The principal focus of lean is the identification and elimination of waste from the process with respect to maximizing customer value. Similarly, the purpose of agile is to maximize customer value and minimize unnecessary work and 
time delays
. In both cases the concept of waste is important. Through an empirical study, we explore how waste is approached in 
agile software development
 organizations.
Objective
This paper explores the concept of waste in agile/lean software development organizations and how it is defined, used, prioritized, reduced, or eliminated in practice
Method
The data were collected using semi-structured open-interviews. 23 practitioners from 14 
embedded software
 development organizations were interviewed representing two core roles in each organization.
Results
Various wastes, categorized in 10 different categories, were identified by the respondents. From the mentioned wastes, not all were necessarily waste per se but could be symptoms caused by wastes. From the seven wastes of lean, Task-switching was ranked as the most important, and Extra-features, as the least important wastes according to the respondents’ opinion. However, most companies do not have their own or use an established definition of waste, more importantly, very few actively identify or try to eliminate waste in their organizations beyond local initiatives on project level.
Conclusion
In order to identify, recognize and eliminate waste, a common understanding, and a 
joint
 and holistic view of the concept is needed. It is also important to optimize the whole organization and the whole product, as waste on one level can be important on another, thus sub-optimization should be avoided. Furthermore, to achieve a sustainable and effective waste handling, both the short-term and the long-term perspectives need to be considered.",21 Mar 2025,6,"The exploration of waste in agile software development organizations can provide insights for startups in improving efficiency, but the direct impact on early-stage ventures might not be as significant as other abstracts."
https://www.sciencedirect.com/science/article/pii/S0950584918301708,Combining Automated GUI Exploration of Android apps with Capture and Replay through Machine Learning,January 2019,Information and Software Technology,Not Found,Domenico=Amalfitano: Not Found; Vincenzo=Riccio: Not Found; Nicola=Amatucci: Not Found; Vincenzo De=Simone: Not Found; Anna Rita=Fasolino: annarita.fasolino@unina.it,"Abstract
Context
Automated GUI Exploration Techniques have been widely adopted in the context of mobile apps for supporting critical engineering tasks such as reverse engineering, testing, and network traffic signature generation. Although several techniques have been proposed in the literature, most of them fail to guarantee the exploration of relevant parts of the applications when GUIs require to be exercised with particular and complex input event sequences. We refer to these GUIs as Gate GUIs and to the sequences required to effectively exercise them as Unlocking GUI Input Event Sequences.
Objective
In this paper, we aim at proposing a GUI exploration approach that exploits the human involvement in the automated process to solve the limitations introduced by Gate GUIs, without requiring the preliminary configuration of the technique or the user involvement for the entire duration of the exploration process.
Method
We propose juGULAR, a Hybrid GUI Exploration Technique combining Automated GUI Exploration with Capture and Replay. Our approach is able to automatically detect Gate GUIs during the app exploration by exploiting a 
Machine Learning approach
 and to unlock them by leveraging input event sequences provided by the user. We implement juGULAR in a modular software architecture that targets the 
Android
 mobile platform. We evaluate the performance of juGULAR by an experiment involving 14 real 
Android
 apps.
Results
The experiment shows that the hybridization introduced by juGULAR allows to improve the exploration capabilities in terms of Covered Activities, Covered Lines of Code, and generated Network Traffic Bytes at a reasonable manual intervention cost. The experimental results also prove that juGULAR is able to outperform the state-of-the-practice tool Monkey.
Conclusion
We conclude that the combination of Automated GUI Exploration approaches with Capture and Replay techniques is promising to achieve a thorough app exploration. Machine Learning approaches aid to pragmatically integrate these two techniques.",21 Mar 2025,7,"The proposal of juGULAR as a hybrid GUI exploration technique for mobile apps can have practical implications for startups in improving app exploration capabilities, especially in the early stages of development."
https://www.sciencedirect.com/science/article/pii/S0950584917301118,CERSE - Catalog for empirical research in software engineering: A Systematic mapping study,January 2019,Information and Software Technology,Not Found,Jefferson Seide=Molléri: jefferson.molleri@bth.se; Kai=Petersen: Not Found; Emilia=Mendes: Not Found,"Abstract
Context
 Empirical research in 
software engineering
 contributes towards developing 
scientific knowledge
 in this field, which in turn is relevant to inform decision-making in industry. A number of empirical studies have been carried out to date in 
software engineering
, and the need for guidelines for conducting and evaluating such research has been stressed.
Objective:
 The main goal of this mapping study is to identify and summarize the body of knowledge on research guidelines, assessment instruments and knowledge organization systems on how to conduct and evaluate empirical research in software engineering.
Method:
 A 
systematic mapping study
 employing manual search and snowballing techniques was carried out to identify the suitable papers. To build up the catalog, we extracted and categorized information provided by the identified papers.
Results:
 The mapping study comprises a list of 341 methodological papers, classified according to research methods, research phases covered, and type of instrument provided. Later, we derived a brief explanatory review of the instruments provided for each of the research methods.
Conclusion:
 We provide: an aggregated body of knowledge on the state of the art relating to guidelines, assessment instruments and knowledge organization systems for carrying out empirical software engineering research; an exemplary 
usage scenario
 that can be used to guide those carrying out such studies is also provided. Finally, we discuss the catalog’s implications for research practice and the needs for further research.",21 Mar 2025,6,"The study provides guidelines and knowledge on how to conduct empirical research in software engineering, which can be valuable for startups looking to improve their research practices."
https://www.sciencedirect.com/science/article/pii/S095058491830185X,A first look at unfollowing behavior on GitHub,January 2019,Information and Software Technology,Not Found,Jing=Jiang: jiangjing@buaa.edu.cn; David=Lo: davidlo@smu.edu.sg; Yun=Yang: ayonel@qq.com; Jianfeng=Li: powerfaster@163.com; Li=Zhang: lily@buaa.edu.cn,"Abstract
Context
Many 
open source software projects
 rely on contributors to fix bugs and contribute new features. On GitHub, developers often broadcast their activities to followers, which may entice followers to be project contributors. It is important to understand unfollowing behavior, maintain current followers, and attract some followers to become contributors in OSS projects.
Objective
Our objective in this paper is to provide a comprehensive analysis of unfollowing behavior on GitHub.
Method
To the best of our knowledge, we present a first look at unfollowing behavior on GitHub. We collect a dataset containing 701,364 developers and their 4,602,440 following relationships in March 2016. We also crawl their following relationships in May 2013, August 2015 and November 2015. We conduct surveys, define potential impact factors, and analyze the correlation of factors with the likelihood of unfollowing behavior.
Results
Our main observations are: (1) Between May 2013 and August 2015, 19.8% of active developers ever unfollowed some users. (2) Developers are more likely to unfollow those who have fewer activities, lower programming language similarity, and asymmetric relationships.
Conclusion
Our results give suggestions for developers to reduce the likelihood of being unfollowed by their followers, and attract researchers’ attention on relationship dissolution.",21 Mar 2025,7,The analysis of unfollowing behavior on GitHub can be relevant for startups trying to understand user engagement and retention strategies in open source projects.
https://www.sciencedirect.com/science/article/pii/S0950584918301861,Adaptive monitoring: A systematic mapping,January 2019,Information and Software Technology,Not Found,Edith=Zavala: zavala@essi.upc.edu; Xavier=Franch: franch@essi.upc.edu; Jordi=Marco: jmarco@cs.upc.edu,"Abstract
Context
Adaptive monitoring is a method used in a variety of domains for responding to changing conditions. It has been applied in different ways, from monitoring systems’ 
customization
 to re-composition, in different application domains. However, to the best of our knowledge, there are no studies analyzing how adaptive monitoring differs or resembles among the existing approaches.
Objective
To characterize the current state of the art on adaptive monitoring, specifically to: (a) identify the main concepts in the adaptive monitoring topic; (b) determine the demographic characteristics of the studies published in this topic; (c) identify how adaptive monitoring is conducted and evaluated by the different approaches; (d) identify patterns in the approaches supporting adaptive monitoring.
Method
We have conducted a 
systematic mapping study
 of adaptive monitoring approaches following recommended practices. We have applied automatic search and snowballing sampling on different sources and used rigorous selection criteria to retrieve the final set of papers. Moreover, we have used an existing qualitative analysis method for extracting relevant data from studies. Finally, we have applied 
data mining
 techniques for identifying patterns in the solutions.
Results
We have evaluated 110 studies organized in 81 approaches that support adaptive monitoring. By analyzing them, we have: (1) surveyed related terms and definitions of adaptive monitoring and proposed a generic one; (2) visualized studies’ demographic data and arranged the studies into approaches; (3) characterized the main approaches’ contributions; (4) determined how approaches conduct the adaptation process and evaluate their solutions.
Conclusions
This cross-domain overview of the current state of the art on adaptive monitoring may be a solid and comprehensive baseline for researchers and practitioners in the field. Especially, it may help in identifying opportunities of research; for instance, the need of proposing generic and flexible 
software engineering
 solutions for supporting adaptive monitoring in a variety of systems.",21 Mar 2025,8,The study on adaptive monitoring provides valuable insights into different approaches and patterns that can be applied to improve monitoring systems in early-stage ventures.
https://www.sciencedirect.com/science/article/pii/S0950584918301848,Software product line evolution: A systematic literature review,January 2019,Information and Software Technology,Not Found,Maíra=Marques: mmarques@dcc.uchile.cl; Jocelyn=Simmonds: jsimmond@dcc.uchile.cl; Pedro O.=Rossel: prossel@ucsc.cl; María Cecilia=Bastarrica: cecilia@dcc.uchile.cl,"Abstract
Context:
 Software Product Lines (SPL) evolve when there are changes in the requirements, product structure or the technology being used. Different approaches have been proposed for managing SPL assets and some also address how evolution affects these assets. Existing mapping studies have focused on specific aspects of SPL evolution, but there is no cohesive body of work that gives an overview of the area as a whole.
Objective:
 The goals of this work are to review the characteristics of the approaches reported as supporting SPL evolution, and to synthesize the evidence provided by primary studies about the nature of their processes, as well as how they are reported and validated.
Method:
 We conducted a systematic literature review, considering six research questions formulated to evaluate evolution approaches for SPL. We considered journal, conference and workshop papers published up until March 2017 in leading digital libraries for computer science.
Results:
 After a thorough analysis of the papers retrieved from the digital libraries, we ended up with a set of 60 primary studies. Feature models are widely used to represent SPLs, so feature evolution is frequently addressed. Other assets are less frequently addressed. The area has matured over time: papers presenting more rigorous work are becoming more common. The processes used to support SPL evolution are systematic, but with a low level of automation.
Conclusions:
 Our research shows that there is no consensus about SPL formalization, what assets can evolve, nor how and when these evolve. 
Case studies
 are quite popular, but few industrial-sized case studies are publicly available. Also, few of the proposed techniques offer tool support. We believe that the SPL community needs to work together to improve the state of the art, creating methods and tools that support SPL evolution in a more comparable manner.",21 Mar 2025,7,The review of approaches for managing Software Product Lines evolution can offer startups useful information on how to adapt their products to changing requirements and technology.
https://www.sciencedirect.com/science/article/pii/S0950584916302178,Integration of feature models: A systematic mapping study,January 2019,Information and Software Technology,Not Found,Vinicius=Bischoff: viniciusbischof@edu.unisinos.br; Kleinner=Farias: kleinnerfarias@unisinos.br; Lucian José=Gonçales: lucianj@edu.unisinos.br; Jorge Luis=Victória Barbosa: jbarbosa@unisinos.br,"Abstract
Context
The integration of feature models has been widely investigated in the last decades, given its 
pivotal role
 for supporting the evolution of software product lines. Unfortunately, academia and industry have overlooked the production of a thematic analysis of the current literature. Hence, a thorough understanding of the state-of-the-art works remains still limited.
Objective
This study seeks to create a panoramic view of the current literature to pinpoint gaps and supply insights of this research field.
Method
A 
systematic mapping study
 was performed based on well-established empirical guidelines for answering six research questions. In total, 47 primary studies were selected by applying a filtering process from a sample of 2874 studies.
Results
The main results obtained are: (1) most studies use a generic notation (68.09%, 32/47) for representing feature models; (2) only one study (2%, 1/47) compares feature models based on their 
syntactic
 and semantics; (3) there is no preponderant use of a particular integration technique in the selected studies; (4) most studies (70%, 33/47) provide a product-based strategy to evaluate the integrated feature models; (5) majority (70%, 33/47) automates the integration process; and (6) most studies (90%, 42/47) propose techniques, rather than focusing on producing practical knowledge derived from empirical studies.
Conclusion
The results were encouraging and suggest that integration of feature models is still an evolving research area. This study provides insightful information for the definition of a more ambitious 
research agenda
. Lastly, empirical studies exploring the required effort to apply the current integration techniques in real-world settings are highly recommended in future work.",21 Mar 2025,6,The systematic mapping study on feature model integration offers insights into current literature gaps and research directions that can be beneficial for startups working on software product lines.
https://www.sciencedirect.com/science/article/pii/S0950584918301885,Empirical research on concurrent software testing: A systematic mapping study,January 2019,Information and Software Technology,Not Found,Silvana M.=Melo: morita@icmc.usp.br; Jeffrey C.=Carver: Not Found; Paulo S.L.=Souza: Not Found; Simone R.S.=Souza: Not Found,"Abstract
Background:
 
Concurrent software
 testing is a costly and difficult task, especially due to the exponential increase in the test sequences caused by non-determinism. Such an issue has motivated researchers to develop testing techniques that select a subset of the input domain that has a high probability of revealing faults. Academics and industrial practitioners rarely use most concurrent software testing techniques because of the lack of data about their applicability. Empirical evidence can provide an important scientific basis for the strengths and weaknesses of each technique to help researchers and practitioners choose concurrent testing techniques appropriate for their environments.
Aim:
 This paper gathers and synthesizes empirical research on concurrent software testing to characterize the field and the types of empirical studies performed.
Method:
 We performed a 
systematic mapping study
 to identify and analyze empirical research on concurrent software testing techniques. We provide a detailed analysis of the studies and their design choices.
Results:
 The primary findings are: (1) there is a general lack of empirical validation of concurrent software testing techniques, (2) the type of evaluation method varies with the type of technique, (3) there are some key challenges to empirical study design in concurrent software testing, and (4) there is a dearth of controlled experiments in concurrent software testing.
Conclusions:
 There is little empirical evidence available about some specific concurrent testing techniques like model-based testing and formal testing. Overall, researchers need to perform more empirical work, especially real-world 
case studies
 and controlled experiments, to validate properties of concurrent software testing techniques. In addition, researchers need to perform more analyses and synthesis of the existing evidence. This paper is a first step in that direction.",21 Mar 2025,7,"The research on concurrent software testing techniques provides valuable insights for researchers and practitioners in improving testing methodologies, but the practical impact on early-stage ventures may be limited at this stage."
https://www.sciencedirect.com/science/article/pii/S0950584918301897,A new algorithm for software clustering considering the knowledge of dependency between artifacts in the source code,January 2019,Information and Software Technology,Not Found,Sina=Mohammadi: Not Found; Habib=Izadkhah: izadkhah@tabrizu.ac.ir,"Abstract
Context:
 Software systems evolve over time to meet the new requirements of users. These new requirements, usually, are not reflected in the original documents of these software systems. Therefore, the new version of a software system deviates from the original and documented architecture. This way, it will be more difficult to understand it after a while and it will be difficult to make new changes conveniently. 
Clustering techniques
 are used to extract the architecture of a software system in order to understand it. An artifact 
dependency graph
 (ADG) is often used for clustering, which is extracted from a source code. In the literature, some hierarchical and search-based 
clustering methods
 have been presented to extract the software architecture. Hierarchical algorithms have reasonable search time; however, they are not able to find a good architecture. In contrast, search-based algorithms are often better in this regard; however, their time and space limitations make them useless in practice for large-scale software systems. Both hierarchical and search-based 
clustering methods
 overlook the existing knowledge in an ADG for clustering.
Objective:
 To overcome the limitations of the existing clustering methods, this paper presents a new deterministic 
clustering algorithm
 named Neighborhood tree algorithm.
Method:
 The new algorithm creates a neighborhood tree using available knowledge in an ADG and uses this tree for clustering.
Results:
 Our initial results indicate that the algorithm is better able to extract an acceptable architecture in a reasonable time, compared with hierarchical and search-based algorithms.
Conclusions:
 The proposed 
clustering algorithm
 is expected to greatly assist software engineers in extracting meaningful and understandable subsystems from a source code.",21 Mar 2025,8,"The development of a new clustering algorithm to extract software architecture more effectively has practical implications for startups dealing with evolving software systems, offering a tangible solution to a common problem."
https://www.sciencedirect.com/science/article/pii/S0883902625000138,Journal of Business Venturing 2024 year in review: The year of exercising entrepreneurial agency in response to crises,July 2025,Business Venturing,Not Found,Oana=Branzei: obranzei@ivey.ca; Jeffery S.=McMullen: mcmullej@iu.edu; Scott L.=Newbert: scott.newbert@baruch.cuny.edu; Christian=Schwens: schwens@wiso.uni-koeln.de,"Abstract
When various forms of crisis hit, they can stimulate changes in entrepreneurial agency – the capacity to act (or choose not to) – and the actions entrepreneurs take to mitigate the threats and pursue the new opportunities those crises create. While assessing articles for the 
Journal of Business Venturing
's annual “Best Paper” award, we observed this to be a recurring theme across a significant number of the studies published in 2024. Inspired by this research, we summarize the 17 articles that explored this theme and develop a framework that highlights material, relational, and discursive concerns brought about by crises. In response entrepreneurs across individual or collective levels take action to preserve or cultivate distinct forms of entrepreneurial agency – adaptive, allied, and censored – and to resolve various paradoxes of entrepreneurial agency. We close with a brief discussion of the growing relevance of a social symbolic lens in reconciling how entrepreneurs construe and respond to crises and how the specific forms of agency and paradox identified could inform theory both within and beyond entrepreneurship.
Executive summary
This article began as a search for the “Best Paper” published in 
Journal of Business Venturing (JBV)
 in 2024. The editor-in-chief selected a panel of editors who then reviewed each of the 49 articles published in volume 39, issues 1–6, to identify those that were bold, broad, and rigorous. We arrived at a shortlist of five articles that best exemplified these criteria, from which the entire 
JBV
 editorial team voted for the winner: “Sight unseen: The visibility paradox of entrepreneurship in an informal economy,” by Robert Nason, Siddharth Vedula, Joel Bothello, Sophie Bacq, and Andrew Charman.
In addition to enabling the selection of a best paper, this process revealed a common theme cutting across more than one-third of the articles published throughout the year; namely, “how entrepreneurs exercise agency in response to crises.” Traditionally, crises have been defined as periods of turmoil that disrupt patterns of economic activities and represent acute potential threats to the livelihoods of those affected. Over the past decade, however, scholars (both within and outside the field of entrepreneurship) have gradually shifted attention from single, separate, and short-lasting episodes that momentarily disrupt entrepreneurial endeavors (
Doern et al., 2016
) to plural, entangled and long-lasting combinations, sometimes referred to as poly-crises (
Klyver and McMullen, 2025
). To consider both conceptualizations, we take a broader perspective of crisis, using the term holistically to include both acute and enduring widespread structural challenges. We reason that, by defying conformity and attempting actions that question a society's taken for granted assumptions about how the world works, entrepreneurs embody and enact a paradox of agency in which restrictions on their capacity to act – which dispirit, discourage, or even devastate most people – instead stimulate them to seek to preserve or cultivate their agency not only by mitigating the threats that crises can pose, but also by leveraging them as opportunities to improve their situation.
We proceed as follows. After a brief introduction, we summarize how each individual article reflects and contributes unique insights to the overarching theme of exercising entrepreneurial agency in response to crises. Grouping the articles according to the type of crisis examined, we sensitize ourselves to the underlying mechanisms that entrepreneurs use in response by adopting a social symbolic lens (
Lawrence and Phillips, 2019
). Specifically, we hone our attention to the relative importance and interplay of material, relational, and discursive dimensions of social symbolic work as entrepreneurs construe and respond to different types of crises. We suggest that entrepreneurs encounter various limitations to and paradoxes of agency that they seek to resolve by 
adapting, allying,
 or 
censoring
 their capacity to act. Finally, we conclude by articulating potential avenues for scholars to elaborate on this tri-dimensional approach to entrepreneurial agency both within and beyond entrepreneurship theory and practice.",21 Mar 2025,10,"The study on how entrepreneurs exercise agency in response to crises provides critical insights for early-stage ventures facing challenges, offering a valuable framework to understand and navigate crisis situations, thereby making a significant impact on startups."
https://www.sciencedirect.com/science/article/pii/S088390262500014X,"The rise and fall of the girlboss: Gender, social expectations and entrepreneurial hype",July 2025,Business Venturing,"Hype, Hype cycle, Women's entrepreneurship, Gender, Feminist perspectives, Postfeminism, Media, Social expectations",Janice=Byrne: jbyrne@ivey.ca; Antonio Paco=Giuliani: antonio.giuliani@unibo.it,"Abstract
Hype is a collective vision and promise of a possible future, around which attention, excitement, and expectations increase over time (Logue & Grimes, 2022). Entrepreneurs employ cultural strategies, using framing to legitimize their endeavors and sustain the surrounding hype. Despite the importance of media in entrepreneurial hype, extant literature has yet to investigate media framing devices and how they shape and inform social expectations in the hype cycle. We also know that framing efforts are shaped by discursive struggles between actors (Kriechbaum et al., 2021) and that under-represented social groups are more constrained by dominant discourses. Yet, extant literature on entrepreneurial hype has thus far undertheorized power and inequality. We focus on one under-represented group - women – as they embody a glaring example of how media influence the social expectations associated with their entrepreneurial endeavors. Specifically, this study investigates how the media employ framing devices to generate social expectations for non-dominant groups (women entrepreneurs in our case) - and shape the hype cycle. To do so, we empirically analyze the evolution of the ‘girlboss’ hype, through a content analysis of 2671 media articles. Our contributions advance studies on entrepreneurial hype by explicating the role of media in the construction of hype. We contend that gender affords a critical power lens in the study of entrepreneurial hype that can be transferred to other contexts mired by inequality. We advance that feminist interrogations of media and entrepreneurship can contribute to understanding and addressing issues beyond gender.",21 Mar 2025,6,"The investigation into how media framing shapes social expectations in entrepreneurial hype, particularly focusing on women entrepreneurs, has theoretical value but may have limited immediate practical impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0883902625000102,"Time to say goodbye? The role of SBIR funding, VC rounds, and initial alliance for director exit in new ventures",May 2025,Business Venturing,"Venture boards, Venture board turnover, Governmental funding, VC funding, Alliances, Early stage ventures, Board life cycle, New venture life cycle",Vilma=Chila: v.chila@uva.nl; Koen=van den Oever: k.f.vdnoever@tilburguniversity.edu,"Abstract
Despite the significant interest in the composition and dynamics of new venture boards, our understanding of when directors exit the boards of new ventures is limited. Drawing on the organizational life cycles framework and resource dependence arguments, we posit that key life cycle events alter a venture's resource needs and dependencies on the board, occasioning director exit. Specifically, we argue that SBIR funding, Venture Capital rounds of funding, and first alliance act as markers of new venture evolution that render existing dependencies obsolete, increasing the likelihood of director exit. Interviews with board members in the semiconductor industry informed and substantiated our theoretical claims. The results show that SBIR funding and subsequent rounds of VC funding are linked to an increased likelihood of director exit, whereas a venture's first alliance is not. The paper sheds light on the interdependencies between the board's life cycle and the life cycle of the new venture.",21 Mar 2025,9,"The study on when directors exit new venture boards based on key life cycle events offers practical implications for startups, helping them understand the dynamics of board composition and resource dependencies, which can be crucial for the growth and success of early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S0883902625000126,Outside board director experience and the growth of new ventures,May 2025,Business Venturing,"Board of directors, Industry experience, Directorial experience, New ventures, New venture growth, Environmental characteristics",Tatevik=Harutyunyan: Tatevik.Harutyunyan@kristiania.no; Bram=Timmermans: Bram.Timmermans@nhh.no; Lars=Frederiksen: L.Frederiksen@mgmt.au.dk,"Abstract
Most research on entrepreneurship focuses on entrepreneurs' human and social capital as the drivers of new venture performance. However, less is known about how much the endowments of other strategic human resources, namely board directors, influence new venture performance. To generate new insights on this topic, we theorize and empirically investigate to what extent, and under which conditions, the experience of outside board directors affects new venture growth. Our analysis of Norwegian registry data on 15,594 new ventures does not provide immediate evidence that the presence of outside board directors or their experiences drive new venture growth. However, post hoc analysis suggests that the timing of board entry, combined with industry and directorial experience, plays a significant role in shaping growth outcomes. Additionally, the impact of industrial and directorial experience varies depending on the industry environment.",21 Mar 2025,7,"The study provides insights on the impact of outside board directors on new venture growth, highlighting the importance of timing, industry, and experiential factors. This knowledge can benefit early-stage ventures by optimizing board composition and entry strategies."
https://www.sciencedirect.com/science/article/pii/S0883902624000880,Atypical entrepreneurs in the venture idea elaboration phase,March 2025,Business Venturing,"Venture idea elaboration, Nascent entrepreneurs, Atypical entrepreneurship, Social group stereotypes, Person-profession congruence, Feedback types",Saggi=Nevo: nevos@rpi.edu,"Highlights
•
The paper explains why atypical nascent entrepreneurs may not receive the feedback they need for elaborating a venture idea.
•
The paper shows that a Black/White woman nascent entrepreneur is likely to be sanctioned for entering a profession with which she is seen as incongruent.
•
Although both are atypical entrepreneurs, nascent Black and White women entrepreneurs are associated with different stereotypes and receive different types of venture-related feedback.
•
Feedback providers are influenced by ingroup consensus, which can reduce their susceptibility to social stereotypes.",21 Mar 2025,3,The focus on stereotype impacts on feedback for atypical entrepreneurs may have limited direct practical implications for early-stage ventures in Europe.
https://www.sciencedirect.com/science/article/pii/S0883902624000892,The leisure paradox for entrepreneurs: A neo-institutional theory perspective of disclosing leisure activities in crowdfunding pitches,March 2025,Business Venturing,Not Found,Jacob A.=Waddingham: jwaddingham@txstate.edu; Jeffrey A.=Chandler: jeffrey.chandler@unt.edu; Katherine C.=Alexander: kalexander7@luc.edu; Sana=Zafar: szafar@georgiasouthern.edu; Aaron=Anglin: a.anglin@tcu.edu,"Abstract
Drawing from neo-institutional theory, we examine how entrepreneurs' disclosure of leisure activities influences the performance of their crowdfunding campaigns. We propose that entrepreneurs' disclosure of leisure activities in their campaigns negatively impacts crowdfunding performance because an institutional norm exists pressuring early-stage entrepreneurs to conform to workaholism. Using a sample of 8511 Kickstarter campaigns and a randomized experiment (
n
 = 436), we find evidence that entrepreneurs who disclose leisure activities are viewed as less workaholic. This, in turn, hurts backers' perceptions of the entrepreneurs' legitimacy, leading to lower crowdfunding performance. We also find women backers are more tolerant of entrepreneurs disclosing their leisure activities than men.",21 Mar 2025,4,"The study on leisure activities disclosure in crowdfunding campaigns sheds light on how perceptions affect campaign performance, which could be relevant for European startups seeking crowdfunding."
https://www.sciencedirect.com/science/article/pii/S0883902624000910,Regulatory institutions and cross-country differences in high-growth entrepreneurship rates: A configurational approach,March 2025,Business Venturing,Not Found,Thomas=Standaert: Not Found; Veroniek=Collewaert: veroniek.collewaert@vlerick.com; Tom=Vanacker: Not Found,"Abstract
Regulatory institutions are double-edged swords: stricter regulations can improve entrepreneurs' access to key resources but also constrain their discretion. Past research has focused on the individual and/or independent influence of regulatory institutions, calling for stricter regulation 
or
 deregulation. However, institutional theory suggests that the full configuration of regulatory institutions, including their possibly complex interactions, drives the trade-off between resource access and the constraints imposed by resource providers. Using an inductive approach and fsQCA analysis, we aim to better understand how configurations of regulatory institutions and contextual conditions influence high-growth entrepreneurship (HGE) rates across European countries. We find that three distinct configurations explain high country-level HGE rates, which include different regulatory institutions that sometimes work in opposing ways and do not necessarily work universally across contexts. Overall, this study deepens research at the nexus of institutional theory and high-growth entrepreneurship.",21 Mar 2025,5,"The research on regulatory institutions and high-growth entrepreneurship rates provides valuable insights for policymakers and entrepreneurs, highlighting the complex interactions that influence resource access and constraints."
https://www.sciencedirect.com/science/article/pii/S0883902624000909,"Angel funding and entrepreneurs' well-being: The mediating role of autonomy, competence, and relatedness",March 2025,Business Venturing,"Business angel investors, Entrepreneurs, Well-being, Psychological needs, Tweets, LIWC, Investor–investee relationship",Corinna Vera Hedwig=Schmidt: corinna-vera.schmidt@tu-dortmund.de; Patrick Sven=Gaßmann: patrick.gassmann@tu-dortmund.de; Nele=McElvany: nele.mcelvany@tu-dortmund.de; Tessa Christina=Flatten: tessa.flatten@tu-dortmund.de,"Abstract
While external funding is indispensable for most entrepreneurs to scale their ventures, entrepreneurship literature highlights the additional benefits of investors' continued involvement, such as access to their expertise and network. Angel investors, whose primary value-add often emerges through their relationship with the entrepreneurs, generate particularly pronounced benefits. Entrepreneurship research has established that bringing angel investors on board comes at the cost of relinquishing partial equity, which restricts entrepreneurs' control over their ventures; however, the individual-level consequences of funding for entrepreneurs remain largely unexplored. To address this gap, we study how angels' funding and their post-investment involvement in the venture affect entrepreneurs' eudaimonic well-being in the long term. Drawing on self-determination theory, we explore further how the psychological need for autonomy, competence, and relatedness mediates the relationship between angel funding and entrepreneurs' well-being. Self-determination theory states that individuals' verbalized language reflects their needs; accordingly, we use Linguistic Inquiry and Word Count (LIWC) analysis on a unique dataset of almost 125 million words derived from the tweets of 1667 entrepreneurs on X (formerly Twitter). As hypothesized, we find a positive association between angel funding and entrepreneurs' well-being. Autonomy negatively mediates this relationship, while competence and relatedness mediate it positively. We advance research on entrepreneurs' eudaimonic well-being and extend the literature on self-determination theory and individual-level consequences of angel funding.
Executive summary
Entrepreneurs often face a difficult trade-off: They must decide whether to accept funding from angel investors or relinquish some control over their venture. While much research centers on the business implications of this trade-off (Davila et al., 2003; Politis, 2008), the personal impact on entrepreneurs' eudaimonic well-being remains underexplored (Collewaert and Sapienza, 2016). This knowledge gap is concerning because entrepreneurs' well-being closely relates to their ventures' performance (Stephan et al., 2020b; Wach et al., 2016).
Recent calls for research (Stephan et al., 2023) emphasize the need to understand how external factors, like investor involvement, affect entrepreneurs' well-being by influencing the extent to which their psychological needs for autonomy, competence, and relatedness are satisfied, as outlined by self-determination theory (SDT) (
Deci and Ryan, 1985
, 2000). Despite the recognized importance of these factors, the impact of angel investors, who often form close relationships with entrepreneurs and engage deeply in their ventures (Fairchild, 2011; Politis, 2008), has been largely overlooked.
Our study addresses this gap by examining how angel funding and subsequent involvement influence entrepreneurs' eudaimonic long-term well-being. Using a dataset of 125 million words compiled from 1667 entrepreneurs' tweets on Twitter (now X) from 2006 to 2022, we apply Linguistic Inquiry and Word Count (LIWC) analysis to gain insights into the psychological states of these entrepreneurs (Block et al., 2019; Obschonka et al., 2017). This approach aligns with SDT, which posits that psychological needs fulfillment manifests in communication (Vansteenkiste et al., 2020).
Our findings reveal that angel investor involvement can significantly influence entrepreneurs' eudaimonic well-being—positively and negatively—by affecting entrepreneurs' psychological needs fulfillment. Our study thus complements research on entrepreneurs' well-being with longitudinal insights. First, it extends the literature on entrepreneurial well-being by providing a nuanced understanding of how angel funding and involvement, mediated by autonomy, competence, and relatedness, affect entrepreneurs' well-being over time (Stephan et al., 2023). Second, our study contributes to SDT literature by contextualizing investor involvement as an external factor and using large-scale social media data to assess entrepreneurs' psychological needs (Stephan et al., 2020a). Third, our study highlights the importance of the dynamics between angel investors and entrepreneurs, showing that such relationships significantly shape entrepreneurs' personal outcomes (Collewaert and Sapienza, 2016; Politis, 2008).
Beyond academic contributions, our study offers practical insights for entrepreneurs, angel investors, policy-makers, and universities by emphasizing the importance of understanding and managing the entrepreneurs' personal impacts of bringing angel investors on board.",21 Mar 2025,9,"The study on angel funding, post-investment involvement, and entrepreneurs' well-being offers significant practical implications for European early-stage ventures, providing insights into the personal impact of funding decisions and relationships with investors."
https://www.sciencedirect.com/science/article/pii/S0883902624000922,How environmental awareness and concern affect environmental entrepreneurial intent,March 2025,Business Venturing,"Environmental entrepreneurship, Entrepreneurial intent, Experiment, Environmental awareness, Environmental concern",Yasmin O.=Schwegler: Yasmin.Schwegler@heig-vd.ch; Jeffrey S.=Petty: Jeffrey.Petty@unil.ch,"Abstract
Environmental awareness and concern are implicit in virtually the entire environmental-entrepreneurship literature but typically not explicitly analyzed. To better understand what makes environmental entrepreneurs start their ventures, we need to understand those omnipresent variables. We therefore deconstruct environmental awareness and concern into different aspects, which we manipulate separately in two experimental studies. Our main finding is that key stakeholders' environmental awareness and concern are drivers of environmental entrepreneurship, as they signal to entrepreneurs that stakeholders are ready to support it. We thus identify a way of increasing environmental entrepreneurial intent in order to transform environmental problems into economic opportunities.
Executive summary
There is a growing belief among scholars and practitioners that environmental entrepreneurs can play a crucial role in addressing global environmental degradation by developing and providing innovative solutions that lead the way toward a more sustainable business world (Dean and McMullen, 2007; Hockerts and Wüstenhagen, 2010; Johnson and Schaltegger, 2019). A growing literature is investigating the drivers of such entrepreneurship (e.g., Muñoz and Cohen, 2018; Schaltegger, 2002; Shepherd et al., 2013; York et al., 2016), but two drivers — environmental awareness and concern — are typically only implicit in this literature, even though they are underlying most drivers that are investigated explicitly (e.g., Cohen and Winn, 2007; Dean and McMullen, 2007; Markman et al., 2019). Explicitly analyzing the role of environmental awareness and concern is crucial to better understand what makes environmental entrepreneurs start their ventures.
In this paper, we deconstruct environmental awareness and concern. The experimental method is ideally suited for that aim (Grégoire et al., 2019; Stevenson et al., 2020; Williams et al., 2019), as it allows us to manipulate several forms of awareness and concern in different experimental groups with slightly different pre-tested articles about an environmental problem. We then measure participants' intent to start a venture that addresses that problem, relative to their intent to start similar ventures that do not address the problem. In this way, we test the effect of the following types of environmental awareness and concern on environmental entrepreneurial intent (EEI): entrepreneurs' personal awareness, awareness of a solution to the problem, and awareness of other entrepreneurs addressing the problem; public awareness; entrepreneurs' own concern; public concern; customers' concern; and investors' concern.
The results indicate that especially customers' and investors' awareness of and concern about environmental problems increase entrepreneurs' intent to start environmental ventures. One explanation is that information about stakeholder concern, customers' willingness to buy from environmental ventures, and investors' willingness to invest in them increases the economic feasibility of such ventures and thus their business potential. This is in line with classic entrepreneurship theory, which postulates that entrepreneurship consists in the exploitation of economic opportunities (Shane and Venkataraman, 2000). Information about the environmental problem intended to raise entrepreneurs' own awareness of and concern about the problem have no effect on EEI in our study, which seemingly is in contrast to previous findings that idealistic motives and personal concern are primary drivers of EEI (Phillips, 2013; Shepherd and Patzelt, 2011; York et al., 2016). The findings can be reconciled by recognizing that our study assesses exclusively the overall effects of the experimental manipulations and controls for the potential entrepreneurs' environmental inclination, which we also find to be an important predictor of EEI. By controlling for it, we show that EEI can also be raised by factors external to the focal entrepreneur, in particular the environmental concern of key stakeholders.
Our findings thus complete the picture of the drivers of EEI by showing that if environmental awareness and concern are experienced not only by potential environmental entrepreneurs but their stakeholders, entrepreneurs' intent to address the relevant environmental problems rises. It also explains why, despite individual environmental awareness and concern, there is still widespread economic inaction in the face of known environmental problems (Heidbreder et al., 2021; York, 2018). According to our study, more EEI could be induced by a more widespread environmental concern, which encompasses both environmental entrepreneurs and their stakeholders.",21 Mar 2025,7,The research on environmental entrepreneurship and the role of stakeholders in driving entrepreneurial intent could provide valuable insights for early-stage ventures in Europe looking to address environmental issues and economic opportunities.
https://www.sciencedirect.com/science/article/pii/S0883902624000934,Fear the loss or welcome the gains? How stock options influence CEO risk-taking in corporate cleantech investments,March 2025,Business Venturing,"Corporate cleantech investments, Behavioral agency theory, CEO stock options, Founder CEO, Green transformation",David=Bendig: dbendig@uni-muenster.de; Colin=Schulz: Not Found; Maximilian=Möhwald: Not Found; Patrick=Pollok: Not Found,"Abstract
This study draws on the behavioral agency model to investigate how stock options incentivize CEO risk-taking related to investments in external clean technology (cleantech) ventures. Using longitudinal data from 540 publicly traded firms, we find that current option wealth is negatively associated with corporate cleantech investments while prospective option wealth is positively associated. The results show that founder CEOs, who exhibit different endowment and risk-bearing patterns than hired CEOs, do not perceive cleantech investments as mixed gambles. These findings advance understanding of the interplay between equity-based incentives, CEO characteristics, and incumbents' pursuit of sustainable business practices.",21 Mar 2025,5,"The study on CEO risk-taking related to cleantech ventures, while interesting, may have limited practical relevance for early-stage ventures in Europe compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S0883902624000946,"Coordination, sensemaking, and idea work: How founding teams pivot their venture ideas",March 2025,Business Venturing,"Pivoting, Founding teams, Spaces, Sensemaking, Idea work",Eva=Weissenböck: eva.weissenbock@imd.org; Nicola=Breugst: nicola.breugst@tum.de; Anna=Brattström: ab638@st-andrews.ac.uk,"Abstract
This study offers novel insights into how team structure and flexibility affect pivoting. It details how founding team coordination practices shape individual and collective sensemaking of feedback and efforts to improve a venture idea. Following seven founding teams, we identified how teams with overlapping responsibilities enjoyed the flexibility of both fragmented and holistic sensemaking. This enabled them to pivot when needed but otherwise persevere with their venture idea. In contrast, teams with clear separation of responsibilities engaged in fragmented sensemaking and only persevered with their idea. Our findings advance research on founding team coordination, pivoting, and teams' understanding of their venture ideas.",21 Mar 2025,6,Insights into team structure and pivoting could be beneficial for European startups in understanding how to adapt and persevere with their venture ideas.
https://www.sciencedirect.com/science/article/pii/S0883902624000533,Directors in new technology-based ventures: An empirical inquiry,March 2025,Business Venturing,"Boards of directors, Ventures, Venture capital, Corporate venture capital, Founders",Sam=Garg: samgarg@essec.edu; Michael=Howard: mdh189@iastate.edu; Emily Cox=Pahnke: eacox@uw.edu,"Abstract
In the emerging literature on venture boards, little research examines the association between different categories of venture directors and strategic firm outcomes. We conduct an empirical inquiry into how founder-directors, venture capitalist investor-directors and corporate venture investor-directors are related to inter-organizational alliances, innovation, and exits. In our longitudinal study based on hand-collected data on 156 medical device ventures in the US, we find that founder-directors are positively associated with patents and negatively associated with supply chain agreements. VC-directors are positively associated with exits but are negatively associated with R&D, supply chain agreements and patents. CVC-directors are negatively associated with patents and first product introductions. Adopting an abductive approach, we suggest potential mechanisms based on interviews with venture directors and CEOs and suggest future directions for venture boards scholarship.
Executive summary
Scandals at private firms such as Theranos and Uber (when it was private) have highlighted both the influence that boards of directors have on these firms and the relative opacity with which they operate. While there is a considerable literature, both theoretical and empirical, on the boards of public companies, there is a relative paucity of research on governance in private firms. At the same time, the distinctive features of private firm governance may limit the applicability of insights from public boards; one difference is that in venture boards, directors often have significant ownership stakes in the companies as founders and representatives of venture capital firms (VCs) or the investment arms of other corporations (CVCs). As part of this special issue on the boards of private firms, we undertake an empirical investigation of the impact that these types of directors have on a variety of firm outcomes.
We build on research on venture investing which hints at, but does not disentangle, the distinct impact of investors that have board seats versus investors that do not. Our analyses explores the impact that three types of venture directors- Founder-directors, VC-directors and CVC directors- have on strategic firm outcomes they are likely to influence in our context: inter-organizational ties, innovation and exit events.
We conduct our study within a sector of the US medical device industry, where both venture-directors and venture-investors are prevalent and where previous research indicates they are likely to impact ventures. We take an abductive approach to analyze hand-collected longitudinal data on the directors of ventures and on the firms in this industry between 1997 and 2018. Overall, the results suggest that different types of directors can be significantly associated with ventures strategic outcomes, with each type of director bringing their unique focus and expertise to the table. For example, the results indicate that founder-directors may focus more on technology development and less on commercial development. For CVC-directors we do not find a significant association with interorganizational tie formation or exits, but are significantly negatively associated with innovation outcomes. Finally, while VC-directors are negatively associated with some kinds of interorganizational ties and patenting, they are significantly positively associated with faster exits via acquisitions and IPOs.
We contribute to the emerging literature on venture boards through an abductive inquiry into how different types of venture directors are associated with some of the most important strategic outcomes involved in the growth, development and exits of ventures. These results control for investor and investment characteristics. The empirical relationships we document suggest a multitude of theoretical explanations that future researchers can build on to test hypotheses. Some of our surprising findings may have implications for key governance theories and their relevance to venture boards. For example, our results suggest that the agency perspective may be applicable to ventures through the 
principal-principal
 model but due to conflicts among different VC-directors, not among VC-directors and CVC-directors. By contrast, the resource dependence perspective can be extended by considering the relevance of 
organizational roles
 of resource providers as directors versus investors (e.g., CVC 
directors
 are neither particularly effective at providing complementary resources nor notably ""shark-like"" in misappropriating ventures) and that ventures may face significant coordination costs in orchestrating resources from different directors. Overall, our study suggests a more nuanced picture of conflict and cooperation in venture boards than has previously been identified and offers an opportunity for scholars to explore a broad array of theoretical explanations, including power and conflict.",21 Mar 2025,8,"The examination of different venture directors and their impact on strategic outcomes, especially in the medical device industry, is highly relevant for European early-stage ventures seeking to form alliances and drive innovation."
https://www.sciencedirect.com/science/article/pii/S0883902620306674,Does new venture team power hierarchy enhance or impair new venture performance? A contingency perspective,November 2020,Business Venturing,Not Found,Xiao-Yun=Xie: xiexy@zju.edu.cn; Wen=Feng: fengwen@zju.edu.cn; Qiongjing=Hu: qjhu@zju.edu.cn,"Abstract
Power hierarchy has the potential to both benefit and harm the functioning of new venture teams (NVTs) and hence new venture performance. Integrating structural 
contingency theory
 with the literature on power hierarchy, we propose that the effect of NVT power hierarchy on new venture performance is contingent on NVT homogeneity (as indicated by functional background homogeneity and shared team experience) and the powerholder's prior founding experience. Specifically, we propose that the effect of NVT power hierarchy on new venture performance will be positive when NVT homogeneity is low but negative when NVT homogeneity is high. Furthermore, this positive (negative) effect under low (high) NVT homogeneity will be strengthened by the powerholder's prior founding experience. Based on a five-year panel data of 285 new Internet ventures listed on the National Equities Exchange and Quotations (NEEQ) in China combined with qualitative fieldwork, our hypotheses received general support. We discuss the theoretical and practical implications of our findings on how NVTs should design 
power structures
 to achieve optimal new venture performance.",21 Mar 2025,6,The study on power hierarchy in new venture teams and its impact on performance may provide useful guidance for European startups in designing effective power structures.
https://www.sciencedirect.com/science/article/pii/S235267342400057X,Not all leavers are equal: How rank and destination influence enforcement of restrictive covenants,June 2025,Business Venturing Insights,"Spinout ventures, Employee mobility, Employee spinouts, Employee entrepreneurship, Restrictive covenants, Non-competes",Sepideh=Yeganegi: syeganegi@wlu.ca; André O.=Laplume: alaplume@torontomu.ca; Bradley=Bernard: bradley.bernard@torontomu.ca,"Abstract
Restrictive covenants like non-competes, non-solicitations, and non-disclosures may pose barriers to spinout ventures and mobility to competitors. However, we know little about the enforceability of these agreements despite their widespread use and associated chilling effects. Examining 332 Canadian court decisions, we find a higher rate of enforcement in cases involving high rank leavers (i.e., managers and owners) versus low rank leavers (regular employees and contractors) especially those who form spinout ventures. Our key insight is that enforcement rates differ significantly across different types of leavers. Low rank leavers and their previous employers may overestimate the potential for enforcement, creating chilling effects (i.e., where employees think they are more restricted by their employment agreements than they really are) that can deter employee mobility and entrepreneurship.",21 Mar 2025,8,"The study sheds light on the enforceability of restrictive covenants and their impact on employee mobility and entrepreneurship, offering valuable insights for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673425000010,Opportunity amidst explosions: How armed conflicts spark informal entrepreneurship in emerging economies,June 2025,Business Venturing Insights,"Armed conflict, FDI, Informal entrepreneurship, Emerging economies, External enablers",Esther=Salvi: esther.salvi@imd.org; Diana M.=Hechavarria: dhechavarria@babson.edu; Daniela=Gimenez-Jimenez: daniela.gimenez@tu-dortmund.de,"Abstract
Armed conflicts, such as a surge in bombings, shelling, and other violent outbreaks, dramatically affect national economies and business activities. This article aims to merge the previously separate research streams on foreign direct investment (FDI) and informal entrepreneurship, illustrating how these elements interact within the context of armed conflict. Utilizing data from eight emerging economies over 16 years, we construct a two-step mediation model to demonstrate that increased armed conflict reduces FDI, enabling informal entrepreneurship. Our study offers two important insights: First, it introduces a novel perspective on armed conflict as an external enabler of informal entrepreneurship. Second, it uncovers three pivotal mechanisms—
legitimation, demand substitution,
 and 
resource access
—through which the decline in FDI resulting from armed conflict intensification stimulates informal entrepreneurial activity. The findings of this study contribute not only to academic discourse at the intersection of international business and entrepreneurship, but also provide practical insights that are essential for shaping policies, guiding international business strategies, and fostering economic resilience in regions affected by violence and instability.",21 Mar 2025,7,"The research linking armed conflict, FDI, and informal entrepreneurship provides important perspectives for understanding economic dynamics in conflict-affected regions, with implications for startup strategies."
https://www.sciencedirect.com/science/article/pii/S2352673425000022,Jingle bells and juggling stress: An IPA analysis of well-being and ill-being among market entrepreneurs in outdoor finnish christmas markets,June 2025,Business Venturing Insights,"IPA analysis, Well-being, Ill-being, Market entrepreneurs, Outdoor markets, Christmas",Regina=Casteleijn-Osorno: racaos@utu.fi; Linh=Duong: linh.duong@abo.fi,"Abstract
Christmas markets evoke joyful, emotional, and nostalgic impressions to just about everyone. But what about the people behind the booths? Through semi-structured interviews with six sellers and a market organizer in Finland, we apply an interpretive phenomenological analysis (IPA) approach to illustrate that positive and detrimental effects on hedonic and eudaimonic well-being co-exist, addressing both physical and emotional stressors of entrepreneurs when selling at Christmas markets. Also, the results showcase well-being trade-offs that bring the sense of fulfillment when doing entrepreneurial activities. This study contributes to entrepreneurship well-being research and research concerning societal implications to the market seller community.",21 Mar 2025,5,"While the study on Christmas markets provides interesting insights on well-being and entrepreneurship, its direct practical implications for startups may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673425000046,Sexual harassment by multiple stakeholders in entrepreneurship: The case of Japan,June 2025,Business Venturing Insights,Not Found,Takanori=Kashino: kashino@eireneuniversity.org,"Abstract
Sexual harassment in entrepreneurial contexts remains a critical yet underexplored issue. While workplace harassment in traditional employment settings has been widely researched, little is known about how power dynamics and cultural norms shape the sexual harassment risks faced by entrepreneurs. To address this gap, we conducted an exploratory study within Japan's entrepreneurial context, where cultural norms and limited institutional protections create distinct vulnerabilities. Through an anonymous online survey of 197 participants (105 of whom identified as female entrepreneurs), we collected both quantitative and qualitative data. We found that 52.4% of female entrepreneurs reported experiencing sexual harassment by multiple stakeholders in the past year. Investors emerged as the primary perpetrators (43.2% of cases), followed by customers, mentors, and members of entrepreneurial support organizations. Qualitative insights suggest that power asymmetries, especially in funding and mentorship relationships—create unique vulnerabilities for entrepreneurs to sexual harassment that differ from those in a traditional workplace. Our study not only advances research on Japanese entrepreneurship, but also provides actionable insights for other contexts with similar cultural and institutional barriers. These findings can inform efforts to combat gender stereotypes and strengthen legal protections against harassment.",21 Mar 2025,9,"The exploration of sexual harassment in entrepreneurial contexts offers crucial insights for startup founders, addressing a pressing issue and providing actionable recommendations."
https://www.sciencedirect.com/science/article/pii/S2352673425000034,"“I could, but why should I?”: Entrepreneurial women's career pathways and how founding fits in (or doesn't)",June 2025,Business Venturing Insights,"Social cognitive career theory (SCCT), Entrepreneurial self-efficacy, Women's entrepreneurship, Careers, STEM, Career success",Hana=Milanov: hana.milanov@tum.de; Katharina=Prantl: Not Found; Sheri=Sheppard: Not Found; Xiao=Ge: Not Found,"Abstract
Why do highly qualified women with entrepreneurial self-efficacy choose not to pursue tech-venture founding? We adopt a career-perspective and interview 17 female Stanford engineering graduates—women who possess high entrepreneurial self-efficacy (ESE), educational prestige, access to Silicon Valley's entrepreneurial networks, and who already overcame hurdles associated with entering and succeeding in a gender-incongruent setting. Despite these seemingly uniform conditions for tech entrepreneurship, we reveal four distinct career pathways: Skill Hunters, Life Masters, Strategists, and Idealists. While Skill Hunters remain attracted to entrepreneurship, the other groups are disillusioned with it, despite having experience as corporate entrepreneurs, business owners, and founders of not-for-profits. Our findings demonstrate how (in)congruency of women's career principles and context-based entrepreneurial outcome expectations shapes their entrepreneurial engagement.",21 Mar 2025,6,"The study on women with entrepreneurial self-efficacy provides valuable insights, but its focus on career pathways may have slightly less immediate practical relevance for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S235267342500006X,"Too nice, but not too wise: The curvilinear effects of employee orientation on new venture performance",June 2025,Business Venturing Insights,Not Found,Myeongho David=Park: myepark@okstate.edu; Shawn L.=Berman: Not Found,"Abstract
This paper examines the mechanisms of stakeholder management within the context of entrepreneurship, with a particular focus on a primary stakeholder group: employees. We propose a novel curvilinear relationship between employee orientation and new venture performance and further explore the moderating role of entrepreneurial team human capital. Using longitudinal data from the Kauffman Firm Survey, we confirm the presence of an inverted U-shaped relationship between employee orientation and new venture performance. We also find that the human capital of entrepreneurial teams moderates this curvilinear relationship, shifting the optimal point of the curve toward a lower employee orientation. Our study contributes to the literature on the intersection of stakeholder management and entrepreneurship.",21 Mar 2025,8,"The study provides valuable insights into stakeholder management within entrepreneurship and how it impacts new venture performance, with practical implications for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673425000058,Weathering the pivot: Stability and turnover in new venture teams,June 2025,Business Venturing Insights,Not Found,Griffin W.=Cottle: gcottle@umassd.edu; Jessica=Jones: jessica.jones@utk.edu; Brian S.=Anderson: brian.anderson@ku.edu; Jeffrey S.=Hornsby: hornsbyj@umkc.edu,"Abstract
Research on strategic change and NVT turnover suggests that members of the new venture team are likely to depart during a pivot as a result of structural changes within the firm. However, this perspective overlooks the relational aspect of venturing, which bonds team members to their ventures. Although NVT members are critical to both the creation of economic profit and the operational viability of the firm, at present we have little understanding of whether the decision to pivot strengthens or weakens their commitment to the firm. Using the employment data of 91 new venture teams that underwent a pivot, we examine how the sudden changes that are brought about affect the stability of the team in the 12-weeks leading up to and following their occurrence. In doing so we help integrate the literature on NVT turnover and entrepreneurial bonding, and provide a fuller distribution of the outcomes that accompany new venture pivots.",21 Mar 2025,6,"Examining the impact of sudden changes in new venture teams during a pivot provides relevant information for startups, although the practical applications may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673425000071,Crowdfunding beyond borders: Geographic disparities in crowdfunding success,June 2025,Business Venturing Insights,Not Found,Liran=Maymoni: Not Found; Eliran=Solodoha: liran.maymoni@gmail.com,"Abstract
This study investigates how geographic location influences the effectiveness of entrepreneurial signals in crowdfunding, integrating Regional Development Theory and Signaling Theory. Using data from 2578 reward-based crowdfunding campaigns on the Israeli platform Headstart (2012–2022), we find that structural advantages in central areas enhance the effect of entrepreneurial signals, such as prior experience and reward diversity, while systemic barriers in peripheral regions weaken these signals' effectiveness. The principal insight of this study is that geographic context not only shapes crowdfunding success but also moderates the effectiveness of signaling strategies, thereby intensifying regional disparities. These findings contribute to the literature by demonstrating how geographic context moderates the strength and interpretation of entrepreneurial signals, ultimately influencing funding success and engagement. Practical recommendations include enhancing campaign visibility for peripheral entrepreneurs and developing tailored training programs to optimize their signaling strategies.",21 Mar 2025,9,"The study sheds light on how geographic location influences crowdfunding success and the effectiveness of signaling strategies, offering practical recommendations for entrepreneurs, especially in European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673425000083,Close but not nearby? Rethinking proximity in the digital era of entrepreneurial ecosystems,June 2025,Business Venturing Insights,"Entrepreneurial ecosystems, Proximity, Digitalization",Olivier=Lamotte: olamotte@em-normandie.fr,"Abstract
Recent literature has increasingly challenged the traditional view of entrepreneurial ecosystems (EEs) as geographically bounded entities, highlighting how entrepreneurial activities span spatial boundaries through networks, digital platforms, and various forms of collaboration. Building on these insights, this article proposes an analytical framework based on the proximity approach to understand better how EEs function in an increasingly digitalized world. We argue that while geographic proximity remains relevant, other forms of proximity (cognitive, social, organizational, and institutional) are equally crucial in shaping resource interactions and coordination within EEs. We also examine how digitalization transforms these proximity dynamics, creating new possibilities for entrepreneurial activities while potentially reducing the importance of geographic proximity. The key insight of our study is that beyond mere geographic closeness, diverse forms of proximity shape actors’ interactions within EEs, while digitalization reconfigures these dynamics, extending EEs boundaries. This research contributes to the literature by offering a more nuanced theoretical framework for understanding the spatial and non-spatial dimensions of EEs, with implications for both research and policy.",21 Mar 2025,7,"The proposed analytical framework for understanding entrepreneurial ecosystems in a digitalized world is insightful, but the direct impact on European early-stage ventures may be somewhat abstract."
https://www.sciencedirect.com/science/article/pii/S2352673424000374,Gender effects and firm financial performance: A SUMAD meta-analysis of social responsibility and family-to-work conflict,November 2024,Business Venturing Insights,Not Found,Mark=Geiger: geigerm1@duq.edu,"Abstract
The current study uses 
SUMAD
 meta-analytic methods (Oh, 2020) to examine 
gender differences
 in social responsibility and family-to-work conflict. Synthesizing evidence from across social science literature, the results of this study provide an evidence-based foundation to support more theorizing and practical discourse regarding gender effects in entrepreneurship. As explained by theories of socialization and social roles, 
gender differences
 in (a) socially responsible attitudes and behaviors and (b) the balance between family and work responsibilities, are likely two of the more pervasive gender effects that influence entrepreneurial careers. The goal of this study is to motivate more research and practical discussion on these and related gender effects to improve our understanding of entrepreneurship phenomena. Using firm performance as an example, the results of the 
SUMAD
 meta-analysis suggest that gender effects related to social responsibility and family-to-work conflict have significant consequences for entrepreneurship outcomes. Based on the evidence and theory rooted in socialization and social roles, the current study calls for more theorizing and primary-level studies on these and related gender effects in entrepreneurship research.
Comment to Readers:
 
Does gender matter? Of course it does (depending on the issue).
 A simple search of “does gender matter” reveals ample discussion on this topic across a variety of gender issues. In this article I highlight 
gender
 regarding differences between women and men in social responsibility and family-to-work conflict. As the evidence suggests, gender does indeed matter as women – on average – are more socially responsible and have more family-to-work conflict than their men counterparts. The results of this study show that greater social responsibility is tied to better business performance whereas greater family-to-work conflict is tied to worse business performance. So, what should we do? First, acknowledge the fact that women and men are different in the contexts of social issues and family matters to clear the way for constructive discourse about these 
gender differences
. Second, embrace that women are higher than men in socially responsible attitudes and behaviors, and that more women in business could inherently result in more socially responsible business practices. Moreover, while this is a 
societal win
 in and of itself, the results suggest it could also carry over to improved financial and economic performance. Lastly, focus more on “why” there are differences between women and men regarding family-to-work conflict. Specifically, emphasize both societal-driven influences (e.g., stereotypes; biases) and individual-driven influences (e.g., individual differences; personal preferences). Understanding these influences, which are not mutually exclusive, is key for maximizing the personal and professional well-being of both women and men.",21 Mar 2025,5,"While the study on gender differences in social responsibility and family-to-work conflict is relevant, the direct impact on practical applications for European early-stage ventures, especially startups, may be less pronounced compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673424000477,Extending behavioral theory of the firm to new ventures: Dispositional optimism as a moderating influence on new product introductions in high-tech ventures,November 2024,Business Venturing Insights,Not Found,R. Isil=Yavuz: ryavuz@bryant.edu; Dev K.=Dutta: dev.dutta@unh.edu; Mehmet A.=Soytas: mehmet.soytas@kfupm.edu.sa,"Abstract
Extant literature has typically drawn from the behavioral theory of the firm (BTOF) to examine new product introductions in the context of well-established companies. This paper extends the behavioral theory of the firm to entrepreneurial firms and argues that jointly considering founders' dispositional optimism together with the performance feedback promises to yield a better understanding of new product introductions in new ventures. We analyze a longitudinal dataset on the activities of 344 newly founded high technology ventures in the United States. The key insight of our study is that when BTOF is applied to the context of nascent, entrepreneurial ventures, the personality and dispositional characteristics of the entrepreneur must be considered. Specifically, we find that performance attainment discrepancy leads to new product introductions, but only when the entrepreneur's dispositional optimism level is high.",21 Mar 2025,7,"This abstract provides valuable insights into the impact of founders' dispositional optimism on new product introductions in entrepreneurial ventures, which is crucial for early-stage startups."
https://www.sciencedirect.com/science/article/pii/S2352673424000519,Uncovering wellbeing: The complex realities of mompreneurs with additional needs children through Lego® Serious Play®,November 2024,Business Venturing Insights,"Lego® Serious Play®, Mompreneur, Wellbeing, Additional needs, Disability, Sensitive topics, Hermeneutic constructivist, Social constructionist, Caregiving, Entrepreneur",Regina=Casteleijn-Osorno: racaos@utu.fi,"Abstract
This paper explores the complexities of identifying the wellbeing of mompreneurs (mother-entrepreneurs) who are also caregivers to children with additional needs. A social constructionist perspective, Lego® Serious Play® was employed in individual interviews to uncover their complex wellbeing realities while pursuing entrepreneurship. A hermeneutic constructivist lens was applied to further conceptualize the language behind their experiences. The findings present three dimensions of wellbeing: 1) Internal conflict and self-neglect 2) Empowerment, Independence, Fulfilment 3) Resilient Control: Keeping the Balance. These dimensions provide an idiographic understanding, contributing to the broader knowledge of wellbeing as a component of entrepreneurship alongside caregiving responsibilities.",21 Mar 2025,5,"While exploring the wellbeing of mompreneurs is important, the practical application and impact on early-stage ventures may be limited compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673424000520,How sector fluidity (knowledge-intensiveness and innovation) shapes startups’ resilience during crises,November 2024,Business Venturing Insights,"Resilience, Value for customers, Digital transformation, Startups, Business models, COVID-19",Asif=Tanveer: muhammadasif.tanveer@hdr.qut.edu.au; Rui=Torres de Oliveira: rui.torresdeoliveira@deakin.edu.au; Shaheer=Rizvi: shaheer.rizvi@iub.edu.pk,"Abstract
In the context of the crises, this study sheds light on the varying impact of crises on startups in different sectors and outlines the specific resilience practices utilized in response, recovery, and growth phases. The research team conducted qualitative analysis on 18 public discussion interviews featuring key stakeholders in the Indian startup ecosystem, which included 51 chief executive officers, founders, cofounders, and venture capitalists who reflected on COVID-19. The study found that high-fluidity sectors (highly knowledge-intensive and innovative) leverage their agility through resourcefulness and customer value creation. In contrast, low-fluidity sectors (low in knowledge intensiveness) primarily focus on operational adjustments. As the transition goes from response to recovery, high-fluidity sectors prioritize digital transformation and strategic shifts, while low-fluidity sectors continue to cope. In the growth phase, high-fluidity startups exhibit growth aspirations, while low-fluidity ones emphasize business model innovation. This research provides valuable sector-specific insights into resilience and highlights the evolution of these strategies throughout a crisis, thereby enhancing our understanding of startup resilience in the context of the crisis.",21 Mar 2025,8,"This abstract offers sector-specific insights into startup resilience during crises, which can greatly benefit early-stage ventures in understanding and navigating challenging times."
https://www.sciencedirect.com/science/article/pii/S2352673424000507,Entrepreneurial finance and sustainability: Do institutional investors impact the ESG performance of SMEs?,November 2024,Business Venturing Insights,"Entrepreneurial finance, Venture capital, Private equity, Sustainability, Environmental, social, and governance (ESG), Corporate social responsibility (CSR)",Wolfgang=Drobetz: Not Found; Sadok=El Ghoul: Not Found; Omrane=Guedhami: Not Found; Jan P.=Hackmann: Not Found; Paul P.=Momtaz: momtaz@tum.de,"Abstract
Institutional investors improve the environmental, social, and governance (ESG) performance of small- and medium-sized enterprises (SMEs). Our difference-in-differences framework shows that the backing from private equity and venture capital funds leads to an increase in SMEs’ externally validated ESG scores compared to their matched non-investor-backed peers. Consistent with “ESG-as-insurance” theory, the ESG performance of SMEs with a higher probability of failure is more likely to benefit from the backing of institutional investors. This positive effect is heterogeneous; while SMEs with high ex-ante ESG performance further improve their ESG performance following institutional investor backing, SMEs with low ex-ante ESG performance are unlikely to implement any improvements. Entrepreneurial finance seems to help sustainable entrepreneurs transform into “sustainability champions,” while neglecting the betterment of non-sustainable SMEs.",21 Mar 2025,6,"The study on institutional investors and ESG performance is relevant, but the focus on small- and medium-sized enterprises may limit its direct applicability to early-stage startups."
https://www.sciencedirect.com/science/article/pii/S2352673424000556,What’s the risk? It depends. Entrepreneurs’ and employees’ perceptions of domestic city-level institutional risk,November 2024,Business Venturing Insights,Not Found,Kaitlyn=DeGhetto: kdeghetto1@udayton.edu; Zachary A.=Russell: russellz1@xavier.edu,"Abstract
With a focus on entrepreneurs' decisions related to domestic location choices, this study draws from the international business institutional risk literature to evaluate city-level risk perceptions while accounting for individual-level political views. Specifically, we surveyed entrepreneurs and prospective employees in an effort to evaluate how important 1) safety risk, 2) political risk, and 3) social risk are when considering where to live, work, and start businesses. This process also included a comparison to ease of doing business, a previously studied driver of investment decisions. To identify low (and high) risk domestic investment locations, we had participants rate 25 large U.S. cities on the risk factors. Our findings indicate that entrepreneurs and prospective employees care about the city-level institutional risk factors. However, the focus and perceptions of entrepreneurs and prospective employees are greatly influenced by political views, perceptions do not always mirror objective data, and the two groups weight risk differently. The key insight of our study is that, in order to access and maintain valuable human capital, entrepreneurs should begin considering employees' perceptions related to city-level institutional risk. Importantly, these perceptions are biased by factors such as one's political views. Likewise, to attract business investment, city leaders should consider these risk perceptions.",21 Mar 2025,4,"While evaluating city-level risk perceptions is interesting, the direct impact on European early-stage ventures may be less significant compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673423000732,Pouring the Paycheck Protection Program into craft beer: PPP employment effects in service-intensive industries,June 2024,Business Venturing Insights,Not Found,Aaron J.=Staples: astaples@utk.edu; Kristopher=Deming: Not Found; Trey=Malone: Not Found; Craig W.=Carpenter: Not Found; Stephan=Weiler: Not Found,"Abstract
Small businesses in the food and beverage service 
industry
 are particularly vulnerable to crises such as the COVID-19 pandemic. One of the most salient vulnerabilities was the drastic decline in consumer spending at eating and drinking places, generating unprecedented swings in employment in this service-intensive sector. Governments across the globe implemented rapid response fiscal policies to mitigate these economic damages and improve small business crisis management. One such policy was the Paycheck Protection Program (PPP) in the United States. This study links restricted microdata from the Colorado Quarterly Census of Employment and Wages to microdata on PPP loan recipients to assess whether the loan program effectively reduced unemployment rates in Colorado's craft beer 
industry
. The results of a staggered difference-in-differences framework indicate immediate and longer-term positive and statistically significant effects of the loan program on employment outcomes, with employment effects ranging from 16.8 to 19.5%. These results emphasize the importance of understanding the loan program’s effectiveness among hard-hit industries comprised of small businesses.",21 Mar 2025,7,"This study provides valuable insights into the effectiveness of government policies, such as the Paycheck Protection Program, in supporting small businesses in the food and beverage industry during crises like the COVID-19 pandemic. The findings can be informative for European early-stage ventures facing similar challenges."
https://www.sciencedirect.com/science/article/pii/S2352673424000040,How and why do social entrepreneurs experience goal conflict differently?,June 2024,Business Venturing Insights,Not Found,Rebecca=Pieniazek: R.Pieniazek@leeds.ac.uk; Kerrie L.=Unsworth: Not Found; Hannah=Dean: Not Found,"Abstract
It is well-known that the need for both social and financial missions creates tension within social enterprises. Less well-known are the specifics around how and why social entrepreneurs themselves construct and experience their situation. Given people vary in their psychological representations of their goals from concrete (i.e., tasks) to more abstract (i.e., values), we anticipated that goal conflict with engaging in financial activities could vary along these lines, leading to potentially different solutions for support. Through collecting interviews and focus group data using goal hierarchies from 37 social entrepreneurs, we find six constructed realities with different salient goals at different levels of cognitive abstraction which either dictate, conflict with, or are dissociated from financial activities. These can explain why social entrepreneurs perceive their financial activities differently – financial activities as out of sight out of mind, aversive, a ball to juggle, a necessary evil, part and parcel, and as king - which are associated with four experiences of goal conflict (i.e., goal conflict as continual questioning, inevitable, manageable, and irrelevant).",21 Mar 2025,5,"While the study sheds light on the internal conflicts faced by social entrepreneurs in balancing social and financial goals, the practical implications for European early-stage ventures, especially startups, may be limited as it focuses more on psychological representations and goal conflicts."
https://www.sciencedirect.com/science/article/pii/S2352673424000143,"Upward, downward or steady: How social class experience shapes transnational social venturing",June 2024,Business Venturing Insights,"Social class, Social mobility, Venturing, Transnational, Entrepreneur",Nkosana=Mafico: nkosana.mafico@ed.ac.uk; Anna=Krzeminska: Not Found; Charmine=Härtel: Not Found; Josh=Keller: Not Found,"Abstract
Transnational social entrepreneurs leverage their cross-border knowledge and experiences to create and exploit opportunities in multiple markets. However, this knowledge and experience is not homogeneous or equally distributed among them. In this paper, we examine how the social class experiences of 18 transnational social entrepreneurs from the African diaspora living in the West influence their transnational social venturing. We identify four types of Transnational Social Class Experience (TSCE)—Grounded, Elite, Fallen and Elevated—each associated with a different approach to transnational social venturing. Our key contribution is introducing and unpacking the concept of Transnational Social Venturing Advantage (TSVA): the unique benefits that transnational social entrepreneurs can gain when their economic experiences across multiple countries intersect with the varied sociocultural environments they encounter. We also develop a framework that elucidates the connections between TSCE and social venturing approaches through TSVA. Taken together, our study advances the literature on transnational social venturing by unpacking the social class experience dynamics that enable transnational social entrepreneurs to access resources and understand their beneficiaries. It also advocates for a shift beyond a low versus high social class 
dichotomy
 in the broader (transnational) entrepreneurship discourse to a spectrum-based approach that accounts for social class experiences gained across borders.",21 Mar 2025,8,The examination of how social class experiences influence transnational social venturing provides actionable insights for European early-stage ventures looking to enter multiple markets. The concept of Transnational Social Venturing Advantage (TSVA) can be particularly valuable for startups seeking to expand globally.
https://www.sciencedirect.com/science/article/pii/S2352673424000209,Unbinding ideology: The impact of communist indoctrination revocation in polish schools on later life self-employment,June 2024,Business Venturing Insights,Not Found,Pankaj C.=Patel: pankaj.patel@villanova.edu,"Abstract
Given communist ideologies discourage individual 
enterprise
, this research investigates whether eliminating compulsory Marxist-Leninist indoctrination from schools influences later life self-employment. Focusing on a mid-1950s reform in Poland that revoked the Communist indoctrination curriculum while holding other aspects constant, the study leverages variation in exposure based on annual 
school enrollment
 cut-off birthdates. Contrary to expectation, the empirical analysis finds no discernible effect of indoctrination removal on later-life self-employment. Additionally, the study examines whether Polish immigrants exposed to reform and arriving in the US after 1960 exhibit increased self-employment propensity, but finds no significant differences. Overall, the study's findings highlight negligible impacts of the revocation of Communist indoctrination in Polish schools on self-employment.",21 Mar 2025,3,"While the research investigates the impact of Communist indoctrination on self-employment, the findings may have limited relevance for European early-stage ventures or startups in the present context. The negligible effects observed may not provide significant practical value."
https://www.sciencedirect.com/science/article/pii/S2352673424000179,Entrepreneurship after prison: It’s complicated,June 2024,Business Venturing Insights,Not Found,Fiona=Robinson: firobinson@mtroyal.ca; Stephanie A.=Fernhaber: sfernhab@butler.edu,"Abstract
Entrepreneurship is increasingly seen as a creative solution for individuals after they have been released from prison given the difficulties they face in finding viable employment. However, considering that entrepreneurship inherently involves maneuvering around and overcoming obstacles, it is likely an even more complicated endeavor for these individuals. A 
thick problem description
 of entrepreneurship after prison is needed to better understand the unique challenges associated with this unconventional entrepreneurial journey. Drawing on the existing literature coupled with semistructured interviews with five individuals who started businesses after being incarcerated, we utilize an empathy mapping tool to explicate our findings. We then outline key insights and offer recommendations on how to move forward.",21 Mar 2025,6,"The exploration of entrepreneurship after prison and the unique challenges faced by individuals post-incarceration is insightful. However, the practical implications for European early-stage ventures or startups may be limited, as the focus is more on understanding obstacles faced by a specific group of entrepreneurs."
https://www.sciencedirect.com/science/article/pii/S2352673423000355,Bridging worlds: The intersection of religion and entrepreneurship as meaningful heterodoxy,November 2023,Business Venturing Insights,"Religion, Entrepreneurship, Heterodoxy, Subfield",Brett=Smith: smithbr2@miamioh.edu; Ali Aslan=Gümüsay: Not Found; David M.=Townsend: Not Found,"Abstract
There is a resurgence in both the advancement and critique of research at the intersection of religion and entrepreneurship. It is precisely because there are important conflicts, tensions, and paradoxes in religion and entrepreneurship that this stream of research is important to the field of entrepreneurship as a source of meaningful heterodoxy. Without grappling with these values and concerns, entrepreneurship scholars are left with an incomplete and possibly emaciated understanding of entrepreneurship. When properly harnessed to build bridges across social divides, the intersection of religion and entrepreneurship is an important source of fresh, new, and heterodox insights into the processes through which entrepreneurs strive to transform organizations and society through entrepreneurial action and an emerging subfield of entrepreneurship research.",21 Mar 2025,5,"While the intersection of religion and entrepreneurship is an interesting topic, it may not have direct practical value for early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S2352673423000379,Startup grants and the development of academic startup projects during funding: Quasi-experimental evidence from the German ‘EXIST – Business startup grant’,November 2023,Business Venturing Insights,Not Found,Christoph E.=Mueller: chr.mueller@fz-juelich.de,"Abstract
Public support programs for (academic) startups are an important component of innovation and technology policy. There are many types of startup policy instrument, one of which is supporting prospective founders with startup grants. The present study contributes to the literature by examining the effectiveness of a specific startup grant entitled ‘EXIST – Business Startup Grant’. This measure is Germany’s largest public startup support program and aims to increase the number of technology-oriented and knowledge-based academic startups. The present research investigates the extent to which products of funded startups, the projects’ business planning, the founders’ skills, the degree of networking, and the uptake of external funding evolve over the duration of the grant. Evidence of the effects on these variables is generated by means of a pipeline-based comparison group design. Findings indicate that the startup grant contributes substantially to the development of the products and the business planning of the funded startups, moderately increases their degree of networking and their uptake of external funding, and slightly improves the skills of the founding team during the funding period. Several robustness checks – including a replication by another type of research design, namely a pre-post comparison – strongly support the findings. Hence it is very likely that the program advances startups in their development and thus contributes to later economic success.",21 Mar 2025,9,The study on the effectiveness of a startup support program in Germany can provide valuable insights for European early-stage ventures and startups.
https://www.sciencedirect.com/science/article/pii/S235267342300046X,Finding the sweet spot: Evaluating the role of structured idea-generation framework in generating high-quality new venture ideas,November 2023,Business Venturing Insights,Not Found,Puspa=Shah: shahp@uwosh.edu; Nischal=Thapa: thapan@uwgb.edu,"Abstract
Previous research has predominantly focused on cognitive antecedents related to the quality of new venture ideas (NVIs), resulting in a notable gap in understanding the influence of structural elements on NVIs. Drawing upon activation theory, we examine how the structure of the idea-generation framework, a dimension of routinization, influences NVI outcomes. To investigate this relationship, we conduct two separate studies: the first involving a student sample and the second involving practicing entrepreneurs. Our findings demonstrate an inverse U-shaped relationship between the structure in the idea-generation framework and the quality/quantity of NVIs. These findings contribute to the understanding of the antecedents that shape NVIs.",21 Mar 2025,7,Understanding how the structure of idea-generation frameworks influences new venture ideas can offer practical guidance for European startups.
https://www.sciencedirect.com/science/article/pii/S2352673423000537,CEO's industry experience and emerging market SME performance: The effects of corruption and political uncertainty,November 2023,Business Venturing Insights,Not Found,Juan Carlos=Morales-Solis: jcmorales@wtamu.edu; Vincent L.=Barker III: vbarker3@ku.edu; Arkangel M.=Cordero: arkangel.cordero@utsa.edu,"Abstract
We examine how Chief Executive Officer (CEO) industry-specific experience influences firm performance in small and medium 
enterprises
 (SMEs) in emerging markets. Drawing on the upper echelons perspective and learning theory, we propose an inverted U-shaped relationship between an SME CEO's industry-specific experience and firm performance. We also argue that country 
corruption
 and political instability moderate this relationship, resulting in lower performance for SME CEOs with little 
industry
 experience or many years of 
industry
 experience in countries with high 
corruption
 or political instability. We test our hypotheses using data from the World Bank's Enterprise Survey of firms in emerging economies from 2006 to 2019. The results support our hypotheses that corruption and political instability primarily hurt the performance of SMEs with CEOs having very long industry experience. We discuss implications of this research for scholars studying SMEs in lesser-developed institutional environments and how leaders may influence SME performance.",21 Mar 2025,8,Examining how CEO industry-specific experience impacts SME performance in emerging markets can be relevant for early-stage ventures in Europe.
https://www.sciencedirect.com/science/article/pii/S2352673423000549,Students' assumptions of Entrepreneurs’ performance: The paradox of excess entry and missed opportunity,November 2023,Business Venturing Insights,Not Found,Kaushik=Gala: kgala@iastate.edu; Carlos D.=Valladares: Not Found; Brandon A.=Mueller: Not Found,"Abstract
Most variables in entrepreneurship are not distributed normally. Instead, they are characterized by positive skew and heavy 
tails
 featuring influential outliers. Yet, this fundamental asymmetry in entrepreneurial endeavors is rarely discussed in 
entrepreneurship education
, which often oscillates between highlighting everyday entrepreneurs and high-growth ‘unicorn’ startups while overlooking the distributional context for these extremes. Therefore, this paper explores whether students accurately comprehend the non-normality that pervades entrepreneurship. We conducted two studies wherein undergraduate business students at a large, public university in the Midwest US estimated entrepreneurial performance. We elicited students' estimates of the range of performance exhibited by entrepreneurs using a real-world vignette and performance data for an online learning platform. By providing empirical evidence that students may carry largely 
in
accurate assumptions of performance distributions, this paper highlights the paradoxical risks of excess entrepreneurial entry on the one hand and missed opportunity on the other.",21 Mar 2025,6,"The exploration of non-normality in entrepreneurship education is insightful, but may have limited immediate impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673423000616,Health resourcefulness behaviors: Implications of work-health resource trade-offs for the self-employed,November 2023,Business Venturing Insights,Not Found,Timothy L.=Michaelis: tmichaelis@niu.edu; Jon C.=Carr: jccarr@ncsu.edu; Alexander=McKelvie: mckelvie@syr.edu; April=Spivack: april.spivack@hanken.fi; Michael P.=Lerman: mlerman@iastate.edu,"Abstract
In this paper, we present an exploratory study to investigate why those who are self-employed in the United States may make more personal health-related trade-offs than adults working in traditional wage-employment jobs. A random sample of 10,663 working adults in the United States indicate that the self-employed engage in higher amounts of health-related resourcefulness behaviors than wage employees (e.g., skipping medication to save money). We find that concerns over healthcare access and 
affordability
 serve as antecedents to health resourcefulness behaviors among all working adults, but that age moderates these relationships differently for the self-employed. Specifically, younger self-employed adults engage in health resourcefulness behaviors due to healthcare 
affordability
 concerns while older self-employed adults engage in such behaviors due to healthcare 
access
 concerns. In sum, we contribute to the entrepreneurial resourcefulness literature and well-being literature by highlighting data on how the self-employed make trade-offs with their personal health resources. We offer multiple directions for theory development, future research, and implications for healthcare policy.",21 Mar 2025,6,"This abstract provides valuable insights into the trade-offs made by self-employed individuals regarding personal health resources, which could be relevant for European early-stage ventures in terms of understanding different aspects of entrepreneurship."
https://www.sciencedirect.com/science/article/pii/S2352673423000367,The weaker sex? A tale of means and tails,November 2023,Business Venturing Insights,Not Found,Indu=Khurana: ikhurana@hsc.edu; Jagannadha Pawan=Tamvada: jp.tamvada@soton.ac.uk; David B.=Audretsch: daudrets@indiana.edu,"Abstract
One of the most commonly held beliefs prevalent in entrepreneurship research is that women-led ventures tend to generate lower earnings than men-led ventures. We contend that this thinking emanates from empirical analyses that obscure the variation in entrepreneurial performance across the earnings distribution. Relying solely on the mean as a measure of central tendency conceals the heterogeneity among so-called underperformers. Using density plots from a nationally representative database, we demonstrate that women-led ventures perform better at some quantiles of the earnings distribution, contrary to the common myth that men-led ventures consistently outperform them. Our study debunks this myth and contributes to entrepreneurship research that adopts a gendered perspective by showing that the reality experienced by women entrepreneurs is not as dismal as it appears when compared to focusing exclusively on the mean. Our study has implications for policymakers, who need to adjust their policy approach by designing targeted policies explicitly incorporating the heterogeneity inherent in entrepreneurship.",21 Mar 2025,8,"This abstract challenges a commonly held belief in entrepreneurship research and highlights the heterogeneity in entrepreneurial performance, particularly focusing on women-led ventures. The findings could be impactful for European startups in terms of addressing gender disparities and reevaluating performance metrics."
https://www.sciencedirect.com/science/article/pii/S2352673423000215,The inefficiencies of venture capital funding,June 2023,Business Venturing Insights,Not Found,Chris=Welter: chriswelter@miamioh.edu; Tim R.=Holcomb: holcomtr@miamioh.edu; John=McIlwraith: john@allosventures.com,"Abstract
Despite plentiful research into venture capital (VC), the process of obtaining funding still frustrates both VC firms and startups. While uncertainty is necessary for the returns that VC investors desire, there are inefficiencies in the process that stem from 
information asymmetry
. We articulate those inefficiencies from both the founder and the VC perspective and offer some pathways toward reducing those inefficiencies to drive better outcomes for startups, VCs, and society as a whole.",21 Mar 2025,5,"While important, the abstract focuses on inefficiencies in the VC funding process, which may have limited direct practical value for early-stage ventures in Europe compared to other abstracts."
https://www.sciencedirect.com/science/article/pii/S2352673422000592,A translational framework for entrepreneurship research,June 2023,Business Venturing Insights,"Translational research, Entrepreneurship research, Scholarly impact, Design science, Engaged scholarship",Pablo=Muñoz: pablo.munoz-roman@durham.ac.uk; Dimo=Dimov: dpd24@bath.ac.uk,"Abstract
In this paper, we put forward a new 
translational research
 framework for entrepreneurship, which leverages translational research from biomedical science and design science to lay the ground for a new research ecosystem of entrepreneurship. Instead of describing, explaining and predicting, our framework places emphasis on framing, experimenting and interacting. It comprises three modes of translational research, which allow for moving discoveries made in basic entrepreneurship research to entrepreneurial practice (T1), entrepreneurial communities (T2) and entrepreneurship policy (T3). These are alternative modes of research, marking different scientific domains, that can ensure the outcomes of our 
basic science
 are understood, adapted to and adopted by stakeholders in the best way possible. This new ecosystem can expand our 
scope of action
 as entrepreneurship researchers, open new pathways to materialize the elusive “scholarly impact” and advance the conversation and practice of engaged scholarship.",21 Mar 2025,9,"This abstract proposes a new translational research framework for entrepreneurship, which could significantly impact how research is translated into practice, policy, and communities. The framework has the potential to shape the future direction of entrepreneurship research and practice in Europe."
https://www.sciencedirect.com/science/article/pii/S2352673422000671,Whistleblowing in entrepreneurial ventures,June 2023,Business Venturing Insights,Not Found,Daniel R.=Clark: dclark@ivey.ca; Bradley R.=Skousen: bradleyskousen@gmail.com,"Abstract
Despite the occurrence of high-profile whistleblowing events in entrepreneurial firms (e.g., We Work, Uber, and Theranos), there is a dearth of understanding of when and why whistleblowing occurs 
outside
 the domain of traditional large firms. Indeed, we argue that new ventures represent a unique and meaningful heterodoxy as entrepreneurs, rebels with a cause, inspire others to join their cause who will ultimately betray the entrepreneur to protect that cause. We test and find support for our hypotheses regarding firm size and new venture status on whistleblowing on a unique dataset of 3113 reported frauds. From these findings we inductively theorize a new model of 
whistleblower
 motivation driven by the unique context of entrepreneurs drawing people to join a cause, and the price when fidelity is not maintained.",21 Mar 2025,7,"The abstract introduces a new model of whistleblower motivation in entrepreneurial firms, shedding light on a less explored area. This could be valuable for European startups to understand the dynamics of whistleblowing and loyalty within their organizations."
https://www.sciencedirect.com/science/article/pii/S2352673423000070,You take after your father: Paternal grit and young adult self-employment,June 2023,Business Venturing Insights,Not Found,Pankaj C.=Patel: pankaj.patel@villanova.edu; Marcus T.=Wolfe: Marcus.Wolfe@unt.edu; Ryan C.=Bailey: Not Found,"Abstract
Considerable scholarly attention has been given to the influence parents have on the self-employment of their children. We further investigate this relationship by examining whether paternal grit, via a young adult's grit, could influence young adult self-employment. Using a sample of 1504 participants from the Cultural Pathways to Economic Self-Sufficiency and Entrepreneurship (CUPESSE) survey, our findings indicate that paternal grit is positively associated with young adult grit, and that paternal grit has a positive indirect relationship with young adult self-employment, as mediated by young adult grit. Our study has important implications to ongoing conversations regarding the generational transmission aspects of self-employment, as well as the influence that grit can have on the entrepreneurial process.",21 Mar 2025,8,"The study provides important insights into the influence of parental grit on young adult self-employment, contributing to the understanding of generational transmission aspects of self-employment and the role of grit in the entrepreneurial process."
https://www.sciencedirect.com/science/article/pii/S235267342300015X,"Creating economic, social, and environmental change through entrepreneurship: An entrepreneurial autonomy perspective informed by Paulo Freire",June 2023,Business Venturing Insights,"Social responsibility, Environmental sustainability, Women entrepreneurs, Paulo freire, Entrepreneurial autonomy, Sustainable development",Ana Cristina O.=Siqueira: siqueiraa@wpunj.edu; Benson=Honig: bhonig@mcmaster.ca; Sandra=Mariano: sandramariano@id.uff.br; Joysi=Moraes: jmoraes@id.uff.br; Robson Moreira=Cunha: robsoncunha@id.uff.br,"Abstract
We extend to the context of entrepreneurship Paulo Freire's concepts including “limit-situation” representing constraints to be surpassed, such as inequalities or crises, “untested feasibility” representing a new vision based on awareness that a given reality can be altered, and “limit-acts” representing actions to change reality. In our abductive analysis, we focus on Brazilian women technology entrepreneurs as individuals transcending barriers such as 
gender inequality
. Our entrepreneurial 
autonomy
 perspective represents a process in which individuals (1) identify economic, social, and/or 
environmental issues
 that they can improve via entrepreneurship, (2) develop a new vision that articulates better economic, social, and/or environmental conditions, and (3) take actions to enhance these conditions and benefit diverse stakeholders by creating a nonprofit or for-profit 
enterprise
. We provide future directions for the integration of Freire's concepts and the entrepreneurial autonomy perspective in research, and offer our entrepreneurial autonomy worksheet for educators to empower individuals to develop ideas of socially responsible new ventures that create value for diverse stakeholders.",21 Mar 2025,7,"The study focuses on Brazilian women technology entrepreneurs transcending barriers and providing a perspective on entrepreneurial autonomy, offering future directions for research and an entrepreneurial autonomy worksheet for empowerment."
https://www.sciencedirect.com/science/article/pii/S2352673423000227,Keep on keeping on: A psychological approach to entrepreneurial persistence,June 2023,Business Venturing Insights,Not Found,Alan D.=Boss: adboss@ualr.edu; Jiaju=Yan: Justin_Yan@baylor.edu; Rhonda K.=Reger: Rhonda.Reger@unt.edu,"Abstract
Persistence typifies the behavior of most successful entrepreneurs. Yet systematic theory about entrepreneurial persistence is lacking. This paper theorizes about psychological differences that lead some entrepreneurs to persist appropriately while others quit too soon or persist excessively. Building on self-regulation literature, we develop a theory of entrepreneurial persistence, called entrepreneurial psychological resource approach. Our paper makes two primary contributions to guide future research: a research model expanding upon the resource-based perspective in entrepreneurship research, and an evaluative component relating psychological resources to entrepreneurial persistence.",21 Mar 2025,6,"The paper develops a theory of entrepreneurial persistence based on psychological resources, contributing to the understanding of why some entrepreneurs persist while others quit, providing a research model and evaluative component for future studies."
https://www.sciencedirect.com/science/article/pii/S235267342200035X,The values work of restorative ventures: The role of founders’ embodied embeddedness with at-risk social groups,November 2022,Business Venturing Insights,"Restorative entrepreneuring, Values, Work, Stakeholders, Qualitative, Homelessness",Mohamed Hassan=Awad: mawad5@calstatela.edu; Mabel=Sanchez: msanch173@calstatela.edu; Matthew A.=Abikenari: matthewabi@g.ucla.edu,"Abstract
In line with the recent turn to pro-social ventures, restorative entrepreneuring is a promising approach for delivering social impact to marginalized and at-risk social groups with its focus on establishing ventures to rehabilitate and integrate individuals back into the community. However, the restorative project requires entrepreneurs to engage broadly with diverse sets of stakeholders with divergent worldviews, ideologies, and interests, many of which were the causes of 
marginalization
 and stigma for the at-risk social groups the entrepreneurs seek to serve. In this 
case study
, we focus on the role of values as a critical arena where entrepreneurs navigate 
stakeholders engagement
. We analyze Occupy Medical, a restorative venture in Oregon, United States, which provides healthcare services to the unhoused. We analyze how the founders captured and enacted values to establish and sustain the venture in a resource-constrained and often hostile environment. We identify a unique form of values work, embodied 
embeddedness
, as central to these efforts. Our study unpacks the role of values in restorative entrepreneuring as a tool for mitigating social exclusion of at-risk groups, enabling community reclamation of the social problem, and maneuvering local pushback and stigma.",21 Mar 2025,9,"The case study provides valuable insights into restorative entrepreneuring, focusing on values as a critical aspect for engaging stakeholders in delivering social impact to marginalized groups, offering a unique perspective on values work in establishing and sustaining ventures."
https://www.sciencedirect.com/science/article/pii/S2352673422000439,Entrepreneurial miasma: Organizational miasma as a theoretical lens for increasing the odds of venture survival after the founder exits,November 2022,Business Venturing Insights,Not Found,James J.=Hoffman: jhoffman@nmsu.edu; Michaela=Driver: mdriver@nmsu.edu,"Abstract
The present study offers new insights on ventures undergoing founder exit. Specifically, it explores miasma as one potentially negative outcome. Miasma, a concept adapted from the organizational literature, refers to a state of contagion or pollution that affects all members of an organization causing potentially irreparable damage. This study develops a model of miasma in venture contexts when founders exit, a term we refer to as entrepreneurial miasma. This model includes the antecedents, moderating and mediating variables and outcomes of miasma. The purpose of this model is to develop insights into how miasma may be prevented and how ventures may work through it once it has occurred. Specifically, the study offers guidance for new management leading ventures on how to best understand, reestablish and build relationships with employees who are struggling with the exit of the founder to protect employee productivity and firm performance. The study also contributes to the organizational miasma literature by strengthening and clarifying the construct and its implications in both organizational and venture contexts.",21 Mar 2025,5,"The study explores the concept of entrepreneurial miasma in venture contexts when founders exit, providing insights into how miasma may be prevented and managed, offering guidance for new management leading ventures in protecting employee productivity and firm performance."
https://www.sciencedirect.com/science/article/pii/S235267342200049X,Linking passion to performance in the social commerce community: The role of collaborative information exchange,November 2022,Business Venturing Insights,"Entrepreneurial passion, Collaborative information exchange, Social commerce, Digital entrepreneurship",Yiwen=Chen: yiwenchen@sfsu.edu; Li=Chen: lchen28@suffolk.edu; Robert=Smith: rssmith@suffolk.edu,"Abstract
The proliferation of 
social commerce
 has enabled millions of passionate entrepreneurs to launch businesses. Yet, literature is sparse regarding whether and how passionate entrepreneurs succeed in this new context. Through an in-depth analysis of the social commerce community, this study articulates the role of 
collaborative information exchange
 as one behavioral mechanism through which entrepreneurial passion translates into performance. Using both primary survey data and secondary store traffic and trade volume data from a social commerce website, the authors find that collaborative information exchange partially mediates the effect of passion on entrepreneurs’ business satisfaction (subjective performance) and fully mediates the effect of passion on store traffic and trade volume (objective performance). This study deepens our understanding of entrepreneurial passion by identifying an important mechanism that is specific to the digital entrepreneurship domain and provides practical implications to both individual entrepreneurs and social commerce platforms.",21 Mar 2025,8,"This study deepens our understanding of entrepreneurial passion in the context of social commerce, providing practical implications for individual entrepreneurs and platforms."
https://www.sciencedirect.com/science/article/pii/S2352673421000809,Giving colour to emotions in entrepreneurship,June 2022,Business Venturing Insights,"Emotion, Entrepreneurship, Colour, Visual methodology, Multimodality",Bernadetta A.=Ginting-Szczesny: bernadetta.ginting@aalto.fi,"Abstract
This paper introduces colour as a visual resource for accessing the emotional experience of entrepreneurs. Colour has been demonstrated throughout the past decades to contain strong affective meanings and the ability to communicate specific emotional experiences. In this paper I show how colours are used by entrepreneurs through the colour timeline approach as a tool to facilitate the process of making sense of and expressing emotion. In particular, I show how colour can give form to complex emotions, draw out significant emotional events, and provide visual space for holistic reflection. This paper thus highlights the potential of colour for research on emotion in the context of entrepreneurship.",21 Mar 2025,5,"While exploring the use of color in entrepreneurship is interesting, the practical value and impact on early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S235267342100072X,Psychological well-being of hybrid entrepreneurs,June 2022,Business Venturing Insights,Not Found,Retno=Ardianti: retnoa@petra.ac.id; Martin=Obschonka: martin.obschonka@qut.edu.au; Per=Davidsson: Not Found,"Abstract
Although the phenomenon of hybrid entrepreneurs—individuals who work in paid and self-employment simultaneously—is prevalent, the psychological well-being of hybrid entrepreneurs has not been researched systematically to date. This is unlike research on paid employment and (assumed) full-time entrepreneurship, where psychological well-being has been researched as a key factor. Using data from the United Kingdom Household Longitudinal Survey, we address this void by studying whether hybrid entrepreneurs display distinct psychological well-being patterns (measured via mental strain, job satisfaction, and life satisfaction), utilizing a comparison with full-time paid employed, full-time self-employed and individuals working in two paid jobs. We further examine whether the specific work arrangements of hybrid entrepreneurs shape their well-being. To this end, we study the changes in well-being of hybrid entrepreneurs and other individuals in the comparison groups who switch to other jobs. For this purpose, we employed matching (entropy balancing approach) to account for self-selection effects. Our results suggest that the well-being of hybrid entrepreneurs is indeed distinct and can be explained by both self-selection effects and unique aspects of their work arrangements. Our study is thus the first to deliver evidence showing that hybrid entrepreneurs need to be studied as a separate group in entrepreneurship research concerned with well-being and psychological functioning. Our results have important implications not only for future research but also for practice.",21 Mar 2025,9,"This study addresses a significant gap in research regarding the psychological well-being of hybrid entrepreneurs, providing important implications for future research and practice."
https://www.sciencedirect.com/science/article/pii/S2352673421000767,Lassie shrugged: The premise and importance of considering non-human entrepreneurial action,June 2022,Business Venturing Insights,Not Found,Richard A.=Hunt: rickhunt@vt.edu; Daniel A.=Lerner: Daniel.Lerner@ie.edu; Avery=Ortiz-Hunt: avery_ortiz-hun1@baylor.edu,"Abstract
While management and entrepreneurship scholars have displayed comfort in and receptivity towards anthropomorphizing organizations, technologies, and even algorithms, our field has not yet grappled with a mountain of empirical evidence gathered over decades of research in the natural sciences that non-humans may behave entrepreneurially. For reflection and valuable perspective, our study relaxes the central assumption that entrepreneurial behaviors are the exclusive domain of human beings. Doing so invites fresh insights concerning the transversal nature of 
entrepreneurial action
, the biological origins of innovation and entrepreneurship, the categorical assumptions demarcating the field of entrepreneurship, and the persistent emphases on intendedly rational conceptions of 
entrepreneurial action
. The inspiration for our study involves “moving back from the species,” as E.O. Wilson advised. Through this “more distanced view” and by focusing on the reproducible benefits of entrepreneurship rather than narrower, human-centric conceptions of firm formation and profit generation, we find that the consideration of non-human behaviors contributes to the evolving definitions and future study of entrepreneurial action.",21 Mar 2025,7,"The exploration of non-human entrepreneurial behaviors provides valuable perspective, but the immediate practical implications for early-stage ventures may be less direct."
https://www.sciencedirect.com/science/article/pii/S235267342200004X,Informal competition and product innovation decisions of new ventures and incumbents across developing and transitioning countries,June 2022,Business Venturing Insights,Not Found,Punyashlok=Dwibedy: punyashlok.dwibedy@ahduni.edu.in,"Abstract
While existing research has found that informal competition positively impacts 
product innovation
 by formal firms in developing and transitioning countries, there is a lack of understanding of how this decision to innovate varies between incumbents and new ventures. Through replication and extension of the work by McCann and Bahl (2017), the article primarily tries to address this significant gap by analyzing the differential impact of informal competition on 
product innovation
 decisions of incumbents and new ventures. Using data from the latest round of the World Bank 
Enterprise
 Surveys, 2019–2020, conducted across multiple transitioning and developing countries and logistic regression as the method of analysis, this study finds that (a) informal competition positively impacts the likelihood of product innovation by formal firms, confirming the results of McCann and Bahl (2017) and (b) new ventures are less likely to engage in product innovation than incumbents when competing with informal firms. The article contributes to the literature by trying to empirically resolve the friction between Attention-Based View of the firm and liability of newness. This paper argues that when facing informal competition, new ventures are less likely to respond through innovation compared to incumbents due to competing resource requirements that limit their attention towards competition from informal firms.",21 Mar 2025,6,"While the study contributes to the literature on informal competition and product innovation, the focus on incumbents and new ventures may have limited direct impact on early-stage startups."
https://www.sciencedirect.com/science/article/pii/S2352673422000142,Green start-ups and the role of founder personality,June 2022,Business Venturing Insights,"Emission reduction, Environmentally friendly products, Green innovation, Big five personality traits, Sustainability",Gary=Chapman: gary.chapman@dmu.ac.uk; Hanna=Hottenrott: hanna.hottenrott@tum.de,"Abstract
Green start-ups play a vital role in the needed transition towards more environmentally sustainable economies. Yet our understanding of why some founders start green ventures and others do not remains incomplete. We build on the cognitive and decision-making perspectives on start-ups pro-environmental engagement to shed light on the role of founders' 
personality traits
 - focusing on the ‘Big 5’ and risk tolerance - in explaining whether founders' start new ventures with 
environmentally friendly products
. Our analysis of a large, representative, manufacturing and 
service sector
 sample of German start-ups illustrates the important role of founder 
personality traits
. Specifically, openness and extraversion promote environmentally friendly products while 
neuroticism
 inhibits them. We discuss the implications of these insights.",21 Mar 2025,8,"This abstract focuses on the role of founder personality traits in environmentally friendly start-ups, providing valuable insights for early-stage ventures in the European market."
https://www.sciencedirect.com/science/article/pii/S2352673421000317,Are behavioral and electrophysiological measures of impulsivity useful for predicting entrepreneurship?,November 2021,Business Venturing Insights,"Entrepreneurship, Impulsivity, EEG, Eriksen flanker task, Go/no-go task, Reward task, Balloon analogue risk task",Christian=Fisch: Not Found; Roy=Thurik: thurik@ese.eur.nl,"Abstract
We examine the association between several behavioral and electrophysiological indices of impulsivity-related constructs and multiple entrepreneurial constructs. Specifically, we investigate if these behavioral and electrophysiological measures are more useful as predictors of entrepreneurship than self-reported measures of impulsivity. Our findings are based on two datasets (
n
 = 133 and 
n
 = 142) and indicate that behavioral and electrophysiological impulsivity measures are not robustly associated with entrepreneurship constructs, in contrast to self-reported measures of impulsivity. Though disappointing at first, our findings pave the way for future research on the relevance of behavioral and electrophysiological measures for entrepreneurship.",21 Mar 2025,4,"While the research on impulsivity and entrepreneurship is interesting, the findings do not directly impact practical implications for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673421000366,What entrepreneurship is really “productive”? An alternative view on Baumol's typology,November 2021,Business Venturing Insights,Not Found,Dmitrii=Trubnikov: dtrubnikov@hse.ru,"Abstract
The paper aims to contribute to the discussion on “productive, unproductive, and destructive” entrepreneurship started by William Baumol. It supports the position that all three categories in Baumol's typology are important to understand how institutions affect entrepreneurship but advocates for an alternative approach to distinguish among them. While the traditional demarcation is often based on the net effect on productivity, the proposed approach focuses on the public choice reasoning that not only distinguishes between “rent-seeking” and “profit-seeking,” but also emphasizes that rent-seeking should not be considered a pure transfer. Rent-seeking not only affects regulatory decisions, it also generates activities in the sphere that is traditionally perceived in the productive side. It is important to take this sphere into account when welfare implications of regulatory initiatives are discussed and when policymakers aim to stimulate “productive” entrepreneurship.",21 Mar 2025,7,"This paper contributes to the discussion on different types of entrepreneurship and provides insights on how institutions affect entrepreneurship, which can be beneficial for European startups."
https://www.sciencedirect.com/science/article/pii/S2352673421000433,The governance of entrepreneurial community ventures: How do conflicting community interests influence opportunity exploitation?,November 2021,Business Venturing Insights,Not Found,Helen M.=Haugh: h.haugh@jbs.cam.ac.uk,"Abstract
Participatory governance is upheld as a fundamental organizing principle in community entrepreneurship. This paper brings new insights from a 
case study
 that investigated how governance structures, processes, and practices, and divergent community interests influence community venture opportunity exploitation. We find that while community stakeholder governance enables 
community participation
 and accountability, the interests of the majority triumph over the minority in determining opportunity exploitation. Adopting supplementary governance mechanisms of multistakeholder advisory committees and community engagement process reporting would enable minority interests to be acknowledged and communicated, and increase accountability by acknowledging conflicting views about opportunity exploitation.",21 Mar 2025,6,"The insights on participatory governance in community entrepreneurship are valuable, but the practical implications for European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S2352673421000469,Does gender matter? Evidence from crowdfunding,November 2021,Business Venturing Insights,Not Found,Ramy=Elitzur: ramy.elitzur@rotman.utoronto.ca; Eliran=Solodoha: solodoha@post.bgu.ac.il,"Abstract
The lack of resources and support for female entrepreneurs and their ventures lowers their chances of success and ultimately leads to the underrepresentation of female led ventures in the economy. Thus, to remedy this problem, it is crucial for female entrepreneurs to attract potential investors. The latter may use in their decision-making process cues such as the number of supporters, which provides social validation of the ventures. To test this, we analyze 2275 reward-based crowdfunding projects to investigate the effects of female entrepreneurs’ presence, and the consequences of social validation (i.e., number of supporters), on their crowdfunding success.",21 Mar 2025,5,"While the focus on female entrepreneurs and crowdfunding is important, the direct impact on European early-stage ventures may not be significant."
https://www.sciencedirect.com/science/article/pii/S2352673421000585,Self-employment through the COVID-19 pandemic: An analysis of linked monthly CPS data,November 2021,Business Venturing Insights,"Self-employment, Entrepreneurship, COVID-19 pandemic, Minorities, Women",Samuel C.H.=Mindes: smindes@iastate.edu; Paul=Lewin: Not Found,"Abstract
The COVID-19 pandemic that began in the United States in March of 2020 had a profound adverse effect on the economy. In particular, the pandemic had a harsh impact on women, minorities, and self-employed individuals. However, research on why the pandemic hit some groups harder is in its nascent stages. We contribute to the growing body of knowledge by comparatively analyzing the inability to work due to the pandemic in the wage and self-employment sectors. We utilize data from the Current Population Survey from May 2020 to May 2021 to investigate the effect of individual, business, and geographic characteristics on the probability of work interruption in each sector. We find that self-employers were much harder hit but fared better than wage workers in several of the harder-hit sectors and when they had incorporated businesses. We also find that women, non-Whites, and Hispanics were more adversely affected in both sectors.",21 Mar 2025,5,"While the research on the impact of the COVID-19 pandemic on certain groups is important, the focus on wage and self-employment sectors may not have direct practical value for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673421000639,Occupy Wall Street ten years on: How its disruptive institutional entrepreneurship spread and why it fizzled,November 2021,Business Venturing Insights,Not Found,Thomas H.=Allison: t.allison@tcu.edu; Matthew=Grimes: m.grimes@jbs.cam.ac.uk; Aaron F.=McKenny: AMcKenny@iu.edu; Jeremy C.=Short: Jeremy.Short@unt.edu,"Abstract
How does media impact institutional entrepreneurs and their ability to create change? We draw from research on social movements and media frames to examine the paradox that media-informed discursive opportunities pose for institutional entrepreneurs engaged in efforts to transform or create social institutions. Through content analysis of 8473 newspaper articles covering the 2011 Occupy Wall Street movement, we highlight the paradox of discursive opportunities: the same types of media frames that initially encourage more disruptive tactics also subsequently increase the perceived threat of such disruption, thereby encouraging swifter counteraction. Our findings hold implications for the importance of media as a potential catalyst for 
entrepreneurial activity
 in the realm of social movements hoping to engage in reform.",21 Mar 2025,8,"The study on media impact on institutional entrepreneurs has practical implications for understanding the role of media in catalyzing entrepreneurial activity, which can be beneficial for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673420300731,A xenophilic perspective of social entrepreneurship,June 2021,Business Venturing Insights,Not Found,Reginald=Tucker: regtucker@lsu.edu; Randall M.=Croom: Not Found,"Abstract
Social entrepreneurship has grown as an established field of inquiry, accompanied by growth in both academic and practice. In this paper, we offer a novel perspective on why some social entrepreneurs venture for foreigners. We employ xenophilia, a love of foreigners, to explain why some people care for foreigners by venturing for them. We conceptualize that religious and social class logics influence a xenophilic perspective of foreigners. We also analyze how xenophilia can have a darker side, particularly in social entrepreneurship. Our arguments and analysis allow us to provoke future research questions and offer practical implications.",21 Mar 2025,6,"The exploration of xenophilia and its influence on social entrepreneurship offers a unique perspective, but may have limited immediate impact on European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673420300755,ADHD and entrepreneurship: Beyond person-entrepreneurship fit,June 2021,Business Venturing Insights,Not Found,Reginald=Tucker: regtucker@lsu.edu; Lu=Zuo: Not Found; Louis D.=Marino: Not Found; Graham H.=Lowman: Not Found; Alexander=Sleptsov: Not Found,"Abstract
Research examining 
mental health
 and entrepreneurship has found important links between 
mental health
 and entrepreneurship. These findings have led scholars to suggest a fit between some aspects of mental health, and in particular, mental dysfunction, and entrepreneurship. This paper complements extant studies in this area by examining the mental health and entrepreneurship relationship from a sociocognitive perspective. We examine to what extent does 
ADHD
 influence entrepreneurial self-efficacy and opportunity recognition tendency. Our findings are consistent with our hypotheses, suggesting that people with 
ADHD
 may not be efficacious in the entrepreneurial context, and specifically in recognizing opportunities. However, confidence in one’s ability regarding the entrepreneurship vocation can grow with education and experience. Our findings allow us to advance theory and offer practical implications.",21 Mar 2025,7,"Examining the link between mental health, ADHD, and entrepreneurship from a sociocognitive perspective provides valuable insights that can inform support strategies for entrepreneurs in Europe."
https://www.sciencedirect.com/science/article/pii/S2352673421000044,Relief and exploration after firm failure: Taking into account pre-failure experiences to understand post-failure responses,June 2021,Business Venturing Insights,Not Found,Anna S=Jenkins: a.jenkins@business.uq.edu.au,"Abstract
Conceptualizing firm failure as the loss of something important to the entrepreneur, the literature on emotional responses to firm failure has focused on the negative emotions experienced in response to this loss. We shift emphasis to introduce the positively valanced emotions relief and exploration as emotional responses to firm failure. These emotions reflect the inherently stressful nature of firm failure enabling an exploration into the timing of when stress is experienced during the failure process. Empirically we test our hypotheses, using a combination of a telephone and mail survey, on a sample of 114 entrepreneurs who had recently experienced the 
bankruptcy
 of their firm. We extend the literature on responses to firm failure by establishing relief and exploration as common emotional responses to firm failure and provide initial empirical support for the importance of considering pre-failure experiences in the process of entrepreneurial failure. Our findings also lend empirical support to the anticipatory grief argument put forward by Shepherd and colleagues (2009).",21 Mar 2025,8,"Shifting focus to positive emotions like relief and exploration in response to firm failure adds value to the understanding of entrepreneurial experiences, which can be relevant for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673421000135,A whole new world: Counterintuitive crowdfunding insights for female founders,June 2021,Business Venturing Insights,"Crowdfunding, Female entrepreneurship, Gender, Signaling theory",Henrik=Wesemann: henrik.wesemann@unisg.ch; Joakim=Wincent: joakim.wincent@hanken.fi,"Abstract
Female entrepreneurs are subjected to stereotypes that make it difficult to secure funding. Crowdfunding challenges many of the causes of this discrimination but we know little about if and how it changes optimal funding strategies for female entrepreneurs. Using a sample of 3191 crowdfunding campaigns by female entrepreneurs, we draw from signaling theory to develop and test a series of counterintuitive conjectures for female crowdfunding success. Our results contradict advice that may be derived from traditional entrepreneurial 
finance
: women in crowdfunding should use their gender as advertising, use more female-centric language, avoid self-promotion, start businesses in male-dominated sectors, and ask for more money. These findings highlight new theoretical mechanisms in crowdfunding and develop recommendations for female entrepreneurs who want to raise funds.",21 Mar 2025,8,"This abstract provides valuable insights into how female entrepreneurs can optimize their funding strategies through crowdfunding, challenging traditional advice. This has practical value for early-stage ventures in Europe."
https://www.sciencedirect.com/science/article/pii/S2352673421000184,"The effects of subclinical ADHD symptomatology on the subjective financial, physical, and mental well-being of entrepreneurs and employees",June 2021,Business Venturing Insights,"Subjective well-being, ADHD, Income perception, Heath perception, Demands-abilities fit, Needs-supplies fit",Zsófia=Vörös: voros.zsofia@ktk.pte.hu; Lívia=Lukovszki: Not Found,"Abstract
Results on the relationship between 
ADHD
 and entrepreneurial success are conflicting and several aspects of entrepreneurial success, especially on the personal level, have not been studied. By using a randomly selected Hungarian sample, the study examines the effects of subclinical 
ADHD
 symptomatology on the subjective quality-of-life outcomes in employment and entrepreneurship. The results indicate that subclinical ADHD impairs only entrepreneurs’ subjective income and harms entrepreneurs’ health perception to a larger extent than that of employees. Yet, the negative effects of ADHD symptomatology on 
life satisfaction
 are rather felt among employees. We argue that these results reflect a relatively good fit between entrepreneurship and subclinical ADHD symptomatology on the needs-supplies dimension but not on the demands-abilities dimension.",21 Mar 2025,5,"While the study on ADHD and entrepreneurial success is interesting, it may have limited impact on early-stage ventures in Europe as it focuses on a specific sample from Hungary."
https://www.sciencedirect.com/science/article/pii/S2352673420300299,Permission to hustle: Igniting entrepreneurship in an organization,November 2020,Business Venturing Insights,"Hustle, Entrepreneurial action, Ethnography, Corporate entrepreneurship, Crisis response",Greg=Fisher: fisherg@indiana.edu; Regan=Stevenson: rstev@indiana.edu; Devin=Burnell: dsburnel@iu.edu,"Abstract
Perceived institutional barriers, especially in existing organizations, often impede 
entrepreneurial action
 in the face of crisis and uncertainty. Understanding how collective entrepreneurial action occurs despite deeply institutionalized 
mindsets
 is important to advance 
entrepreneurship theory
. We report on an autoethnographic account of an entrepreneurship professor and several colleagues who gave themselves 
permission to hustle
 to overcome perceived institutional barriers to entrepreneurial action. As the findings reveal, a permission to hustle mindset provided a platform for the group of professors to act entrepreneurially in response to the COVID-19 pandemic. In a matter of several days, the group acted under uncertainty to create a new “idea blitz” program which attracted over 150 participants from around the world. We argue that permission to hustle is an important sense-breaking device that ignites and sustains entrepreneurial action by breaking taken-for-granted assumptions about institutionalized practices and redirecting attention toward urgent and creative action, especially in existing organizations where institutional barriers are perceived to impede such action.",21 Mar 2025,9,"This abstract offers a compelling account of how entrepreneurial action can overcome institutional barriers, specifically in response to the COVID-19 pandemic. The insights can be highly relevant for European early-stage ventures facing similar challenges."
https://www.sciencedirect.com/science/article/pii/S2352673420300378,Reorienting entrepreneurial support infrastructure to tackle a social crisis: A rapid response,November 2020,Business Venturing Insights,"Entrepreneurship, Crisis, Entrepreneurship policy, Rapid response, Chile",Pablo=Muñoz: pmunoz@liverpool.ac.uk; Wim=Naudé: naude@time.rwth-aachen.de; Nick=Williams: N.E.Williams@leeds.ac.uk; Trenton=Williams: trenwill@iu.edu; Rodrigo=Frías: rodrigo.frias@corfo.cl,"Abstract
Chile is experiencing its worst economic and social crisis in decades, which is adversely impacting entrepreneurs and SMEs. Chile’s Economic Development Agency is seeking to support recovery efforts by reorienting its entrepreneurship programs and ecosystem support capacity. What makes the reorientation especially challenging is the need to ensure all actions are sensitive to the causes of the social unrest, where arguably extant entrepreneurship policy has played a role. Theory and evidence in entrepreneurship literature seem insufficient to inform immediate actions. In this rapid response paper, we leverage and translate research on ecosystem democracy, spontaneous venturing and entrepreneurship-enabled social cohesion to inform decision-making and contribute to the development of policy solutions. We propose an entrepreneurship policy reorientation model, including interventions during and post crisis, potentially capable of minimizing the effects of the crisis and changing the orientation of future support.",21 Mar 2025,7,"The rapid response paper addressing Chile's economic crisis and entrepreneurship policy reorientation provides valuable insights that could be beneficial for early-stage ventures in Europe, although the direct impact may be more specific to the Chilean context."
https://www.sciencedirect.com/science/article/pii/S2352673420300433,Staying alive during an unfolding crisis: How SMEs ward off impending disaster,November 2020,Business Venturing Insights,"Shock, Crisis, Entrepreneurship, Coronavirus, COVID-19, Outcome, Actions",Sara=Thorgren: sara.thorgren@ltu.se; Trenton Alma=Williams: trenwill@iu.edu,"Abstract
What measures are SMEs most likely to take in order to make ends meet in the face of a “black swan” external shock? That is the question we explore in this study, drawing upon unique data from 456 SMEs 
in the midst
 of an unfolding crisis. Our findings demonstrate how SMEs acted immediately by deferring investments, reducing 
labor costs
, reducing expenses, and negotiating contracts and terms. Moreover, the data highlight how SMEs in an unfolding crisis are reluctant to commit to any action that will increase their debt-to-equity ratio. The findings suggest new questions to be explored in relation to actions during an unfolding crisis, post-crisis businesses, entrepreneurial failure, and entrepreneur/entrepreneurial team characteristics. Implications for policy and practice are provided.",21 Mar 2025,6,"The study on SMEs' actions during a crisis offers some interesting findings, but the practical implications for early-stage ventures in Europe may be limited as it focuses on SMEs in a crisis situation."
https://www.sciencedirect.com/science/article/pii/S2352673420300482,A dynamic analysis of the role of entrepreneurial ecosystems in reducing innovation obstacles for startups,November 2020,Business Venturing Insights,Not Found,Franco-Leal=Noelia: noelia.franco@uca.es; Diaz-Carrion=Rosalia: rosaliadiaz@us.es,"Abstract
Innovative startups 
face
 serious obstacles in their innovation processes because of the high costs of innovations, lack of commercial and managerial competencies, and difficulties in cooperating with industrial agents. Understanding the evolution of these obstacles, the dynamic nature of 
entrepreneurial ecosystems
, and how such ecosystems can reduce obstacles to innovation becomes crucial for managers and policymakers. This research examines the evolution of different types of obstacles innovative startups 
face
 and analyzes the effects market and research resources have on the entrepreneurial ecosystem in reducing these obstacles over time. Linear mixed models are used to analyze the evolution of the influence of the entrepreneurial ecosystem on the innovation obstacles of 911 Spanish innovative startups. The results indicate that different obstacles display different patterns over time. We found that the role played by market and/or research sources on the entrepreneurial ecosystem in reducing different obstacles to innovation could be linked to the tendency of these obstacles to diminish over time. The results point to the need for a set of agents to reinforce each other by creating an ecosystem in which innovation obstacles faced by startups are reduced over time.",21 Mar 2025,7,"This research examines the evolution of obstacles faced by innovative startups and the role of entrepreneurial ecosystems in reducing these obstacles over time, providing valuable insights for startup managers and policymakers."
https://www.sciencedirect.com/science/article/pii/S235267342030055X,Classifying self-employed persons using segmentation criteria available in the Labour Force Survey (LFS) data,November 2020,Business Venturing Insights,Not Found,Ondřej=Dvouletý: ondrej.dvoulety@vse.cz,"Abstract
This paper responds to the call of researchers, business practitioners, and policymakers to treat different kinds of entrepreneurs separately by the empirical implementation of Cieślik and Dvouletý (2019) segmentation criteria for classifying self-employed persons. The article shows how the segmentation variables (i.e. work engagement; skills and job classification; growth aspirations and economic dependency) might be used when working with the European 
Labour Force Survey
 (LFS) data set. The paper exploits the Czech sample of the ad-hoc module 2017 data set, and it shows differences between various types of entrepreneurs by using tools of applied statistical techniques (Chi-Square tests of association, Cramer’s V and t-tests). The article contributes to the community by showing how to use the segmentation variables in their own empirical research. The study encourages all researchers to explore the diversity of self-employment to advance the entrepreneurship field further forward. The article also includes several recommendations and directions for future research at the individual, regional, country or cross-country levels.",21 Mar 2025,5,"This paper explores the segmentation of self-employed persons in Europe, which could provide some insights for policymakers and researchers, but may have a more limited impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673420300585,The practice of “we”: A framework for balancing rigour and relevance in entrepreneurship scholarship,November 2020,Business Venturing Insights,Not Found,Isla=Kapasi: i.kapasi@leeds.ac.uk; Ainurul=Rosli: Ainurul.rosli@brunel.ac.uk,"Abstract
The rigour-relevance divide remains a longstanding concern for the entrepreneurship field. In this article we elucidate the practice of “we” in entrepreneurship scholarship and propose a means to encourage and realise it. Our contribution is in the combination of reflection (content reflection, process reflection, and premise reflection) and design science phases; thus, we develop and outline the concept and communal practice of entrepreneur
ial
 scholarship informed by a structured reflection framework. Our original model and related framework detail a series of overlapping phases of inquiry and questioning, demonstrating how can we work together with non-academics to collectively strengthen the relevance of entrepreneurship scholarship and, ultimately, be more accountable and relevant to those whom we research.",21 Mar 2025,8,"By proposing a structured reflection framework for entrepreneurial scholarship, this article offers a practical approach to strengthen the relevance of entrepreneurship research, potentially benefiting early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673420300640,Disinhibition predicts both psychopathy and entrepreneurial intentions,November 2020,Business Venturing Insights,Not Found,Benjamin R.=Walker: ben.walker1@monash.edu; Chris J.=Jackson: c.jackson@unsw.edu.au; Genevieve=Sovereign: genevieve.sovereign@alumni.utoronto.ca,"Abstract
Most research has suggested that 
disinhibition
, defined as persistence despite negative feedback, generally leads to dysfunctional outcomes. However, some traits related to 
disinhibition
 such as sensation seeking, impulsivity, and risk-taking are also associated with functional outcomes. This study examined 157 full-time workers to determine whether disinhibition positively predicted 
psychopathy
 and entrepreneurial intentions, using an adapted Balloon Analogue Risk Task (BART) as a measure of disinhibition. This approach was then replicated in a sample of 143 university staff and students. Across both samples, disinhibition was found to predict both subclinical 
psychopathy
 and entrepreneurial intentions. These results suggest disinhibition can be a driver that potentially leads to entrepreneurial action or antisocial outcomes.",21 Mar 2025,9,"This study on disinhibition, psychopathy, and entrepreneurial intentions provides valuable insights for understanding the psychological traits associated with entrepreneurship, which could have a significant impact on early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673419300629,Psychiatric symptoms and entrepreneurial intention: The role of the behavioral activation system,June 2020,Business Venturing Insights,"Behavioral activation system, ADHD, Narcissism, Hypomania, Mental disorders, Entrepreneurial intention",Y.K.=Leung: Not Found; A.R.=Thurik: thurik@ese.eur.nl,"Abstract
Both the scientific literature and the popular press have recently started to associate entrepreneurship with symptoms of mental disorders. In addition, there is an emerging stream of literature devoted to the non-intendedly rational logic of entrepreneurs. Despite the high co-occurrence rate of psychiatric symptoms, prior research has only examined the independent effects of psychiatric symptoms. Furthermore, the two emerging literature streams remain largely independent. In the present study, we investigate the independent and joint association of four psychiatric symptoms (i.e., inattention, hyperactivity, 
narcissism
, and hypomania) and entrepreneurial intention. Drawing on the 
reinforcement sensitivity theory
, we explore whether the relationship between psychiatric symptoms and entrepreneurial intention is mediated by the behavioral activation system (BAS). Using the survey responses of 182 university students, our results show differential findings between the independent and joint effects of psychiatric symptoms on entrepreneurial intention. Importantly, we find support for the mediating role of BAS. Overall, our results suggest that BAS may serve as a unifying theoretical construct that helps to understand the relationship between multiple psychiatric symptoms and entrepreneurial intention.",21 Mar 2025,6,"Investigating the relationship between psychiatric symptoms and entrepreneurial intention, this study contributes to understanding the psychological aspects of entrepreneurship, which may have some relevance for early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S2352673419300587,Resilience as a moderator of government and family support in explaining entrepreneurial interest and readiness among single mothers,June 2020,Business Venturing Insights,Not Found,Yulita=Not Found: yulita@umt.edu.my; Suriyani=Muhamad: Not Found; Noorhaslinda=Kulub Abdul Rashid: Not Found; Nor Ermawati=Hussain: Not Found; Noor Haslina=Mohamad Akhir: Not Found; Nizam=Ahmat: Not Found,"Abstract
Resilience is the individual’s ability to cope with, adapt to and recover from stressful or 
traumatic experiences
. It is considered crucial in various fields, and particularly in entrepreneurship. In the current study, we sought to deepen our understanding of the role of an individual’s resilience in moderating the interaction between government support and family support to predict entrepreneurial-related outcomes. Specifically, using self-determination theory (SDT) and the social support perspective, we proposed that a positive beneficial three-way interaction effect, stated as resilience*government support*family support, would enhance the level of single mothers’ entrepreneurial interest and readiness for entrepreneurial challenge. Data collected from 519 Malaysian single mothers in Malaysia’s East Coast region were analysed using IBM 
SPSS Statistics
 software. The results showed that resilience moderated government support and family support interaction to predict both entrepreneurial interest and readiness for entrepreneurial challenge. Although both types of support were viewed as important, they were more effective for individuals who were highly resilient than for those who were not. These findings support the important role of resilience as ‘a moderator of the moderator’ and of government support as an external source of motivation, with both complementing the positive beneficial outcomes of family support.",21 Mar 2025,8,"The study provides valuable insights into the role of resilience in moderating the interaction between government support and family support for single mothers' entrepreneurial outcomes, contributing to the understanding of entrepreneurial success factors."
https://www.sciencedirect.com/science/article/pii/S2352673420300202,Towards an integrative definition of scaling social impact in social enterprises,June 2020,Business Venturing Insights,Not Found,Syrus M.=Islam: syrus.islam@aut.ac.nz,"Abstract
Scaling social impact is a key concept in the social 
enterprise
 literature. While central, the wide range of meanings and the lack of conceptual uniformity detract from its usefulness. To tackle this issue, this paper conducts a 
systematic review
 to derive an integrative definition of scaling social impact: Scaling social impact is an ongoing process of increasing the magnitude of both quantitative and qualitative positive changes in society by addressing pressing social problems at individual and/or systemic levels through one or more scaling paths. Alongside improving conceptual clarity, the definition offers an operational structure with five underlying elements, setting the basis for new empirical work and theorising in social 
enterprise
 research.",21 Mar 2025,6,"The paper addresses the lack of clarity in defining scaling social impact in social enterprise literature, offering a structured definition that can potentially enhance future empirical work and research in the field."
https://www.sciencedirect.com/science/article/pii/S2352673420300238,A meta-analysis of the gender gap(s) in venture funding: Funder- and entrepreneur-driven perspectives,June 2020,Business Venturing Insights,Not Found,Mark=Geiger: geigerm1@duq.edu,"Abstract
Using gender homophily and 
gender socialization
 as theoretical foundations, the current study takes the position that both funder-driven (supply-side) and entrepreneur-driven (demand-side) processes perpetuate the gender gap in venture funding. Using this positional anchor, I performed a meta-analysis on gender-funding associations. The results show that gender-funding associations are different across funding contexts, which is consistent with what gender homophily and a funder-driven perspective might suggest. However, the nature of the difference depends on whether the outcome is funding amount or funding success. In addition, business size and 
industry
 sector were found to fully mediate the relation between entrepreneur gender and funding needed. This finding is consistent with what 
gender socialization
 and an entrepreneur-driven perspective might suggest. The mediation results ultimately suggest that female entrepreneurs need less funding for their ventures, which in turn results in less funding amounts but greater funding success. As such, there is one gender gap to the disadvantage of female entrepreneurs (funding amount) and another gender gap to the advantage of female entrepreneurs (funding success). Together, the perspectives and findings presented in this paper provide insights for both research and practice on the gender 
gap(s)
 in venture funding.",21 Mar 2025,5,"The study sheds light on gender gaps in venture funding based on different funding contexts and perspectives, offering insights for research and practice, but the practical implications for European early-stage ventures may be limited."
https://www.sciencedirect.com/science/article/pii/S2352673419300344,Political climate and academic entrepreneurship: The case of strange bedfellows?,November 2019,Business Venturing Insights,Not Found,Peter T.=Gianiodis: gianiodisp@duq.edu; William R.=Meek: wmeek1@udayton.edu; Wendy=Chen: dchen16@gmu.edu,"Abstract
Universities have fully embraced academic entrepreneurship, transforming their structures, systems, and processes to generate licensing revenues and create new ventures. While prior research has mainly focused on the relationship between public policy and 
entrepreneurial activities
, this study examines a major gap – the performance implications of regional politics on academic entrepreneurship. We use a unique data set of U.S. universities and their regional governments to test how the influence of two elements of a region's political climate – consensus and stability – affects entrepreneurial and commercial performance. Our results suggest that political consensus and stability are positively associated with higher licensing revenues, while political stability is negatively associated with new venture creation. Our results reveal how regional politics influence university commercial outcomes, which suggests that entrepreneurship-enhancing public policy is intimately linked to the regional political process. We discuss the implications for theory and practice, and suggest possible future research directions.",21 Mar 2025,7,"The research explores the impact of regional politics on academic entrepreneurship, revealing the associations between political climate elements and entrepreneurial performance in universities, providing implications for theory and practice in the field."
https://www.sciencedirect.com/science/article/pii/S2352673419300010,A chip off the old block? How parent-child interactions affect the intergenerational transmission of entrepreneurial intentions,June 2019,Business Venturing Insights,Not Found,Christian=Hopp: hopp@time.rwth-aachen.de; Dana=Minarikova: minarikova@time.rwth-aachen.de; Alexander=Speil: speil@time.rwth-aachen.de,"Abstract
Entrepreneurial parents can serve as sources of information and inspiration to transmit entrepreneurial intentions to their 
offspring
. Yet we find that role modelling is not purely observational, but it requires social interactions between parents and children to take effect. Our results using more than 2,500 child-parent dyads from the German socio-economic panel show that only if the socialization intensity between self-employed parents and their child is high entrepreneurial intentions will be transmitted. The intergenerational transmission of entrepreneurial intentions is dependent on the socialization intensity of the parents. The opportunity to share experiences with the children influences children's forming of 
entrepreneurial attitudes
 especially within same-sex parent-child dyads.",21 Mar 2025,4,"The study highlights the importance of social interactions between entrepreneurial parents and children in transmitting entrepreneurial intentions, but the focus on intergenerational transmission may have limited direct implications for European early-stage ventures."
https://www.sciencedirect.com/science/article/pii/S235267341830132X,"The tacit knowledge of entrepreneurial design: Interrelating theory, practice and prescription in entrepreneurship research",June 2019,Business Venturing Insights,Not Found,Paul D.=Selden: paul.d.selden@gmail.com; Denise E.=Fletcher: denise.fletcher@uni.lu,"Abstract
An important challenge facing entrepreneurship researchers is the “three-body” knowledge problem of how to use “theoretical knowledge” to produce “prescriptive knowledge” that communicates the “practical knowledge” of situated practice to students and practitioners of entrepreneurship. We argue that a contribution can be made to solving this problem by theorizing practical knowledge as the “know-how” to do a situated entrepreneurial practice. “Know-how” is a cognitive “capacity to act” that 
prescribes
 for a practitioner how to produce a type of outcome in a range of circumstances. This “know-how” can potentially, therefore, be reconstructed theoretically as explicit micro-prescriptive guidelines for third-party practice. To exploit this connection between practical knowledge and prescriptive knowledge, however, we first need to overcome the problem that “know-how” is largely 
tacit
 in the moment of real-time forward-looking practice. In other words, the practitioner is not directly aware of their tacit “know-how”, or “tacit knowledge”, at the time of practice. In this article, we explore the contribution design theory can make to empirically eliciting, and conceptually inferring, the real-time “tacit knowledge” of entrepreneurial practice as a precursor to producing micro-prescriptive knowledge.",21 Mar 2025,5,"The abstract discusses theoretical concepts related to practical knowledge, which may have limited direct impact on early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S2352673418300933,Identifying design principles for business incubation in the European space sector,June 2019,Business Venturing Insights,Not Found,Daniel=Sagath: d.sagath@vu.nl; Elco=van Burg: j.c.van.burg@vu.nl; Joep P.=Cornelissen: cornelissen@rsm.nl; Christina=Giannopapa: Christina.giannopapa@esa.in,"Abstract
Organizations and policy makers seek to support business and entrepreneurship through facilitating new product and service development, for instance in business incubators. Taking stock of existing research and combining this with practitioner's insights, this study aims to identify a comprehensive set of design principles for incubation practices in a particular sector, the European space sector. We provide a synthesis of business incubation practices, resulting in a set of actionable design principles that also serves to tailor solutions for other contexts.",21 Mar 2025,7,"This abstract focuses on identifying design principles for incubation practices in the European space sector, providing actionable insights that could benefit early-stage ventures and startups."
https://www.sciencedirect.com/science/article/pii/S2352673418301112,Further exploring international entrepreneurial cognitions: The case of the Middle-East,June 2019,Business Venturing Insights,Not Found,Hamid=Vahidnia: hvahidnia@tulane.edu; J. Robert=Mitchell: rmitchell@ivey.uwo.ca; J. Brock=Smith: smithb@uvic.ca; Abdallah M.=Assaf: abdallah.assaf@ttu.edu; Ronald K.=Mitchell: ronald.mitchell@ttu.edu; Özlem=Araci: ozlem.araci@istanbul.edu.tr,"Abstract
Scholars argue that at least some entrepreneurial cognition is global; but there is little evidence to test this claim in the Middle East. For this region, composed of several countries with distinct socioeconomic contexts, even baseline descriptions are rare. Indeed, some have used the case of Middle East to challenge the global nature of entrepreneurial cognitions among individual entrepreneurs. Using data from 577 entrepreneurs and professionals in Egypt, Iran, 
Saudi Arabia
, and Turkey, four of the largest countries in the region, we study the extent to which entrepreneurial cognitions occur and explain an entrepreneurial 
mindset
 in this context.",21 Mar 2025,6,"The study on entrepreneurial cognitions in the Middle East may have relevance to European early-stage ventures, but the impact may be somewhat limited given the regional focus."
https://www.sciencedirect.com/science/article/pii/S2352673418301021,Understanding the design of opportunities: Re-evaluating the agent-opportunity nexus through a design lens,June 2019,Business Venturing Insights,Not Found,Thomas=Ding: thomas.ding@strath.ac.uk,"Abstract
This paper reassesses the fundamental tenets of the opportunity construct from a design perspective. It acknowledges the incommensurability between the opportunity discovery and creation views, and uses a processual approach based on ontological pragmatism to conceptualize an opportunity. In essence, this paper outlines the associative nature of an opportunity and its implications to elucidate its ephemerality and its dependence on human agency for materialization.",21 Mar 2025,8,"This abstract reassesses the opportunity construct from a design perspective, offering insights that could be valuable for European early-stage ventures and startups in understanding and exploiting opportunities."
